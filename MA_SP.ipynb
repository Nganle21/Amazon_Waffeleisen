{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nganle21/Amazon_Waffeleisen/blob/main/MA_SP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ELpi4So4gm0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896d687e-df28-4beb-df1b-de5b828c997e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2022-12-21 08:30:13.651706: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 587.7 MB 15 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-lg==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.5.1.post0-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 612 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (0.12.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.3.5)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.7.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.5->category_encoders) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.5.1.post0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (4.64.1)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.8 sklearn-crfsuite-0.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn<0.24\n",
            "  Downloading scikit_learn-0.23.2-cp38-cp38-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<0.24) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<0.24) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<0.24) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<0.24) (1.2.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.23.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 64.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 57.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 152 kB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.64.1)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30534 sha256=81e413f3968960259177693c7d44efdae6bc2d1987891729d4c24d69bd9a6a5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/a4/72/df07592cea3ae06b5e846f5e52262f8b16748e829ca354b7df\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19472 sha256=e27063b0c96c85cbc3e15c13d6a0a19ef89f5bf39c9e8159ad9c856e673a0a37\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/f3/85/b8cf1d8bfe55dc2ece0f1fcd4e91d6f8fc7b59ff3fd75329e1\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7911 sha256=742f3e1127df72633d065644268ee561b5131fb5d09ad94337abbbafd7611458\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/26/e9/df16869ccbd4abf517f1ff3be9a2c7ee5c5980fc87eea04fb1\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "!pip install category_encoders\n",
        "!pip install sklearn_crfsuite\n",
        "!pip install -U 'scikit-learn<0.24'\n",
        "!pip install transformers\n",
        "!pip install vaderSentiment\n",
        "!pip install bert-for-tf2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use first created file train and test from Jedox"
      ],
      "metadata": {
        "id": "_xbFvBipel2z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qbjr_yrRgiF6",
        "outputId": "21a72b63-84aa-4caf-9261-1727d24acafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import os\n",
        "import re\n",
        "import operator\n",
        "import math\n",
        "import spacy\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.util import ngrams\n",
        "n_gram = 1\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "kfold=10\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, auc, f1_score, recall_score, precision_score\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import make_scorer\n",
        "import matplotlib.pyplot as plt \n",
        "import itertools\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "import category_encoders as ce\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import pad_sequences, to_categorical\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Embedding, Dense, TimeDistributed, Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, Activation, SpatialDropout1D, Bidirectional, Lambda\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.models import load_model\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "import keras as k\n",
        "import keras.backend as K\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import TFBertModel\n",
        "from bert.model import BertModelLayer\n",
        "from bert.loader import params_from_pretrained_ckpt, load_stock_weights\n",
        "\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKYZNbzfXnFz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pc2b-0CoK9a"
      },
      "source": [
        "# Split Train Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaOHKaeroPey"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/admin.xlsx\",engine=\"openpyxl\")\n",
        "#df.head()\n",
        "train, test = train_test_split(df, test_size=0.3, random_state = 29)\n",
        "#train.to_excel(\"/content/drive/MyDrive/Colab Notebooks/absa_train.xlsx\")\n",
        "#test.to_excel(\"/content/drive/MyDrive/Colab Notebooks/absa_test.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF0qYYRwowyO"
      },
      "source": [
        "\n",
        "\n",
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPC9eL80gjuS"
      },
      "outputs": [],
      "source": [
        "#train = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/train_final.xlsx\",engine=\"openpyxl\")\n",
        "#test = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/test_final.xlsx\",engine=\"openpyxl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERzyi3pHhxfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6d6e3a-6cff-4ee6-e1d1-cac9aa0d68de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
          ]
        }
      ],
      "source": [
        "train_sp = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/train.xlsx\",engine=\"openpyxl\")\n",
        "test_sp = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/test.xlsx\",engine=\"openpyxl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "jpkiJ1gajGvO",
        "outputId": "d4f88a9c-2363-41c9-b8e7-4d67b698a92a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                               text  \\\n",
              "0    142090  Perfect. Waffles succeed straight away. Gladly...   \n",
              "1    142096  The waffles are not evenly browned, unfortunat...   \n",
              "2    142096  The waffles are not evenly browned, unfortunat...   \n",
              "3    142096  The waffles are not evenly browned, unfortunat...   \n",
              "4    142096  The waffles are not evenly browned, unfortunat...   \n",
              "..      ...                                                ...   \n",
              "462  145346  It is very simple to handle and the price qual...   \n",
              "463  145357  Easy to clean, heat enough and the materials s...   \n",
              "464  145357  Easy to clean, heat enough and the materials s...   \n",
              "465  145357  Easy to clean, heat enough and the materials s...   \n",
              "466  145357  Easy to clean, heat enough and the materials s...   \n",
              "\n",
              "                        aspectTerm  From   To        SP  \n",
              "0                          Waffles     9   16  positive  \n",
              "1                          waffles     4   11  negative  \n",
              "2    regulation of the temperature    62   91  negative  \n",
              "3                              use   111  114   neutral  \n",
              "4                              use   137  140   neutral  \n",
              "..                             ...   ...  ...       ...  \n",
              "462                        coating    81   88  positive  \n",
              "463                          clean     8   13  positive  \n",
              "464                           heat    15   19  positive  \n",
              "465                      materials    35   44  positive  \n",
              "466                          price   123  128  positive  \n",
              "\n",
              "[467 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-75e9e8e2-8ac0-44e3-a3a5-cb2d0a8d19d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>aspectTerm</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>SP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142090</td>\n",
              "      <td>Perfect. Waffles succeed straight away. Gladly...</td>\n",
              "      <td>Waffles</td>\n",
              "      <td>9</td>\n",
              "      <td>16</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142096</td>\n",
              "      <td>The waffles are not evenly browned, unfortunat...</td>\n",
              "      <td>waffles</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142096</td>\n",
              "      <td>The waffles are not evenly browned, unfortunat...</td>\n",
              "      <td>regulation of the temperature</td>\n",
              "      <td>62</td>\n",
              "      <td>91</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142096</td>\n",
              "      <td>The waffles are not evenly browned, unfortunat...</td>\n",
              "      <td>use</td>\n",
              "      <td>111</td>\n",
              "      <td>114</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142096</td>\n",
              "      <td>The waffles are not evenly browned, unfortunat...</td>\n",
              "      <td>use</td>\n",
              "      <td>137</td>\n",
              "      <td>140</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>145346</td>\n",
              "      <td>It is very simple to handle and the price qual...</td>\n",
              "      <td>coating</td>\n",
              "      <td>81</td>\n",
              "      <td>88</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>clean</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>heat</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>materials</td>\n",
              "      <td>35</td>\n",
              "      <td>44</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>price</td>\n",
              "      <td>123</td>\n",
              "      <td>128</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>467 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75e9e8e2-8ac0-44e3-a3a5-cb2d0a8d19d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-75e9e8e2-8ac0-44e3-a3a5-cb2d0a8d19d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-75e9e8e2-8ac0-44e3-a3a5-cb2d0a8d19d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNrpxMQPOzjs"
      },
      "source": [
        "##Data Exploration + Feature Selection SP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k73sTWFG0eVq"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MokBMjza0iSJ"
      },
      "source": [
        "#### Create COC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al3U00KDiExN",
        "outputId": "e64e6089-3134-4e7d-a391-c00c59a7eb75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                               text   aspectTerm  \\\n",
              "0     142087  I am very satisfied with it ... I have used th...        price   \n",
              "1     142089  Cheap processing, wobbling unstable unstable l...   processing   \n",
              "2     142089  Cheap processing, wobbling unstable unstable l...         lids   \n",
              "3     142089  Cheap processing, wobbling unstable unstable l...  baking time   \n",
              "4     142091  The iron does not remain, the waffles are too ...      waffles   \n",
              "...      ...                                                ...          ...   \n",
              "1083  145299  When I used the machine to make triangular san...        bakes   \n",
              "1084  145303           The device is very bad in terms of price        price   \n",
              "1085  145309                 Its size is small than I expected.         size   \n",
              "1086  145316                   The products come out delicious.     products   \n",
              "1087  145355  If the plates could be removed, it would be fa...       plates   \n",
              "\n",
              "      From   To        SP  \n",
              "0       82   87  positive  \n",
              "1        6   16  negative  \n",
              "2       45   49  negative  \n",
              "3      142  153  negative  \n",
              "4       30   37  negative  \n",
              "...    ...  ...       ...  \n",
              "1083   106  111  positive  \n",
              "1084    35   40  negative  \n",
              "1085     4    8  negative  \n",
              "1086     4   12  positive  \n",
              "1087     7   13   neutral  \n",
              "\n",
              "[1088 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-28b56f63-5b24-455e-a068-d76dbb63ca7d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>aspectTerm</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>SP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142087</td>\n",
              "      <td>I am very satisfied with it ... I have used th...</td>\n",
              "      <td>price</td>\n",
              "      <td>82</td>\n",
              "      <td>87</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>processing</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>lids</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>baking time</td>\n",
              "      <td>142</td>\n",
              "      <td>153</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142091</td>\n",
              "      <td>The iron does not remain, the waffles are too ...</td>\n",
              "      <td>waffles</td>\n",
              "      <td>30</td>\n",
              "      <td>37</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>145299</td>\n",
              "      <td>When I used the machine to make triangular san...</td>\n",
              "      <td>bakes</td>\n",
              "      <td>106</td>\n",
              "      <td>111</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1084</th>\n",
              "      <td>145303</td>\n",
              "      <td>The device is very bad in terms of price</td>\n",
              "      <td>price</td>\n",
              "      <td>35</td>\n",
              "      <td>40</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085</th>\n",
              "      <td>145309</td>\n",
              "      <td>Its size is small than I expected.</td>\n",
              "      <td>size</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1086</th>\n",
              "      <td>145316</td>\n",
              "      <td>The products come out delicious.</td>\n",
              "      <td>products</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1087</th>\n",
              "      <td>145355</td>\n",
              "      <td>If the plates could be removed, it would be fa...</td>\n",
              "      <td>plates</td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1088 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28b56f63-5b24-455e-a068-d76dbb63ca7d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-28b56f63-5b24-455e-a068-d76dbb63ca7d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-28b56f63-5b24-455e-a068-d76dbb63ca7d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "train_sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmwfYrlypNXk"
      },
      "outputs": [],
      "source": [
        "#remove stopwords\n",
        "en = spacy.load('en_core_web_sm')\n",
        "stopwords = en.Defaults.stop_words\n",
        "\n",
        "def stop_word(string):\n",
        "  lst=[]\n",
        "  for token in string.split():\n",
        "      if token.lower() not in stopwords:    #checking whether the word is not \n",
        "          lst.append(token.lower()) \n",
        "  return ' '.join(lst)\n",
        "\n",
        "#create word COC\n",
        "def word_coc(s, aspect):\n",
        "    window_size = 5\n",
        "    before_keyword, keyword, after_keyword = s.partition(aspect)\n",
        "    if (len(before_keyword.split()) <= window_size):\n",
        "      b = before_keyword.split()[0:len(before_keyword.split())]\n",
        "    else:\n",
        "      b = before_keyword.split()[(len(before_keyword.split())-window_size):len(before_keyword.split())]\n",
        "    a = after_keyword.split()[0:window_size]\n",
        "    ba = TreebankWordDetokenizer().detokenize(b+a)\n",
        "    return ba\n",
        "\n",
        "\n",
        "train_sp['text_wo_sw'] = train_sp['text'].apply(lambda x: stop_word(x))\n",
        "test_sp['text_wo_sw'] = test_sp['text'].apply(lambda x: stop_word(x))\n",
        "train_sp['coc'] = train_sp.apply(lambda x: word_coc(x.text_wo_sw, x.aspectTerm), axis=1)\n",
        "test_sp['coc'] = test_sp.apply(lambda x: word_coc(x.text_wo_sw, x.aspectTerm), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVv1taq4hYAq",
        "outputId": "ddb90588-3e5a-48e6-bd09-178727ea906a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                               text   aspectTerm  \\\n",
              "0     142087  I am very satisfied with it ... I have used th...        price   \n",
              "1     142089  Cheap processing, wobbling unstable unstable l...   processing   \n",
              "2     142089  Cheap processing, wobbling unstable unstable l...         lids   \n",
              "3     142089  Cheap processing, wobbling unstable unstable l...  baking time   \n",
              "4     142091  The iron does not remain, the waffles are too ...      waffles   \n",
              "...      ...                                                ...          ...   \n",
              "1083  145299  When I used the machine to make triangular san...        bakes   \n",
              "1084  145303           The device is very bad in terms of price        price   \n",
              "1085  145309                 Its size is small than I expected.         size   \n",
              "1086  145316                   The products come out delicious.     products   \n",
              "1087  145355  If the plates could be removed, it would be fa...       plates   \n",
              "\n",
              "      From   To        SP                                         text_wo_sw  \\\n",
              "0       82   87  positive  satisfied ... device times ... price device .....   \n",
              "1        6   16  negative  cheap processing, wobbling unstable unstable l...   \n",
              "2       45   49  negative  cheap processing, wobbling unstable unstable l...   \n",
              "3      142  153  negative  cheap processing, wobbling unstable unstable l...   \n",
              "4       30   37  negative     iron remain, waffles small waffles brown, six.   \n",
              "...    ...  ...       ...                                                ...   \n",
              "1083   106  111  positive  machine triangular sandwiches, handle broke. d...   \n",
              "1084    35   40  negative                             device bad terms price   \n",
              "1085     4    8  negative                               size small expected.   \n",
              "1086     4   12  positive                           products come delicious.   \n",
              "1087     7   13   neutral                         plates removed, fantastic!   \n",
              "\n",
              "                                                    coc  \n",
              "0     satisfied...device times...device...can't expe...  \n",
              "1                cheap, wobbling unstable unstable lids  \n",
              "2     cheap processing, wobbling unstable unstable f...  \n",
              "3     independently time open incredible unreasonabl...  \n",
              "4                iron remain, small waffles brown, six.  \n",
              "...                                                 ...  \n",
              "1083       sandwiches, handle broke. device good, well.  \n",
              "1084                                   device bad terms  \n",
              "1085                                    small expected.  \n",
              "1086                                    come delicious.  \n",
              "1087                                removed, fantastic!  \n",
              "\n",
              "[1088 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8db24c3d-8ea0-4af3-9561-ab24e237fe10\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>aspectTerm</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>SP</th>\n",
              "      <th>text_wo_sw</th>\n",
              "      <th>coc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142087</td>\n",
              "      <td>I am very satisfied with it ... I have used th...</td>\n",
              "      <td>price</td>\n",
              "      <td>82</td>\n",
              "      <td>87</td>\n",
              "      <td>positive</td>\n",
              "      <td>satisfied ... device times ... price device .....</td>\n",
              "      <td>satisfied...device times...device...can't expe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>processing</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap, wobbling unstable unstable lids</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>lids</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap processing, wobbling unstable unstable f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>baking time</td>\n",
              "      <td>142</td>\n",
              "      <td>153</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>independently time open incredible unreasonabl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142091</td>\n",
              "      <td>The iron does not remain, the waffles are too ...</td>\n",
              "      <td>waffles</td>\n",
              "      <td>30</td>\n",
              "      <td>37</td>\n",
              "      <td>negative</td>\n",
              "      <td>iron remain, waffles small waffles brown, six.</td>\n",
              "      <td>iron remain, small waffles brown, six.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>145299</td>\n",
              "      <td>When I used the machine to make triangular san...</td>\n",
              "      <td>bakes</td>\n",
              "      <td>106</td>\n",
              "      <td>111</td>\n",
              "      <td>positive</td>\n",
              "      <td>machine triangular sandwiches, handle broke. d...</td>\n",
              "      <td>sandwiches, handle broke. device good, well.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1084</th>\n",
              "      <td>145303</td>\n",
              "      <td>The device is very bad in terms of price</td>\n",
              "      <td>price</td>\n",
              "      <td>35</td>\n",
              "      <td>40</td>\n",
              "      <td>negative</td>\n",
              "      <td>device bad terms price</td>\n",
              "      <td>device bad terms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085</th>\n",
              "      <td>145309</td>\n",
              "      <td>Its size is small than I expected.</td>\n",
              "      <td>size</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>negative</td>\n",
              "      <td>size small expected.</td>\n",
              "      <td>small expected.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1086</th>\n",
              "      <td>145316</td>\n",
              "      <td>The products come out delicious.</td>\n",
              "      <td>products</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>positive</td>\n",
              "      <td>products come delicious.</td>\n",
              "      <td>come delicious.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1087</th>\n",
              "      <td>145355</td>\n",
              "      <td>If the plates could be removed, it would be fa...</td>\n",
              "      <td>plates</td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>neutral</td>\n",
              "      <td>plates removed, fantastic!</td>\n",
              "      <td>removed, fantastic!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1088 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8db24c3d-8ea0-4af3-9561-ab24e237fe10')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8db24c3d-8ea0-4af3-9561-ab24e237fe10 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8db24c3d-8ea0-4af3-9561-ab24e237fe10');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_sp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPV4J98R0koB"
      },
      "source": [
        "#### Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlSxlVUROgZU"
      },
      "source": [
        "Declare feature vector and target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuAwBHEZ0Tzw",
        "outputId": "1cc7c076-20b5-4a57-cccd-6736a6dd598b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1088, 1396) (1088,) (467, 1396) (467,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=0.95)\n",
        "\n",
        "# Create the training set with the words encoded as features of the reviews\n",
        "X_train = vectorizer.fit_transform(train_sp['coc'])\n",
        "X_test = vectorizer.transform(test_sp['coc'])\n",
        "y_train = train_sp['SP']\n",
        "y_test = test_sp['SP']\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQR-bfyg_j9f",
        "outputId": "afe26075-cc43-43fc-952e-f5195d9505c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    0.588235\n",
              "negative    0.317096\n",
              "neutral     0.094669\n",
              "Name: SP, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_sp['SP'].value_counts()/train_sp.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOnfkUbCNH3P"
      },
      "source": [
        "# SP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNg0h6DXNMdn"
      },
      "source": [
        "## Lexicon-based"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VADER"
      ],
      "metadata": {
        "id": "1JSukvkCOIIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vader = SentimentIntensityAnalyzer()\n",
        "lbm_data = train_sp.append(test_sp, ignore_index=True)\n",
        "lbm_data['scores'] = lbm_data['coc'].apply(lambda coc: vader.polarity_scores(coc))\n",
        "lbm_data['compound']  = lbm_data['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "\n",
        "pos = []\n",
        "for i in range(len(lbm_data)):\n",
        "        if (lbm_data['SP'][i])==\"positive\":\n",
        "            pos.append('1')\n",
        "        elif (lbm_data['SP'][i])==\"negative\":\n",
        "            pos.append('-1')\n",
        "        else:\n",
        "            pos.append('0')\n",
        "lbm_data['true'] = pos\n",
        "lbm_data.head()\n",
        "\n",
        "pred = []\n",
        "for i in range(len(lbm_data)):\n",
        "        if (lbm_data['compound'][i])>0:\n",
        "            pred.append('1')\n",
        "        elif (lbm_data['compound'][i])<0:\n",
        "            pred.append('-1')\n",
        "        else:\n",
        "            pred.append('0')\n",
        "lbm_data['pred'] = pred\n",
        "lbm_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "TiXg_MMiYqG4",
        "outputId": "d4073c94-de1d-4db6-d8e2-ed05ba1c5912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id                                               text   aspectTerm  \\\n",
              "0  142087  I am very satisfied with it ... I have used th...        price   \n",
              "1  142089  Cheap processing, wobbling unstable unstable l...   processing   \n",
              "2  142089  Cheap processing, wobbling unstable unstable l...         lids   \n",
              "3  142089  Cheap processing, wobbling unstable unstable l...  baking time   \n",
              "4  142091  The iron does not remain, the waffles are too ...      waffles   \n",
              "\n",
              "   From   To        SP                                         text_wo_sw  \\\n",
              "0    82   87  positive  satisfied ... device times ... price device .....   \n",
              "1     6   16  negative  cheap processing, wobbling unstable unstable l...   \n",
              "2    45   49  negative  cheap processing, wobbling unstable unstable l...   \n",
              "3   142  153  negative  cheap processing, wobbling unstable unstable l...   \n",
              "4    30   37  negative     iron remain, waffles small waffles brown, six.   \n",
              "\n",
              "                                                 coc  \\\n",
              "0  satisfied...device times...device...can't expe...   \n",
              "1             cheap, wobbling unstable unstable lids   \n",
              "2  cheap processing, wobbling unstable unstable f...   \n",
              "3  independently time open incredible unreasonabl...   \n",
              "4             iron remain, small waffles brown, six.   \n",
              "\n",
              "                                              scores  compound true pred  \n",
              "0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000    1    0  \n",
              "1  {'neg': 0.625, 'neu': 0.375, 'pos': 0.0, 'comp...   -0.6124   -1   -1  \n",
              "2  {'neg': 0.485, 'neu': 0.515, 'pos': 0.0, 'comp...   -0.6808   -1   -1  \n",
              "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   -1    0  \n",
              "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   -1    0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b7a4f848-26ab-45e1-b8e6-00dd7dd6ed0e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>aspectTerm</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>SP</th>\n",
              "      <th>text_wo_sw</th>\n",
              "      <th>coc</th>\n",
              "      <th>scores</th>\n",
              "      <th>compound</th>\n",
              "      <th>true</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142087</td>\n",
              "      <td>I am very satisfied with it ... I have used th...</td>\n",
              "      <td>price</td>\n",
              "      <td>82</td>\n",
              "      <td>87</td>\n",
              "      <td>positive</td>\n",
              "      <td>satisfied ... device times ... price device .....</td>\n",
              "      <td>satisfied...device times...device...can't expe...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>processing</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap, wobbling unstable unstable lids</td>\n",
              "      <td>{'neg': 0.625, 'neu': 0.375, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.6124</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>lids</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap processing, wobbling unstable unstable f...</td>\n",
              "      <td>{'neg': 0.485, 'neu': 0.515, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.6808</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>baking time</td>\n",
              "      <td>142</td>\n",
              "      <td>153</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>independently time open incredible unreasonabl...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142091</td>\n",
              "      <td>The iron does not remain, the waffles are too ...</td>\n",
              "      <td>waffles</td>\n",
              "      <td>30</td>\n",
              "      <td>37</td>\n",
              "      <td>negative</td>\n",
              "      <td>iron remain, waffles small waffles brown, six.</td>\n",
              "      <td>iron remain, small waffles brown, six.</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7a4f848-26ab-45e1-b8e6-00dd7dd6ed0e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b7a4f848-26ab-45e1-b8e6-00dd7dd6ed0e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b7a4f848-26ab-45e1-b8e6-00dd7dd6ed0e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(lbm_data)):\n",
        "  if \"easy clean\" in lbm_data.coc[i]:\n",
        "    print(i) \n",
        "    \n"
      ],
      "metadata": {
        "id": "9-oZuysuZgwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lbm_data.iloc[30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSc47Wy_d9Pw",
        "outputId": "40af434c-c601-41b1-90b0-3234d10cd67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                                                       142109\n",
              "text          Excellent, fun product.  Made a 2 & 1/2 year o...\n",
              "aspectTerm                                                store\n",
              "From                                                        121\n",
              "To                                                          126\n",
              "SP                                                     positive\n",
              "text_wo_sw    excellent, fun product. 2 & 1/2 year old happy...\n",
              "coc                              gets thumbs nephew. easy clean\n",
              "scores        {'neg': 0.0, 'neu': 0.349, 'pos': 0.651, 'comp...\n",
              "compound                                                 0.6808\n",
              "true                                                          1\n",
              "pred                                                          1\n",
              "Name: 30, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy')\n",
        "print(accuracy_score(lbm_data.true, lbm_data.pred))\n",
        "print('f1 score')\n",
        "print(f1_score(lbm_data.true, lbm_data.pred,average='weighted'))\n",
        "print('recall')\n",
        "print(recall_score(lbm_data.true, lbm_data.pred,average='weighted'))\n",
        "print('precision')\n",
        "print(precision_score(lbm_data.true, lbm_data.pred,average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57-dqgRfZ5cD",
        "outputId": "735bfd5c-bc3f-4b07-8e63-e584d6ff02a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "0.6881028938906752\n",
            "f1 score\n",
            "0.6772254874921957\n",
            "recall\n",
            "0.6881028938906752\n",
            "precision\n",
            "0.7219257067193191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(lbm_data.pred, lbm_data.true))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4ef_5_fZV2L",
        "outputId": "e4205592-ab2a-4c10-9909-1e9c349ff45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.40      0.83      0.54       229\n",
            "           0       0.21      0.15      0.18       191\n",
            "           1       0.90      0.75      0.82      1135\n",
            "\n",
            "    accuracy                           0.69      1555\n",
            "   macro avg       0.50      0.58      0.51      1555\n",
            "weighted avg       0.74      0.69      0.70      1555\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.scatter(lbm_data.compound, lbm_data.polarity)\n",
        "plt.xlabel(\"VADER's compound\")\n",
        "plt.ylabel(\"TextBlob's score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "1_I6tfQoMqGx",
        "outputId": "3cf9c498-4aa4-4f08-869a-a029e003eb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zip47ZtXaCsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 2)\n",
        "axs[0, 0].scatter(lbm_data.compound, lbm_data.polarity)\n",
        "axs[0, 0].set(xlabel='VADER Compounds', ylabel='TextBlob Scores')\n",
        "axs[0, 1].scatter(lbm_data.compound, lbm_data.senti_score)\n",
        "axs[0, 1].set(xlabel='VADER Compounds', ylabel='SentiWordNet Scores')\n",
        "axs[1, 0].scatter(lbm_data.polarity, lbm_data.senti_score)\n",
        "axs[1, 0].set(xlabel='TextBlob Scores', ylabel='SentiWordNet Scores')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "oStlZaJBaDAu",
        "outputId": "8ed9785b-be75-486d-e0cb-394deba3794f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0.5, 'SentiWordNet Scores'), Text(0.5, 0, 'TextBlob Scores')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRjblq_KX3Nx"
      },
      "source": [
        "### TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azk_pOo3E3O1"
      },
      "outputs": [],
      "source": [
        "#Create a function to get the polarity\n",
        "def getPolarity(text):\n",
        "   return TextBlob(text).sentiment.polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "m8D98agFPNpC",
        "outputId": "699fa490-7e85-4096-c868-27ba6fc911fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id                                               text   aspectTerm  \\\n",
              "0  142087  I am very satisfied with it ... I have used th...        price   \n",
              "1  142089  Cheap processing, wobbling unstable unstable l...   processing   \n",
              "2  142089  Cheap processing, wobbling unstable unstable l...         lids   \n",
              "3  142089  Cheap processing, wobbling unstable unstable l...  baking time   \n",
              "4  142091  The iron does not remain, the waffles are too ...      waffles   \n",
              "\n",
              "   From   To        SP                                         text_wo_sw  \\\n",
              "0    82   87  positive  satisfied ... device times ... price device .....   \n",
              "1     6   16  negative  cheap processing, wobbling unstable unstable l...   \n",
              "2    45   49  negative  cheap processing, wobbling unstable unstable l...   \n",
              "3   142  153  negative  cheap processing, wobbling unstable unstable l...   \n",
              "4    30   37  negative     iron remain, waffles small waffles brown, six.   \n",
              "\n",
              "                                                 coc  \\\n",
              "0  satisfied...device times...device...can't expe...   \n",
              "1             cheap, wobbling unstable unstable lids   \n",
              "2  cheap processing, wobbling unstable unstable f...   \n",
              "3  independently time open incredible unreasonabl...   \n",
              "4             iron remain, small waffles brown, six.   \n",
              "\n",
              "                                              scores  compound true pred  \\\n",
              "0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000    1    0   \n",
              "1  {'neg': 0.625, 'neu': 0.375, 'pos': 0.0, 'comp...   -0.6124   -1    1   \n",
              "2  {'neg': 0.485, 'neu': 0.515, 'pos': 0.0, 'comp...   -0.6808   -1    1   \n",
              "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   -1    1   \n",
              "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   -1   -1   \n",
              "\n",
              "   polarity  \n",
              "0     0.000  \n",
              "1     0.400  \n",
              "2     0.325  \n",
              "3     0.225  \n",
              "4    -0.250  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-118cae57-835d-4479-854e-b93b49fede11\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>aspectTerm</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>SP</th>\n",
              "      <th>text_wo_sw</th>\n",
              "      <th>coc</th>\n",
              "      <th>scores</th>\n",
              "      <th>compound</th>\n",
              "      <th>true</th>\n",
              "      <th>pred</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142087</td>\n",
              "      <td>I am very satisfied with it ... I have used th...</td>\n",
              "      <td>price</td>\n",
              "      <td>82</td>\n",
              "      <td>87</td>\n",
              "      <td>positive</td>\n",
              "      <td>satisfied ... device times ... price device .....</td>\n",
              "      <td>satisfied...device times...device...can't expe...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>processing</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap, wobbling unstable unstable lids</td>\n",
              "      <td>{'neg': 0.625, 'neu': 0.375, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.6124</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>lids</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap processing, wobbling unstable unstable f...</td>\n",
              "      <td>{'neg': 0.485, 'neu': 0.515, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.6808</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>baking time</td>\n",
              "      <td>142</td>\n",
              "      <td>153</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>independently time open incredible unreasonabl...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142091</td>\n",
              "      <td>The iron does not remain, the waffles are too ...</td>\n",
              "      <td>waffles</td>\n",
              "      <td>30</td>\n",
              "      <td>37</td>\n",
              "      <td>negative</td>\n",
              "      <td>iron remain, waffles small waffles brown, six.</td>\n",
              "      <td>iron remain, small waffles brown, six.</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-118cae57-835d-4479-854e-b93b49fede11')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-118cae57-835d-4479-854e-b93b49fede11 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-118cae57-835d-4479-854e-b93b49fede11');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "lbm_data['polarity'] = lbm_data['coc'].apply(lambda coc: getPolarity(coc))\n",
        "pred = []\n",
        "for i in range(len(lbm_data)):\n",
        "        if (lbm_data['polarity'][i])>0:\n",
        "            pred.append('1')\n",
        "        elif (lbm_data['polarity'][i])<0:\n",
        "            pred.append('-1')\n",
        "        else:\n",
        "            pred.append('0')\n",
        "lbm_data['pred'] = pred\n",
        "lbm_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy')\n",
        "print(accuracy_score(lbm_data.true, lbm_data.pred))\n",
        "print('f1 score')\n",
        "print(f1_score(lbm_data.true, lbm_data.pred,average='weighted'))\n",
        "print('recall')\n",
        "print(recall_score(lbm_data.true, lbm_data.pred,average='weighted'))\n",
        "print('precision')\n",
        "print(precision_score(lbm_data.true, lbm_data.pred,average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETtLvCZZlvWC",
        "outputId": "5a964ac7-d6c8-48f2-92b7-7f0ee1cd575a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "0.7080385852090032\n",
            "f1 score\n",
            "0.694932448305261\n",
            "recall\n",
            "0.7080385852090032\n",
            "precision\n",
            "0.7308192244540054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(lbm_data.pred, lbm_data.true))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecl4OM71_S-7",
        "outputId": "0fab7ee4-cf05-4068-e837-cce1335568b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.44      0.85      0.58       247\n",
            "           0       0.20      0.17      0.18       163\n",
            "           1       0.92      0.76      0.83      1145\n",
            "\n",
            "    accuracy                           0.71      1555\n",
            "   macro avg       0.52      0.59      0.53      1555\n",
            "weighted avg       0.77      0.71      0.72      1555\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SentiWordNet"
      ],
      "metadata": {
        "id": "NWhnGVzRmLrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def Lemmatization(data,name):\n",
        "    def getting2(sen):\n",
        "        \n",
        "        example = sen\n",
        "        output_sentence =[]\n",
        "        word_tokens2 = word_tokenize(example)\n",
        "        lemmatized_output = [lemmatizer.lemmatize(w) for w in word_tokens2]\n",
        "        \n",
        "        # Remove characters which have length less than 2  \n",
        "        without_single_chr = [word for word in lemmatized_output if len(word) > 2]\n",
        "        # Remove numbers\n",
        "        cleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]\n",
        "        \n",
        "        return cleaned_data_title\n",
        "    # Using \"getting2(sen)\" function to append edited sentence to data\n",
        "    x=[]\n",
        "    for i in data[name].values:\n",
        "        x.append(getting2(i))\n",
        "    data[name]=x"
      ],
      "metadata": {
        "id": "pnqT6Tk5td6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize \n",
        "pos=neg=obj=count=0\n",
        "\n",
        "postagging = []\n",
        "\n",
        "for review in lbm_data['coc']:\n",
        "    list = word_tokenize(review)\n",
        "    postagging.append(nltk.pos_tag(list))\n",
        "\n",
        "lbm_data['pos_tags'] = postagging\n",
        "\n",
        "def penn_to_wn(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    return None\n",
        "\n",
        "\n",
        "# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\n",
        "def get_sentiment(word,tag):\n",
        "    wn_tag = penn_to_wn(tag)\n",
        "    \n",
        "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
        "        return []\n",
        "\n",
        "    #Lemmatization\n",
        "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "    if not lemma:\n",
        "        return []\n",
        "\n",
        "    #Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n",
        "    #Synset instances are the groupings of synonymous words that express the same concept. \n",
        "    #Some of the words have only one Synset and some have several.\n",
        "    synsets = wn.synsets(word, pos=wn_tag)\n",
        "    if not synsets:\n",
        "        return []\n",
        "\n",
        "    # Take the first sense, the most common\n",
        "    synset = synsets[0]\n",
        "    swn_synset = swn.senti_synset(synset.name())\n",
        "\n",
        "    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n",
        "\n",
        "    pos=neg=obj=count=0\n",
        "    \n",
        "    ###################################################################################\n",
        "senti_score = []\n",
        "\n",
        "for pos_val in lbm_data['pos_tags']:\n",
        "    senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n",
        "    for score in senti_val:\n",
        "        try:\n",
        "            pos = pos + score[1]  #positive score is stored at 2nd position\n",
        "            neg = neg + score[2]  #negative score is stored at 3rd position\n",
        "        except:\n",
        "            continue\n",
        "    senti_score.append(pos - neg)\n",
        "    pos=neg=0    \n",
        "    \n",
        "lbm_data['senti_score'] = senti_score\n",
        "#print(lbm_data['senti_score'])\n",
        "\n",
        "lbm_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "DOgLV2pHsQAB",
        "outputId": "d6157c43-53b6-4591-cc27-293347feeef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id                                               text   aspectTerm  \\\n",
              "0  142087  I am very satisfied with it ... I have used th...        price   \n",
              "1  142089  Cheap processing, wobbling unstable unstable l...   processing   \n",
              "2  142089  Cheap processing, wobbling unstable unstable l...         lids   \n",
              "3  142089  Cheap processing, wobbling unstable unstable l...  baking time   \n",
              "4  142091  The iron does not remain, the waffles are too ...      waffles   \n",
              "\n",
              "   From   To        SP                                         text_wo_sw  \\\n",
              "0    82   87  positive  satisfied ... device times ... price device .....   \n",
              "1     6   16  negative  cheap processing, wobbling unstable unstable l...   \n",
              "2    45   49  negative  cheap processing, wobbling unstable unstable l...   \n",
              "3   142  153  negative  cheap processing, wobbling unstable unstable l...   \n",
              "4    30   37  negative     iron remain, waffles small waffles brown, six.   \n",
              "\n",
              "                                                 coc  \\\n",
              "0  satisfied...device times...device...can't expe...   \n",
              "1             cheap, wobbling unstable unstable lids   \n",
              "2  cheap processing, wobbling unstable unstable f...   \n",
              "3  independently time open incredible unreasonabl...   \n",
              "4             iron remain, small waffles brown, six.   \n",
              "\n",
              "                                              scores  compound true pred  \\\n",
              "0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000    1    0   \n",
              "1  {'neg': 0.625, 'neu': 0.375, 'pos': 0.0, 'comp...   -0.6124   -1    1   \n",
              "2  {'neg': 0.485, 'neu': 0.515, 'pos': 0.0, 'comp...   -0.6808   -1    1   \n",
              "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   -1    1   \n",
              "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   -1   -1   \n",
              "\n",
              "   polarity                                           pos_tags  senti_score  \n",
              "0     0.000  [(satisfied, JJ), (..., :), (device, NN), (tim...        0.875  \n",
              "1     0.400  [(cheap, RB), (,, ,), (wobbling, VBG), (unstab...       -0.250  \n",
              "2     0.325  [(cheap, JJ), (processing, NN), (,, ,), (wobbl...       -1.000  \n",
              "3     0.225  [(independently, RB), (time, NN), (open, JJ), ...       -0.750  \n",
              "4    -0.250  [(iron, NN), (remain, NN), (,, ,), (small, JJ)...       -0.375  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-09ec1bde-faf2-4ea0-b21f-78b909b545c6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>aspectTerm</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>SP</th>\n",
              "      <th>text_wo_sw</th>\n",
              "      <th>coc</th>\n",
              "      <th>scores</th>\n",
              "      <th>compound</th>\n",
              "      <th>true</th>\n",
              "      <th>pred</th>\n",
              "      <th>polarity</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>senti_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142087</td>\n",
              "      <td>I am very satisfied with it ... I have used th...</td>\n",
              "      <td>price</td>\n",
              "      <td>82</td>\n",
              "      <td>87</td>\n",
              "      <td>positive</td>\n",
              "      <td>satisfied ... device times ... price device .....</td>\n",
              "      <td>satisfied...device times...device...can't expe...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>[(satisfied, JJ), (..., :), (device, NN), (tim...</td>\n",
              "      <td>0.875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>processing</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap, wobbling unstable unstable lids</td>\n",
              "      <td>{'neg': 0.625, 'neu': 0.375, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.6124</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.400</td>\n",
              "      <td>[(cheap, RB), (,, ,), (wobbling, VBG), (unstab...</td>\n",
              "      <td>-0.250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>lids</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap processing, wobbling unstable unstable f...</td>\n",
              "      <td>{'neg': 0.485, 'neu': 0.515, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.6808</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.325</td>\n",
              "      <td>[(cheap, JJ), (processing, NN), (,, ,), (wobbl...</td>\n",
              "      <td>-1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>baking time</td>\n",
              "      <td>142</td>\n",
              "      <td>153</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>independently time open incredible unreasonabl...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.225</td>\n",
              "      <td>[(independently, RB), (time, NN), (open, JJ), ...</td>\n",
              "      <td>-0.750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142091</td>\n",
              "      <td>The iron does not remain, the waffles are too ...</td>\n",
              "      <td>waffles</td>\n",
              "      <td>30</td>\n",
              "      <td>37</td>\n",
              "      <td>negative</td>\n",
              "      <td>iron remain, waffles small waffles brown, six.</td>\n",
              "      <td>iron remain, small waffles brown, six.</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.250</td>\n",
              "      <td>[(iron, NN), (remain, NN), (,, ,), (small, JJ)...</td>\n",
              "      <td>-0.375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09ec1bde-faf2-4ea0-b21f-78b909b545c6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-09ec1bde-faf2-4ea0-b21f-78b909b545c6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-09ec1bde-faf2-4ea0-b21f-78b909b545c6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "for i in range(len(lbm_data)):\n",
        "        if (lbm_data['senti_score'][i])>=0.05:\n",
        "            pred.append('1')\n",
        "        elif (lbm_data['senti_score'][i])<= -0.05:\n",
        "            pred.append('-1')\n",
        "        else:\n",
        "            pred.append('0')\n",
        "lbm_data['pred'] = pred"
      ],
      "metadata": {
        "id": "2OxXCF7qxUPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy')\n",
        "print(accuracy_score(lbm_data.true, lbm_data.pred))\n",
        "print('f1 score')\n",
        "print(f1_score(lbm_data.true, lbm_data.pred,average='macro'))\n",
        "print('recall')\n",
        "print(recall_score(lbm_data.true, lbm_data.pred,average='macro'))\n",
        "print('precision')\n",
        "print(precision_score(lbm_data.true, lbm_data.pred,average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8vgmmqsxcft",
        "outputId": "5f489d0d-ce5d-4184-9f85-a46df84bd8a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "0.6083601286173633\n",
            "f1 score\n",
            "0.4683292838690701\n",
            "recall\n",
            "0.4686373507848824\n",
            "precision\n",
            "0.4832129513277263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(lbm_data.pred, lbm_data.true))"
      ],
      "metadata": {
        "id": "AcLKnAx0ZPR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531be18c-e513-4152-d20d-6ea36c5b3956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.45      0.59      0.51       364\n",
            "           0       0.21      0.12      0.15       251\n",
            "           1       0.75      0.75      0.75       940\n",
            "\n",
            "    accuracy                           0.61      1555\n",
            "   macro avg       0.47      0.48      0.47      1555\n",
            "weighted avg       0.59      0.61      0.60      1555\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxValues = lbm_data.max()\n",
        "print(maxValues)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG3YC5GPg3Lk",
        "outputId": "43025aeb-81ec-4cea-b2f0-a84cdd63cab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id                                                        145357\n",
            "text                           very good price/performance ratio\n",
            "aspectTerm                                                 works\n",
            "From                                                        1079\n",
            "To                                                          1092\n",
            "SP                                                      positive\n",
            "text_wo_sw                works well. makes cute little waffles.\n",
            "coc                                 yet, having tested others!;)\n",
            "compound                                                  0.9715\n",
            "true                                                           1\n",
            "pred                                                           1\n",
            "polarity                                                     1.0\n",
            "pos_tags       [(yet, RB), (,, ,), (having, VBG), (tested, VB...\n",
            "senti_score                                                3.125\n",
            "dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-1715b4ca6174>:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  maxValues = lbm_data.max()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzKvSA7nWcpw"
      },
      "source": [
        "## ML\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70VCc6OAYmY5"
      },
      "source": [
        "Train Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nPTjtkhWeQD"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBGs867LJy9O"
      },
      "outputs": [],
      "source": [
        "#from sklearn.utils.class_weight import compute_class_weight\n",
        "#sklearn_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "#sklearn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgO87hn5XTn7"
      },
      "outputs": [],
      "source": [
        "w = {'positive':0.566, 'negative':1.051, 'neutral':3.521}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4_EFQpfYWv4"
      },
      "source": [
        "Creating a dictionary with the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fSc4xOUYbFK"
      },
      "outputs": [],
      "source": [
        "# parameter grid\n",
        "params = {\n",
        "    'max_iter': [100,200,300],\n",
        "    'penalty' : ['l1','l2','elastic','None'], \n",
        "    'C'       : np.logspace(-3,3,7),\n",
        "    'solver'  : ['newton-cg', 'lbfgs', 'liblinear','saga','sag'],\n",
        "    'verbose' : [1]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg3IJSMGryLb",
        "outputId": "76e9d8b9-416e-4155-9383-a6678bea80cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 14 epochs took 0 seconds\n",
            "convergence after 14 epochs took 0 seconds\n",
            "convergence after 13 epochs took 0 seconds\n",
            "convergence after 15 epochs took 0 seconds\n",
            "convergence after 16 epochs took 0 seconds\n",
            "convergence after 15 epochs took 0 seconds\n",
            "convergence after 15 epochs took 0 seconds\n",
            "convergence after 16 epochs took 0 seconds\n",
            "convergence after 16 epochs took 0 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 438, in _check_solver\n",
            "    raise ValueError(\"Logistic Regression supports only penalties in %s,\"\n",
            "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
            "\n",
            "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1348, in fit\n",
            "    multi_class = _check_multi_class(self.multi_class, solver,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 472, in _check_multi_class\n",
            "    raise ValueError(\"Solver %s does not support \"\n",
            "ValueError: Solver liblinear does not support a multinomial backend.\n",
            "\n",
            "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 438, in _check_solver\n",
            "    raise ValueError(\"Logistic Regression supports only penalties in %s,\"\n",
            "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got elastic.\n",
            "\n",
            "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 438, in _check_solver\n",
            "    raise ValueError(\"Logistic Regression supports only penalties in %s,\"\n",
            "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
            "\n",
            "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 16 epochs took 0 seconds\n",
            "max_iter reached after 0 seconds\n",
            "max_iter reached after 0 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_iter reached after 0 seconds\n",
            "max_iter reached after 1 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_iter reached after 0 seconds\n",
            "max_iter reached after 0 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_iter reached after 0 seconds\n",
            "max_iter reached after 0 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_iter reached after 0 seconds\n",
            "max_iter reached after 1 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 438, in _check_solver\n",
            "    raise ValueError(\"Logistic Regression supports only penalties in %s,\"\n",
            "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
            "\n",
            "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
            "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
            "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 17 epochs took 0 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=10,\n",
              "                   estimator=LogisticRegression(class_weight={'negative': 1.051,\n",
              "                                                              'neutral': 3.521,\n",
              "                                                              'positive': 0.566},\n",
              "                                                multi_class='multinomial',\n",
              "                                                random_state=29),\n",
              "                   param_distributions={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
              "                                        'max_iter': [100, 200, 300],\n",
              "                                        'penalty': ['l1', 'l2', 'elastic',\n",
              "                                                    'None'],\n",
              "                                        'solver': ['newton-cg', 'lbfgs',\n",
              "                                                   'liblinear', 'saga', 'sag'],\n",
              "                                        'verbose': [1]},\n",
              "                   random_state=29, scoring='f1_macro')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "lr=LogisticRegression(random_state=29, multi_class = 'multinomial', class_weight =w)\n",
        "lr_rs=RandomizedSearchCV(lr, params, cv=10, random_state = 29, scoring='f1_macro')\n",
        "lr_rs.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYpo8ij9sDoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f8e5e7-0bf3-4702-8c03-d40f63a90d6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tuned hpyerparameters :(best parameters)  {'verbose': 1, 'solver': 'sag', 'penalty': 'l2', 'max_iter': 100, 'C': 0.1}\n",
            "f1_macro : 0.554586858982507\n"
          ]
        }
      ],
      "source": [
        "print(\"tuned hpyerparameters :(best parameters) \",lr_rs.best_params_)\n",
        "print(\"f1_macro :\",lr_rs.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPLV4abld98B"
      },
      "outputs": [],
      "source": [
        "predict_result_aspect = lr_rs.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw2YfMrcb9g_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30266c78-aa80-4b9e-ec6c-ac142d253f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "0.7109207708779444\n",
            "f1 score\n",
            "0.7292850808070249\n",
            "recall\n",
            "0.7109207708779444\n",
            "precision\n",
            "0.762156511636159\n"
          ]
        }
      ],
      "source": [
        "print('accuracy')\n",
        "print(accuracy_score(y_test, predict_result_aspect))\n",
        "print('f1 score')\n",
        "print(f1_score(y_test, predict_result_aspect,average='weighted'))\n",
        "print('recall')\n",
        "print(recall_score(y_test, predict_result_aspect,average='weighted'))\n",
        "print('precision')\n",
        "print(precision_score(y_test, predict_result_aspect,average='weighted'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qPNTKzHurMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b726c666-1bbf-477c-b017-79fdf26d41f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.59      0.65       158\n",
            "     neutral       0.34      0.21      0.26        57\n",
            "    positive       0.75      0.90      0.82       252\n",
            "\n",
            "    accuracy                           0.71       467\n",
            "   macro avg       0.60      0.57      0.58       467\n",
            "weighted avg       0.69      0.71      0.69       467\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(predict_result_aspect, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ9OZK0G6S-6"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhgJrOG06U2k"
      },
      "outputs": [],
      "source": [
        "# parameter grid\n",
        "params= {'n_estimators': [100,500,1000],\n",
        "               'max_features': ['auto', 'log2'],\n",
        "               'max_depth': np.arange(2,5,1),\n",
        "               'min_samples_split': np.arange(2,5,1),\n",
        "               'min_samples_leaf': np.arange(1,4,1),\n",
        "               'bootstrap': [True, False],\n",
        "                'verbose':[1]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L9Bk2YI7WD1",
        "outputId": "36a120a3-d31d-4a77-f8d7-1b4b1120c46b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=10,\n",
              "                   estimator=RandomForestClassifier(class_weight={'negative': 1.051,\n",
              "                                                                  'neutral': 3.521,\n",
              "                                                                  'positive': 0.566},\n",
              "                                                    criterion='entropy',\n",
              "                                                    random_state=29),\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': array([2, 3, 4]),\n",
              "                                        'max_features': ['auto', 'log2'],\n",
              "                                        'min_samples_leaf': array([1, 2, 3]),\n",
              "                                        'min_samples_split': array([2, 3, 4]),\n",
              "                                        'n_estimators': [100, 500, 1000],\n",
              "                                        'verbose': [1]},\n",
              "                   random_state=29, scoring='f1_macro')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "rf = RandomForestClassifier(random_state=29, criterion = 'entropy', class_weight =w)\n",
        "rf_rs=RandomizedSearchCV(rf, params, cv=10, random_state = 29,scoring='f1_macro')\n",
        "rf_rs.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjBDjz8r7Z8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cdf1b5a-4989-47ab-8231-569a4a0e2e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tuned hpyerparameters :(best parameters)  {'verbose': 1, 'n_estimators': 1000, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 3, 'bootstrap': False}\n",
            "f1_macro : 0.5473215959193335\n"
          ]
        }
      ],
      "source": [
        "print(\"tuned hpyerparameters :(best parameters) \",rf_rs.best_params_)\n",
        "print(\"f1_macro :\",rf_rs.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsJZ4m-J7dm3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c445295-bd48-405d-c59f-5bbe22ce48ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ]
        }
      ],
      "source": [
        "predict_result_aspect = rf_rs.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpWGYFe_7d08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda4db54-2958-4026-c88a-4935c147e346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "0.7002141327623126\n",
            "f1 score\n",
            "0.7110787869818663\n",
            "recall\n",
            "0.7002141327623126\n",
            "precision\n",
            "0.7310551495941402\n"
          ]
        }
      ],
      "source": [
        "print('accuracy')\n",
        "print(accuracy_score(y_test, predict_result_aspect))\n",
        "print('f1 score')\n",
        "print(f1_score(y_test, predict_result_aspect,average='weighted'))\n",
        "print('recall')\n",
        "print(recall_score(y_test, predict_result_aspect,average='weighted'))\n",
        "print('precision')\n",
        "print(precision_score(y_test, predict_result_aspect,average='weighted'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSkUWxTQyqYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a1dcad-2544-4056-9433-536d1f0b8bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.58      0.64       159\n",
            "     neutral       0.26      0.20      0.23        45\n",
            "    positive       0.75      0.86      0.80       263\n",
            "\n",
            "    accuracy                           0.70       467\n",
            "   macro avg       0.57      0.55      0.56       467\n",
            "weighted avg       0.69      0.70      0.69       467\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(predict_result_aspect, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxn-__i1lBvH"
      },
      "source": [
        "### Multinomial NB "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRYXu2Il6OFL"
      },
      "outputs": [],
      "source": [
        "# parameter grid\n",
        "params = {\n",
        "  'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "  'fit_prior': [True, False]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiEmf9QX_p69",
        "outputId": "9d856603-e1ef-40e0-b65d-6bb15212291b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=10, estimator=MultinomialNB(),\n",
              "                   param_distributions={'alpha': [0.0001, 0.001, 0.01, 0.1, 1,\n",
              "                                                  10, 100, 1000],\n",
              "                                        'fit_prior': [True, False]},\n",
              "                   random_state=29, scoring='f1_macro')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "mnb = MultinomialNB()\n",
        "mnb_rs=RandomizedSearchCV(mnb, params, cv=10, random_state = 29,scoring='f1_macro')\n",
        "mnb_rs.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzXFHuUM_3ic",
        "outputId": "bfa91d00-766c-45a0-84f3-466b6b8c4e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tuned hpyerparameters :(best parameters)  {'fit_prior': True, 'alpha': 0.1}\n",
            "f1_macro : 0.5360826960818954\n"
          ]
        }
      ],
      "source": [
        "print(\"tuned hpyerparameters :(best parameters) \",mnb_rs.best_params_)\n",
        "print(\"f1_macro :\",mnb_rs.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kri-_GFQALEP"
      },
      "outputs": [],
      "source": [
        "predict_result_aspect = mnb_rs.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8ubXKNnALZv",
        "outputId": "71ad6188-778b-4aa8-8a98-6f66436ef34c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "0.715203426124197\n",
            "f1 score\n",
            "0.6873204383265501\n",
            "recall\n",
            "0.715203426124197\n",
            "precision\n",
            "0.6778227177757392\n"
          ]
        }
      ],
      "source": [
        "print('accuracy')\n",
        "print(accuracy_score(y_test, predict_result_aspect))\n",
        "print('f1 score')\n",
        "print(f1_score(y_test, predict_result_aspect,average='weighted'))\n",
        "print('recall')\n",
        "print(recall_score(y_test, predict_result_aspect,average='weighted'))\n",
        "print('precision')\n",
        "print(precision_score(y_test, predict_result_aspect,average='weighted'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DhxXNzxwg44",
        "outputId": "0a1d398a-6160-450d-f24a-41fedd196edf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.49      0.62      0.55       103\n",
            "     neutral       0.06      0.22      0.09         9\n",
            "    positive       0.89      0.75      0.82       355\n",
            "\n",
            "    accuracy                           0.72       467\n",
            "   macro avg       0.48      0.53      0.49       467\n",
            "weighted avg       0.78      0.72      0.74       467\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(predict_result_aspect, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHSYpo3ZlA1U"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcC0P3K0ijIG"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n",
        "    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
        "    \"penalty\" : [\"l2\", \"l1\", \"elasticnet\",\"none\"],\n",
        "    'verbose' :[1]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvcf2GthqEMm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327af65b-1abe-4971-9222-f1fc8e2b6487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 7.11, NNZs: 338, Bias: -0.115027, T: 979, Avg. loss: 0.643936\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 6.16, NNZs: 225, Bias: -0.114849, T: 1958, Avg. loss: 0.547316\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.82, NNZs: 247, Bias: -0.116922, T: 2937, Avg. loss: 0.541444\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 5.66, NNZs: 251, Bias: -0.123421, T: 3916, Avg. loss: 0.535785\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.55, NNZs: 257, Bias: -0.122840, T: 4895, Avg. loss: 0.530856\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 5.47, NNZs: 259, Bias: -0.121635, T: 5874, Avg. loss: 0.528370\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 5.42, NNZs: 261, Bias: -0.122666, T: 6853, Avg. loss: 0.527197\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 5.38, NNZs: 262, Bias: -0.123178, T: 7832, Avg. loss: 0.525774\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 5.35, NNZs: 263, Bias: -0.123560, T: 8811, Avg. loss: 0.524745\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 5.34, NNZs: 262, Bias: -0.124380, T: 9790, Avg. loss: 0.523772\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 5.32, NNZs: 267, Bias: -0.124358, T: 10769, Avg. loss: 0.522796\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 5.30, NNZs: 267, Bias: -0.124734, T: 11748, Avg. loss: 0.522429\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 5.29, NNZs: 266, Bias: -0.125065, T: 12727, Avg. loss: 0.522120\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 5.28, NNZs: 267, Bias: -0.125386, T: 13706, Avg. loss: 0.521250\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 14 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.36, NNZs: 402, Bias: -0.350235, T: 979, Avg. loss: 0.621504\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 8.34, NNZs: 303, Bias: -0.400278, T: 1958, Avg. loss: 0.318505\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 7.57, NNZs: 307, Bias: -0.416831, T: 2937, Avg. loss: 0.302628\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 7.18, NNZs: 311, Bias: -0.429661, T: 3916, Avg. loss: 0.293684\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 6.93, NNZs: 313, Bias: -0.444964, T: 4895, Avg. loss: 0.292821\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6.76, NNZs: 318, Bias: -0.454885, T: 5874, Avg. loss: 0.286498\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 6.66, NNZs: 322, Bias: -0.460737, T: 6853, Avg. loss: 0.280434\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 6.58, NNZs: 325, Bias: -0.460628, T: 7832, Avg. loss: 0.270179\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.52, NNZs: 326, Bias: -0.463986, T: 8811, Avg. loss: 0.274848\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 6.48, NNZs: 329, Bias: -0.467989, T: 9790, Avg. loss: 0.274929\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 6.44, NNZs: 326, Bias: -0.471646, T: 10769, Avg. loss: 0.274029\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 6.41, NNZs: 327, Bias: -0.473942, T: 11748, Avg. loss: 0.270493\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 6.39, NNZs: 328, Bias: -0.475738, T: 12727, Avg. loss: 0.268790\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 6.37, NNZs: 327, Bias: -0.477048, T: 13706, Avg. loss: 0.267211\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 6.35, NNZs: 329, Bias: -0.478299, T: 14685, Avg. loss: 0.266347\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 6.33, NNZs: 330, Bias: -0.480437, T: 15664, Avg. loss: 0.268672\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 6.32, NNZs: 332, Bias: -0.482051, T: 16643, Avg. loss: 0.266934\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 6.31, NNZs: 334, Bias: -0.483485, T: 17622, Avg. loss: 0.266425\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 6.30, NNZs: 333, Bias: -0.484433, T: 18601, Avg. loss: 0.264352\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 6.29, NNZs: 337, Bias: -0.485337, T: 19580, Avg. loss: 0.264268\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 6.28, NNZs: 336, Bias: -0.486458, T: 20559, Avg. loss: 0.265086\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 6.27, NNZs: 335, Bias: -0.487391, T: 21538, Avg. loss: 0.263895\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 6.27, NNZs: 336, Bias: -0.488442, T: 22517, Avg. loss: 0.264394\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 6.26, NNZs: 336, Bias: -0.489036, T: 23496, Avg. loss: 0.261949\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 6.26, NNZs: 338, Bias: -0.489876, T: 24475, Avg. loss: 0.263245\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 6.25, NNZs: 337, Bias: -0.490756, T: 25454, Avg. loss: 0.263188\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 6.25, NNZs: 336, Bias: -0.491559, T: 26433, Avg. loss: 0.262993\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 6.24, NNZs: 338, Bias: -0.492001, T: 27412, Avg. loss: 0.260885\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 6.24, NNZs: 337, Bias: -0.492487, T: 28391, Avg. loss: 0.261209\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 6.24, NNZs: 339, Bias: -0.492929, T: 29370, Avg. loss: 0.260828\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 6.23, NNZs: 338, Bias: -0.493751, T: 30349, Avg. loss: 0.263084\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 6.23, NNZs: 338, Bias: -0.494497, T: 31328, Avg. loss: 0.262290\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 6.23, NNZs: 338, Bias: -0.494968, T: 32307, Avg. loss: 0.260754\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 33 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 6.03, NNZs: 277, Bias: -0.268417, T: 979, Avg. loss: 0.728527\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.28, NNZs: 148, Bias: -0.298110, T: 1958, Avg. loss: 0.654985\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.03, NNZs: 153, Bias: -0.304983, T: 2937, Avg. loss: 0.657562\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 4.89, NNZs: 156, Bias: -0.309079, T: 3916, Avg. loss: 0.653379\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 4.84, NNZs: 158, Bias: -0.309002, T: 4895, Avg. loss: 0.659202\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 4.80, NNZs: 163, Bias: -0.310473, T: 5874, Avg. loss: 0.653238\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 4.76, NNZs: 165, Bias: -0.312086, T: 6853, Avg. loss: 0.648642\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.73, NNZs: 165, Bias: -0.313129, T: 7832, Avg. loss: 0.649610\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.72, NNZs: 166, Bias: -0.313814, T: 8811, Avg. loss: 0.649361\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.70, NNZs: 169, Bias: -0.314472, T: 9790, Avg. loss: 0.648403\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 4.69, NNZs: 174, Bias: -0.315471, T: 10769, Avg. loss: 0.645292\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 4.67, NNZs: 174, Bias: -0.316193, T: 11748, Avg. loss: 0.645436\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 4.67, NNZs: 176, Bias: -0.316336, T: 12727, Avg. loss: 0.648225\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 4.66, NNZs: 177, Bias: -0.316602, T: 13706, Avg. loss: 0.647283\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 4.66, NNZs: 179, Bias: -0.317099, T: 14685, Avg. loss: 0.644815\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 4.65, NNZs: 180, Bias: -0.317472, T: 15664, Avg. loss: 0.645117\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 16 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 7.17, NNZs: 339, Bias: -0.109603, T: 979, Avg. loss: 0.648366\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 6.18, NNZs: 231, Bias: -0.100509, T: 1958, Avg. loss: 0.536983\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.82, NNZs: 251, Bias: -0.102365, T: 2937, Avg. loss: 0.534685\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 5.66, NNZs: 253, Bias: -0.109018, T: 3916, Avg. loss: 0.528615\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.54, NNZs: 261, Bias: -0.107559, T: 4895, Avg. loss: 0.523396\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 5.47, NNZs: 264, Bias: -0.107050, T: 5874, Avg. loss: 0.521300\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 5.42, NNZs: 266, Bias: -0.107229, T: 6853, Avg. loss: 0.519931\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 5.38, NNZs: 268, Bias: -0.108075, T: 7832, Avg. loss: 0.518264\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 5.36, NNZs: 269, Bias: -0.108486, T: 8811, Avg. loss: 0.517222\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 5.34, NNZs: 271, Bias: -0.109243, T: 9790, Avg. loss: 0.515957\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 5.32, NNZs: 271, Bias: -0.109025, T: 10769, Avg. loss: 0.515133\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 5.31, NNZs: 273, Bias: -0.109323, T: 11748, Avg. loss: 0.514457\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 5.29, NNZs: 272, Bias: -0.109537, T: 12727, Avg. loss: 0.514002\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 5.28, NNZs: 274, Bias: -0.109535, T: 13706, Avg. loss: 0.513370\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 5.27, NNZs: 278, Bias: -0.109732, T: 14685, Avg. loss: 0.513276\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 15 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.54, NNZs: 399, Bias: -0.330306, T: 979, Avg. loss: 0.611894\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 8.39, NNZs: 322, Bias: -0.379467, T: 1958, Avg. loss: 0.329811\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 7.56, NNZs: 328, Bias: -0.399650, T: 2937, Avg. loss: 0.314534\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 7.14, NNZs: 335, Bias: -0.414501, T: 3916, Avg. loss: 0.305426\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 6.88, NNZs: 338, Bias: -0.427317, T: 4895, Avg. loss: 0.301312\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6.71, NNZs: 341, Bias: -0.436865, T: 5874, Avg. loss: 0.296687\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 6.59, NNZs: 347, Bias: -0.443524, T: 6853, Avg. loss: 0.291454\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 6.52, NNZs: 349, Bias: -0.443434, T: 7832, Avg. loss: 0.280429\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.45, NNZs: 352, Bias: -0.445600, T: 8811, Avg. loss: 0.282591\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 6.41, NNZs: 352, Bias: -0.449644, T: 9790, Avg. loss: 0.284896\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 6.36, NNZs: 352, Bias: -0.453932, T: 10769, Avg. loss: 0.285211\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 6.34, NNZs: 355, Bias: -0.456023, T: 11748, Avg. loss: 0.279520\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 6.31, NNZs: 356, Bias: -0.457865, T: 12727, Avg. loss: 0.278426\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 6.29, NNZs: 358, Bias: -0.459392, T: 13706, Avg. loss: 0.277255\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 6.27, NNZs: 358, Bias: -0.460600, T: 14685, Avg. loss: 0.275830\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 6.25, NNZs: 358, Bias: -0.462683, T: 15664, Avg. loss: 0.278252\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 6.24, NNZs: 358, Bias: -0.464131, T: 16643, Avg. loss: 0.275623\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 6.23, NNZs: 359, Bias: -0.465349, T: 17622, Avg. loss: 0.274916\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 6.22, NNZs: 360, Bias: -0.466313, T: 18601, Avg. loss: 0.273657\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 6.21, NNZs: 360, Bias: -0.467568, T: 19580, Avg. loss: 0.275058\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 6.20, NNZs: 360, Bias: -0.468584, T: 20559, Avg. loss: 0.273784\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 6.19, NNZs: 360, Bias: -0.469615, T: 21538, Avg. loss: 0.273608\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 6.19, NNZs: 360, Bias: -0.470813, T: 22517, Avg. loss: 0.274172\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 6.18, NNZs: 359, Bias: -0.471399, T: 23496, Avg. loss: 0.271092\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 6.18, NNZs: 358, Bias: -0.472202, T: 24475, Avg. loss: 0.272267\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 6.17, NNZs: 359, Bias: -0.472988, T: 25454, Avg. loss: 0.271927\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 6.17, NNZs: 358, Bias: -0.473859, T: 26433, Avg. loss: 0.272505\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 6.16, NNZs: 359, Bias: -0.474345, T: 27412, Avg. loss: 0.270218\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 6.16, NNZs: 358, Bias: -0.474848, T: 28391, Avg. loss: 0.270378\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 29 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 6.00, NNZs: 278, Bias: -0.288668, T: 979, Avg. loss: 0.723973\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.29, NNZs: 151, Bias: -0.310770, T: 1958, Avg. loss: 0.648296\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.05, NNZs: 157, Bias: -0.314968, T: 2937, Avg. loss: 0.651644\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 4.92, NNZs: 159, Bias: -0.318461, T: 3916, Avg. loss: 0.644029\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 4.86, NNZs: 160, Bias: -0.318201, T: 4895, Avg. loss: 0.649156\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 4.82, NNZs: 168, Bias: -0.319903, T: 5874, Avg. loss: 0.642539\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 4.78, NNZs: 168, Bias: -0.321124, T: 6853, Avg. loss: 0.639676\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.76, NNZs: 170, Bias: -0.321989, T: 7832, Avg. loss: 0.638720\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.74, NNZs: 172, Bias: -0.322170, T: 8811, Avg. loss: 0.640155\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.73, NNZs: 171, Bias: -0.322634, T: 9790, Avg. loss: 0.638170\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 4.72, NNZs: 174, Bias: -0.323258, T: 10769, Avg. loss: 0.636087\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 4.71, NNZs: 174, Bias: -0.323684, T: 11748, Avg. loss: 0.635902\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 4.70, NNZs: 178, Bias: -0.323710, T: 12727, Avg. loss: 0.637967\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 4.70, NNZs: 178, Bias: -0.323882, T: 13706, Avg. loss: 0.636639\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 4.69, NNZs: 179, Bias: -0.324213, T: 14685, Avg. loss: 0.634984\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 4.68, NNZs: 180, Bias: -0.324440, T: 15664, Avg. loss: 0.635233\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 16 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 7.13, NNZs: 346, Bias: -0.108314, T: 979, Avg. loss: 0.644332\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 6.14, NNZs: 224, Bias: -0.099004, T: 1958, Avg. loss: 0.542596\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.79, NNZs: 236, Bias: -0.100577, T: 2937, Avg. loss: 0.539430\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 5.63, NNZs: 243, Bias: -0.105204, T: 3916, Avg. loss: 0.533900\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.51, NNZs: 253, Bias: -0.103686, T: 4895, Avg. loss: 0.528513\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 5.44, NNZs: 252, Bias: -0.104023, T: 5874, Avg. loss: 0.526603\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 5.40, NNZs: 258, Bias: -0.104569, T: 6853, Avg. loss: 0.524196\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 5.36, NNZs: 257, Bias: -0.104742, T: 7832, Avg. loss: 0.522902\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 5.33, NNZs: 261, Bias: -0.105391, T: 8811, Avg. loss: 0.521803\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 5.32, NNZs: 262, Bias: -0.106118, T: 9790, Avg. loss: 0.520968\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 5.30, NNZs: 262, Bias: -0.106026, T: 10769, Avg. loss: 0.519519\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 5.28, NNZs: 264, Bias: -0.106040, T: 11748, Avg. loss: 0.518610\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 5.27, NNZs: 265, Bias: -0.106047, T: 12727, Avg. loss: 0.518332\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 5.26, NNZs: 267, Bias: -0.106201, T: 13706, Avg. loss: 0.517692\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 5.25, NNZs: 266, Bias: -0.106486, T: 14685, Avg. loss: 0.517878\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 5.24, NNZs: 271, Bias: -0.106584, T: 15664, Avg. loss: 0.517184\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 16 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.53, NNZs: 389, Bias: -0.360720, T: 979, Avg. loss: 0.612413\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 8.37, NNZs: 306, Bias: -0.413458, T: 1958, Avg. loss: 0.338977\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 7.55, NNZs: 313, Bias: -0.429823, T: 2937, Avg. loss: 0.316068\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 7.13, NNZs: 318, Bias: -0.447008, T: 3916, Avg. loss: 0.311568\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 6.86, NNZs: 317, Bias: -0.457128, T: 4895, Avg. loss: 0.302443\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6.68, NNZs: 317, Bias: -0.468376, T: 5874, Avg. loss: 0.302423\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 6.56, NNZs: 320, Bias: -0.475316, T: 6853, Avg. loss: 0.294564\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 6.49, NNZs: 323, Bias: -0.475758, T: 7832, Avg. loss: 0.285257\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.43, NNZs: 323, Bias: -0.478157, T: 8811, Avg. loss: 0.286677\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 6.38, NNZs: 322, Bias: -0.481106, T: 9790, Avg. loss: 0.286738\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 6.34, NNZs: 324, Bias: -0.484242, T: 10769, Avg. loss: 0.286074\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 6.31, NNZs: 322, Bias: -0.486149, T: 11748, Avg. loss: 0.282851\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 6.28, NNZs: 323, Bias: -0.487454, T: 12727, Avg. loss: 0.281063\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 6.26, NNZs: 325, Bias: -0.489006, T: 13706, Avg. loss: 0.281175\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 6.24, NNZs: 326, Bias: -0.490382, T: 14685, Avg. loss: 0.280150\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 6.22, NNZs: 324, Bias: -0.492348, T: 15664, Avg. loss: 0.281851\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 6.20, NNZs: 325, Bias: -0.493699, T: 16643, Avg. loss: 0.279604\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 6.19, NNZs: 326, Bias: -0.495125, T: 17622, Avg. loss: 0.279925\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 18 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 5.98, NNZs: 280, Bias: -0.272950, T: 979, Avg. loss: 0.743040\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.24, NNZs: 160, Bias: -0.303652, T: 1958, Avg. loss: 0.658333\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 4.99, NNZs: 162, Bias: -0.310492, T: 2937, Avg. loss: 0.660952\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 4.86, NNZs: 165, Bias: -0.315180, T: 3916, Avg. loss: 0.653059\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 4.81, NNZs: 171, Bias: -0.314671, T: 4895, Avg. loss: 0.659823\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 4.76, NNZs: 175, Bias: -0.316663, T: 5874, Avg. loss: 0.651837\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 4.72, NNZs: 179, Bias: -0.318042, T: 6853, Avg. loss: 0.649791\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.70, NNZs: 180, Bias: -0.319131, T: 7832, Avg. loss: 0.648658\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.68, NNZs: 182, Bias: -0.319768, T: 8811, Avg. loss: 0.649083\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.67, NNZs: 182, Bias: -0.320306, T: 9790, Avg. loss: 0.648444\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 4.65, NNZs: 185, Bias: -0.320952, T: 10769, Avg. loss: 0.646783\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 4.64, NNZs: 185, Bias: -0.321613, T: 11748, Avg. loss: 0.644981\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 4.64, NNZs: 187, Bias: -0.321758, T: 12727, Avg. loss: 0.647830\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 4.63, NNZs: 188, Bias: -0.322151, T: 13706, Avg. loss: 0.645996\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 4.62, NNZs: 188, Bias: -0.322604, T: 14685, Avg. loss: 0.644926\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 4.62, NNZs: 191, Bias: -0.322936, T: 15664, Avg. loss: 0.645334\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 4.62, NNZs: 191, Bias: -0.323187, T: 16643, Avg. loss: 0.645467\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 17 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 7.13, NNZs: 356, Bias: -0.122093, T: 979, Avg. loss: 0.665838\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 6.14, NNZs: 239, Bias: -0.118931, T: 1958, Avg. loss: 0.556177\n",
            "Total training time: 0.00 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 3\n",
            "Norm: 5.77, NNZs: 250, Bias: -0.118545, T: 2937, Avg. loss: 0.555601\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 5.61, NNZs: 262, Bias: -0.124509, T: 3916, Avg. loss: 0.548671\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.49, NNZs: 265, Bias: -0.124365, T: 4895, Avg. loss: 0.543209\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 5.41, NNZs: 274, Bias: -0.123427, T: 5874, Avg. loss: 0.541439\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 5.36, NNZs: 284, Bias: -0.123459, T: 6853, Avg. loss: 0.538799\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 5.33, NNZs: 283, Bias: -0.124336, T: 7832, Avg. loss: 0.537404\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 5.31, NNZs: 293, Bias: -0.125232, T: 8811, Avg. loss: 0.536005\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 5.29, NNZs: 293, Bias: -0.125540, T: 9790, Avg. loss: 0.534548\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 5.27, NNZs: 294, Bias: -0.125343, T: 10769, Avg. loss: 0.533273\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 5.25, NNZs: 293, Bias: -0.125267, T: 11748, Avg. loss: 0.531961\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 5.24, NNZs: 293, Bias: -0.125466, T: 12727, Avg. loss: 0.531541\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 5.23, NNZs: 295, Bias: -0.125473, T: 13706, Avg. loss: 0.530821\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 5.22, NNZs: 297, Bias: -0.125739, T: 14685, Avg. loss: 0.530938\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 5.22, NNZs: 298, Bias: -0.125893, T: 15664, Avg. loss: 0.530221\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 5.21, NNZs: 298, Bias: -0.125846, T: 16643, Avg. loss: 0.529619\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 17 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.53, NNZs: 374, Bias: -0.385796, T: 979, Avg. loss: 0.616067\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 8.37, NNZs: 290, Bias: -0.438547, T: 1958, Avg. loss: 0.330384\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 7.56, NNZs: 299, Bias: -0.456800, T: 2937, Avg. loss: 0.308857\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 7.15, NNZs: 304, Bias: -0.470722, T: 3916, Avg. loss: 0.301189\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 6.89, NNZs: 315, Bias: -0.481955, T: 4895, Avg. loss: 0.296671\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6.72, NNZs: 315, Bias: -0.488597, T: 5874, Avg. loss: 0.290329\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 6.60, NNZs: 315, Bias: -0.496332, T: 6853, Avg. loss: 0.289857\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 6.53, NNZs: 316, Bias: -0.497591, T: 7832, Avg. loss: 0.280578\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.47, NNZs: 318, Bias: -0.499335, T: 8811, Avg. loss: 0.279992\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 6.42, NNZs: 319, Bias: -0.501374, T: 9790, Avg. loss: 0.278687\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 6.38, NNZs: 319, Bias: -0.504029, T: 10769, Avg. loss: 0.279230\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 6.35, NNZs: 321, Bias: -0.505538, T: 11748, Avg. loss: 0.276149\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 6.32, NNZs: 323, Bias: -0.506434, T: 12727, Avg. loss: 0.274176\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 6.30, NNZs: 325, Bias: -0.507803, T: 13706, Avg. loss: 0.274666\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 6.28, NNZs: 326, Bias: -0.508858, T: 14685, Avg. loss: 0.273251\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 6.26, NNZs: 326, Bias: -0.510888, T: 15664, Avg. loss: 0.275979\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 6.25, NNZs: 328, Bias: -0.512316, T: 16643, Avg. loss: 0.273870\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 6.24, NNZs: 328, Bias: -0.513520, T: 17622, Avg. loss: 0.273191\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 18 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 6.00, NNZs: 283, Bias: -0.262898, T: 979, Avg. loss: 0.750102\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.25, NNZs: 157, Bias: -0.288147, T: 1958, Avg. loss: 0.676933\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 4.98, NNZs: 161, Bias: -0.294661, T: 2937, Avg. loss: 0.677300\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 4.84, NNZs: 164, Bias: -0.298364, T: 3916, Avg. loss: 0.671580\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 4.78, NNZs: 167, Bias: -0.297800, T: 4895, Avg. loss: 0.675993\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 4.73, NNZs: 168, Bias: -0.299137, T: 5874, Avg. loss: 0.669746\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 4.70, NNZs: 174, Bias: -0.299879, T: 6853, Avg. loss: 0.668429\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.67, NNZs: 178, Bias: -0.300190, T: 7832, Avg. loss: 0.668780\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.65, NNZs: 173, Bias: -0.300634, T: 8811, Avg. loss: 0.667408\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.64, NNZs: 180, Bias: -0.301137, T: 9790, Avg. loss: 0.666233\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 4.62, NNZs: 180, Bias: -0.301708, T: 10769, Avg. loss: 0.664582\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 4.61, NNZs: 182, Bias: -0.302213, T: 11748, Avg. loss: 0.663560\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 4.60, NNZs: 184, Bias: -0.302187, T: 12727, Avg. loss: 0.666256\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 4.60, NNZs: 186, Bias: -0.302343, T: 13706, Avg. loss: 0.665075\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 4.59, NNZs: 185, Bias: -0.302533, T: 14685, Avg. loss: 0.664398\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 4.59, NNZs: 189, Bias: -0.302789, T: 15664, Avg. loss: 0.663655\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 4.58, NNZs: 190, Bias: -0.302926, T: 16643, Avg. loss: 0.664049\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 17 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 7.17, NNZs: 381, Bias: -0.132195, T: 979, Avg. loss: 0.653753\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 6.18, NNZs: 239, Bias: -0.131230, T: 1958, Avg. loss: 0.552398\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.81, NNZs: 249, Bias: -0.133025, T: 2937, Avg. loss: 0.551761\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 5.64, NNZs: 255, Bias: -0.137929, T: 3916, Avg. loss: 0.543783\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.53, NNZs: 261, Bias: -0.138719, T: 4895, Avg. loss: 0.538244\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 5.45, NNZs: 264, Bias: -0.137114, T: 5874, Avg. loss: 0.536302\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 5.40, NNZs: 265, Bias: -0.138304, T: 6853, Avg. loss: 0.534308\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 5.37, NNZs: 267, Bias: -0.139160, T: 7832, Avg. loss: 0.533265\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 5.34, NNZs: 269, Bias: -0.139674, T: 8811, Avg. loss: 0.532033\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 5.32, NNZs: 271, Bias: -0.139945, T: 9790, Avg. loss: 0.531067\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 5.30, NNZs: 273, Bias: -0.139931, T: 10769, Avg. loss: 0.530026\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 5.28, NNZs: 275, Bias: -0.139698, T: 11748, Avg. loss: 0.528957\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 5.27, NNZs: 276, Bias: -0.139894, T: 12727, Avg. loss: 0.528831\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 5.26, NNZs: 278, Bias: -0.140090, T: 13706, Avg. loss: 0.528247\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 5.25, NNZs: 280, Bias: -0.140318, T: 14685, Avg. loss: 0.528040\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 5.25, NNZs: 281, Bias: -0.140417, T: 15664, Avg. loss: 0.527714\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 5.24, NNZs: 282, Bias: -0.140398, T: 16643, Avg. loss: 0.527350\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 17 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.50, NNZs: 380, Bias: -0.362849, T: 979, Avg. loss: 0.596615\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 8.29, NNZs: 291, Bias: -0.439415, T: 1958, Avg. loss: 0.344155\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 7.55, NNZs: 302, Bias: -0.439030, T: 2937, Avg. loss: 0.303295\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 7.15, NNZs: 313, Bias: -0.457065, T: 3916, Avg. loss: 0.308259\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 6.90, NNZs: 317, Bias: -0.469834, T: 4895, Avg. loss: 0.299261\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6.75, NNZs: 322, Bias: -0.474260, T: 5874, Avg. loss: 0.288610\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 6.63, NNZs: 323, Bias: -0.483014, T: 6853, Avg. loss: 0.291679\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 6.56, NNZs: 324, Bias: -0.484592, T: 7832, Avg. loss: 0.280914\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.50, NNZs: 323, Bias: -0.486592, T: 8811, Avg. loss: 0.280101\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 6.46, NNZs: 326, Bias: -0.489718, T: 9790, Avg. loss: 0.280468\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 6.42, NNZs: 327, Bias: -0.492131, T: 10769, Avg. loss: 0.278596\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 6.39, NNZs: 330, Bias: -0.493630, T: 11748, Avg. loss: 0.275685\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 6.37, NNZs: 330, Bias: -0.494548, T: 12727, Avg. loss: 0.274026\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 6.35, NNZs: 334, Bias: -0.495958, T: 13706, Avg. loss: 0.274137\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 6.33, NNZs: 335, Bias: -0.497701, T: 14685, Avg. loss: 0.274765\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 6.32, NNZs: 335, Bias: -0.499767, T: 15664, Avg. loss: 0.275756\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 6.30, NNZs: 335, Bias: -0.501127, T: 16643, Avg. loss: 0.273090\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 6.29, NNZs: 337, Bias: -0.502477, T: 17622, Avg. loss: 0.273200\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 18 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 6.02, NNZs: 289, Bias: -0.268893, T: 979, Avg. loss: 0.743261\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.26, NNZs: 161, Bias: -0.289207, T: 1958, Avg. loss: 0.678700\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 4.97, NNZs: 165, Bias: -0.297414, T: 2937, Avg. loss: 0.675995\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 4.84, NNZs: 169, Bias: -0.300223, T: 3916, Avg. loss: 0.675052\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 4.78, NNZs: 170, Bias: -0.299025, T: 4895, Avg. loss: 0.680318\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 4.72, NNZs: 173, Bias: -0.300601, T: 5874, Avg. loss: 0.671895\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 4.69, NNZs: 174, Bias: -0.301208, T: 6853, Avg. loss: 0.672091\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.67, NNZs: 178, Bias: -0.301319, T: 7832, Avg. loss: 0.672372\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.65, NNZs: 179, Bias: -0.301710, T: 8811, Avg. loss: 0.670007\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.63, NNZs: 180, Bias: -0.302036, T: 9790, Avg. loss: 0.669558\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 4.62, NNZs: 181, Bias: -0.302466, T: 10769, Avg. loss: 0.668037\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 4.60, NNZs: 182, Bias: -0.303007, T: 11748, Avg. loss: 0.665849\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 4.60, NNZs: 182, Bias: -0.303077, T: 12727, Avg. loss: 0.668409\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 4.59, NNZs: 182, Bias: -0.303329, T: 13706, Avg. loss: 0.667075\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 4.58, NNZs: 183, Bias: -0.303451, T: 14685, Avg. loss: 0.667331\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 4.58, NNZs: 183, Bias: -0.303577, T: 15664, Avg. loss: 0.667182\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 4.57, NNZs: 183, Bias: -0.303720, T: 16643, Avg. loss: 0.666652\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 17 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 7.07, NNZs: 348, Bias: -0.128354, T: 979, Avg. loss: 0.649537\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 6.14, NNZs: 232, Bias: -0.128636, T: 1958, Avg. loss: 0.555840\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.76, NNZs: 240, Bias: -0.126700, T: 2937, Avg. loss: 0.552232\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 5.59, NNZs: 244, Bias: -0.131046, T: 3916, Avg. loss: 0.547092\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.47, NNZs: 262, Bias: -0.130445, T: 4895, Avg. loss: 0.541476\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 5.40, NNZs: 266, Bias: -0.129817, T: 5874, Avg. loss: 0.539916\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 5.35, NNZs: 268, Bias: -0.130631, T: 6853, Avg. loss: 0.537134\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 5.32, NNZs: 269, Bias: -0.130841, T: 7832, Avg. loss: 0.535787\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 5.29, NNZs: 273, Bias: -0.131450, T: 8811, Avg. loss: 0.534534\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 5.27, NNZs: 274, Bias: -0.131522, T: 9790, Avg. loss: 0.532926\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 5.25, NNZs: 279, Bias: -0.131458, T: 10769, Avg. loss: 0.532115\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 5.24, NNZs: 278, Bias: -0.131112, T: 11748, Avg. loss: 0.531077\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 5.23, NNZs: 281, Bias: -0.131253, T: 12727, Avg. loss: 0.530973\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 5.22, NNZs: 281, Bias: -0.131331, T: 13706, Avg. loss: 0.530035\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 5.21, NNZs: 284, Bias: -0.131529, T: 14685, Avg. loss: 0.529980\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 5.20, NNZs: 287, Bias: -0.131403, T: 15664, Avg. loss: 0.529393\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 5.19, NNZs: 289, Bias: -0.131344, T: 16643, Avg. loss: 0.529076\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 17 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.53, NNZs: 342, Bias: -0.373896, T: 979, Avg. loss: 0.576661\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 8.23, NNZs: 269, Bias: -0.452133, T: 1958, Avg. loss: 0.342618\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 7.48, NNZs: 278, Bias: -0.454468, T: 2937, Avg. loss: 0.307583\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 7.08, NNZs: 283, Bias: -0.467827, T: 3916, Avg. loss: 0.304627\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 6.83, NNZs: 292, Bias: -0.481195, T: 4895, Avg. loss: 0.302115\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6.67, NNZs: 294, Bias: -0.484140, T: 5874, Avg. loss: 0.288548\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 6.55, NNZs: 295, Bias: -0.491964, T: 6853, Avg. loss: 0.292223\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 6.47, NNZs: 298, Bias: -0.492705, T: 7832, Avg. loss: 0.280948\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.41, NNZs: 301, Bias: -0.493377, T: 8811, Avg. loss: 0.279689\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 6.36, NNZs: 304, Bias: -0.496485, T: 9790, Avg. loss: 0.283049\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 6.32, NNZs: 309, Bias: -0.498411, T: 10769, Avg. loss: 0.279765\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 6.29, NNZs: 310, Bias: -0.500259, T: 11748, Avg. loss: 0.278688\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 6.27, NNZs: 310, Bias: -0.500364, T: 12727, Avg. loss: 0.274082\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 6.25, NNZs: 310, Bias: -0.502200, T: 13706, Avg. loss: 0.277653\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 6.23, NNZs: 311, Bias: -0.503586, T: 14685, Avg. loss: 0.275806\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 6.21, NNZs: 311, Bias: -0.505495, T: 15664, Avg. loss: 0.277284\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 6.20, NNZs: 311, Bias: -0.506914, T: 16643, Avg. loss: 0.275269\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 6.18, NNZs: 312, Bias: -0.508170, T: 17622, Avg. loss: 0.274942\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 18 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 6.00, NNZs: 300, Bias: -0.270461, T: 979, Avg. loss: 0.728212\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.27, NNZs: 164, Bias: -0.289838, T: 1958, Avg. loss: 0.668366\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.01, NNZs: 167, Bias: -0.297143, T: 2937, Avg. loss: 0.667028\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 4.88, NNZs: 167, Bias: -0.300400, T: 3916, Avg. loss: 0.664672\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 4.82, NNZs: 171, Bias: -0.299841, T: 4895, Avg. loss: 0.669819\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 4.77, NNZs: 178, Bias: -0.301637, T: 5874, Avg. loss: 0.661710\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 4.73, NNZs: 180, Bias: -0.303081, T: 6853, Avg. loss: 0.659840\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.70, NNZs: 181, Bias: -0.303373, T: 7832, Avg. loss: 0.661823\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.68, NNZs: 181, Bias: -0.303955, T: 8811, Avg. loss: 0.659643\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.67, NNZs: 182, Bias: -0.304208, T: 9790, Avg. loss: 0.660018\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 4.66, NNZs: 183, Bias: -0.304614, T: 10769, Avg. loss: 0.659078\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 4.65, NNZs: 182, Bias: -0.305240, T: 11748, Avg. loss: 0.656106\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 4.64, NNZs: 185, Bias: -0.305301, T: 12727, Avg. loss: 0.658867\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 4.63, NNZs: 186, Bias: -0.305699, T: 13706, Avg. loss: 0.656913\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 4.63, NNZs: 188, Bias: -0.305906, T: 14685, Avg. loss: 0.657474\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 4.62, NNZs: 192, Bias: -0.306165, T: 15664, Avg. loss: 0.656738\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 4.62, NNZs: 189, Bias: -0.306276, T: 16643, Avg. loss: 0.657434\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 17 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 7.07, NNZs: 350, Bias: -0.124728, T: 979, Avg. loss: 0.654492\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 6.15, NNZs: 239, Bias: -0.129732, T: 1958, Avg. loss: 0.564725\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.78, NNZs: 242, Bias: -0.128594, T: 2937, Avg. loss: 0.559155\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 5.62, NNZs: 261, Bias: -0.133239, T: 3916, Avg. loss: 0.553758\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.51, NNZs: 270, Bias: -0.134581, T: 4895, Avg. loss: 0.546886\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 5.44, NNZs: 281, Bias: -0.134564, T: 5874, Avg. loss: 0.545667\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 5.40, NNZs: 283, Bias: -0.135821, T: 6853, Avg. loss: 0.541733\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 5.36, NNZs: 282, Bias: -0.135833, T: 7832, Avg. loss: 0.540556\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 5.34, NNZs: 289, Bias: -0.136863, T: 8811, Avg. loss: 0.539257\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 5.32, NNZs: 290, Bias: -0.137439, T: 9790, Avg. loss: 0.537874\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 5.30, NNZs: 293, Bias: -0.137232, T: 10769, Avg. loss: 0.536942\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 5.28, NNZs: 293, Bias: -0.136944, T: 11748, Avg. loss: 0.535927\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 5.27, NNZs: 293, Bias: -0.137236, T: 12727, Avg. loss: 0.535785\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 5.26, NNZs: 296, Bias: -0.137515, T: 13706, Avg. loss: 0.534887\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 5.25, NNZs: 298, Bias: -0.137830, T: 14685, Avg. loss: 0.534775\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 5.25, NNZs: 299, Bias: -0.137866, T: 15664, Avg. loss: 0.534225\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 5.24, NNZs: 298, Bias: -0.137819, T: 16643, Avg. loss: 0.534002\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 17 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.69, NNZs: 365, Bias: -0.373716, T: 979, Avg. loss: 0.584361\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 8.30, NNZs: 280, Bias: -0.468977, T: 1958, Avg. loss: 0.350945\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 7.54, NNZs: 288, Bias: -0.465744, T: 2937, Avg. loss: 0.304327\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 7.14, NNZs: 291, Bias: -0.477508, T: 3916, Avg. loss: 0.303628\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 6.88, NNZs: 298, Bias: -0.489316, T: 4895, Avg. loss: 0.300421\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6.72, NNZs: 303, Bias: -0.491447, T: 5874, Avg. loss: 0.288072\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 6.59, NNZs: 307, Bias: -0.499271, T: 6853, Avg. loss: 0.292672\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 6.51, NNZs: 311, Bias: -0.501453, T: 7832, Avg. loss: 0.283569\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.45, NNZs: 315, Bias: -0.501621, T: 8811, Avg. loss: 0.279152\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 6.40, NNZs: 315, Bias: -0.504426, T: 9790, Avg. loss: 0.282584\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 6.36, NNZs: 316, Bias: -0.506657, T: 10769, Avg. loss: 0.280721\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 6.33, NNZs: 318, Bias: -0.508350, T: 11748, Avg. loss: 0.278473\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 6.31, NNZs: 316, Bias: -0.508599, T: 12727, Avg. loss: 0.274836\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 6.29, NNZs: 320, Bias: -0.509858, T: 13706, Avg. loss: 0.276450\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 6.27, NNZs: 320, Bias: -0.511109, T: 14685, Avg. loss: 0.275956\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 6.25, NNZs: 320, Bias: -0.512859, T: 15664, Avg. loss: 0.276983\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 6.24, NNZs: 321, Bias: -0.514397, T: 16643, Avg. loss: 0.276205\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 6.23, NNZs: 320, Bias: -0.515520, T: 17622, Avg. loss: 0.274873\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 18 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 5.98, NNZs: 306, Bias: -0.271188, T: 979, Avg. loss: 0.745529\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.27, NNZs: 171, Bias: -0.280375, T: 1958, Avg. loss: 0.695701\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 4.99, NNZs: 175, Bias: -0.287030, T: 2937, Avg. loss: 0.689124\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 4.84, NNZs: 179, Bias: -0.292039, T: 3916, Avg. loss: 0.681391\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 4.79, NNZs: 180, Bias: -0.289396, T: 4895, Avg. loss: 0.693231\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 4.74, NNZs: 182, Bias: -0.291413, T: 5874, Avg. loss: 0.680406\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 4.70, NNZs: 190, Bias: -0.292391, T: 6853, Avg. loss: 0.680258\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.68, NNZs: 194, Bias: -0.292900, T: 7832, Avg. loss: 0.680479\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.65, NNZs: 196, Bias: -0.293752, T: 8811, Avg. loss: 0.677363\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.64, NNZs: 199, Bias: -0.294133, T: 9790, Avg. loss: 0.678293\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 4.63, NNZs: 199, Bias: -0.294572, T: 10769, Avg. loss: 0.677166\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 4.61, NNZs: 202, Bias: -0.295217, T: 11748, Avg. loss: 0.674139\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 4.61, NNZs: 203, Bias: -0.295344, T: 12727, Avg. loss: 0.676824\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 4.60, NNZs: 205, Bias: -0.295736, T: 13706, Avg. loss: 0.674522\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 4.59, NNZs: 205, Bias: -0.295797, T: 14685, Avg. loss: 0.676323\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 4.59, NNZs: 204, Bias: -0.295948, T: 15664, Avg. loss: 0.675187\n",
            "Total training time: 0.02 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 17\n",
            "Norm: 4.59, NNZs: 204, Bias: -0.296106, T: 16643, Avg. loss: 0.674866\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 17 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 7.00, NNZs: 329, Bias: -0.113145, T: 979, Avg. loss: 0.640757\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 6.14, NNZs: 218, Bias: -0.115318, T: 1958, Avg. loss: 0.550881\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.79, NNZs: 231, Bias: -0.115054, T: 2937, Avg. loss: 0.545667\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 5.63, NNZs: 243, Bias: -0.119084, T: 3916, Avg. loss: 0.538830\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.53, NNZs: 253, Bias: -0.120768, T: 4895, Avg. loss: 0.533490\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 5.46, NNZs: 262, Bias: -0.119991, T: 5874, Avg. loss: 0.531212\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 5.42, NNZs: 263, Bias: -0.121375, T: 6853, Avg. loss: 0.528718\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 5.39, NNZs: 264, Bias: -0.122157, T: 7832, Avg. loss: 0.526891\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 5.36, NNZs: 267, Bias: -0.123187, T: 8811, Avg. loss: 0.525489\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 5.34, NNZs: 268, Bias: -0.123834, T: 9790, Avg. loss: 0.524715\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 5.33, NNZs: 268, Bias: -0.124092, T: 10769, Avg. loss: 0.523685\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 5.31, NNZs: 269, Bias: -0.123802, T: 11748, Avg. loss: 0.522476\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 5.30, NNZs: 275, Bias: -0.124122, T: 12727, Avg. loss: 0.522404\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 5.29, NNZs: 278, Bias: -0.124274, T: 13706, Avg. loss: 0.521646\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 5.28, NNZs: 274, Bias: -0.124664, T: 14685, Avg. loss: 0.521666\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 5.27, NNZs: 280, Bias: -0.124741, T: 15664, Avg. loss: 0.520940\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 5.27, NNZs: 276, Bias: -0.124772, T: 16643, Avg. loss: 0.520835\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 17 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.53, NNZs: 357, Bias: -0.382994, T: 979, Avg. loss: 0.546616\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 8.16, NNZs: 273, Bias: -0.482742, T: 1958, Avg. loss: 0.337010\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 7.46, NNZs: 283, Bias: -0.472024, T: 2937, Avg. loss: 0.292352\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 7.08, NNZs: 288, Bias: -0.482109, T: 3916, Avg. loss: 0.293861\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 6.83, NNZs: 291, Bias: -0.494430, T: 4895, Avg. loss: 0.292029\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6.68, NNZs: 293, Bias: -0.497386, T: 5874, Avg. loss: 0.279582\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 6.56, NNZs: 298, Bias: -0.504704, T: 6853, Avg. loss: 0.282866\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 6.48, NNZs: 299, Bias: -0.508107, T: 7832, Avg. loss: 0.277040\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.43, NNZs: 301, Bias: -0.508065, T: 8811, Avg. loss: 0.270023\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 6.39, NNZs: 304, Bias: -0.510406, T: 9790, Avg. loss: 0.272739\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 6.35, NNZs: 307, Bias: -0.513617, T: 10769, Avg. loss: 0.274249\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 6.32, NNZs: 309, Bias: -0.515007, T: 11748, Avg. loss: 0.269245\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 6.30, NNZs: 308, Bias: -0.515531, T: 12727, Avg. loss: 0.267277\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 6.28, NNZs: 308, Bias: -0.516677, T: 13706, Avg. loss: 0.267613\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 6.26, NNZs: 307, Bias: -0.518185, T: 14685, Avg. loss: 0.268164\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 6.24, NNZs: 308, Bias: -0.519789, T: 15664, Avg. loss: 0.267893\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 6.23, NNZs: 308, Bias: -0.521360, T: 16643, Avg. loss: 0.267623\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 6.22, NNZs: 308, Bias: -0.522400, T: 17622, Avg. loss: 0.266010\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 6.21, NNZs: 309, Bias: -0.523006, T: 18601, Avg. loss: 0.264042\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 6.20, NNZs: 312, Bias: -0.524028, T: 19580, Avg. loss: 0.265481\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 6.19, NNZs: 311, Bias: -0.524772, T: 20559, Avg. loss: 0.264333\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 6.19, NNZs: 312, Bias: -0.525723, T: 21538, Avg. loss: 0.265121\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 6.18, NNZs: 312, Bias: -0.526536, T: 22517, Avg. loss: 0.264112\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 6.17, NNZs: 313, Bias: -0.527157, T: 23496, Avg. loss: 0.263333\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 5.95, NNZs: 270, Bias: -0.275620, T: 979, Avg. loss: 0.721577\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.27, NNZs: 158, Bias: -0.287083, T: 1958, Avg. loss: 0.667979\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.03, NNZs: 165, Bias: -0.292789, T: 2937, Avg. loss: 0.665138\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 4.89, NNZs: 164, Bias: -0.296722, T: 3916, Avg. loss: 0.655889\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 4.85, NNZs: 172, Bias: -0.294704, T: 4895, Avg. loss: 0.663908\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 4.79, NNZs: 175, Bias: -0.296495, T: 5874, Avg. loss: 0.653500\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 4.76, NNZs: 181, Bias: -0.296872, T: 6853, Avg. loss: 0.655450\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.74, NNZs: 180, Bias: -0.296898, T: 7832, Avg. loss: 0.654998\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.72, NNZs: 185, Bias: -0.297736, T: 8811, Avg. loss: 0.650511\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.71, NNZs: 187, Bias: -0.298103, T: 9790, Avg. loss: 0.651631\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 4.69, NNZs: 188, Bias: -0.298463, T: 10769, Avg. loss: 0.650865\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 4.68, NNZs: 190, Bias: -0.298865, T: 11748, Avg. loss: 0.649126\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 4.68, NNZs: 190, Bias: -0.298902, T: 12727, Avg. loss: 0.650690\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 4.67, NNZs: 191, Bias: -0.299422, T: 13706, Avg. loss: 0.647487\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 4.66, NNZs: 192, Bias: -0.299504, T: 14685, Avg. loss: 0.649732\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 4.66, NNZs: 192, Bias: -0.299768, T: 15664, Avg. loss: 0.648006\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 4.65, NNZs: 192, Bias: -0.299908, T: 16643, Avg. loss: 0.648629\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 4.65, NNZs: 193, Bias: -0.300099, T: 17622, Avg. loss: 0.647627\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 4.64, NNZs: 194, Bias: -0.300282, T: 18601, Avg. loss: 0.647329\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 19 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 6.92, NNZs: 337, Bias: -0.126281, T: 980, Avg. loss: 0.638606\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 6.06, NNZs: 235, Bias: -0.129318, T: 1960, Avg. loss: 0.565526\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.75, NNZs: 240, Bias: -0.137323, T: 2940, Avg. loss: 0.559506\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 5.57, NNZs: 250, Bias: -0.138099, T: 3920, Avg. loss: 0.550919\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.47, NNZs: 255, Bias: -0.138619, T: 4900, Avg. loss: 0.545212\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 5.39, NNZs: 261, Bias: -0.137691, T: 5880, Avg. loss: 0.542667\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 5.35, NNZs: 262, Bias: -0.139042, T: 6860, Avg. loss: 0.541876\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 5.32, NNZs: 264, Bias: -0.140048, T: 7840, Avg. loss: 0.539636\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 5.29, NNZs: 266, Bias: -0.140437, T: 8820, Avg. loss: 0.537947\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 5.27, NNZs: 268, Bias: -0.140470, T: 9800, Avg. loss: 0.537268\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 5.26, NNZs: 270, Bias: -0.141160, T: 10780, Avg. loss: 0.536472\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 5.24, NNZs: 270, Bias: -0.140881, T: 11760, Avg. loss: 0.535674\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 5.23, NNZs: 271, Bias: -0.141220, T: 12740, Avg. loss: 0.535163\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 5.22, NNZs: 275, Bias: -0.141721, T: 13720, Avg. loss: 0.534249\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 14 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.61, NNZs: 383, Bias: -0.337643, T: 980, Avg. loss: 0.640153\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 8.38, NNZs: 301, Bias: -0.427318, T: 1960, Avg. loss: 0.355894\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 7.55, NNZs: 301, Bias: -0.444418, T: 2940, Avg. loss: 0.313771\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 7.13, NNZs: 304, Bias: -0.459116, T: 3920, Avg. loss: 0.308497\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 6.89, NNZs: 307, Bias: -0.466043, T: 4900, Avg. loss: 0.299373\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6.73, NNZs: 309, Bias: -0.468790, T: 5880, Avg. loss: 0.291139\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 6.62, NNZs: 310, Bias: -0.475321, T: 6860, Avg. loss: 0.294505\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 6.54, NNZs: 317, Bias: -0.478242, T: 7840, Avg. loss: 0.287101\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.48, NNZs: 316, Bias: -0.481086, T: 8820, Avg. loss: 0.286213\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 6.43, NNZs: 319, Bias: -0.482226, T: 9800, Avg. loss: 0.281493\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 6.39, NNZs: 320, Bias: -0.484800, T: 10780, Avg. loss: 0.283720\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 6.36, NNZs: 321, Bias: -0.486106, T: 11760, Avg. loss: 0.280440\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 6.33, NNZs: 321, Bias: -0.489519, T: 12740, Avg. loss: 0.285765\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 6.31, NNZs: 323, Bias: -0.490820, T: 13720, Avg. loss: 0.279704\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 6.29, NNZs: 323, Bias: -0.492156, T: 14700, Avg. loss: 0.279662\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 6.27, NNZs: 323, Bias: -0.493246, T: 15680, Avg. loss: 0.278242\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 6.26, NNZs: 324, Bias: -0.494631, T: 16660, Avg. loss: 0.278968\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 6.24, NNZs: 323, Bias: -0.495129, T: 17640, Avg. loss: 0.275490\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 6.23, NNZs: 324, Bias: -0.496431, T: 18620, Avg. loss: 0.278529\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 6.22, NNZs: 324, Bias: -0.497763, T: 19600, Avg. loss: 0.278435\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 6.21, NNZs: 324, Bias: -0.498606, T: 20580, Avg. loss: 0.276444\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 6.20, NNZs: 326, Bias: -0.499342, T: 21560, Avg. loss: 0.275873\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 6.20, NNZs: 326, Bias: -0.500090, T: 22540, Avg. loss: 0.275881\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 23 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 5.96, NNZs: 286, Bias: -0.233654, T: 980, Avg. loss: 0.751237\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.28, NNZs: 165, Bias: -0.256106, T: 1960, Avg. loss: 0.686559\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 4.99, NNZs: 169, Bias: -0.269809, T: 2940, Avg. loss: 0.676120\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 4.86, NNZs: 174, Bias: -0.275986, T: 3920, Avg. loss: 0.675050\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 4.79, NNZs: 176, Bias: -0.278388, T: 4900, Avg. loss: 0.674860\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 4.75, NNZs: 178, Bias: -0.280127, T: 5880, Avg. loss: 0.671776\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 4.71, NNZs: 179, Bias: -0.282663, T: 6860, Avg. loss: 0.667207\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.68, NNZs: 183, Bias: -0.284115, T: 7840, Avg. loss: 0.667979\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.66, NNZs: 184, Bias: -0.285389, T: 8820, Avg. loss: 0.667526\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.64, NNZs: 188, Bias: -0.286317, T: 9800, Avg. loss: 0.667034\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 4.63, NNZs: 190, Bias: -0.287026, T: 10780, Avg. loss: 0.667182\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 4.62, NNZs: 192, Bias: -0.287706, T: 11760, Avg. loss: 0.665958\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 4.61, NNZs: 191, Bias: -0.287922, T: 12740, Avg. loss: 0.668726\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 4.60, NNZs: 191, Bias: -0.288486, T: 13720, Avg. loss: 0.665361\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 4.60, NNZs: 195, Bias: -0.288890, T: 14700, Avg. loss: 0.666208\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 4.59, NNZs: 195, Bias: -0.289128, T: 15680, Avg. loss: 0.666319\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 4.59, NNZs: 197, Bias: -0.289655, T: 16660, Avg. loss: 0.663770\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 4.58, NNZs: 197, Bias: -0.289876, T: 17640, Avg. loss: 0.665632\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 4.58, NNZs: 197, Bias: -0.290261, T: 18620, Avg. loss: 0.663865\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 4.58, NNZs: 198, Bias: -0.290555, T: 19600, Avg. loss: 0.663964\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 4.58, NNZs: 200, Bias: -0.290677, T: 20580, Avg. loss: 0.665479\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 4.57, NNZs: 201, Bias: -0.290939, T: 21560, Avg. loss: 0.663595\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 22 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 6.98, NNZs: 358, Bias: -0.109401, T: 980, Avg. loss: 0.633653\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 6.07, NNZs: 238, Bias: -0.104585, T: 1960, Avg. loss: 0.557645\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.77, NNZs: 244, Bias: -0.114012, T: 2940, Avg. loss: 0.554049\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 5.61, NNZs: 254, Bias: -0.117778, T: 3920, Avg. loss: 0.544285\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.50, NNZs: 254, Bias: -0.118471, T: 4900, Avg. loss: 0.539015\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 5.42, NNZs: 262, Bias: -0.117610, T: 5880, Avg. loss: 0.536541\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 5.38, NNZs: 268, Bias: -0.119638, T: 6860, Avg. loss: 0.536060\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 5.35, NNZs: 271, Bias: -0.120956, T: 7840, Avg. loss: 0.533611\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 5.32, NNZs: 274, Bias: -0.121664, T: 8820, Avg. loss: 0.532325\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 5.30, NNZs: 276, Bias: -0.122047, T: 9800, Avg. loss: 0.531508\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 5.29, NNZs: 275, Bias: -0.122964, T: 10780, Avg. loss: 0.530042\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 5.27, NNZs: 275, Bias: -0.122936, T: 11760, Avg. loss: 0.529384\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 5.26, NNZs: 277, Bias: -0.123510, T: 12740, Avg. loss: 0.528978\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 5.25, NNZs: 279, Bias: -0.124212, T: 13720, Avg. loss: 0.528287\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 5.24, NNZs: 278, Bias: -0.124319, T: 14700, Avg. loss: 0.527709\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 5.24, NNZs: 280, Bias: -0.124745, T: 15680, Avg. loss: 0.527256\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 16 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.74, NNZs: 352, Bias: -0.361294, T: 980, Avg. loss: 0.620426\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 8.39, NNZs: 275, Bias: -0.455074, T: 1960, Avg. loss: 0.346125\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 7.57, NNZs: 276, Bias: -0.460093, T: 2940, Avg. loss: 0.300125\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 7.14, NNZs: 279, Bias: -0.477571, T: 3920, Avg. loss: 0.305246\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 6.91, NNZs: 285, Bias: -0.482747, T: 4900, Avg. loss: 0.292777\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6.74, NNZs: 291, Bias: -0.486164, T: 5880, Avg. loss: 0.287782\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 6.62, NNZs: 299, Bias: -0.492105, T: 6860, Avg. loss: 0.289376\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 6.54, NNZs: 304, Bias: -0.493490, T: 7840, Avg. loss: 0.281135\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.48, NNZs: 309, Bias: -0.497040, T: 8820, Avg. loss: 0.283753\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 6.43, NNZs: 307, Bias: -0.497420, T: 9800, Avg. loss: 0.276269\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 6.39, NNZs: 310, Bias: -0.500351, T: 10780, Avg. loss: 0.280739\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 6.36, NNZs: 311, Bias: -0.501654, T: 11760, Avg. loss: 0.276835\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 6.33, NNZs: 312, Bias: -0.504443, T: 12740, Avg. loss: 0.280312\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 6.30, NNZs: 313, Bias: -0.505566, T: 13720, Avg. loss: 0.275508\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 6.28, NNZs: 314, Bias: -0.506815, T: 14700, Avg. loss: 0.275652\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 15 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 5.98, NNZs: 275, Bias: -0.242357, T: 980, Avg. loss: 0.733257\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.28, NNZs: 168, Bias: -0.268685, T: 1960, Avg. loss: 0.673727\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.01, NNZs: 180, Bias: -0.281301, T: 2940, Avg. loss: 0.666587\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 4.88, NNZs: 183, Bias: -0.286818, T: 3920, Avg. loss: 0.664642\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 4.82, NNZs: 184, Bias: -0.288996, T: 4900, Avg. loss: 0.664167\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 4.77, NNZs: 186, Bias: -0.291039, T: 5880, Avg. loss: 0.659870\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 4.73, NNZs: 186, Bias: -0.293558, T: 6860, Avg. loss: 0.655327\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.70, NNZs: 190, Bias: -0.295039, T: 7840, Avg. loss: 0.656008\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.68, NNZs: 190, Bias: -0.296298, T: 8820, Avg. loss: 0.654955\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.67, NNZs: 190, Bias: -0.297109, T: 9800, Avg. loss: 0.655719\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 4.66, NNZs: 194, Bias: -0.297815, T: 10780, Avg. loss: 0.655433\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 4.64, NNZs: 193, Bias: -0.298571, T: 11760, Avg. loss: 0.653624\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 4.64, NNZs: 195, Bias: -0.298809, T: 12740, Avg. loss: 0.656612\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 4.63, NNZs: 193, Bias: -0.299380, T: 13720, Avg. loss: 0.653037\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 4.63, NNZs: 196, Bias: -0.299780, T: 14700, Avg. loss: 0.654020\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 4.62, NNZs: 196, Bias: -0.300008, T: 15680, Avg. loss: 0.654419\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 4.62, NNZs: 199, Bias: -0.300491, T: 16660, Avg. loss: 0.652245\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 17 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.71, NNZs: 1280, Bias: -0.149583, T: 979, Avg. loss: 0.499606\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 11.54, NNZs: 1280, Bias: -0.145145, T: 1958, Avg. loss: 0.416385\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 12.01, NNZs: 1280, Bias: -0.142934, T: 2937, Avg. loss: 0.399851\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 12.35, NNZs: 1280, Bias: -0.140986, T: 3916, Avg. loss: 0.390621\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 12.61, NNZs: 1280, Bias: -0.139658, T: 4895, Avg. loss: 0.384202\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 12.82, NNZs: 1280, Bias: -0.138583, T: 5874, Avg. loss: 0.379377\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 13.00, NNZs: 1280, Bias: -0.137672, T: 6853, Avg. loss: 0.375557\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 13.15, NNZs: 1280, Bias: -0.136878, T: 7832, Avg. loss: 0.372379\n",
            "Total training time: 0.01 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 9\n",
            "Norm: 13.28, NNZs: 1280, Bias: -0.136204, T: 8811, Avg. loss: 0.369686\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13.40, NNZs: 1280, Bias: -0.135597, T: 9790, Avg. loss: 0.367349\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 13.51, NNZs: 1280, Bias: -0.135060, T: 10769, Avg. loss: 0.365278\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 13.61, NNZs: 1280, Bias: -0.134560, T: 11748, Avg. loss: 0.363443\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 13.70, NNZs: 1280, Bias: -0.134108, T: 12727, Avg. loss: 0.361783\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 13.78, NNZs: 1280, Bias: -0.133690, T: 13706, Avg. loss: 0.360271\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 13.86, NNZs: 1280, Bias: -0.133312, T: 14685, Avg. loss: 0.358886\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 13.93, NNZs: 1280, Bias: -0.132953, T: 15664, Avg. loss: 0.357614\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 13.99, NNZs: 1280, Bias: -0.132630, T: 16643, Avg. loss: 0.356426\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 14.06, NNZs: 1280, Bias: -0.132305, T: 17622, Avg. loss: 0.355337\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 14.12, NNZs: 1280, Bias: -0.132015, T: 18601, Avg. loss: 0.354297\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 14.17, NNZs: 1280, Bias: -0.131739, T: 19580, Avg. loss: 0.353331\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 14.23, NNZs: 1280, Bias: -0.131464, T: 20559, Avg. loss: 0.352433\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 14.28, NNZs: 1280, Bias: -0.131208, T: 21538, Avg. loss: 0.351567\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 14.33, NNZs: 1280, Bias: -0.130965, T: 22517, Avg. loss: 0.350752\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 14.37, NNZs: 1280, Bias: -0.130737, T: 23496, Avg. loss: 0.349979\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 14.36, NNZs: 1280, Bias: -0.299443, T: 979, Avg. loss: 0.428900\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 15.07, NNZs: 1280, Bias: -0.335936, T: 1958, Avg. loss: 0.328350\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 15.49, NNZs: 1280, Bias: -0.354888, T: 2937, Avg. loss: 0.307929\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 15.79, NNZs: 1280, Bias: -0.367442, T: 3916, Avg. loss: 0.297423\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 16.03, NNZs: 1280, Bias: -0.376341, T: 4895, Avg. loss: 0.291638\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 16.22, NNZs: 1280, Bias: -0.383258, T: 5874, Avg. loss: 0.286941\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 16.39, NNZs: 1280, Bias: -0.389039, T: 6853, Avg. loss: 0.282584\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16.53, NNZs: 1280, Bias: -0.394026, T: 7832, Avg. loss: 0.278675\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 16.66, NNZs: 1280, Bias: -0.398333, T: 8811, Avg. loss: 0.276236\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 16.77, NNZs: 1280, Bias: -0.402099, T: 9790, Avg. loss: 0.273893\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 16.87, NNZs: 1280, Bias: -0.405454, T: 10769, Avg. loss: 0.272112\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 16.96, NNZs: 1280, Bias: -0.408489, T: 11748, Avg. loss: 0.269988\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 17.05, NNZs: 1280, Bias: -0.411258, T: 12727, Avg. loss: 0.268381\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 17.13, NNZs: 1280, Bias: -0.413786, T: 13706, Avg. loss: 0.266758\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 17.20, NNZs: 1280, Bias: -0.416135, T: 14685, Avg. loss: 0.265319\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 17.27, NNZs: 1280, Bias: -0.418289, T: 15664, Avg. loss: 0.264302\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 17.33, NNZs: 1280, Bias: -0.420303, T: 16643, Avg. loss: 0.263111\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 17.39, NNZs: 1280, Bias: -0.422185, T: 17622, Avg. loss: 0.262038\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 17.45, NNZs: 1280, Bias: -0.423960, T: 18601, Avg. loss: 0.260983\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17.50, NNZs: 1280, Bias: -0.425636, T: 19580, Avg. loss: 0.260036\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17.56, NNZs: 1280, Bias: -0.427217, T: 20559, Avg. loss: 0.259194\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 17.61, NNZs: 1280, Bias: -0.428715, T: 21538, Avg. loss: 0.258298\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 17.65, NNZs: 1280, Bias: -0.430138, T: 22517, Avg. loss: 0.257573\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 17.70, NNZs: 1280, Bias: -0.431499, T: 23496, Avg. loss: 0.256709\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 8.71, NNZs: 1280, Bias: -0.169000, T: 979, Avg. loss: 0.573348\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 9.50, NNZs: 1280, Bias: -0.185789, T: 1958, Avg. loss: 0.485111\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 9.97, NNZs: 1280, Bias: -0.195438, T: 2937, Avg. loss: 0.467111\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 10.29, NNZs: 1280, Bias: -0.202297, T: 3916, Avg. loss: 0.457025\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 10.54, NNZs: 1280, Bias: -0.207740, T: 4895, Avg. loss: 0.450461\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 10.74, NNZs: 1280, Bias: -0.212074, T: 5874, Avg. loss: 0.445203\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 10.91, NNZs: 1280, Bias: -0.215634, T: 6853, Avg. loss: 0.440947\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 11.05, NNZs: 1280, Bias: -0.218708, T: 7832, Avg. loss: 0.437559\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 11.18, NNZs: 1280, Bias: -0.221374, T: 8811, Avg. loss: 0.434667\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 11.29, NNZs: 1280, Bias: -0.223736, T: 9790, Avg. loss: 0.432184\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 11.39, NNZs: 1280, Bias: -0.225837, T: 10769, Avg. loss: 0.429826\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11.48, NNZs: 1280, Bias: -0.227742, T: 11748, Avg. loss: 0.427827\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11.57, NNZs: 1280, Bias: -0.229496, T: 12727, Avg. loss: 0.426225\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 11.65, NNZs: 1280, Bias: -0.231099, T: 13706, Avg. loss: 0.424553\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 11.72, NNZs: 1280, Bias: -0.232572, T: 14685, Avg. loss: 0.423003\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 11.79, NNZs: 1280, Bias: -0.233941, T: 15664, Avg. loss: 0.421654\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 11.85, NNZs: 1280, Bias: -0.235226, T: 16643, Avg. loss: 0.420367\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 11.91, NNZs: 1280, Bias: -0.236432, T: 17622, Avg. loss: 0.419221\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 11.96, NNZs: 1280, Bias: -0.237559, T: 18601, Avg. loss: 0.418114\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 12.02, NNZs: 1280, Bias: -0.238624, T: 19580, Avg. loss: 0.417059\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 12.07, NNZs: 1280, Bias: -0.239637, T: 20559, Avg. loss: 0.416134\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 12.11, NNZs: 1280, Bias: -0.240592, T: 21538, Avg. loss: 0.415190\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 12.16, NNZs: 1280, Bias: -0.241499, T: 22517, Avg. loss: 0.414273\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 12.20, NNZs: 1280, Bias: -0.242367, T: 23496, Avg. loss: 0.413519\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 12.24, NNZs: 1280, Bias: -0.243194, T: 24475, Avg. loss: 0.412711\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 25 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.68, NNZs: 1274, Bias: -0.131531, T: 979, Avg. loss: 0.495114\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 11.51, NNZs: 1274, Bias: -0.127171, T: 1958, Avg. loss: 0.411823\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 12.00, NNZs: 1274, Bias: -0.124917, T: 2937, Avg. loss: 0.395059\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 12.34, NNZs: 1274, Bias: -0.122778, T: 3916, Avg. loss: 0.385697\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 12.60, NNZs: 1274, Bias: -0.121424, T: 4895, Avg. loss: 0.379187\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 12.81, NNZs: 1274, Bias: -0.120213, T: 5874, Avg. loss: 0.374322\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 12.99, NNZs: 1274, Bias: -0.119215, T: 6853, Avg. loss: 0.370455\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 13.15, NNZs: 1274, Bias: -0.118338, T: 7832, Avg. loss: 0.367256\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 13.28, NNZs: 1274, Bias: -0.117603, T: 8811, Avg. loss: 0.364535\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13.40, NNZs: 1274, Bias: -0.116933, T: 9790, Avg. loss: 0.362176\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 13.51, NNZs: 1274, Bias: -0.116345, T: 10769, Avg. loss: 0.360088\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 13.61, NNZs: 1274, Bias: -0.115800, T: 11748, Avg. loss: 0.358240\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 13.70, NNZs: 1274, Bias: -0.115306, T: 12727, Avg. loss: 0.356565\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 13.78, NNZs: 1274, Bias: -0.114865, T: 13706, Avg. loss: 0.355041\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 13.86, NNZs: 1274, Bias: -0.114449, T: 14685, Avg. loss: 0.353652\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 13.93, NNZs: 1274, Bias: -0.114059, T: 15664, Avg. loss: 0.352370\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 14.00, NNZs: 1274, Bias: -0.113709, T: 16643, Avg. loss: 0.351176\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 14.06, NNZs: 1274, Bias: -0.113362, T: 17622, Avg. loss: 0.350080\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 14.12, NNZs: 1274, Bias: -0.113044, T: 18601, Avg. loss: 0.349039\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 14.18, NNZs: 1274, Bias: -0.112743, T: 19580, Avg. loss: 0.348069\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 14.23, NNZs: 1274, Bias: -0.112451, T: 20559, Avg. loss: 0.347162\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 14.28, NNZs: 1274, Bias: -0.112172, T: 21538, Avg. loss: 0.346294\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 14.33, NNZs: 1274, Bias: -0.111910, T: 22517, Avg. loss: 0.345475\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 14.38, NNZs: 1274, Bias: -0.111664, T: 23496, Avg. loss: 0.344699\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 13.41, NNZs: 1274, Bias: -0.297489, T: 979, Avg. loss: 0.425424\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 14.15, NNZs: 1274, Bias: -0.330375, T: 1958, Avg. loss: 0.330702\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 14.59, NNZs: 1274, Bias: -0.347254, T: 2937, Avg. loss: 0.311501\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 14.91, NNZs: 1274, Bias: -0.358368, T: 3916, Avg. loss: 0.301902\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 15.16, NNZs: 1274, Bias: -0.366364, T: 4895, Avg. loss: 0.295937\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 15.36, NNZs: 1274, Bias: -0.372597, T: 5874, Avg. loss: 0.291522\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 15.53, NNZs: 1274, Bias: -0.377809, T: 6853, Avg. loss: 0.287401\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 15.68, NNZs: 1274, Bias: -0.382334, T: 7832, Avg. loss: 0.283508\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 15.81, NNZs: 1274, Bias: -0.386261, T: 8811, Avg. loss: 0.280945\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 15.93, NNZs: 1274, Bias: -0.389696, T: 9790, Avg. loss: 0.278773\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 16.04, NNZs: 1274, Bias: -0.392744, T: 10769, Avg. loss: 0.277138\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 16.14, NNZs: 1274, Bias: -0.395522, T: 11748, Avg. loss: 0.274991\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 16.22, NNZs: 1274, Bias: -0.398062, T: 12727, Avg. loss: 0.273404\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 16.31, NNZs: 1274, Bias: -0.400387, T: 13706, Avg. loss: 0.271846\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 16.38, NNZs: 1274, Bias: -0.402543, T: 14685, Avg. loss: 0.270434\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 16.46, NNZs: 1274, Bias: -0.404528, T: 15664, Avg. loss: 0.269409\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 16.52, NNZs: 1274, Bias: -0.406391, T: 16643, Avg. loss: 0.268167\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 16.59, NNZs: 1274, Bias: -0.408136, T: 17622, Avg. loss: 0.267082\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 16.65, NNZs: 1274, Bias: -0.409774, T: 18601, Avg. loss: 0.266082\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 16.70, NNZs: 1274, Bias: -0.411320, T: 19580, Avg. loss: 0.265265\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 16.76, NNZs: 1274, Bias: -0.412789, T: 20559, Avg. loss: 0.264315\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16.81, NNZs: 1274, Bias: -0.414179, T: 21538, Avg. loss: 0.263469\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 16.86, NNZs: 1274, Bias: -0.415496, T: 22517, Avg. loss: 0.262798\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 16.91, NNZs: 1274, Bias: -0.416758, T: 23496, Avg. loss: 0.261897\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 8.65, NNZs: 1274, Bias: -0.173080, T: 979, Avg. loss: 0.567275\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 9.45, NNZs: 1274, Bias: -0.191023, T: 1958, Avg. loss: 0.480666\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 9.92, NNZs: 1274, Bias: -0.201596, T: 2937, Avg. loss: 0.462911\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 10.25, NNZs: 1274, Bias: -0.208804, T: 3916, Avg. loss: 0.452631\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 10.50, NNZs: 1274, Bias: -0.214587, T: 4895, Avg. loss: 0.446104\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 10.70, NNZs: 1274, Bias: -0.219127, T: 5874, Avg. loss: 0.440729\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 10.87, NNZs: 1274, Bias: -0.222901, T: 6853, Avg. loss: 0.436543\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 11.01, NNZs: 1274, Bias: -0.226124, T: 7832, Avg. loss: 0.433081\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 11.14, NNZs: 1274, Bias: -0.228955, T: 8811, Avg. loss: 0.430274\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 11.25, NNZs: 1274, Bias: -0.231434, T: 9790, Avg. loss: 0.427697\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 11.35, NNZs: 1274, Bias: -0.233642, T: 10769, Avg. loss: 0.425411\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11.45, NNZs: 1274, Bias: -0.235639, T: 11748, Avg. loss: 0.423401\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11.53, NNZs: 1274, Bias: -0.237492, T: 12727, Avg. loss: 0.421761\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 11.61, NNZs: 1274, Bias: -0.239183, T: 13706, Avg. loss: 0.420069\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 11.68, NNZs: 1274, Bias: -0.240739, T: 14685, Avg. loss: 0.418529\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 11.75, NNZs: 1274, Bias: -0.242183, T: 15664, Avg. loss: 0.417182\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 11.81, NNZs: 1274, Bias: -0.243541, T: 16643, Avg. loss: 0.415934\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 11.87, NNZs: 1274, Bias: -0.244809, T: 17622, Avg. loss: 0.414752\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 11.93, NNZs: 1274, Bias: -0.246002, T: 18601, Avg. loss: 0.413650\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 11.98, NNZs: 1274, Bias: -0.247129, T: 19580, Avg. loss: 0.412609\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 12.03, NNZs: 1274, Bias: -0.248196, T: 20559, Avg. loss: 0.411646\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 12.08, NNZs: 1274, Bias: -0.249206, T: 21538, Avg. loss: 0.410707\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 12.12, NNZs: 1274, Bias: -0.250165, T: 22517, Avg. loss: 0.409797\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 12.17, NNZs: 1274, Bias: -0.251080, T: 23496, Avg. loss: 0.409016\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 12.21, NNZs: 1274, Bias: -0.251954, T: 24475, Avg. loss: 0.408222\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 25 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.52, NNZs: 1291, Bias: -0.150656, T: 979, Avg. loss: 0.495490\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 11.35, NNZs: 1291, Bias: -0.143588, T: 1958, Avg. loss: 0.414213\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 11.83, NNZs: 1291, Bias: -0.139878, T: 2937, Avg. loss: 0.397717\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 12.17, NNZs: 1291, Bias: -0.137080, T: 3916, Avg. loss: 0.388447\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 12.43, NNZs: 1291, Bias: -0.135121, T: 4895, Avg. loss: 0.382023\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 12.64, NNZs: 1291, Bias: -0.133500, T: 5874, Avg. loss: 0.377216\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 12.82, NNZs: 1291, Bias: -0.132143, T: 6853, Avg. loss: 0.373387\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 12.97, NNZs: 1291, Bias: -0.130992, T: 7832, Avg. loss: 0.370205\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 13.11, NNZs: 1291, Bias: -0.129999, T: 8811, Avg. loss: 0.367517\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13.23, NNZs: 1291, Bias: -0.129122, T: 9790, Avg. loss: 0.365176\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 13.33, NNZs: 1291, Bias: -0.128349, T: 10769, Avg. loss: 0.363105\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 13.43, NNZs: 1291, Bias: -0.127648, T: 11748, Avg. loss: 0.361261\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 13.52, NNZs: 1291, Bias: -0.127010, T: 12727, Avg. loss: 0.359598\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 13.61, NNZs: 1291, Bias: -0.126430, T: 13706, Avg. loss: 0.358089\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 13.68, NNZs: 1291, Bias: -0.125896, T: 14685, Avg. loss: 0.356708\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 13.76, NNZs: 1291, Bias: -0.125400, T: 15664, Avg. loss: 0.355429\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 13.82, NNZs: 1291, Bias: -0.124946, T: 16643, Avg. loss: 0.354243\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 13.89, NNZs: 1291, Bias: -0.124508, T: 17622, Avg. loss: 0.353151\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 13.95, NNZs: 1291, Bias: -0.124105, T: 18601, Avg. loss: 0.352116\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 14.00, NNZs: 1291, Bias: -0.123721, T: 19580, Avg. loss: 0.351151\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 14.06, NNZs: 1291, Bias: -0.123355, T: 20559, Avg. loss: 0.350245\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 14.11, NNZs: 1291, Bias: -0.123004, T: 21538, Avg. loss: 0.349383\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 14.16, NNZs: 1291, Bias: -0.122671, T: 22517, Avg. loss: 0.348570\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 14.20, NNZs: 1291, Bias: -0.122359, T: 23496, Avg. loss: 0.347794\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 13.03, NNZs: 1291, Bias: -0.313396, T: 979, Avg. loss: 0.429914\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 13.74, NNZs: 1291, Bias: -0.347455, T: 1958, Avg. loss: 0.340498\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 14.17, NNZs: 1291, Bias: -0.365447, T: 2937, Avg. loss: 0.320651\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 14.48, NNZs: 1291, Bias: -0.377095, T: 3916, Avg. loss: 0.311916\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 14.73, NNZs: 1291, Bias: -0.385632, T: 4895, Avg. loss: 0.305471\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 14.93, NNZs: 1291, Bias: -0.392237, T: 5874, Avg. loss: 0.301591\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 15.10, NNZs: 1291, Bias: -0.397770, T: 6853, Avg. loss: 0.297171\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 15.25, NNZs: 1291, Bias: -0.402573, T: 7832, Avg. loss: 0.293553\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 15.38, NNZs: 1291, Bias: -0.406724, T: 8811, Avg. loss: 0.290910\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 15.50, NNZs: 1291, Bias: -0.410377, T: 9790, Avg. loss: 0.288651\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 15.61, NNZs: 1291, Bias: -0.413643, T: 10769, Avg. loss: 0.286712\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 15.70, NNZs: 1291, Bias: -0.416609, T: 11748, Avg. loss: 0.284653\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 15.79, NNZs: 1291, Bias: -0.419324, T: 12727, Avg. loss: 0.283016\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 15.88, NNZs: 1291, Bias: -0.421805, T: 13706, Avg. loss: 0.281551\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 15.95, NNZs: 1291, Bias: -0.424105, T: 14685, Avg. loss: 0.280138\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 16.03, NNZs: 1291, Bias: -0.426229, T: 15664, Avg. loss: 0.279103\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 16.09, NNZs: 1291, Bias: -0.428215, T: 16643, Avg. loss: 0.277880\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 16.16, NNZs: 1291, Bias: -0.430070, T: 17622, Avg. loss: 0.276853\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 16.22, NNZs: 1291, Bias: -0.431822, T: 18601, Avg. loss: 0.275737\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 16.27, NNZs: 1291, Bias: -0.433476, T: 19580, Avg. loss: 0.274950\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 16.33, NNZs: 1291, Bias: -0.435049, T: 20559, Avg. loss: 0.273923\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16.38, NNZs: 1291, Bias: -0.436532, T: 21538, Avg. loss: 0.273133\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 16.43, NNZs: 1291, Bias: -0.437942, T: 22517, Avg. loss: 0.272457\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 16.48, NNZs: 1291, Bias: -0.439293, T: 23496, Avg. loss: 0.271550\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 16.52, NNZs: 1291, Bias: -0.440584, T: 24475, Avg. loss: 0.270902\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 16.57, NNZs: 1291, Bias: -0.441818, T: 25454, Avg. loss: 0.270241\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 26 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 8.54, NNZs: 1291, Bias: -0.162975, T: 979, Avg. loss: 0.571716\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 9.34, NNZs: 1291, Bias: -0.181126, T: 1958, Avg. loss: 0.485256\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 9.80, NNZs: 1291, Bias: -0.191509, T: 2937, Avg. loss: 0.467443\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 10.13, NNZs: 1291, Bias: -0.198579, T: 3916, Avg. loss: 0.457358\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 10.38, NNZs: 1291, Bias: -0.204190, T: 4895, Avg. loss: 0.450940\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 10.58, NNZs: 1291, Bias: -0.208603, T: 5874, Avg. loss: 0.445634\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 10.74, NNZs: 1291, Bias: -0.212295, T: 6853, Avg. loss: 0.441595\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 10.89, NNZs: 1291, Bias: -0.215434, T: 7832, Avg. loss: 0.438184\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 11.01, NNZs: 1291, Bias: -0.218172, T: 8811, Avg. loss: 0.435357\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 11.13, NNZs: 1291, Bias: -0.220596, T: 9790, Avg. loss: 0.432920\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 11.23, NNZs: 1291, Bias: -0.222755, T: 10769, Avg. loss: 0.430686\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11.32, NNZs: 1291, Bias: -0.224696, T: 11748, Avg. loss: 0.428649\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11.40, NNZs: 1291, Bias: -0.226502, T: 12727, Avg. loss: 0.427085\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 11.48, NNZs: 1291, Bias: -0.228140, T: 13706, Avg. loss: 0.425393\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 11.55, NNZs: 1291, Bias: -0.229655, T: 14685, Avg. loss: 0.423904\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 11.62, NNZs: 1291, Bias: -0.231060, T: 15664, Avg. loss: 0.422588\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 11.68, NNZs: 1291, Bias: -0.232381, T: 16643, Avg. loss: 0.421365\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 11.74, NNZs: 1291, Bias: -0.233617, T: 17622, Avg. loss: 0.420200\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 11.80, NNZs: 1291, Bias: -0.234778, T: 18601, Avg. loss: 0.419128\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 11.85, NNZs: 1291, Bias: -0.235873, T: 19580, Avg. loss: 0.418103\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 11.90, NNZs: 1291, Bias: -0.236913, T: 20559, Avg. loss: 0.417159\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 11.95, NNZs: 1291, Bias: -0.237899, T: 21538, Avg. loss: 0.416259\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm: 11.99, NNZs: 1291, Bias: -0.238830, T: 22517, Avg. loss: 0.415330\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 12.04, NNZs: 1291, Bias: -0.239721, T: 23496, Avg. loss: 0.414585\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 12.08, NNZs: 1291, Bias: -0.240571, T: 24475, Avg. loss: 0.413803\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 25 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.67, NNZs: 1328, Bias: -0.160935, T: 979, Avg. loss: 0.501651\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 11.48, NNZs: 1328, Bias: -0.156235, T: 1958, Avg. loss: 0.419918\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 11.95, NNZs: 1328, Bias: -0.153681, T: 2937, Avg. loss: 0.403475\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 12.29, NNZs: 1328, Bias: -0.151378, T: 3916, Avg. loss: 0.394285\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 12.55, NNZs: 1328, Bias: -0.149693, T: 4895, Avg. loss: 0.387888\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 12.76, NNZs: 1328, Bias: -0.148353, T: 5874, Avg. loss: 0.383067\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 12.94, NNZs: 1328, Bias: -0.147233, T: 6853, Avg. loss: 0.379235\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 13.09, NNZs: 1328, Bias: -0.146228, T: 7832, Avg. loss: 0.376062\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 13.22, NNZs: 1328, Bias: -0.145355, T: 8811, Avg. loss: 0.373363\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13.34, NNZs: 1328, Bias: -0.144595, T: 9790, Avg. loss: 0.371004\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 13.45, NNZs: 1328, Bias: -0.143918, T: 10769, Avg. loss: 0.368929\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 13.55, NNZs: 1328, Bias: -0.143309, T: 11748, Avg. loss: 0.367071\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 13.64, NNZs: 1328, Bias: -0.142741, T: 12727, Avg. loss: 0.365404\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 13.72, NNZs: 1328, Bias: -0.142235, T: 13706, Avg. loss: 0.363881\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 13.80, NNZs: 1328, Bias: -0.141755, T: 14685, Avg. loss: 0.362493\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 13.87, NNZs: 1328, Bias: -0.141314, T: 15664, Avg. loss: 0.361205\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 13.94, NNZs: 1328, Bias: -0.140908, T: 16643, Avg. loss: 0.360010\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 14.00, NNZs: 1328, Bias: -0.140515, T: 17622, Avg. loss: 0.358908\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 14.06, NNZs: 1328, Bias: -0.140154, T: 18601, Avg. loss: 0.357864\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 14.12, NNZs: 1328, Bias: -0.139809, T: 19580, Avg. loss: 0.356890\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 14.17, NNZs: 1328, Bias: -0.139475, T: 20559, Avg. loss: 0.355978\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 14.22, NNZs: 1328, Bias: -0.139159, T: 21538, Avg. loss: 0.355104\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 14.27, NNZs: 1328, Bias: -0.138857, T: 22517, Avg. loss: 0.354284\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 14.32, NNZs: 1328, Bias: -0.138573, T: 23496, Avg. loss: 0.353501\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 13.07, NNZs: 1328, Bias: -0.322462, T: 979, Avg. loss: 0.430773\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 13.78, NNZs: 1328, Bias: -0.358842, T: 1958, Avg. loss: 0.341046\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 14.22, NNZs: 1328, Bias: -0.377842, T: 2937, Avg. loss: 0.321498\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 14.53, NNZs: 1328, Bias: -0.390356, T: 3916, Avg. loss: 0.312248\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 14.77, NNZs: 1328, Bias: -0.399523, T: 4895, Avg. loss: 0.306294\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 14.97, NNZs: 1328, Bias: -0.406889, T: 5874, Avg. loss: 0.301445\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 15.14, NNZs: 1328, Bias: -0.412920, T: 6853, Avg. loss: 0.297399\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 15.29, NNZs: 1328, Bias: -0.418113, T: 7832, Avg. loss: 0.294104\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 15.42, NNZs: 1328, Bias: -0.422650, T: 8811, Avg. loss: 0.291060\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 15.54, NNZs: 1328, Bias: -0.426673, T: 9790, Avg. loss: 0.288590\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 15.64, NNZs: 1328, Bias: -0.430251, T: 10769, Avg. loss: 0.286713\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 15.74, NNZs: 1328, Bias: -0.433506, T: 11748, Avg. loss: 0.284608\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 15.83, NNZs: 1328, Bias: -0.436479, T: 12727, Avg. loss: 0.282913\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 15.91, NNZs: 1328, Bias: -0.439209, T: 13706, Avg. loss: 0.281495\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 15.98, NNZs: 1328, Bias: -0.441729, T: 14685, Avg. loss: 0.280010\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 16.06, NNZs: 1328, Bias: -0.444060, T: 15664, Avg. loss: 0.279073\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 16.12, NNZs: 1328, Bias: -0.446238, T: 16643, Avg. loss: 0.277865\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 16.19, NNZs: 1328, Bias: -0.448277, T: 17622, Avg. loss: 0.276724\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 16.25, NNZs: 1328, Bias: -0.450195, T: 18601, Avg. loss: 0.275654\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 16.30, NNZs: 1328, Bias: -0.452006, T: 19580, Avg. loss: 0.274865\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 16.36, NNZs: 1328, Bias: -0.453732, T: 20559, Avg. loss: 0.273807\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16.41, NNZs: 1328, Bias: -0.455366, T: 21538, Avg. loss: 0.272921\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 16.46, NNZs: 1328, Bias: -0.456915, T: 22517, Avg. loss: 0.272291\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 16.50, NNZs: 1328, Bias: -0.458395, T: 23496, Avg. loss: 0.271468\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 16.55, NNZs: 1328, Bias: -0.459808, T: 24475, Avg. loss: 0.270815\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 16.59, NNZs: 1328, Bias: -0.461165, T: 25454, Avg. loss: 0.270061\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 26 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 8.54, NNZs: 1328, Bias: -0.156029, T: 979, Avg. loss: 0.577491\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 9.34, NNZs: 1328, Bias: -0.172571, T: 1958, Avg. loss: 0.491528\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 9.80, NNZs: 1328, Bias: -0.182311, T: 2937, Avg. loss: 0.473883\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 10.13, NNZs: 1328, Bias: -0.189015, T: 3916, Avg. loss: 0.463928\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 10.38, NNZs: 1328, Bias: -0.194297, T: 4895, Avg. loss: 0.457490\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 10.58, NNZs: 1328, Bias: -0.198435, T: 5874, Avg. loss: 0.452255\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 10.75, NNZs: 1328, Bias: -0.201947, T: 6853, Avg. loss: 0.448294\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 10.89, NNZs: 1328, Bias: -0.204952, T: 7832, Avg. loss: 0.444993\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 11.02, NNZs: 1328, Bias: -0.207562, T: 8811, Avg. loss: 0.442116\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 11.13, NNZs: 1328, Bias: -0.209857, T: 9790, Avg. loss: 0.439641\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 11.23, NNZs: 1328, Bias: -0.211899, T: 10769, Avg. loss: 0.437397\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11.33, NNZs: 1328, Bias: -0.213733, T: 11748, Avg. loss: 0.435382\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11.41, NNZs: 1328, Bias: -0.215436, T: 12727, Avg. loss: 0.433793\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 11.49, NNZs: 1328, Bias: -0.216992, T: 13706, Avg. loss: 0.432151\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 11.56, NNZs: 1328, Bias: -0.218437, T: 14685, Avg. loss: 0.430711\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 11.63, NNZs: 1328, Bias: -0.219771, T: 15664, Avg. loss: 0.429341\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 11.69, NNZs: 1328, Bias: -0.221026, T: 16643, Avg. loss: 0.428125\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 11.75, NNZs: 1328, Bias: -0.222194, T: 17622, Avg. loss: 0.426945\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 11.81, NNZs: 1328, Bias: -0.223295, T: 18601, Avg. loss: 0.425871\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 11.86, NNZs: 1328, Bias: -0.224334, T: 19580, Avg. loss: 0.424847\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 11.91, NNZs: 1328, Bias: -0.225318, T: 20559, Avg. loss: 0.423900\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 11.96, NNZs: 1328, Bias: -0.226250, T: 21538, Avg. loss: 0.422992\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 12.00, NNZs: 1328, Bias: -0.227132, T: 22517, Avg. loss: 0.422082\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 12.05, NNZs: 1328, Bias: -0.227977, T: 23496, Avg. loss: 0.421336\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 12.09, NNZs: 1328, Bias: -0.228783, T: 24475, Avg. loss: 0.420555\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 25 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.63, NNZs: 1329, Bias: -0.162834, T: 979, Avg. loss: 0.505018\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 11.43, NNZs: 1329, Bias: -0.159321, T: 1958, Avg. loss: 0.422026\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 11.90, NNZs: 1329, Bias: -0.157271, T: 2937, Avg. loss: 0.405381\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 12.23, NNZs: 1329, Bias: -0.155414, T: 3916, Avg. loss: 0.396034\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 12.49, NNZs: 1329, Bias: -0.153961, T: 4895, Avg. loss: 0.389579\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 12.70, NNZs: 1329, Bias: -0.152867, T: 5874, Avg. loss: 0.384676\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 12.88, NNZs: 1329, Bias: -0.151889, T: 6853, Avg. loss: 0.380810\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 13.03, NNZs: 1329, Bias: -0.151022, T: 7832, Avg. loss: 0.377588\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 13.16, NNZs: 1329, Bias: -0.150270, T: 8811, Avg. loss: 0.374849\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13.28, NNZs: 1329, Bias: -0.149609, T: 9790, Avg. loss: 0.372466\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 13.39, NNZs: 1329, Bias: -0.149021, T: 10769, Avg. loss: 0.370371\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 13.48, NNZs: 1329, Bias: -0.148499, T: 11748, Avg. loss: 0.368489\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 13.57, NNZs: 1329, Bias: -0.148011, T: 12727, Avg. loss: 0.366804\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 13.65, NNZs: 1329, Bias: -0.147561, T: 13706, Avg. loss: 0.365270\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 13.73, NNZs: 1329, Bias: -0.147150, T: 14685, Avg. loss: 0.363862\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 13.80, NNZs: 1329, Bias: -0.146770, T: 15664, Avg. loss: 0.362559\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 13.87, NNZs: 1329, Bias: -0.146415, T: 16643, Avg. loss: 0.361354\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 13.93, NNZs: 1329, Bias: -0.146074, T: 17622, Avg. loss: 0.360239\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 13.99, NNZs: 1329, Bias: -0.145757, T: 18601, Avg. loss: 0.359186\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 14.05, NNZs: 1329, Bias: -0.145449, T: 19580, Avg. loss: 0.358205\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 14.10, NNZs: 1329, Bias: -0.145162, T: 20559, Avg. loss: 0.357275\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 14.15, NNZs: 1329, Bias: -0.144884, T: 21538, Avg. loss: 0.356398\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 14.20, NNZs: 1329, Bias: -0.144623, T: 22517, Avg. loss: 0.355567\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 14.25, NNZs: 1329, Bias: -0.144374, T: 23496, Avg. loss: 0.354777\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 13.01, NNZs: 1329, Bias: -0.321037, T: 979, Avg. loss: 0.416598\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 13.75, NNZs: 1329, Bias: -0.350951, T: 1958, Avg. loss: 0.339625\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 14.19, NNZs: 1329, Bias: -0.368480, T: 2937, Avg. loss: 0.317571\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 14.51, NNZs: 1329, Bias: -0.379862, T: 3916, Avg. loss: 0.308921\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 14.76, NNZs: 1329, Bias: -0.388089, T: 4895, Avg. loss: 0.303243\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 14.97, NNZs: 1329, Bias: -0.394846, T: 5874, Avg. loss: 0.298112\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 15.14, NNZs: 1329, Bias: -0.400326, T: 6853, Avg. loss: 0.294396\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 15.29, NNZs: 1329, Bias: -0.405101, T: 7832, Avg. loss: 0.290950\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 15.42, NNZs: 1329, Bias: -0.409252, T: 8811, Avg. loss: 0.287967\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 15.54, NNZs: 1329, Bias: -0.412917, T: 9790, Avg. loss: 0.285652\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 15.65, NNZs: 1329, Bias: -0.416209, T: 10769, Avg. loss: 0.283600\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 15.75, NNZs: 1329, Bias: -0.419214, T: 11748, Avg. loss: 0.281463\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 15.84, NNZs: 1329, Bias: -0.421957, T: 12727, Avg. loss: 0.279749\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 15.92, NNZs: 1329, Bias: -0.424474, T: 13706, Avg. loss: 0.278299\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 16.00, NNZs: 1329, Bias: -0.426792, T: 14685, Avg. loss: 0.276985\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 16.07, NNZs: 1329, Bias: -0.428944, T: 15664, Avg. loss: 0.275907\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 16.14, NNZs: 1329, Bias: -0.430959, T: 16643, Avg. loss: 0.274645\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 16.21, NNZs: 1329, Bias: -0.432839, T: 17622, Avg. loss: 0.273559\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 16.27, NNZs: 1329, Bias: -0.434612, T: 18601, Avg. loss: 0.272461\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 16.33, NNZs: 1329, Bias: -0.436287, T: 19580, Avg. loss: 0.271524\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 16.38, NNZs: 1329, Bias: -0.437883, T: 20559, Avg. loss: 0.270584\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16.43, NNZs: 1329, Bias: -0.439387, T: 21538, Avg. loss: 0.269794\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 16.48, NNZs: 1329, Bias: -0.440818, T: 22517, Avg. loss: 0.269109\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 16.53, NNZs: 1329, Bias: -0.442181, T: 23496, Avg. loss: 0.268310\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 8.47, NNZs: 1329, Bias: -0.148226, T: 979, Avg. loss: 0.577238\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 9.28, NNZs: 1329, Bias: -0.166216, T: 1958, Avg. loss: 0.492673\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 9.74, NNZs: 1329, Bias: -0.175972, T: 2937, Avg. loss: 0.474453\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 10.07, NNZs: 1329, Bias: -0.182900, T: 3916, Avg. loss: 0.464913\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 10.32, NNZs: 1329, Bias: -0.188445, T: 4895, Avg. loss: 0.458688\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 10.52, NNZs: 1329, Bias: -0.192664, T: 5874, Avg. loss: 0.453315\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 10.68, NNZs: 1329, Bias: -0.196293, T: 6853, Avg. loss: 0.449528\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 10.83, NNZs: 1329, Bias: -0.199398, T: 7832, Avg. loss: 0.446277\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 10.96, NNZs: 1329, Bias: -0.202084, T: 8811, Avg. loss: 0.443393\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 11.07, NNZs: 1329, Bias: -0.204459, T: 9790, Avg. loss: 0.440977\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 11.17, NNZs: 1329, Bias: -0.206584, T: 10769, Avg. loss: 0.438792\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11.26, NNZs: 1329, Bias: -0.208467, T: 11748, Avg. loss: 0.436712\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11.35, NNZs: 1329, Bias: -0.210217, T: 12727, Avg. loss: 0.435151\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 11.43, NNZs: 1329, Bias: -0.211813, T: 13706, Avg. loss: 0.433510\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 11.50, NNZs: 1329, Bias: -0.213299, T: 14685, Avg. loss: 0.432129\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 11.57, NNZs: 1329, Bias: -0.214679, T: 15664, Avg. loss: 0.430817\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 11.63, NNZs: 1329, Bias: -0.215969, T: 16643, Avg. loss: 0.429582\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 11.69, NNZs: 1329, Bias: -0.217171, T: 17622, Avg. loss: 0.428414\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 11.74, NNZs: 1329, Bias: -0.218301, T: 18601, Avg. loss: 0.427338\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 11.80, NNZs: 1329, Bias: -0.219368, T: 19580, Avg. loss: 0.426341\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 11.85, NNZs: 1329, Bias: -0.220383, T: 20559, Avg. loss: 0.425436\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 11.89, NNZs: 1329, Bias: -0.221340, T: 21538, Avg. loss: 0.424497\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 11.94, NNZs: 1329, Bias: -0.222249, T: 22517, Avg. loss: 0.423619\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 11.98, NNZs: 1329, Bias: -0.223116, T: 23496, Avg. loss: 0.422867\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 24 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.75, NNZs: 1327, Bias: -0.161596, T: 979, Avg. loss: 0.503704\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 11.55, NNZs: 1327, Bias: -0.159212, T: 1958, Avg. loss: 0.419883\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 12.02, NNZs: 1327, Bias: -0.157775, T: 2937, Avg. loss: 0.403343\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 12.35, NNZs: 1327, Bias: -0.156050, T: 3916, Avg. loss: 0.394132\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 12.61, NNZs: 1327, Bias: -0.154871, T: 4895, Avg. loss: 0.387760\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 12.82, NNZs: 1327, Bias: -0.153884, T: 5874, Avg. loss: 0.382961\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 12.99, NNZs: 1327, Bias: -0.153003, T: 6853, Avg. loss: 0.379150\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 13.14, NNZs: 1327, Bias: -0.152195, T: 7832, Avg. loss: 0.375983\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 13.28, NNZs: 1327, Bias: -0.151468, T: 8811, Avg. loss: 0.373298\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13.39, NNZs: 1327, Bias: -0.150845, T: 9790, Avg. loss: 0.370950\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 13.50, NNZs: 1327, Bias: -0.150294, T: 10769, Avg. loss: 0.368893\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 13.60, NNZs: 1327, Bias: -0.149796, T: 11748, Avg. loss: 0.367042\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 13.69, NNZs: 1327, Bias: -0.149325, T: 12727, Avg. loss: 0.365386\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 13.77, NNZs: 1327, Bias: -0.148891, T: 13706, Avg. loss: 0.363878\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 13.84, NNZs: 1327, Bias: -0.148488, T: 14685, Avg. loss: 0.362497\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 13.91, NNZs: 1327, Bias: -0.148125, T: 15664, Avg. loss: 0.361214\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 13.98, NNZs: 1327, Bias: -0.147780, T: 16643, Avg. loss: 0.360031\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 14.04, NNZs: 1327, Bias: -0.147443, T: 17622, Avg. loss: 0.358936\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 14.10, NNZs: 1327, Bias: -0.147133, T: 18601, Avg. loss: 0.357900\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 14.16, NNZs: 1327, Bias: -0.146836, T: 19580, Avg. loss: 0.356936\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 14.21, NNZs: 1327, Bias: -0.146556, T: 20559, Avg. loss: 0.356023\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 14.26, NNZs: 1327, Bias: -0.146285, T: 21538, Avg. loss: 0.355160\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 14.31, NNZs: 1327, Bias: -0.146027, T: 22517, Avg. loss: 0.354348\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 14.35, NNZs: 1327, Bias: -0.145784, T: 23496, Avg. loss: 0.353571\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 12.96, NNZs: 1327, Bias: -0.329125, T: 979, Avg. loss: 0.412268\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 13.65, NNZs: 1327, Bias: -0.356116, T: 1958, Avg. loss: 0.338244\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 14.07, NNZs: 1327, Bias: -0.371912, T: 2937, Avg. loss: 0.317985\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 14.37, NNZs: 1327, Bias: -0.382433, T: 3916, Avg. loss: 0.309190\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 14.62, NNZs: 1327, Bias: -0.390014, T: 4895, Avg. loss: 0.304094\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 14.81, NNZs: 1327, Bias: -0.396331, T: 5874, Avg. loss: 0.298951\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 14.98, NNZs: 1327, Bias: -0.401400, T: 6853, Avg. loss: 0.295721\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 15.13, NNZs: 1327, Bias: -0.405866, T: 7832, Avg. loss: 0.292052\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 15.26, NNZs: 1327, Bias: -0.409792, T: 8811, Avg. loss: 0.289116\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 15.37, NNZs: 1327, Bias: -0.413221, T: 9790, Avg. loss: 0.287145\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 15.48, NNZs: 1327, Bias: -0.416322, T: 10769, Avg. loss: 0.285075\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 15.57, NNZs: 1327, Bias: -0.419155, T: 11748, Avg. loss: 0.283142\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 15.66, NNZs: 1327, Bias: -0.421769, T: 12727, Avg. loss: 0.281250\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 15.74, NNZs: 1327, Bias: -0.424131, T: 13706, Avg. loss: 0.280190\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 15.82, NNZs: 1327, Bias: -0.426331, T: 14685, Avg. loss: 0.278688\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 15.89, NNZs: 1327, Bias: -0.428376, T: 15664, Avg. loss: 0.277656\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 15.96, NNZs: 1327, Bias: -0.430286, T: 16643, Avg. loss: 0.276551\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 16.02, NNZs: 1327, Bias: -0.432075, T: 17622, Avg. loss: 0.275448\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 16.08, NNZs: 1327, Bias: -0.433761, T: 18601, Avg. loss: 0.274389\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 16.14, NNZs: 1327, Bias: -0.435348, T: 19580, Avg. loss: 0.273637\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 16.19, NNZs: 1327, Bias: -0.436871, T: 20559, Avg. loss: 0.272550\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16.24, NNZs: 1327, Bias: -0.438305, T: 21538, Avg. loss: 0.271818\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 16.29, NNZs: 1327, Bias: -0.439673, T: 22517, Avg. loss: 0.271122\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 16.34, NNZs: 1327, Bias: -0.440979, T: 23496, Avg. loss: 0.270303\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 16.39, NNZs: 1327, Bias: -0.442227, T: 24475, Avg. loss: 0.269672\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 16.43, NNZs: 1327, Bias: -0.443429, T: 25454, Avg. loss: 0.268903\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 26 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 8.54, NNZs: 1327, Bias: -0.144879, T: 979, Avg. loss: 0.565970\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 9.35, NNZs: 1327, Bias: -0.165328, T: 1958, Avg. loss: 0.485290\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 9.82, NNZs: 1327, Bias: -0.176298, T: 2937, Avg. loss: 0.467672\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 10.15, NNZs: 1327, Bias: -0.183917, T: 3916, Avg. loss: 0.458366\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 10.40, NNZs: 1327, Bias: -0.189877, T: 4895, Avg. loss: 0.452249\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 10.60, NNZs: 1327, Bias: -0.194446, T: 5874, Avg. loss: 0.447063\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 10.77, NNZs: 1327, Bias: -0.198308, T: 6853, Avg. loss: 0.443192\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 10.92, NNZs: 1327, Bias: -0.201637, T: 7832, Avg. loss: 0.440106\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 11.04, NNZs: 1327, Bias: -0.204502, T: 8811, Avg. loss: 0.437270\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 11.16, NNZs: 1327, Bias: -0.207036, T: 9790, Avg. loss: 0.434935\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 11.26, NNZs: 1327, Bias: -0.209312, T: 10769, Avg. loss: 0.432851\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11.35, NNZs: 1327, Bias: -0.211327, T: 11748, Avg. loss: 0.430806\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11.44, NNZs: 1327, Bias: -0.213192, T: 12727, Avg. loss: 0.429278\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 11.51, NNZs: 1327, Bias: -0.214887, T: 13706, Avg. loss: 0.427647\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 11.59, NNZs: 1327, Bias: -0.216467, T: 14685, Avg. loss: 0.426294\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 11.65, NNZs: 1327, Bias: -0.217928, T: 15664, Avg. loss: 0.424992\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 11.72, NNZs: 1327, Bias: -0.219299, T: 16643, Avg. loss: 0.423834\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 11.78, NNZs: 1327, Bias: -0.220576, T: 17622, Avg. loss: 0.422658\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 11.83, NNZs: 1327, Bias: -0.221778, T: 18601, Avg. loss: 0.421615\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 11.89, NNZs: 1327, Bias: -0.222909, T: 19580, Avg. loss: 0.420609\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 11.94, NNZs: 1327, Bias: -0.223987, T: 20559, Avg. loss: 0.419750\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 11.98, NNZs: 1327, Bias: -0.225000, T: 21538, Avg. loss: 0.418815\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 12.03, NNZs: 1327, Bias: -0.225966, T: 22517, Avg. loss: 0.417973\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 12.07, NNZs: 1327, Bias: -0.226884, T: 23496, Avg. loss: 0.417212\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 12.11, NNZs: 1327, Bias: -0.227760, T: 24475, Avg. loss: 0.416456\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 25 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.88, NNZs: 1343, Bias: -0.163199, T: 979, Avg. loss: 0.505630\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 11.69, NNZs: 1343, Bias: -0.162029, T: 1958, Avg. loss: 0.421758\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 12.16, NNZs: 1343, Bias: -0.161044, T: 2937, Avg. loss: 0.405494\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 12.49, NNZs: 1343, Bias: -0.159753, T: 3916, Avg. loss: 0.396382\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 12.75, NNZs: 1343, Bias: -0.158738, T: 4895, Avg. loss: 0.390101\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 12.95, NNZs: 1343, Bias: -0.157894, T: 5874, Avg. loss: 0.385338\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 13.13, NNZs: 1343, Bias: -0.157112, T: 6853, Avg. loss: 0.381552\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 13.28, NNZs: 1343, Bias: -0.156423, T: 7832, Avg. loss: 0.378396\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 13.41, NNZs: 1343, Bias: -0.155791, T: 8811, Avg. loss: 0.375729\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13.53, NNZs: 1343, Bias: -0.155225, T: 9790, Avg. loss: 0.373393\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 13.63, NNZs: 1343, Bias: -0.154744, T: 10769, Avg. loss: 0.371335\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 13.73, NNZs: 1343, Bias: -0.154302, T: 11748, Avg. loss: 0.369494\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 13.82, NNZs: 1343, Bias: -0.153885, T: 12727, Avg. loss: 0.367842\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 13.90, NNZs: 1343, Bias: -0.153499, T: 13706, Avg. loss: 0.366335\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 13.98, NNZs: 1343, Bias: -0.153138, T: 14685, Avg. loss: 0.364955\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 14.05, NNZs: 1343, Bias: -0.152811, T: 15664, Avg. loss: 0.363675\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 14.12, NNZs: 1343, Bias: -0.152506, T: 16643, Avg. loss: 0.362492\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 14.18, NNZs: 1343, Bias: -0.152204, T: 17622, Avg. loss: 0.361396\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 14.24, NNZs: 1343, Bias: -0.151928, T: 18601, Avg. loss: 0.360361\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 14.29, NNZs: 1343, Bias: -0.151660, T: 19580, Avg. loss: 0.359398\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 14.35, NNZs: 1343, Bias: -0.151413, T: 20559, Avg. loss: 0.358483\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 14.40, NNZs: 1343, Bias: -0.151171, T: 21538, Avg. loss: 0.357622\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 14.45, NNZs: 1343, Bias: -0.150937, T: 22517, Avg. loss: 0.356809\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 14.49, NNZs: 1343, Bias: -0.150719, T: 23496, Avg. loss: 0.356030\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 12.58, NNZs: 1343, Bias: -0.336493, T: 979, Avg. loss: 0.415343\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 13.30, NNZs: 1343, Bias: -0.363633, T: 1958, Avg. loss: 0.345542\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 13.74, NNZs: 1343, Bias: -0.380299, T: 2937, Avg. loss: 0.323083\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 14.06, NNZs: 1343, Bias: -0.391085, T: 3916, Avg. loss: 0.314281\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 14.31, NNZs: 1343, Bias: -0.399044, T: 4895, Avg. loss: 0.308763\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 14.51, NNZs: 1343, Bias: -0.405659, T: 5874, Avg. loss: 0.303342\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 14.69, NNZs: 1343, Bias: -0.410957, T: 6853, Avg. loss: 0.299950\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 14.84, NNZs: 1343, Bias: -0.415575, T: 7832, Avg. loss: 0.296532\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 14.98, NNZs: 1343, Bias: -0.419667, T: 8811, Avg. loss: 0.293318\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 15.10, NNZs: 1343, Bias: -0.423252, T: 9790, Avg. loss: 0.291195\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 15.20, NNZs: 1343, Bias: -0.426476, T: 10769, Avg. loss: 0.289233\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 15.30, NNZs: 1343, Bias: -0.429427, T: 11748, Avg. loss: 0.287200\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 15.40, NNZs: 1343, Bias: -0.432137, T: 12727, Avg. loss: 0.285272\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 15.48, NNZs: 1343, Bias: -0.434600, T: 13706, Avg. loss: 0.284031\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 15.56, NNZs: 1343, Bias: -0.436885, T: 14685, Avg. loss: 0.282581\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 15.63, NNZs: 1343, Bias: -0.439013, T: 15664, Avg. loss: 0.281456\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 15.70, NNZs: 1343, Bias: -0.440997, T: 16643, Avg. loss: 0.280425\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 15.77, NNZs: 1343, Bias: -0.442864, T: 17622, Avg. loss: 0.279192\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 15.83, NNZs: 1343, Bias: -0.444623, T: 18601, Avg. loss: 0.278078\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 15.89, NNZs: 1343, Bias: -0.446276, T: 19580, Avg. loss: 0.277298\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 15.94, NNZs: 1343, Bias: -0.447860, T: 20559, Avg. loss: 0.276218\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 15.99, NNZs: 1343, Bias: -0.449347, T: 21538, Avg. loss: 0.275557\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 16.05, NNZs: 1343, Bias: -0.450773, T: 22517, Avg. loss: 0.274728\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 16.09, NNZs: 1343, Bias: -0.452130, T: 23496, Avg. loss: 0.273959\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 16.14, NNZs: 1343, Bias: -0.453427, T: 24475, Avg. loss: 0.273300\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 16.18, NNZs: 1343, Bias: -0.454678, T: 25454, Avg. loss: 0.272501\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 26 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 8.41, NNZs: 1343, Bias: -0.139965, T: 979, Avg. loss: 0.574616\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 9.22, NNZs: 1343, Bias: -0.160893, T: 1958, Avg. loss: 0.494968\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 9.69, NNZs: 1343, Bias: -0.172068, T: 2937, Avg. loss: 0.477409\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 10.02, NNZs: 1343, Bias: -0.179593, T: 3916, Avg. loss: 0.467752\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 10.27, NNZs: 1343, Bias: -0.185688, T: 4895, Avg. loss: 0.462004\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 10.47, NNZs: 1343, Bias: -0.190269, T: 5874, Avg. loss: 0.456566\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 10.64, NNZs: 1343, Bias: -0.194148, T: 6853, Avg. loss: 0.452745\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 10.79, NNZs: 1343, Bias: -0.197469, T: 7832, Avg. loss: 0.449515\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 10.92, NNZs: 1343, Bias: -0.200311, T: 8811, Avg. loss: 0.446624\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 11.03, NNZs: 1343, Bias: -0.202845, T: 9790, Avg. loss: 0.444266\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 11.14, NNZs: 1343, Bias: -0.205109, T: 10769, Avg. loss: 0.442136\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11.23, NNZs: 1343, Bias: -0.207109, T: 11748, Avg. loss: 0.440043\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11.32, NNZs: 1343, Bias: -0.208963, T: 12727, Avg. loss: 0.438480\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 11.40, NNZs: 1343, Bias: -0.210641, T: 13706, Avg. loss: 0.436791\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 11.47, NNZs: 1343, Bias: -0.212212, T: 14685, Avg. loss: 0.435480\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 11.54, NNZs: 1343, Bias: -0.213666, T: 15664, Avg. loss: 0.434143\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 11.60, NNZs: 1343, Bias: -0.215023, T: 16643, Avg. loss: 0.432930\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 11.66, NNZs: 1343, Bias: -0.216289, T: 17622, Avg. loss: 0.431748\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 11.72, NNZs: 1343, Bias: -0.217476, T: 18601, Avg. loss: 0.430653\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 11.77, NNZs: 1343, Bias: -0.218597, T: 19580, Avg. loss: 0.429664\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 11.82, NNZs: 1343, Bias: -0.219658, T: 20559, Avg. loss: 0.428748\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 11.87, NNZs: 1343, Bias: -0.220663, T: 21538, Avg. loss: 0.427831\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 11.92, NNZs: 1343, Bias: -0.221618, T: 22517, Avg. loss: 0.426969\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 11.96, NNZs: 1343, Bias: -0.222528, T: 23496, Avg. loss: 0.426199\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 11.02, NNZs: 1325, Bias: -0.152824, T: 979, Avg. loss: 0.496002\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 11.82, NNZs: 1325, Bias: -0.153255, T: 1958, Avg. loss: 0.414145\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 12.28, NNZs: 1325, Bias: -0.153067, T: 2937, Avg. loss: 0.398326\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 12.61, NNZs: 1325, Bias: -0.152238, T: 3916, Avg. loss: 0.389476\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 12.86, NNZs: 1325, Bias: -0.151555, T: 4895, Avg. loss: 0.383371\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 13.06, NNZs: 1325, Bias: -0.151012, T: 5874, Avg. loss: 0.378738\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 13.24, NNZs: 1325, Bias: -0.150466, T: 6853, Avg. loss: 0.375068\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 13.38, NNZs: 1325, Bias: -0.149969, T: 7832, Avg. loss: 0.372010\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 13.51, NNZs: 1325, Bias: -0.149497, T: 8811, Avg. loss: 0.369417\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13.63, NNZs: 1325, Bias: -0.149085, T: 9790, Avg. loss: 0.367150\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 13.73, NNZs: 1325, Bias: -0.148721, T: 10769, Avg. loss: 0.365159\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 13.83, NNZs: 1325, Bias: -0.148393, T: 11748, Avg. loss: 0.363367\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 13.92, NNZs: 1325, Bias: -0.148072, T: 12727, Avg. loss: 0.361766\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 14.00, NNZs: 1325, Bias: -0.147787, T: 13706, Avg. loss: 0.360300\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 14.07, NNZs: 1325, Bias: -0.147516, T: 14685, Avg. loss: 0.358964\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 14.14, NNZs: 1325, Bias: -0.147269, T: 15664, Avg. loss: 0.357721\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 14.20, NNZs: 1325, Bias: -0.147038, T: 16643, Avg. loss: 0.356574\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 14.27, NNZs: 1325, Bias: -0.146810, T: 17622, Avg. loss: 0.355510\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 14.32, NNZs: 1325, Bias: -0.146600, T: 18601, Avg. loss: 0.354506\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 14.38, NNZs: 1325, Bias: -0.146397, T: 19580, Avg. loss: 0.353570\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 14.43, NNZs: 1325, Bias: -0.146209, T: 20559, Avg. loss: 0.352683\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 14.48, NNZs: 1325, Bias: -0.146026, T: 21538, Avg. loss: 0.351847\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 14.53, NNZs: 1325, Bias: -0.145847, T: 22517, Avg. loss: 0.351059\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 14.57, NNZs: 1325, Bias: -0.145681, T: 23496, Avg. loss: 0.350304\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 12.81, NNZs: 1325, Bias: -0.354776, T: 979, Avg. loss: 0.398224\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 13.58, NNZs: 1325, Bias: -0.383161, T: 1958, Avg. loss: 0.330879\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 14.03, NNZs: 1325, Bias: -0.400513, T: 2937, Avg. loss: 0.308582\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 14.36, NNZs: 1325, Bias: -0.412066, T: 3916, Avg. loss: 0.299775\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 14.62, NNZs: 1325, Bias: -0.420544, T: 4895, Avg. loss: 0.294561\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 14.83, NNZs: 1325, Bias: -0.427553, T: 5874, Avg. loss: 0.289353\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 15.01, NNZs: 1325, Bias: -0.433187, T: 6853, Avg. loss: 0.286134\n",
            "Total training time: 0.01 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 8\n",
            "Norm: 15.16, NNZs: 1325, Bias: -0.438039, T: 7832, Avg. loss: 0.283127\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 15.30, NNZs: 1325, Bias: -0.442362, T: 8811, Avg. loss: 0.279849\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 15.42, NNZs: 1325, Bias: -0.446149, T: 9790, Avg. loss: 0.277858\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 15.53, NNZs: 1325, Bias: -0.449509, T: 10769, Avg. loss: 0.276184\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 15.63, NNZs: 1325, Bias: -0.452600, T: 11748, Avg. loss: 0.274102\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 15.72, NNZs: 1325, Bias: -0.455426, T: 12727, Avg. loss: 0.272334\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 15.81, NNZs: 1325, Bias: -0.458020, T: 13706, Avg. loss: 0.271026\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 15.88, NNZs: 1325, Bias: -0.460406, T: 14685, Avg. loss: 0.269771\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 15.96, NNZs: 1325, Bias: -0.462631, T: 15664, Avg. loss: 0.268633\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 16.03, NNZs: 1325, Bias: -0.464711, T: 16643, Avg. loss: 0.267620\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 16.09, NNZs: 1325, Bias: -0.466660, T: 17622, Avg. loss: 0.266472\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 16.15, NNZs: 1325, Bias: -0.468495, T: 18601, Avg. loss: 0.265397\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 16.21, NNZs: 1325, Bias: -0.470221, T: 19580, Avg. loss: 0.264672\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 16.27, NNZs: 1325, Bias: -0.471866, T: 20559, Avg. loss: 0.263732\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16.32, NNZs: 1325, Bias: -0.473418, T: 21538, Avg. loss: 0.263024\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 16.37, NNZs: 1325, Bias: -0.474902, T: 22517, Avg. loss: 0.262223\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 16.42, NNZs: 1325, Bias: -0.476316, T: 23496, Avg. loss: 0.261489\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 24 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 8.71, NNZs: 1325, Bias: -0.150943, T: 979, Avg. loss: 0.558854\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 9.53, NNZs: 1325, Bias: -0.169922, T: 1958, Avg. loss: 0.478539\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 10.00, NNZs: 1325, Bias: -0.180209, T: 2937, Avg. loss: 0.461487\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 10.33, NNZs: 1325, Bias: -0.187113, T: 3916, Avg. loss: 0.452041\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 10.58, NNZs: 1325, Bias: -0.192626, T: 4895, Avg. loss: 0.446222\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 10.78, NNZs: 1325, Bias: -0.196745, T: 5874, Avg. loss: 0.440930\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 10.95, NNZs: 1325, Bias: -0.200324, T: 6853, Avg. loss: 0.437363\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 11.10, NNZs: 1325, Bias: -0.203366, T: 7832, Avg. loss: 0.434244\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 11.22, NNZs: 1325, Bias: -0.205953, T: 8811, Avg. loss: 0.431340\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 11.34, NNZs: 1325, Bias: -0.208245, T: 9790, Avg. loss: 0.429020\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 11.44, NNZs: 1325, Bias: -0.210300, T: 10769, Avg. loss: 0.426965\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11.53, NNZs: 1325, Bias: -0.212132, T: 11748, Avg. loss: 0.425017\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11.62, NNZs: 1325, Bias: -0.213820, T: 12727, Avg. loss: 0.423427\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 11.69, NNZs: 1325, Bias: -0.215345, T: 13706, Avg. loss: 0.421767\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 11.77, NNZs: 1325, Bias: -0.216773, T: 14685, Avg. loss: 0.420487\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 11.83, NNZs: 1325, Bias: -0.218085, T: 15664, Avg. loss: 0.419153\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 11.90, NNZs: 1325, Bias: -0.219313, T: 16643, Avg. loss: 0.417991\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 11.96, NNZs: 1325, Bias: -0.220459, T: 17622, Avg. loss: 0.416838\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 12.01, NNZs: 1325, Bias: -0.221540, T: 18601, Avg. loss: 0.415810\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 12.07, NNZs: 1325, Bias: -0.222560, T: 19580, Avg. loss: 0.414854\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 12.12, NNZs: 1325, Bias: -0.223521, T: 20559, Avg. loss: 0.413937\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 12.16, NNZs: 1325, Bias: -0.224431, T: 21538, Avg. loss: 0.413042\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 12.21, NNZs: 1325, Bias: -0.225298, T: 22517, Avg. loss: 0.412220\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 12.25, NNZs: 1325, Bias: -0.226122, T: 23496, Avg. loss: 0.411453\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.50, NNZs: 1320, Bias: -0.162400, T: 980, Avg. loss: 0.505108\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 11.32, NNZs: 1320, Bias: -0.156926, T: 1960, Avg. loss: 0.423728\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 11.80, NNZs: 1320, Bias: -0.153498, T: 2940, Avg. loss: 0.407169\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 12.14, NNZs: 1320, Bias: -0.151268, T: 3920, Avg. loss: 0.397768\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 12.40, NNZs: 1320, Bias: -0.149336, T: 4900, Avg. loss: 0.391344\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 12.62, NNZs: 1320, Bias: -0.148100, T: 5880, Avg. loss: 0.386469\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 12.80, NNZs: 1320, Bias: -0.147033, T: 6860, Avg. loss: 0.382631\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 12.95, NNZs: 1320, Bias: -0.146092, T: 7840, Avg. loss: 0.379445\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 13.08, NNZs: 1320, Bias: -0.145302, T: 8820, Avg. loss: 0.376721\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13.21, NNZs: 1320, Bias: -0.144641, T: 9800, Avg. loss: 0.374365\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 13.31, NNZs: 1320, Bias: -0.144010, T: 10780, Avg. loss: 0.372299\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 13.41, NNZs: 1320, Bias: -0.143480, T: 11760, Avg. loss: 0.370434\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 13.50, NNZs: 1320, Bias: -0.142997, T: 12740, Avg. loss: 0.368774\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 13.59, NNZs: 1320, Bias: -0.142546, T: 13720, Avg. loss: 0.367257\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 13.66, NNZs: 1320, Bias: -0.142147, T: 14700, Avg. loss: 0.365863\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 13.74, NNZs: 1320, Bias: -0.141771, T: 15680, Avg. loss: 0.364583\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 13.80, NNZs: 1320, Bias: -0.141424, T: 16660, Avg. loss: 0.363398\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 13.87, NNZs: 1320, Bias: -0.141110, T: 17640, Avg. loss: 0.362285\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 13.93, NNZs: 1320, Bias: -0.140807, T: 18620, Avg. loss: 0.361255\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 13.99, NNZs: 1320, Bias: -0.140534, T: 19600, Avg. loss: 0.360280\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 14.04, NNZs: 1320, Bias: -0.140275, T: 20580, Avg. loss: 0.359369\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 14.09, NNZs: 1320, Bias: -0.140023, T: 21560, Avg. loss: 0.358509\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 14.14, NNZs: 1320, Bias: -0.139789, T: 22540, Avg. loss: 0.357688\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 14.19, NNZs: 1320, Bias: -0.139570, T: 23520, Avg. loss: 0.356909\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 24 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 11.74, NNZs: 1320, Bias: -0.350117, T: 980, Avg. loss: 0.414326\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 12.55, NNZs: 1320, Bias: -0.375159, T: 1960, Avg. loss: 0.334289\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 13.05, NNZs: 1320, Bias: -0.389106, T: 2940, Avg. loss: 0.316943\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 13.41, NNZs: 1320, Bias: -0.398745, T: 3920, Avg. loss: 0.307815\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 13.69, NNZs: 1320, Bias: -0.406063, T: 4900, Avg. loss: 0.301292\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 13.92, NNZs: 1320, Bias: -0.412039, T: 5880, Avg. loss: 0.295606\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 14.11, NNZs: 1320, Bias: -0.416853, T: 6860, Avg. loss: 0.292518\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 14.28, NNZs: 1320, Bias: -0.421013, T: 7840, Avg. loss: 0.289121\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 14.43, NNZs: 1320, Bias: -0.424634, T: 8820, Avg. loss: 0.286508\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 14.56, NNZs: 1320, Bias: -0.427909, T: 9800, Avg. loss: 0.283904\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 14.68, NNZs: 1320, Bias: -0.430784, T: 10780, Avg. loss: 0.282216\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 14.79, NNZs: 1320, Bias: -0.433438, T: 11760, Avg. loss: 0.280119\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 14.89, NNZs: 1320, Bias: -0.435787, T: 12740, Avg. loss: 0.279126\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 14.98, NNZs: 1320, Bias: -0.437983, T: 13720, Avg. loss: 0.277353\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 15.06, NNZs: 1320, Bias: -0.440011, T: 14700, Avg. loss: 0.276097\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 15.15, NNZs: 1320, Bias: -0.441900, T: 15680, Avg. loss: 0.274774\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 15.22, NNZs: 1320, Bias: -0.443659, T: 16660, Avg. loss: 0.273779\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 15.29, NNZs: 1320, Bias: -0.445340, T: 17640, Avg. loss: 0.272451\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 15.36, NNZs: 1320, Bias: -0.446897, T: 18620, Avg. loss: 0.271725\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 15.42, NNZs: 1320, Bias: -0.448362, T: 19600, Avg. loss: 0.270882\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 15.48, NNZs: 1320, Bias: -0.449764, T: 20580, Avg. loss: 0.269961\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 15.54, NNZs: 1320, Bias: -0.451092, T: 21560, Avg. loss: 0.269084\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 15.60, NNZs: 1320, Bias: -0.452348, T: 22540, Avg. loss: 0.268431\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 23 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 8.34, NNZs: 1320, Bias: -0.129835, T: 980, Avg. loss: 0.583569\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 9.15, NNZs: 1320, Bias: -0.147820, T: 1960, Avg. loss: 0.500092\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 9.62, NNZs: 1320, Bias: -0.157401, T: 2940, Avg. loss: 0.481141\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 9.94, NNZs: 1320, Bias: -0.164065, T: 3920, Avg. loss: 0.471027\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 10.19, NNZs: 1320, Bias: -0.169162, T: 4900, Avg. loss: 0.464185\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 10.40, NNZs: 1320, Bias: -0.173294, T: 5880, Avg. loss: 0.459003\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 10.57, NNZs: 1320, Bias: -0.176630, T: 6860, Avg. loss: 0.454583\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 10.71, NNZs: 1320, Bias: -0.179482, T: 7840, Avg. loss: 0.451131\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 10.84, NNZs: 1320, Bias: -0.181993, T: 8820, Avg. loss: 0.448234\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 10.95, NNZs: 1320, Bias: -0.184203, T: 9800, Avg. loss: 0.445688\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 11.06, NNZs: 1320, Bias: -0.186185, T: 10780, Avg. loss: 0.443467\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11.15, NNZs: 1320, Bias: -0.187970, T: 11760, Avg. loss: 0.441436\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11.23, NNZs: 1320, Bias: -0.189621, T: 12740, Avg. loss: 0.439779\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 11.31, NNZs: 1320, Bias: -0.191114, T: 13720, Avg. loss: 0.438037\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 11.38, NNZs: 1320, Bias: -0.192506, T: 14700, Avg. loss: 0.436598\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 11.45, NNZs: 1320, Bias: -0.193808, T: 15680, Avg. loss: 0.435260\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 11.52, NNZs: 1320, Bias: -0.195011, T: 16660, Avg. loss: 0.433860\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 11.58, NNZs: 1320, Bias: -0.196138, T: 17640, Avg. loss: 0.432774\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 11.63, NNZs: 1320, Bias: -0.197198, T: 18620, Avg. loss: 0.431567\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 11.68, NNZs: 1320, Bias: -0.198197, T: 19600, Avg. loss: 0.430554\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 11.73, NNZs: 1320, Bias: -0.199153, T: 20580, Avg. loss: 0.429641\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 11.78, NNZs: 1320, Bias: -0.200055, T: 21560, Avg. loss: 0.428659\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 11.83, NNZs: 1320, Bias: -0.200909, T: 22540, Avg. loss: 0.427777\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 11.87, NNZs: 1320, Bias: -0.201727, T: 23520, Avg. loss: 0.426974\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 11.91, NNZs: 1320, Bias: -0.202506, T: 24500, Avg. loss: 0.426167\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 25 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.59, NNZs: 1338, Bias: -0.140799, T: 980, Avg. loss: 0.502036\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 11.42, NNZs: 1338, Bias: -0.135437, T: 1960, Avg. loss: 0.419359\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 11.90, NNZs: 1338, Bias: -0.131702, T: 2940, Avg. loss: 0.402775\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 12.24, NNZs: 1338, Bias: -0.129100, T: 3920, Avg. loss: 0.393369\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 12.50, NNZs: 1338, Bias: -0.127033, T: 4900, Avg. loss: 0.386920\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 12.71, NNZs: 1338, Bias: -0.125641, T: 5880, Avg. loss: 0.382040\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 12.89, NNZs: 1338, Bias: -0.124415, T: 6860, Avg. loss: 0.378203\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 13.05, NNZs: 1338, Bias: -0.123339, T: 7840, Avg. loss: 0.375014\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 13.18, NNZs: 1338, Bias: -0.122427, T: 8820, Avg. loss: 0.372292\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13.30, NNZs: 1338, Bias: -0.121646, T: 9800, Avg. loss: 0.369937\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 13.41, NNZs: 1338, Bias: -0.120918, T: 10780, Avg. loss: 0.367867\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 13.51, NNZs: 1338, Bias: -0.120302, T: 11760, Avg. loss: 0.366004\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 13.60, NNZs: 1338, Bias: -0.119736, T: 12740, Avg. loss: 0.364342\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 13.68, NNZs: 1338, Bias: -0.119205, T: 13720, Avg. loss: 0.362827\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 13.76, NNZs: 1338, Bias: -0.118742, T: 14700, Avg. loss: 0.361428\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 13.83, NNZs: 1338, Bias: -0.118292, T: 15680, Avg. loss: 0.360154\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 13.90, NNZs: 1338, Bias: -0.117880, T: 16660, Avg. loss: 0.358967\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 13.97, NNZs: 1338, Bias: -0.117509, T: 17640, Avg. loss: 0.357853\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 14.03, NNZs: 1338, Bias: -0.117149, T: 18620, Avg. loss: 0.356823\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 14.08, NNZs: 1338, Bias: -0.116824, T: 19600, Avg. loss: 0.355850\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 14.14, NNZs: 1338, Bias: -0.116513, T: 20580, Avg. loss: 0.354939\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 14.19, NNZs: 1338, Bias: -0.116210, T: 21560, Avg. loss: 0.354080\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 14.24, NNZs: 1338, Bias: -0.115928, T: 22540, Avg. loss: 0.353260\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 14.28, NNZs: 1338, Bias: -0.115664, T: 23520, Avg. loss: 0.352481\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 24 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 12.19, NNZs: 1338, Bias: -0.359337, T: 980, Avg. loss: 0.409295\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 12.96, NNZs: 1338, Bias: -0.385188, T: 1960, Avg. loss: 0.334991\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 13.43, NNZs: 1338, Bias: -0.400437, T: 2940, Avg. loss: 0.316259\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 13.77, NNZs: 1338, Bias: -0.410706, T: 3920, Avg. loss: 0.307687\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 14.03, NNZs: 1338, Bias: -0.418621, T: 4900, Avg. loss: 0.301106\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 14.25, NNZs: 1338, Bias: -0.425016, T: 5880, Avg. loss: 0.295900\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 14.43, NNZs: 1338, Bias: -0.430210, T: 6860, Avg. loss: 0.292581\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 14.59, NNZs: 1338, Bias: -0.434721, T: 7840, Avg. loss: 0.289089\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 14.73, NNZs: 1338, Bias: -0.438580, T: 8820, Avg. loss: 0.286857\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 14.86, NNZs: 1338, Bias: -0.442111, T: 9800, Avg. loss: 0.284077\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 14.97, NNZs: 1338, Bias: -0.445215, T: 10780, Avg. loss: 0.282611\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 15.07, NNZs: 1338, Bias: -0.448065, T: 11760, Avg. loss: 0.280491\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 15.17, NNZs: 1338, Bias: -0.450596, T: 12740, Avg. loss: 0.279441\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 15.26, NNZs: 1338, Bias: -0.452963, T: 13720, Avg. loss: 0.277668\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 15.34, NNZs: 1338, Bias: -0.455145, T: 14700, Avg. loss: 0.276455\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 15.42, NNZs: 1338, Bias: -0.457181, T: 15680, Avg. loss: 0.275156\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 15.49, NNZs: 1338, Bias: -0.459072, T: 16660, Avg. loss: 0.274240\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 15.56, NNZs: 1338, Bias: -0.460871, T: 17640, Avg. loss: 0.272967\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 15.62, NNZs: 1338, Bias: -0.462553, T: 18620, Avg. loss: 0.272068\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 15.68, NNZs: 1338, Bias: -0.464139, T: 19600, Avg. loss: 0.271278\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 15.74, NNZs: 1338, Bias: -0.465650, T: 20580, Avg. loss: 0.270367\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 15.79, NNZs: 1338, Bias: -0.467075, T: 21560, Avg. loss: 0.269613\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 15.85, NNZs: 1338, Bias: -0.468431, T: 22540, Avg. loss: 0.268900\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 23 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 8.42, NNZs: 1338, Bias: -0.138072, T: 980, Avg. loss: 0.576684\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 9.25, NNZs: 1338, Bias: -0.156860, T: 1960, Avg. loss: 0.491489\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 9.72, NNZs: 1338, Bias: -0.166960, T: 2940, Avg. loss: 0.472637\n",
            "Total training time: 0.00 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 4\n",
            "Norm: 10.05, NNZs: 1338, Bias: -0.173950, T: 3920, Avg. loss: 0.462521\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 10.31, NNZs: 1338, Bias: -0.179322, T: 4900, Avg. loss: 0.455676\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 10.51, NNZs: 1338, Bias: -0.183623, T: 5880, Avg. loss: 0.450373\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 10.69, NNZs: 1338, Bias: -0.187100, T: 6860, Avg. loss: 0.446013\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 10.83, NNZs: 1338, Bias: -0.190064, T: 7840, Avg. loss: 0.442545\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 10.96, NNZs: 1338, Bias: -0.192675, T: 8820, Avg. loss: 0.439613\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 11.08, NNZs: 1338, Bias: -0.194987, T: 9800, Avg. loss: 0.437110\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 11.18, NNZs: 1338, Bias: -0.197063, T: 10780, Avg. loss: 0.434903\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11.28, NNZs: 1338, Bias: -0.198928, T: 11760, Avg. loss: 0.432840\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11.36, NNZs: 1338, Bias: -0.200648, T: 12740, Avg. loss: 0.431180\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 11.44, NNZs: 1338, Bias: -0.202204, T: 13720, Avg. loss: 0.429431\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 11.52, NNZs: 1338, Bias: -0.203652, T: 14700, Avg. loss: 0.427988\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 11.59, NNZs: 1338, Bias: -0.205006, T: 15680, Avg. loss: 0.426659\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 11.65, NNZs: 1338, Bias: -0.206265, T: 16660, Avg. loss: 0.425297\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 11.71, NNZs: 1338, Bias: -0.207442, T: 17640, Avg. loss: 0.424172\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 11.77, NNZs: 1338, Bias: -0.208545, T: 18620, Avg. loss: 0.422975\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 11.82, NNZs: 1338, Bias: -0.209586, T: 19600, Avg. loss: 0.421951\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 11.87, NNZs: 1338, Bias: -0.210579, T: 20580, Avg. loss: 0.421034\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 11.92, NNZs: 1338, Bias: -0.211517, T: 21560, Avg. loss: 0.420057\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 11.97, NNZs: 1338, Bias: -0.212406, T: 22540, Avg. loss: 0.419180\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 12.01, NNZs: 1338, Bias: -0.213258, T: 23520, Avg. loss: 0.418374\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 12.05, NNZs: 1338, Bias: -0.214069, T: 24500, Avg. loss: 0.417572\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 25 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 50.09, NNZs: 365, Bias: -0.185351, T: 979, Avg. loss: 0.530875\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 53.39, NNZs: 217, Bias: -0.139677, T: 1958, Avg. loss: 0.322870\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 55.15, NNZs: 196, Bias: -0.143915, T: 2937, Avg. loss: 0.325030\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 56.30, NNZs: 191, Bias: -0.163485, T: 3916, Avg. loss: 0.320857\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 57.15, NNZs: 190, Bias: -0.171783, T: 4895, Avg. loss: 0.313175\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 57.79, NNZs: 179, Bias: -0.174476, T: 5874, Avg. loss: 0.308616\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 58.34, NNZs: 178, Bias: -0.174321, T: 6853, Avg. loss: 0.305292\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 58.79, NNZs: 176, Bias: -0.177907, T: 7832, Avg. loss: 0.303851\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 59.17, NNZs: 175, Bias: -0.175211, T: 8811, Avg. loss: 0.302597\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 59.53, NNZs: 174, Bias: -0.181128, T: 9790, Avg. loss: 0.300660\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 59.84, NNZs: 177, Bias: -0.178276, T: 10769, Avg. loss: 0.299622\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 60.11, NNZs: 176, Bias: -0.184208, T: 11748, Avg. loss: 0.298456\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 60.38, NNZs: 175, Bias: -0.184514, T: 12727, Avg. loss: 0.297621\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 60.61, NNZs: 177, Bias: -0.188780, T: 13706, Avg. loss: 0.296556\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 60.81, NNZs: 173, Bias: -0.185909, T: 14685, Avg. loss: 0.296016\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 61.02, NNZs: 172, Bias: -0.186297, T: 15664, Avg. loss: 0.295282\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 61.21, NNZs: 172, Bias: -0.186334, T: 16643, Avg. loss: 0.294480\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 61.38, NNZs: 172, Bias: -0.196995, T: 17622, Avg. loss: 0.293923\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 61.55, NNZs: 172, Bias: -0.192413, T: 18601, Avg. loss: 0.293612\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 19 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 66.11, NNZs: 390, Bias: -0.842472, T: 979, Avg. loss: 0.530040\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 68.91, NNZs: 238, Bias: -0.962374, T: 1958, Avg. loss: 0.214979\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 70.07, NNZs: 210, Bias: -0.979997, T: 2937, Avg. loss: 0.175562\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 70.94, NNZs: 207, Bias: -1.008963, T: 3916, Avg. loss: 0.160732\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 71.68, NNZs: 208, Bias: -1.015027, T: 4895, Avg. loss: 0.150806\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 72.32, NNZs: 205, Bias: -1.018791, T: 5874, Avg. loss: 0.142735\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 72.79, NNZs: 202, Bias: -1.011836, T: 6853, Avg. loss: 0.134722\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 73.30, NNZs: 198, Bias: -0.998843, T: 7832, Avg. loss: 0.132085\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 73.74, NNZs: 196, Bias: -0.995811, T: 8811, Avg. loss: 0.129488\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 74.07, NNZs: 198, Bias: -1.004611, T: 9790, Avg. loss: 0.126637\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 74.38, NNZs: 194, Bias: -1.023489, T: 10769, Avg. loss: 0.125276\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 74.73, NNZs: 194, Bias: -1.001754, T: 11748, Avg. loss: 0.122211\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 74.94, NNZs: 190, Bias: -1.010181, T: 12727, Avg. loss: 0.119609\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 75.21, NNZs: 193, Bias: -0.997297, T: 13706, Avg. loss: 0.117443\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 75.44, NNZs: 188, Bias: -0.994104, T: 14685, Avg. loss: 0.117450\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 75.64, NNZs: 191, Bias: -1.002677, T: 15664, Avg. loss: 0.116961\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 75.83, NNZs: 188, Bias: -1.004068, T: 16643, Avg. loss: 0.114684\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 76.01, NNZs: 188, Bias: -1.004403, T: 17622, Avg. loss: 0.114342\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 76.17, NNZs: 186, Bias: -1.001589, T: 18601, Avg. loss: 0.112742\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 76.36, NNZs: 184, Bias: -0.992845, T: 19580, Avg. loss: 0.112549\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 76.53, NNZs: 187, Bias: -1.002496, T: 20559, Avg. loss: 0.113906\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 76.66, NNZs: 183, Bias: -1.003637, T: 21538, Avg. loss: 0.111741\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 76.80, NNZs: 188, Bias: -1.003692, T: 22517, Avg. loss: 0.111594\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 76.94, NNZs: 184, Bias: -0.997652, T: 23496, Avg. loss: 0.110438\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 77.07, NNZs: 187, Bias: -1.002175, T: 24475, Avg. loss: 0.110882\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 77.19, NNZs: 186, Bias: -1.001339, T: 25454, Avg. loss: 0.109663\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 77.30, NNZs: 182, Bias: -1.000544, T: 26433, Avg. loss: 0.109860\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 77.43, NNZs: 184, Bias: -1.000627, T: 27412, Avg. loss: 0.109419\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 77.54, NNZs: 183, Bias: -0.997095, T: 28391, Avg. loss: 0.109448\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 29 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 43.66, NNZs: 296, Bias: -0.675573, T: 979, Avg. loss: 0.570162\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 46.17, NNZs: 147, Bias: -0.831108, T: 1958, Avg. loss: 0.410401\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47.44, NNZs: 133, Bias: -0.911102, T: 2937, Avg. loss: 0.427688\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 48.22, NNZs: 123, Bias: -0.929745, T: 3916, Avg. loss: 0.435419\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 48.82, NNZs: 121, Bias: -0.933327, T: 4895, Avg. loss: 0.437855\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 49.25, NNZs: 123, Bias: -0.962703, T: 5874, Avg. loss: 0.433079\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 49.61, NNZs: 118, Bias: -0.991700, T: 6853, Avg. loss: 0.431583\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 51.06, NNZs: 350, Bias: -0.188120, T: 979, Avg. loss: 0.519483\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 54.02, NNZs: 222, Bias: -0.084866, T: 1958, Avg. loss: 0.321323\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 55.60, NNZs: 204, Bias: -0.069628, T: 2937, Avg. loss: 0.332499\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 56.72, NNZs: 201, Bias: -0.095265, T: 3916, Avg. loss: 0.329906\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 57.51, NNZs: 187, Bias: -0.084670, T: 4895, Avg. loss: 0.321744\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 58.14, NNZs: 180, Bias: -0.093113, T: 5874, Avg. loss: 0.317639\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 58.65, NNZs: 181, Bias: -0.090961, T: 6853, Avg. loss: 0.314961\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 59.08, NNZs: 182, Bias: -0.092058, T: 7832, Avg. loss: 0.312630\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 59.46, NNZs: 183, Bias: -0.099829, T: 8811, Avg. loss: 0.310889\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 59.80, NNZs: 180, Bias: -0.102358, T: 9790, Avg. loss: 0.307894\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 60.09, NNZs: 182, Bias: -0.102912, T: 10769, Avg. loss: 0.306806\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 60.37, NNZs: 183, Bias: -0.104896, T: 11748, Avg. loss: 0.304977\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 60.62, NNZs: 181, Bias: -0.105395, T: 12727, Avg. loss: 0.303612\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 60.84, NNZs: 182, Bias: -0.105377, T: 13706, Avg. loss: 0.302940\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 61.06, NNZs: 183, Bias: -0.107134, T: 14685, Avg. loss: 0.302075\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 61.25, NNZs: 182, Bias: -0.106773, T: 15664, Avg. loss: 0.300819\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 61.44, NNZs: 183, Bias: -0.105513, T: 16643, Avg. loss: 0.300118\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 61.61, NNZs: 185, Bias: -0.114469, T: 17622, Avg. loss: 0.299625\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 61.77, NNZs: 186, Bias: -0.110918, T: 18601, Avg. loss: 0.298890\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 61.92, NNZs: 183, Bias: -0.113314, T: 19580, Avg. loss: 0.298508\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 62.07, NNZs: 183, Bias: -0.117456, T: 20559, Avg. loss: 0.298233\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 21 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 66.44, NNZs: 377, Bias: -0.849172, T: 979, Avg. loss: 0.567471\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 68.17, NNZs: 255, Bias: -0.995328, T: 1958, Avg. loss: 0.217747\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 69.43, NNZs: 216, Bias: -1.001551, T: 2937, Avg. loss: 0.186451\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 70.35, NNZs: 201, Bias: -1.007518, T: 3916, Avg. loss: 0.171099\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 71.10, NNZs: 210, Bias: -1.021087, T: 4895, Avg. loss: 0.160279\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 71.71, NNZs: 213, Bias: -1.021096, T: 5874, Avg. loss: 0.147247\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 72.17, NNZs: 208, Bias: -1.015657, T: 6853, Avg. loss: 0.140468\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 72.65, NNZs: 210, Bias: -0.996436, T: 7832, Avg. loss: 0.137920\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 73.08, NNZs: 210, Bias: -0.992159, T: 8811, Avg. loss: 0.131553\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 73.46, NNZs: 208, Bias: -0.998996, T: 9790, Avg. loss: 0.130651\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 73.73, NNZs: 201, Bias: -1.020312, T: 10769, Avg. loss: 0.127233\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 74.01, NNZs: 203, Bias: -1.001275, T: 11748, Avg. loss: 0.123013\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 74.29, NNZs: 196, Bias: -1.005674, T: 12727, Avg. loss: 0.122816\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 74.48, NNZs: 198, Bias: -0.996307, T: 13706, Avg. loss: 0.118953\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 74.71, NNZs: 197, Bias: -0.994951, T: 14685, Avg. loss: 0.119294\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 74.94, NNZs: 200, Bias: -1.008687, T: 15664, Avg. loss: 0.119513\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 75.12, NNZs: 200, Bias: -1.006037, T: 16643, Avg. loss: 0.116366\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 75.32, NNZs: 201, Bias: -1.000966, T: 17622, Avg. loss: 0.117000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 75.51, NNZs: 201, Bias: -1.002707, T: 18601, Avg. loss: 0.116701\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 75.68, NNZs: 202, Bias: -0.997028, T: 19580, Avg. loss: 0.117322\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 75.84, NNZs: 199, Bias: -1.002176, T: 20559, Avg. loss: 0.116064\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 75.96, NNZs: 200, Bias: -1.006559, T: 21538, Avg. loss: 0.115261\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 22 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 43.66, NNZs: 311, Bias: -0.713341, T: 979, Avg. loss: 0.578024\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 46.18, NNZs: 149, Bias: -0.859411, T: 1958, Avg. loss: 0.407692\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47.37, NNZs: 135, Bias: -0.924057, T: 2937, Avg. loss: 0.427545\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 48.16, NNZs: 121, Bias: -0.957737, T: 3916, Avg. loss: 0.425105\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 48.75, NNZs: 113, Bias: -0.954813, T: 4895, Avg. loss: 0.428961\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 49.16, NNZs: 112, Bias: -0.987871, T: 5874, Avg. loss: 0.424656\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 49.48, NNZs: 106, Bias: -0.999029, T: 6853, Avg. loss: 0.426696\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 49.73, NNZs: 344, Bias: -0.103859, T: 979, Avg. loss: 0.528744\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 52.98, NNZs: 211, Bias: -0.057863, T: 1958, Avg. loss: 0.323473\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 54.75, NNZs: 193, Bias: -0.083492, T: 2937, Avg. loss: 0.328013\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 55.90, NNZs: 192, Bias: -0.092867, T: 3916, Avg. loss: 0.324004\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.71, NNZs: 190, Bias: -0.091498, T: 4895, Avg. loss: 0.317315\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 57.38, NNZs: 186, Bias: -0.099510, T: 5874, Avg. loss: 0.313220\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 57.89, NNZs: 178, Bias: -0.098169, T: 6853, Avg. loss: 0.309808\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 58.34, NNZs: 180, Bias: -0.095303, T: 7832, Avg. loss: 0.308595\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 58.74, NNZs: 183, Bias: -0.098051, T: 8811, Avg. loss: 0.306504\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 59.08, NNZs: 180, Bias: -0.102407, T: 9790, Avg. loss: 0.303924\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 59.38, NNZs: 183, Bias: -0.096608, T: 10769, Avg. loss: 0.302634\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 59.66, NNZs: 185, Bias: -0.102541, T: 11748, Avg. loss: 0.301834\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 59.92, NNZs: 182, Bias: -0.103750, T: 12727, Avg. loss: 0.300627\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 60.15, NNZs: 182, Bias: -0.106513, T: 13706, Avg. loss: 0.299032\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 60.37, NNZs: 181, Bias: -0.108273, T: 14685, Avg. loss: 0.298378\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 60.57, NNZs: 180, Bias: -0.104808, T: 15664, Avg. loss: 0.296819\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 60.76, NNZs: 177, Bias: -0.109701, T: 16643, Avg. loss: 0.296456\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 60.94, NNZs: 178, Bias: -0.112359, T: 17622, Avg. loss: 0.295270\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 61.10, NNZs: 178, Bias: -0.110448, T: 18601, Avg. loss: 0.294890\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 61.26, NNZs: 178, Bias: -0.112445, T: 19580, Avg. loss: 0.294524\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 61.41, NNZs: 180, Bias: -0.115091, T: 20559, Avg. loss: 0.294307\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 61.55, NNZs: 179, Bias: -0.113779, T: 21538, Avg. loss: 0.293266\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 61.69, NNZs: 178, Bias: -0.116670, T: 22517, Avg. loss: 0.292770\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 61.82, NNZs: 177, Bias: -0.115710, T: 23496, Avg. loss: 0.292115\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 61.94, NNZs: 178, Bias: -0.116416, T: 24475, Avg. loss: 0.291746\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 62.06, NNZs: 178, Bias: -0.119731, T: 25454, Avg. loss: 0.291361\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 62.18, NNZs: 178, Bias: -0.117189, T: 26433, Avg. loss: 0.290918\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 27 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 64.77, NNZs: 369, Bias: -0.849095, T: 979, Avg. loss: 0.534036\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 66.89, NNZs: 255, Bias: -0.985969, T: 1958, Avg. loss: 0.233294\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 68.37, NNZs: 222, Bias: -0.971571, T: 2937, Avg. loss: 0.195866\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 69.38, NNZs: 202, Bias: -1.008900, T: 3916, Avg. loss: 0.180441\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 70.19, NNZs: 201, Bias: -0.993037, T: 4895, Avg. loss: 0.168125\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 70.76, NNZs: 203, Bias: -1.023576, T: 5874, Avg. loss: 0.158134\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 71.28, NNZs: 194, Bias: -1.018996, T: 6853, Avg. loss: 0.152805\n",
            "Total training time: 0.01 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 8\n",
            "Norm: 71.80, NNZs: 195, Bias: -1.002518, T: 7832, Avg. loss: 0.149270\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 72.17, NNZs: 189, Bias: -1.005257, T: 8811, Avg. loss: 0.143723\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 72.53, NNZs: 187, Bias: -1.002056, T: 9790, Avg. loss: 0.139848\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 72.85, NNZs: 184, Bias: -1.019403, T: 10769, Avg. loss: 0.137892\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 73.13, NNZs: 185, Bias: -1.002633, T: 11748, Avg. loss: 0.135515\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 73.44, NNZs: 188, Bias: -1.003604, T: 12727, Avg. loss: 0.135488\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 73.68, NNZs: 186, Bias: -1.001970, T: 13706, Avg. loss: 0.134613\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 73.92, NNZs: 188, Bias: -1.001655, T: 14685, Avg. loss: 0.132495\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 74.11, NNZs: 187, Bias: -1.010842, T: 15664, Avg. loss: 0.131379\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 74.32, NNZs: 184, Bias: -1.010080, T: 16643, Avg. loss: 0.130112\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 74.52, NNZs: 183, Bias: -1.005706, T: 17622, Avg. loss: 0.129813\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 74.70, NNZs: 183, Bias: -1.002275, T: 18601, Avg. loss: 0.127410\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 74.86, NNZs: 182, Bias: -0.996053, T: 19580, Avg. loss: 0.126244\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 75.03, NNZs: 179, Bias: -1.003656, T: 20559, Avg. loss: 0.127462\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 75.18, NNZs: 183, Bias: -1.002827, T: 21538, Avg. loss: 0.125434\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 75.32, NNZs: 182, Bias: -1.011140, T: 22517, Avg. loss: 0.124963\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 75.46, NNZs: 182, Bias: -1.000767, T: 23496, Avg. loss: 0.123362\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 75.59, NNZs: 183, Bias: -1.005040, T: 24475, Avg. loss: 0.123873\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 75.72, NNZs: 184, Bias: -1.003909, T: 25454, Avg. loss: 0.123097\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 75.86, NNZs: 187, Bias: -1.003331, T: 26433, Avg. loss: 0.122646\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 75.96, NNZs: 185, Bias: -1.001331, T: 27412, Avg. loss: 0.121289\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 76.09, NNZs: 181, Bias: -0.999862, T: 28391, Avg. loss: 0.121658\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 76.19, NNZs: 179, Bias: -1.002220, T: 29370, Avg. loss: 0.121003\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 76.30, NNZs: 185, Bias: -1.003080, T: 30349, Avg. loss: 0.121625\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 76.41, NNZs: 189, Bias: -1.002456, T: 31328, Avg. loss: 0.120019\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 76.51, NNZs: 186, Bias: -1.001948, T: 32307, Avg. loss: 0.120281\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 33 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 43.98, NNZs: 315, Bias: -0.677684, T: 979, Avg. loss: 0.577755\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 46.39, NNZs: 154, Bias: -0.782153, T: 1958, Avg. loss: 0.414864\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47.58, NNZs: 138, Bias: -0.877349, T: 2937, Avg. loss: 0.434490\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 48.34, NNZs: 127, Bias: -0.923821, T: 3916, Avg. loss: 0.433707\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 48.94, NNZs: 122, Bias: -0.931432, T: 4895, Avg. loss: 0.442585\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 49.37, NNZs: 124, Bias: -0.972571, T: 5874, Avg. loss: 0.435868\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 49.73, NNZs: 119, Bias: -0.992286, T: 6853, Avg. loss: 0.435916\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 50.42, NNZs: 353, Bias: -0.147754, T: 979, Avg. loss: 0.535188\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 53.65, NNZs: 211, Bias: -0.103256, T: 1958, Avg. loss: 0.331312\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 55.32, NNZs: 190, Bias: -0.100107, T: 2937, Avg. loss: 0.342951\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 56.49, NNZs: 196, Bias: -0.131344, T: 3916, Avg. loss: 0.336081\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 57.35, NNZs: 191, Bias: -0.122955, T: 4895, Avg. loss: 0.328042\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 58.00, NNZs: 185, Bias: -0.116227, T: 5874, Avg. loss: 0.322680\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 58.55, NNZs: 184, Bias: -0.123806, T: 6853, Avg. loss: 0.320039\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 59.02, NNZs: 183, Bias: -0.127076, T: 7832, Avg. loss: 0.317197\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 59.42, NNZs: 186, Bias: -0.129190, T: 8811, Avg. loss: 0.313552\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 59.79, NNZs: 183, Bias: -0.132887, T: 9790, Avg. loss: 0.311584\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 60.11, NNZs: 187, Bias: -0.126072, T: 10769, Avg. loss: 0.310529\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 60.41, NNZs: 188, Bias: -0.128974, T: 11748, Avg. loss: 0.308529\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 60.68, NNZs: 184, Bias: -0.129759, T: 12727, Avg. loss: 0.307078\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 60.92, NNZs: 183, Bias: -0.133864, T: 13706, Avg. loss: 0.305879\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 61.14, NNZs: 185, Bias: -0.133888, T: 14685, Avg. loss: 0.304676\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 61.35, NNZs: 183, Bias: -0.133443, T: 15664, Avg. loss: 0.303278\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 61.54, NNZs: 183, Bias: -0.131207, T: 16643, Avg. loss: 0.302506\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 61.74, NNZs: 183, Bias: -0.135777, T: 17622, Avg. loss: 0.302047\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 61.91, NNZs: 183, Bias: -0.132617, T: 18601, Avg. loss: 0.300825\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 62.08, NNZs: 183, Bias: -0.134219, T: 19580, Avg. loss: 0.300238\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 62.23, NNZs: 183, Bias: -0.139669, T: 20559, Avg. loss: 0.299854\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 62.38, NNZs: 183, Bias: -0.135927, T: 21538, Avg. loss: 0.298745\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 62.52, NNZs: 182, Bias: -0.137728, T: 22517, Avg. loss: 0.298215\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 62.66, NNZs: 182, Bias: -0.135260, T: 23496, Avg. loss: 0.297768\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 62.79, NNZs: 183, Bias: -0.140364, T: 24475, Avg. loss: 0.297032\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 62.92, NNZs: 184, Bias: -0.140650, T: 25454, Avg. loss: 0.296988\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 63.03, NNZs: 184, Bias: -0.136912, T: 26433, Avg. loss: 0.296167\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 27 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 63.28, NNZs: 359, Bias: -0.882241, T: 979, Avg. loss: 0.510236\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 65.79, NNZs: 239, Bias: -0.996941, T: 1958, Avg. loss: 0.222922\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 67.10, NNZs: 204, Bias: -1.007728, T: 2937, Avg. loss: 0.186391\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 67.92, NNZs: 188, Bias: -1.011804, T: 3916, Avg. loss: 0.172243\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 68.71, NNZs: 189, Bias: -1.011174, T: 4895, Avg. loss: 0.161068\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 69.36, NNZs: 191, Bias: -1.013073, T: 5874, Avg. loss: 0.153946\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 69.92, NNZs: 183, Bias: -1.034419, T: 6853, Avg. loss: 0.148524\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 70.38, NNZs: 177, Bias: -1.026828, T: 7832, Avg. loss: 0.144983\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 70.76, NNZs: 172, Bias: -1.010481, T: 8811, Avg. loss: 0.138407\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 71.07, NNZs: 174, Bias: -0.998762, T: 9790, Avg. loss: 0.134577\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 71.36, NNZs: 173, Bias: -1.020326, T: 10769, Avg. loss: 0.134768\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 71.67, NNZs: 169, Bias: -0.998367, T: 11748, Avg. loss: 0.132573\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 71.93, NNZs: 165, Bias: -1.001333, T: 12727, Avg. loss: 0.132481\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 72.15, NNZs: 168, Bias: -1.002174, T: 13706, Avg. loss: 0.130997\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 72.35, NNZs: 169, Bias: -1.009593, T: 14685, Avg. loss: 0.129465\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 72.55, NNZs: 169, Bias: -1.006008, T: 15664, Avg. loss: 0.130116\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 72.74, NNZs: 167, Bias: -1.003615, T: 16643, Avg. loss: 0.128431\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 72.93, NNZs: 167, Bias: -1.003722, T: 17622, Avg. loss: 0.128678\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 73.08, NNZs: 164, Bias: -1.006036, T: 18601, Avg. loss: 0.127100\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 73.24, NNZs: 167, Bias: -1.002760, T: 19580, Avg. loss: 0.127653\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 73.40, NNZs: 162, Bias: -1.003679, T: 20559, Avg. loss: 0.127174\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 73.54, NNZs: 162, Bias: -1.000593, T: 21538, Avg. loss: 0.126048\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 73.66, NNZs: 160, Bias: -1.007695, T: 22517, Avg. loss: 0.126400\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 73.80, NNZs: 157, Bias: -1.001799, T: 23496, Avg. loss: 0.126401\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 73.92, NNZs: 162, Bias: -1.005629, T: 24475, Avg. loss: 0.126648\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 74.04, NNZs: 162, Bias: -0.998876, T: 25454, Avg. loss: 0.125968\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 74.14, NNZs: 164, Bias: -1.003822, T: 26433, Avg. loss: 0.126168\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 27 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 43.99, NNZs: 314, Bias: -0.671629, T: 979, Avg. loss: 0.581116\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 46.41, NNZs: 155, Bias: -0.827488, T: 1958, Avg. loss: 0.431597\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47.59, NNZs: 127, Bias: -0.908720, T: 2937, Avg. loss: 0.448487\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 48.36, NNZs: 122, Bias: -0.963402, T: 3916, Avg. loss: 0.452817\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 48.94, NNZs: 115, Bias: -0.973575, T: 4895, Avg. loss: 0.458861\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 49.37, NNZs: 108, Bias: -1.001184, T: 5874, Avg. loss: 0.453535\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 49.72, NNZs: 101, Bias: -1.006123, T: 6853, Avg. loss: 0.454876\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 50.42, NNZs: 352, Bias: -0.231462, T: 979, Avg. loss: 0.529544\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 53.78, NNZs: 216, Bias: -0.181892, T: 1958, Avg. loss: 0.335389\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 55.54, NNZs: 196, Bias: -0.207491, T: 2937, Avg. loss: 0.344688\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 56.67, NNZs: 194, Bias: -0.236026, T: 3916, Avg. loss: 0.335786\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 57.44, NNZs: 190, Bias: -0.240694, T: 4895, Avg. loss: 0.328470\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 58.07, NNZs: 188, Bias: -0.217341, T: 5874, Avg. loss: 0.325196\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 58.62, NNZs: 183, Bias: -0.234101, T: 6853, Avg. loss: 0.321991\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 59.08, NNZs: 187, Bias: -0.240020, T: 7832, Avg. loss: 0.318156\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 59.46, NNZs: 183, Bias: -0.247878, T: 8811, Avg. loss: 0.316803\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 59.80, NNZs: 183, Bias: -0.252859, T: 9790, Avg. loss: 0.314661\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 60.11, NNZs: 185, Bias: -0.250884, T: 10769, Avg. loss: 0.313379\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 60.38, NNZs: 183, Bias: -0.245163, T: 11748, Avg. loss: 0.311204\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 60.64, NNZs: 184, Bias: -0.249590, T: 12727, Avg. loss: 0.309854\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 60.86, NNZs: 183, Bias: -0.258790, T: 13706, Avg. loss: 0.308626\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 61.08, NNZs: 185, Bias: -0.256827, T: 14685, Avg. loss: 0.308026\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 61.28, NNZs: 183, Bias: -0.253378, T: 15664, Avg. loss: 0.306740\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 61.47, NNZs: 182, Bias: -0.259758, T: 16643, Avg. loss: 0.306251\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 61.64, NNZs: 182, Bias: -0.265768, T: 17622, Avg. loss: 0.305066\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 61.80, NNZs: 181, Bias: -0.258867, T: 18601, Avg. loss: 0.304400\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 61.97, NNZs: 181, Bias: -0.266235, T: 19580, Avg. loss: 0.303691\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 62.11, NNZs: 181, Bias: -0.267486, T: 20559, Avg. loss: 0.303343\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 62.25, NNZs: 180, Bias: -0.266184, T: 21538, Avg. loss: 0.302789\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 62.38, NNZs: 180, Bias: -0.266743, T: 22517, Avg. loss: 0.302390\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 23 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 62.92, NNZs: 367, Bias: -0.822010, T: 979, Avg. loss: 0.511046\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 65.37, NNZs: 242, Bias: -0.970757, T: 1958, Avg. loss: 0.219981\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 66.87, NNZs: 218, Bias: -1.002016, T: 2937, Avg. loss: 0.193649\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 67.94, NNZs: 209, Bias: -1.006826, T: 3916, Avg. loss: 0.181335\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 68.67, NNZs: 206, Bias: -1.022223, T: 4895, Avg. loss: 0.167969\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 69.26, NNZs: 204, Bias: -1.002480, T: 5874, Avg. loss: 0.154063\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 69.78, NNZs: 192, Bias: -1.030964, T: 6853, Avg. loss: 0.155090\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 70.29, NNZs: 191, Bias: -1.015435, T: 7832, Avg. loss: 0.150021\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 70.70, NNZs: 190, Bias: -1.008624, T: 8811, Avg. loss: 0.144776\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 71.01, NNZs: 190, Bias: -1.015050, T: 9790, Avg. loss: 0.140008\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 71.37, NNZs: 190, Bias: -1.023870, T: 10769, Avg. loss: 0.141340\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 71.65, NNZs: 190, Bias: -1.003103, T: 11748, Avg. loss: 0.137989\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 71.93, NNZs: 187, Bias: -1.000910, T: 12727, Avg. loss: 0.136961\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 72.14, NNZs: 188, Bias: -1.002610, T: 13706, Avg. loss: 0.135400\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 72.38, NNZs: 189, Bias: -1.004586, T: 14685, Avg. loss: 0.134106\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 72.58, NNZs: 187, Bias: -1.010236, T: 15664, Avg. loss: 0.133510\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 72.80, NNZs: 186, Bias: -1.004228, T: 16643, Avg. loss: 0.132231\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 72.98, NNZs: 187, Bias: -1.004411, T: 17622, Avg. loss: 0.131394\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 73.17, NNZs: 190, Bias: -1.003410, T: 18601, Avg. loss: 0.130293\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 73.34, NNZs: 187, Bias: -1.002916, T: 19580, Avg. loss: 0.130704\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 73.48, NNZs: 186, Bias: -1.004450, T: 20559, Avg. loss: 0.128695\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 73.65, NNZs: 186, Bias: -1.001536, T: 21538, Avg. loss: 0.128894\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 73.77, NNZs: 187, Bias: -1.007978, T: 22517, Avg. loss: 0.128421\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 73.93, NNZs: 185, Bias: -1.001956, T: 23496, Avg. loss: 0.128379\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 74.06, NNZs: 185, Bias: -1.007020, T: 24475, Avg. loss: 0.127843\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 74.20, NNZs: 186, Bias: -0.999885, T: 25454, Avg. loss: 0.127243\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 26 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 44.92, NNZs: 330, Bias: -0.715963, T: 979, Avg. loss: 0.564683\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 47.36, NNZs: 155, Bias: -0.789308, T: 1958, Avg. loss: 0.423305\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48.52, NNZs: 129, Bias: -0.867739, T: 2937, Avg. loss: 0.442963\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 49.31, NNZs: 121, Bias: -0.907486, T: 3916, Avg. loss: 0.446923\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 49.91, NNZs: 111, Bias: -0.916680, T: 4895, Avg. loss: 0.453590\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 50.36, NNZs: 115, Bias: -0.955498, T: 5874, Avg. loss: 0.446510\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 50.71, NNZs: 109, Bias: -0.967735, T: 6853, Avg. loss: 0.448730\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 50.76, NNZs: 331, Bias: -0.158094, T: 979, Avg. loss: 0.527170\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 53.92, NNZs: 216, Bias: -0.157252, T: 1958, Avg. loss: 0.342386\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 55.55, NNZs: 199, Bias: -0.147670, T: 2937, Avg. loss: 0.350584\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 56.64, NNZs: 192, Bias: -0.176832, T: 3916, Avg. loss: 0.345687\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 57.45, NNZs: 192, Bias: -0.176198, T: 4895, Avg. loss: 0.337773\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 58.07, NNZs: 188, Bias: -0.160673, T: 5874, Avg. loss: 0.333741\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 58.58, NNZs: 188, Bias: -0.177955, T: 6853, Avg. loss: 0.329452\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 59.04, NNZs: 186, Bias: -0.180469, T: 7832, Avg. loss: 0.326596\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 59.43, NNZs: 186, Bias: -0.180617, T: 8811, Avg. loss: 0.324819\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 59.76, NNZs: 184, Bias: -0.179396, T: 9790, Avg. loss: 0.321074\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 60.08, NNZs: 184, Bias: -0.179280, T: 10769, Avg. loss: 0.319719\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 60.36, NNZs: 185, Bias: -0.173276, T: 11748, Avg. loss: 0.317398\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 60.62, NNZs: 183, Bias: -0.180839, T: 12727, Avg. loss: 0.316184\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 60.86, NNZs: 182, Bias: -0.183595, T: 13706, Avg. loss: 0.314873\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 61.08, NNZs: 182, Bias: -0.183487, T: 14685, Avg. loss: 0.313342\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 61.28, NNZs: 181, Bias: -0.177939, T: 15664, Avg. loss: 0.312136\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 61.47, NNZs: 180, Bias: -0.180799, T: 16643, Avg. loss: 0.311299\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 61.65, NNZs: 179, Bias: -0.187195, T: 17622, Avg. loss: 0.310034\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 61.82, NNZs: 184, Bias: -0.186275, T: 18601, Avg. loss: 0.309328\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 61.98, NNZs: 186, Bias: -0.187310, T: 19580, Avg. loss: 0.308823\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 62.13, NNZs: 186, Bias: -0.188408, T: 20559, Avg. loss: 0.308201\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 62.27, NNZs: 186, Bias: -0.186648, T: 21538, Avg. loss: 0.307014\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 62.40, NNZs: 186, Bias: -0.192127, T: 22517, Avg. loss: 0.306503\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 62.53, NNZs: 185, Bias: -0.189689, T: 23496, Avg. loss: 0.305914\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 62.66, NNZs: 185, Bias: -0.191326, T: 24475, Avg. loss: 0.305431\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 62.78, NNZs: 184, Bias: -0.192544, T: 25454, Avg. loss: 0.305169\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 62.90, NNZs: 183, Bias: -0.192487, T: 26433, Avg. loss: 0.304392\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 27 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 63.23, NNZs: 339, Bias: -0.823562, T: 979, Avg. loss: 0.507524\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 65.70, NNZs: 211, Bias: -0.968293, T: 1958, Avg. loss: 0.226296\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 67.21, NNZs: 184, Bias: -1.021681, T: 2937, Avg. loss: 0.191363\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 68.13, NNZs: 173, Bias: -1.008877, T: 3916, Avg. loss: 0.171499\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 68.84, NNZs: 177, Bias: -1.026578, T: 4895, Avg. loss: 0.162353\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 69.49, NNZs: 174, Bias: -0.993287, T: 5874, Avg. loss: 0.149848\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 69.98, NNZs: 173, Bias: -1.043688, T: 6853, Avg. loss: 0.151107\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 70.48, NNZs: 174, Bias: -1.007771, T: 7832, Avg. loss: 0.143013\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 70.85, NNZs: 170, Bias: -1.006781, T: 8811, Avg. loss: 0.140429\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 71.20, NNZs: 176, Bias: -1.008219, T: 9790, Avg. loss: 0.137870\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 71.49, NNZs: 179, Bias: -1.023730, T: 10769, Avg. loss: 0.134684\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 71.80, NNZs: 175, Bias: -0.996889, T: 11748, Avg. loss: 0.133142\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 72.05, NNZs: 173, Bias: -0.997450, T: 12727, Avg. loss: 0.131234\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 72.28, NNZs: 174, Bias: -1.000945, T: 13706, Avg. loss: 0.130164\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 72.49, NNZs: 176, Bias: -1.016232, T: 14685, Avg. loss: 0.128423\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 72.70, NNZs: 172, Bias: -1.003568, T: 15664, Avg. loss: 0.127284\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 72.87, NNZs: 175, Bias: -1.007940, T: 16643, Avg. loss: 0.126060\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 73.07, NNZs: 175, Bias: -1.008042, T: 17622, Avg. loss: 0.125691\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 73.24, NNZs: 175, Bias: -0.999451, T: 18601, Avg. loss: 0.124002\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 73.40, NNZs: 177, Bias: -1.006542, T: 19580, Avg. loss: 0.124709\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 73.56, NNZs: 175, Bias: -1.003405, T: 20559, Avg. loss: 0.122848\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 73.73, NNZs: 174, Bias: -1.001786, T: 21538, Avg. loss: 0.123346\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 73.86, NNZs: 176, Bias: -1.010124, T: 22517, Avg. loss: 0.122475\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 73.99, NNZs: 178, Bias: -1.002452, T: 23496, Avg. loss: 0.120387\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 74.12, NNZs: 176, Bias: -1.005167, T: 24475, Avg. loss: 0.121073\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 74.25, NNZs: 176, Bias: -1.000717, T: 25454, Avg. loss: 0.120583\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 74.37, NNZs: 180, Bias: -1.004241, T: 26433, Avg. loss: 0.120340\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 74.49, NNZs: 182, Bias: -1.000946, T: 27412, Avg. loss: 0.119854\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 74.61, NNZs: 182, Bias: -1.002622, T: 28391, Avg. loss: 0.119389\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 29 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 44.40, NNZs: 329, Bias: -0.688948, T: 979, Avg. loss: 0.566274\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 46.76, NNZs: 155, Bias: -0.770482, T: 1958, Avg. loss: 0.434676\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47.90, NNZs: 125, Bias: -0.864519, T: 2937, Avg. loss: 0.450979\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 48.66, NNZs: 119, Bias: -0.897855, T: 3916, Avg. loss: 0.458238\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 49.23, NNZs: 118, Bias: -0.916036, T: 4895, Avg. loss: 0.460866\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 49.66, NNZs: 115, Bias: -0.957817, T: 5874, Avg. loss: 0.457222\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 50.01, NNZs: 117, Bias: -0.978042, T: 6853, Avg. loss: 0.457207\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 49.69, NNZs: 344, Bias: -0.136518, T: 979, Avg. loss: 0.535491\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 53.01, NNZs: 204, Bias: -0.160879, T: 1958, Avg. loss: 0.347216\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 54.80, NNZs: 194, Bias: -0.162166, T: 2937, Avg. loss: 0.357428\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 55.94, NNZs: 189, Bias: -0.201522, T: 3916, Avg. loss: 0.346535\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.80, NNZs: 183, Bias: -0.198253, T: 4895, Avg. loss: 0.336430\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 57.47, NNZs: 180, Bias: -0.198101, T: 5874, Avg. loss: 0.332516\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 58.01, NNZs: 181, Bias: -0.219732, T: 6853, Avg. loss: 0.327069\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 58.48, NNZs: 179, Bias: -0.212675, T: 7832, Avg. loss: 0.324022\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 58.88, NNZs: 179, Bias: -0.223895, T: 8811, Avg. loss: 0.321573\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 59.24, NNZs: 181, Bias: -0.226248, T: 9790, Avg. loss: 0.318312\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 59.57, NNZs: 178, Bias: -0.220504, T: 10769, Avg. loss: 0.315961\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 59.86, NNZs: 176, Bias: -0.217420, T: 11748, Avg. loss: 0.313780\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 60.12, NNZs: 177, Bias: -0.225657, T: 12727, Avg. loss: 0.312898\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 60.36, NNZs: 177, Bias: -0.227072, T: 13706, Avg. loss: 0.311145\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 60.58, NNZs: 178, Bias: -0.227686, T: 14685, Avg. loss: 0.310261\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 60.79, NNZs: 176, Bias: -0.226593, T: 15664, Avg. loss: 0.309418\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 60.99, NNZs: 174, Bias: -0.224368, T: 16643, Avg. loss: 0.308633\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 61.17, NNZs: 176, Bias: -0.232568, T: 17622, Avg. loss: 0.307566\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 61.33, NNZs: 178, Bias: -0.227264, T: 18601, Avg. loss: 0.306791\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 61.49, NNZs: 178, Bias: -0.230964, T: 19580, Avg. loss: 0.306514\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 61.65, NNZs: 178, Bias: -0.228591, T: 20559, Avg. loss: 0.305677\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 61.79, NNZs: 178, Bias: -0.229379, T: 21538, Avg. loss: 0.305053\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 61.93, NNZs: 178, Bias: -0.233854, T: 22517, Avg. loss: 0.304858\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 23 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 60.99, NNZs: 353, Bias: -0.853692, T: 979, Avg. loss: 0.498717\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 63.87, NNZs: 215, Bias: -1.001440, T: 1958, Avg. loss: 0.240908\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 65.15, NNZs: 191, Bias: -1.017591, T: 2937, Avg. loss: 0.196019\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 66.24, NNZs: 188, Bias: -1.022183, T: 3916, Avg. loss: 0.183676\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 66.98, NNZs: 191, Bias: -1.040965, T: 4895, Avg. loss: 0.172529\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 67.60, NNZs: 185, Bias: -1.001979, T: 5874, Avg. loss: 0.157526\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 68.07, NNZs: 187, Bias: -1.016386, T: 6853, Avg. loss: 0.154298\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 68.59, NNZs: 182, Bias: -1.011110, T: 7832, Avg. loss: 0.150047\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 68.96, NNZs: 185, Bias: -1.005627, T: 8811, Avg. loss: 0.145055\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 69.28, NNZs: 184, Bias: -1.008420, T: 9790, Avg. loss: 0.142124\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 69.63, NNZs: 182, Bias: -1.019812, T: 10769, Avg. loss: 0.141491\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 69.93, NNZs: 183, Bias: -0.997093, T: 11748, Avg. loss: 0.138488\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 70.19, NNZs: 180, Bias: -1.000901, T: 12727, Avg. loss: 0.138106\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 70.41, NNZs: 179, Bias: -1.003544, T: 13706, Avg. loss: 0.136232\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 70.64, NNZs: 181, Bias: -1.009752, T: 14685, Avg. loss: 0.134515\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 70.85, NNZs: 179, Bias: -1.003623, T: 15664, Avg. loss: 0.132901\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 71.06, NNZs: 179, Bias: -1.008352, T: 16643, Avg. loss: 0.132323\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 71.27, NNZs: 176, Bias: -1.003694, T: 17622, Avg. loss: 0.131210\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 71.43, NNZs: 178, Bias: -1.001613, T: 18601, Avg. loss: 0.129778\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 71.61, NNZs: 174, Bias: -1.004545, T: 19580, Avg. loss: 0.129300\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 71.78, NNZs: 177, Bias: -1.002173, T: 20559, Avg. loss: 0.128687\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 71.92, NNZs: 174, Bias: -1.001845, T: 21538, Avg. loss: 0.128369\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 72.08, NNZs: 177, Bias: -1.004591, T: 22517, Avg. loss: 0.129039\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 72.22, NNZs: 170, Bias: -1.001853, T: 23496, Avg. loss: 0.127874\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 45.31, NNZs: 324, Bias: -0.735673, T: 979, Avg. loss: 0.575528\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 47.65, NNZs: 165, Bias: -0.765293, T: 1958, Avg. loss: 0.442124\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48.90, NNZs: 134, Bias: -0.870043, T: 2937, Avg. loss: 0.453608\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 49.69, NNZs: 127, Bias: -0.922063, T: 3916, Avg. loss: 0.454637\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 50.28, NNZs: 117, Bias: -0.922641, T: 4895, Avg. loss: 0.462478\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 50.73, NNZs: 119, Bias: -0.964153, T: 5874, Avg. loss: 0.456467\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 51.10, NNZs: 113, Bias: -0.978583, T: 6853, Avg. loss: 0.457186\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 49.88, NNZs: 309, Bias: -0.122427, T: 979, Avg. loss: 0.524468\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 52.97, NNZs: 192, Bias: -0.123847, T: 1958, Avg. loss: 0.337358\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 54.64, NNZs: 189, Bias: -0.109118, T: 2937, Avg. loss: 0.345498\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 55.77, NNZs: 183, Bias: -0.133711, T: 3916, Avg. loss: 0.336288\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.59, NNZs: 173, Bias: -0.123495, T: 4895, Avg. loss: 0.327393\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 57.26, NNZs: 172, Bias: -0.117476, T: 5874, Avg. loss: 0.320825\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 57.79, NNZs: 173, Bias: -0.133757, T: 6853, Avg. loss: 0.316295\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 58.24, NNZs: 171, Bias: -0.133381, T: 7832, Avg. loss: 0.312683\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 58.60, NNZs: 171, Bias: -0.142854, T: 8811, Avg. loss: 0.311533\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 58.95, NNZs: 171, Bias: -0.144402, T: 9790, Avg. loss: 0.309166\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 59.26, NNZs: 170, Bias: -0.135098, T: 10769, Avg. loss: 0.307282\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 59.53, NNZs: 171, Bias: -0.135629, T: 11748, Avg. loss: 0.306058\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 59.79, NNZs: 172, Bias: -0.139160, T: 12727, Avg. loss: 0.304720\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 60.02, NNZs: 174, Bias: -0.144003, T: 13706, Avg. loss: 0.303608\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 60.23, NNZs: 174, Bias: -0.144922, T: 14685, Avg. loss: 0.302157\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 60.42, NNZs: 174, Bias: -0.140145, T: 15664, Avg. loss: 0.301417\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 60.62, NNZs: 175, Bias: -0.142990, T: 16643, Avg. loss: 0.300907\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 60.80, NNZs: 175, Bias: -0.146499, T: 17622, Avg. loss: 0.299789\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 60.97, NNZs: 175, Bias: -0.142824, T: 18601, Avg. loss: 0.298565\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 61.11, NNZs: 176, Bias: -0.142677, T: 19580, Avg. loss: 0.297941\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 61.27, NNZs: 175, Bias: -0.141467, T: 20559, Avg. loss: 0.297985\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 61.41, NNZs: 175, Bias: -0.139443, T: 21538, Avg. loss: 0.296496\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 61.55, NNZs: 175, Bias: -0.146939, T: 22517, Avg. loss: 0.296093\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 61.67, NNZs: 175, Bias: -0.144242, T: 23496, Avg. loss: 0.295537\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 61.80, NNZs: 175, Bias: -0.145221, T: 24475, Avg. loss: 0.295301\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 61.92, NNZs: 175, Bias: -0.144563, T: 25454, Avg. loss: 0.294795\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 62.03, NNZs: 176, Bias: -0.147418, T: 26433, Avg. loss: 0.294106\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 27 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 61.65, NNZs: 347, Bias: -0.834834, T: 979, Avg. loss: 0.457723\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 64.03, NNZs: 231, Bias: -1.008619, T: 1958, Avg. loss: 0.224176\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 65.37, NNZs: 198, Bias: -1.014983, T: 2937, Avg. loss: 0.180691\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 66.30, NNZs: 179, Bias: -1.007373, T: 3916, Avg. loss: 0.168178\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 67.04, NNZs: 178, Bias: -1.023530, T: 4895, Avg. loss: 0.162531\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 67.72, NNZs: 180, Bias: -1.000769, T: 5874, Avg. loss: 0.148171\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 68.15, NNZs: 174, Bias: -1.017487, T: 6853, Avg. loss: 0.146508\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 68.61, NNZs: 177, Bias: -1.026252, T: 7832, Avg. loss: 0.144075\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 69.00, NNZs: 178, Bias: -1.003877, T: 8811, Avg. loss: 0.138710\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 69.35, NNZs: 180, Bias: -1.003421, T: 9790, Avg. loss: 0.136069\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 69.66, NNZs: 181, Bias: -1.027167, T: 10769, Avg. loss: 0.135609\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 69.95, NNZs: 177, Bias: -1.000080, T: 11748, Avg. loss: 0.131915\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 70.19, NNZs: 180, Bias: -1.001953, T: 12727, Avg. loss: 0.131508\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 70.42, NNZs: 180, Bias: -1.004279, T: 13706, Avg. loss: 0.130233\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 70.64, NNZs: 181, Bias: -1.006172, T: 14685, Avg. loss: 0.129938\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 70.84, NNZs: 178, Bias: -1.000628, T: 15664, Avg. loss: 0.128622\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 71.03, NNZs: 175, Bias: -1.005931, T: 16643, Avg. loss: 0.127638\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 71.23, NNZs: 180, Bias: -1.004775, T: 17622, Avg. loss: 0.126974\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 71.40, NNZs: 176, Bias: -1.000387, T: 18601, Avg. loss: 0.125082\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 71.53, NNZs: 176, Bias: -1.004159, T: 19580, Avg. loss: 0.124684\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 71.67, NNZs: 169, Bias: -1.005487, T: 20559, Avg. loss: 0.123425\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 71.82, NNZs: 174, Bias: -1.000012, T: 21538, Avg. loss: 0.122481\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 71.96, NNZs: 170, Bias: -1.007635, T: 22517, Avg. loss: 0.122559\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 72.09, NNZs: 171, Bias: -1.005272, T: 23496, Avg. loss: 0.122691\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 72.21, NNZs: 172, Bias: -1.007597, T: 24475, Avg. loss: 0.121342\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 72.35, NNZs: 174, Bias: -0.997287, T: 25454, Avg. loss: 0.120957\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 72.44, NNZs: 172, Bias: -1.003081, T: 26433, Avg. loss: 0.120222\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 72.56, NNZs: 173, Bias: -0.999194, T: 27412, Avg. loss: 0.119801\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 72.67, NNZs: 166, Bias: -0.999905, T: 28391, Avg. loss: 0.120239\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 72.77, NNZs: 171, Bias: -1.001935, T: 29370, Avg. loss: 0.119523\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 30 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 44.36, NNZs: 310, Bias: -0.693144, T: 979, Avg. loss: 0.546812\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 46.61, NNZs: 151, Bias: -0.755971, T: 1958, Avg. loss: 0.413391\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47.82, NNZs: 126, Bias: -0.844312, T: 2937, Avg. loss: 0.436142\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 48.55, NNZs: 112, Bias: -0.891562, T: 3916, Avg. loss: 0.432291\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 49.11, NNZs: 112, Bias: -0.898414, T: 4895, Avg. loss: 0.440934\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 49.52, NNZs: 114, Bias: -0.928661, T: 5874, Avg. loss: 0.436341\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 49.86, NNZs: 113, Bias: -0.948447, T: 6853, Avg. loss: 0.437731\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 50.06, NNZs: 367, Bias: -0.133381, T: 980, Avg. loss: 0.535097\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 53.33, NNZs: 209, Bias: -0.117425, T: 1960, Avg. loss: 0.350643\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 55.07, NNZs: 195, Bias: -0.152540, T: 2940, Avg. loss: 0.354559\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 56.17, NNZs: 187, Bias: -0.155222, T: 3920, Avg. loss: 0.343887\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 57.05, NNZs: 185, Bias: -0.177132, T: 4900, Avg. loss: 0.335874\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 57.69, NNZs: 184, Bias: -0.166909, T: 5880, Avg. loss: 0.330475\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 58.27, NNZs: 181, Bias: -0.176313, T: 6860, Avg. loss: 0.325886\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 58.73, NNZs: 181, Bias: -0.183290, T: 7840, Avg. loss: 0.323311\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 59.13, NNZs: 177, Bias: -0.192077, T: 8820, Avg. loss: 0.319674\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 59.50, NNZs: 178, Bias: -0.193601, T: 9800, Avg. loss: 0.317771\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 59.81, NNZs: 179, Bias: -0.198831, T: 10780, Avg. loss: 0.315474\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 60.11, NNZs: 175, Bias: -0.190504, T: 11760, Avg. loss: 0.314707\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 60.37, NNZs: 179, Bias: -0.201871, T: 12740, Avg. loss: 0.313446\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 60.61, NNZs: 177, Bias: -0.203994, T: 13720, Avg. loss: 0.312030\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 60.83, NNZs: 177, Bias: -0.203261, T: 14700, Avg. loss: 0.311194\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 61.04, NNZs: 176, Bias: -0.209709, T: 15680, Avg. loss: 0.310558\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 61.24, NNZs: 175, Bias: -0.208906, T: 16660, Avg. loss: 0.309750\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 61.41, NNZs: 175, Bias: -0.209110, T: 17640, Avg. loss: 0.308891\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 61.58, NNZs: 176, Bias: -0.211379, T: 18620, Avg. loss: 0.308511\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 19 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 65.58, NNZs: 377, Bias: -0.812498, T: 980, Avg. loss: 0.564076\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 68.03, NNZs: 275, Bias: -1.015184, T: 1960, Avg. loss: 0.226823\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 69.56, NNZs: 227, Bias: -0.989484, T: 2940, Avg. loss: 0.186978\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 70.50, NNZs: 218, Bias: -0.990692, T: 3920, Avg. loss: 0.168698\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 71.44, NNZs: 219, Bias: -1.018403, T: 4900, Avg. loss: 0.163166\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 72.12, NNZs: 209, Bias: -1.000169, T: 5880, Avg. loss: 0.150911\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 72.66, NNZs: 211, Bias: -1.027925, T: 6860, Avg. loss: 0.147810\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 73.14, NNZs: 209, Bias: -1.006540, T: 7840, Avg. loss: 0.141300\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 73.52, NNZs: 204, Bias: -1.013395, T: 8820, Avg. loss: 0.138688\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 73.89, NNZs: 204, Bias: -0.994900, T: 9800, Avg. loss: 0.135441\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 74.20, NNZs: 205, Bias: -1.009259, T: 10780, Avg. loss: 0.135491\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 74.52, NNZs: 204, Bias: -0.989825, T: 11760, Avg. loss: 0.133498\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 74.79, NNZs: 204, Bias: -1.015739, T: 12740, Avg. loss: 0.133480\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 75.07, NNZs: 204, Bias: -1.002279, T: 13720, Avg. loss: 0.131460\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 75.31, NNZs: 201, Bias: -1.001131, T: 14700, Avg. loss: 0.129470\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 75.51, NNZs: 200, Bias: -1.003864, T: 15680, Avg. loss: 0.127499\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 75.71, NNZs: 201, Bias: -0.999518, T: 16660, Avg. loss: 0.127074\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 75.91, NNZs: 204, Bias: -0.992403, T: 17640, Avg. loss: 0.126394\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 76.10, NNZs: 204, Bias: -0.996799, T: 18620, Avg. loss: 0.126337\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 76.25, NNZs: 204, Bias: -1.004270, T: 19600, Avg. loss: 0.125780\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 76.43, NNZs: 201, Bias: -0.999291, T: 20580, Avg. loss: 0.124888\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 21 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 43.51, NNZs: 292, Bias: -0.628220, T: 980, Avg. loss: 0.588166\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 46.23, NNZs: 158, Bias: -0.672997, T: 1960, Avg. loss: 0.445872\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47.50, NNZs: 135, Bias: -0.764058, T: 2940, Avg. loss: 0.460600\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 48.27, NNZs: 126, Bias: -0.824384, T: 3920, Avg. loss: 0.460675\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 48.85, NNZs: 125, Bias: -0.840197, T: 4900, Avg. loss: 0.461369\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 49.29, NNZs: 124, Bias: -0.873936, T: 5880, Avg. loss: 0.458553\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 49.66, NNZs: 122, Bias: -0.905019, T: 6860, Avg. loss: 0.456355\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 49.43, NNZs: 361, Bias: -0.026323, T: 980, Avg. loss: 0.527671\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 52.90, NNZs: 209, Bias: 0.013049, T: 1960, Avg. loss: 0.339346\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 54.76, NNZs: 200, Bias: -0.060283, T: 2940, Avg. loss: 0.351411\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 55.89, NNZs: 195, Bias: -0.076486, T: 3920, Avg. loss: 0.339158\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.76, NNZs: 188, Bias: -0.087673, T: 4900, Avg. loss: 0.333607\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 57.41, NNZs: 188, Bias: -0.077601, T: 5880, Avg. loss: 0.326620\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 57.96, NNZs: 184, Bias: -0.082017, T: 6860, Avg. loss: 0.323186\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 58.41, NNZs: 185, Bias: -0.097395, T: 7840, Avg. loss: 0.320428\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 58.82, NNZs: 186, Bias: -0.100499, T: 8820, Avg. loss: 0.317561\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 59.17, NNZs: 187, Bias: -0.096745, T: 9800, Avg. loss: 0.315218\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 59.48, NNZs: 190, Bias: -0.105167, T: 10780, Avg. loss: 0.313126\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 59.77, NNZs: 188, Bias: -0.103132, T: 11760, Avg. loss: 0.311979\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 60.03, NNZs: 189, Bias: -0.107993, T: 12740, Avg. loss: 0.310111\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 60.27, NNZs: 188, Bias: -0.110076, T: 13720, Avg. loss: 0.308770\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 60.49, NNZs: 187, Bias: -0.106134, T: 14700, Avg. loss: 0.308436\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 60.70, NNZs: 188, Bias: -0.111374, T: 15680, Avg. loss: 0.306938\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 60.89, NNZs: 188, Bias: -0.111400, T: 16660, Avg. loss: 0.306262\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 61.06, NNZs: 188, Bias: -0.110642, T: 17640, Avg. loss: 0.305382\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 61.24, NNZs: 188, Bias: -0.112361, T: 18620, Avg. loss: 0.304911\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 61.39, NNZs: 189, Bias: -0.113728, T: 19600, Avg. loss: 0.304044\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 61.55, NNZs: 190, Bias: -0.111469, T: 20580, Avg. loss: 0.303479\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 21 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 64.12, NNZs: 354, Bias: -0.855827, T: 980, Avg. loss: 0.529877\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 66.14, NNZs: 233, Bias: -1.054931, T: 1960, Avg. loss: 0.236621\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 67.43, NNZs: 189, Bias: -1.009037, T: 2940, Avg. loss: 0.184436\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 68.45, NNZs: 177, Bias: -1.021837, T: 3920, Avg. loss: 0.167941\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 69.32, NNZs: 178, Bias: -1.003341, T: 4900, Avg. loss: 0.155903\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 69.93, NNZs: 177, Bias: -1.002572, T: 5880, Avg. loss: 0.147221\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 70.44, NNZs: 172, Bias: -1.017424, T: 6860, Avg. loss: 0.143190\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 70.96, NNZs: 180, Bias: -1.011398, T: 7840, Avg. loss: 0.138212\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 71.39, NNZs: 173, Bias: -1.004276, T: 8820, Avg. loss: 0.136158\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 71.72, NNZs: 169, Bias: -0.994041, T: 9800, Avg. loss: 0.133625\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 72.05, NNZs: 170, Bias: -1.009700, T: 10780, Avg. loss: 0.134144\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 72.37, NNZs: 171, Bias: -0.999714, T: 11760, Avg. loss: 0.131234\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 72.60, NNZs: 168, Bias: -1.013058, T: 12740, Avg. loss: 0.130027\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 72.89, NNZs: 170, Bias: -0.997820, T: 13720, Avg. loss: 0.129263\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 73.10, NNZs: 168, Bias: -1.002127, T: 14700, Avg. loss: 0.129158\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 73.34, NNZs: 164, Bias: -1.000048, T: 15680, Avg. loss: 0.126760\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 73.54, NNZs: 167, Bias: -1.000178, T: 16660, Avg. loss: 0.125956\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 73.71, NNZs: 166, Bias: -0.997606, T: 17640, Avg. loss: 0.125408\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 73.89, NNZs: 163, Bias: -0.999603, T: 18620, Avg. loss: 0.124361\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 74.03, NNZs: 161, Bias: -1.006226, T: 19600, Avg. loss: 0.124324\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 74.19, NNZs: 160, Bias: -0.999967, T: 20580, Avg. loss: 0.123479\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 74.36, NNZs: 162, Bias: -0.999897, T: 21560, Avg. loss: 0.123866\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 74.48, NNZs: 160, Bias: -1.001073, T: 22540, Avg. loss: 0.121892\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 74.63, NNZs: 161, Bias: -1.000365, T: 23520, Avg. loss: 0.122838\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 74.76, NNZs: 160, Bias: -1.001770, T: 24500, Avg. loss: 0.122568\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 74.88, NNZs: 158, Bias: -1.003634, T: 25480, Avg. loss: 0.121703\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 75.00, NNZs: 158, Bias: -1.000898, T: 26460, Avg. loss: 0.121696\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 75.10, NNZs: 158, Bias: -0.998891, T: 27440, Avg. loss: 0.121354\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 28 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 44.00, NNZs: 302, Bias: -0.661705, T: 980, Avg. loss: 0.559290\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 46.64, NNZs: 154, Bias: -0.740461, T: 1960, Avg. loss: 0.426540\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47.84, NNZs: 134, Bias: -0.835515, T: 2940, Avg. loss: 0.437332\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 48.61, NNZs: 130, Bias: -0.861680, T: 3920, Avg. loss: 0.438439\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 49.21, NNZs: 124, Bias: -0.885718, T: 4900, Avg. loss: 0.436879\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 49.65, NNZs: 117, Bias: -0.906967, T: 5880, Avg. loss: 0.434266\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 50.02, NNZs: 115, Bias: -0.937800, T: 6860, Avg. loss: 0.432981\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 77.42, NNZs: 1280, Bias: -0.247901, T: 979, Avg. loss: 0.507293\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 81.52, NNZs: 1280, Bias: -0.182471, T: 1958, Avg. loss: 0.154483\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 84.64, NNZs: 1280, Bias: -0.266518, T: 2937, Avg. loss: 0.095111\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 86.83, NNZs: 1280, Bias: -0.341334, T: 3916, Avg. loss: 0.076369\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 88.48, NNZs: 1280, Bias: -0.282599, T: 4895, Avg. loss: 0.067749\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 89.81, NNZs: 1280, Bias: -0.296643, T: 5874, Avg. loss: 0.061638\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 90.96, NNZs: 1280, Bias: -0.298851, T: 6853, Avg. loss: 0.057657\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 91.92, NNZs: 1280, Bias: -0.309373, T: 7832, Avg. loss: 0.055117\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 92.76, NNZs: 1280, Bias: -0.333429, T: 8811, Avg. loss: 0.052977\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 93.50, NNZs: 1280, Bias: -0.339684, T: 9790, Avg. loss: 0.051228\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 94.15, NNZs: 1280, Bias: -0.327435, T: 10769, Avg. loss: 0.049708\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 94.75, NNZs: 1280, Bias: -0.335927, T: 11748, Avg. loss: 0.048637\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 95.29, NNZs: 1280, Bias: -0.339875, T: 12727, Avg. loss: 0.047675\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 95.79, NNZs: 1280, Bias: -0.338373, T: 13706, Avg. loss: 0.046709\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 96.25, NNZs: 1280, Bias: -0.334404, T: 14685, Avg. loss: 0.045857\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 96.67, NNZs: 1280, Bias: -0.345154, T: 15664, Avg. loss: 0.045337\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 97.07, NNZs: 1280, Bias: -0.347115, T: 16643, Avg. loss: 0.044674\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 17 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 143.57, NNZs: 1280, Bias: -0.673976, T: 979, Avg. loss: 0.912214\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 143.91, NNZs: 1280, Bias: -2.071370, T: 1958, Avg. loss: 0.240698\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 145.03, NNZs: 1280, Bias: -2.076792, T: 2937, Avg. loss: 0.089170\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 144.99, NNZs: 1280, Bias: -2.323612, T: 3916, Avg. loss: 0.052504\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 145.36, NNZs: 1280, Bias: -2.384639, T: 4895, Avg. loss: 0.035157\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 145.57, NNZs: 1280, Bias: -2.451674, T: 5874, Avg. loss: 0.031284\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 145.86, NNZs: 1280, Bias: -2.469994, T: 6853, Avg. loss: 0.027118\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 146.09, NNZs: 1280, Bias: -2.488800, T: 7832, Avg. loss: 0.025572\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 146.26, NNZs: 1280, Bias: -2.515735, T: 8811, Avg. loss: 0.024927\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 146.47, NNZs: 1280, Bias: -2.517433, T: 9790, Avg. loss: 0.022245\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 146.60, NNZs: 1280, Bias: -2.541146, T: 10769, Avg. loss: 0.023223\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 146.79, NNZs: 1280, Bias: -2.539658, T: 11748, Avg. loss: 0.021082\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 146.93, NNZs: 1280, Bias: -2.546571, T: 12727, Avg. loss: 0.021300\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 147.08, NNZs: 1280, Bias: -2.547184, T: 13706, Avg. loss: 0.020418\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 147.19, NNZs: 1280, Bias: -2.558062, T: 14685, Avg. loss: 0.021092\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 147.28, NNZs: 1280, Bias: -2.568302, T: 15664, Avg. loss: 0.020816\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 147.39, NNZs: 1280, Bias: -2.571985, T: 16643, Avg. loss: 0.019858\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 17 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 59.82, NNZs: 1280, Bias: -0.754295, T: 979, Avg. loss: 0.497600\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 65.34, NNZs: 1280, Bias: -0.772479, T: 1958, Avg. loss: 0.193065\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 69.13, NNZs: 1280, Bias: -0.673576, T: 2937, Avg. loss: 0.131367\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 71.70, NNZs: 1280, Bias: -0.696758, T: 3916, Avg. loss: 0.108470\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 73.72, NNZs: 1280, Bias: -0.676689, T: 4895, Avg. loss: 0.098690\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 75.31, NNZs: 1280, Bias: -0.684875, T: 5874, Avg. loss: 0.091013\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 76.59, NNZs: 1280, Bias: -0.698311, T: 6853, Avg. loss: 0.085701\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 77.71, NNZs: 1280, Bias: -0.688424, T: 7832, Avg. loss: 0.083077\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 78.67, NNZs: 1280, Bias: -0.695576, T: 8811, Avg. loss: 0.079391\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 79.52, NNZs: 1280, Bias: -0.690956, T: 9790, Avg. loss: 0.077651\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 80.26, NNZs: 1280, Bias: -0.703045, T: 10769, Avg. loss: 0.074561\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 80.93, NNZs: 1280, Bias: -0.705945, T: 11748, Avg. loss: 0.073358\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 81.55, NNZs: 1280, Bias: -0.696179, T: 12727, Avg. loss: 0.073173\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 82.12, NNZs: 1280, Bias: -0.696289, T: 13706, Avg. loss: 0.071165\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 82.64, NNZs: 1280, Bias: -0.697165, T: 14685, Avg. loss: 0.070082\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 83.11, NNZs: 1280, Bias: -0.697737, T: 15664, Avg. loss: 0.069092\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 83.55, NNZs: 1280, Bias: -0.703391, T: 16643, Avg. loss: 0.067648\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 83.97, NNZs: 1280, Bias: -0.701225, T: 17622, Avg. loss: 0.067722\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 84.36, NNZs: 1280, Bias: -0.699528, T: 18601, Avg. loss: 0.066983\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 84.72, NNZs: 1280, Bias: -0.699495, T: 19580, Avg. loss: 0.066152\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 85.07, NNZs: 1280, Bias: -0.698694, T: 20559, Avg. loss: 0.065626\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 85.40, NNZs: 1280, Bias: -0.698330, T: 21538, Avg. loss: 0.065008\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 22 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 76.51, NNZs: 1274, Bias: -0.286563, T: 979, Avg. loss: 0.506749\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 80.88, NNZs: 1274, Bias: -0.057043, T: 1958, Avg. loss: 0.147679\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 84.03, NNZs: 1274, Bias: -0.122912, T: 2937, Avg. loss: 0.088297\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 86.28, NNZs: 1274, Bias: -0.198568, T: 3916, Avg. loss: 0.068537\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 88.00, NNZs: 1274, Bias: -0.144306, T: 4895, Avg. loss: 0.059902\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 89.37, NNZs: 1274, Bias: -0.168158, T: 5874, Avg. loss: 0.054883\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 90.52, NNZs: 1274, Bias: -0.154998, T: 6853, Avg. loss: 0.050979\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 91.51, NNZs: 1274, Bias: -0.178723, T: 7832, Avg. loss: 0.048759\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 92.35, NNZs: 1274, Bias: -0.194303, T: 8811, Avg. loss: 0.046846\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 93.09, NNZs: 1274, Bias: -0.195581, T: 9790, Avg. loss: 0.045207\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 93.75, NNZs: 1274, Bias: -0.188086, T: 10769, Avg. loss: 0.043916\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 94.35, NNZs: 1274, Bias: -0.194057, T: 11748, Avg. loss: 0.042869\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 94.88, NNZs: 1274, Bias: -0.191584, T: 12727, Avg. loss: 0.041980\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 95.38, NNZs: 1274, Bias: -0.190916, T: 13706, Avg. loss: 0.041142\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 95.84, NNZs: 1274, Bias: -0.188383, T: 14685, Avg. loss: 0.040407\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 96.26, NNZs: 1274, Bias: -0.195195, T: 15664, Avg. loss: 0.039880\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 96.65, NNZs: 1274, Bias: -0.195927, T: 16643, Avg. loss: 0.039298\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 17 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 147.01, NNZs: 1274, Bias: -0.519273, T: 979, Avg. loss: 0.934855\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 146.32, NNZs: 1274, Bias: -1.857112, T: 1958, Avg. loss: 0.270378\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 146.30, NNZs: 1274, Bias: -2.208504, T: 2937, Avg. loss: 0.084464\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 146.54, NNZs: 1274, Bias: -2.363841, T: 3916, Avg. loss: 0.046667\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 146.94, NNZs: 1274, Bias: -2.406134, T: 4895, Avg. loss: 0.033478\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 147.15, NNZs: 1274, Bias: -2.471930, T: 5874, Avg. loss: 0.029246\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 147.44, NNZs: 1274, Bias: -2.490763, T: 6853, Avg. loss: 0.024723\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 147.67, NNZs: 1274, Bias: -2.510658, T: 7832, Avg. loss: 0.023354\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 147.88, NNZs: 1274, Bias: -2.525568, T: 8811, Avg. loss: 0.021964\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 148.11, NNZs: 1274, Bias: -2.520924, T: 9790, Avg. loss: 0.019611\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 148.26, NNZs: 1274, Bias: -2.544423, T: 10769, Avg. loss: 0.020815\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 148.43, NNZs: 1274, Bias: -2.546092, T: 11748, Avg. loss: 0.018930\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 148.59, NNZs: 1274, Bias: -2.550833, T: 12727, Avg. loss: 0.018781\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 148.74, NNZs: 1274, Bias: -2.552484, T: 13706, Avg. loss: 0.018161\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 148.85, NNZs: 1274, Bias: -2.562480, T: 14685, Avg. loss: 0.018661\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 15 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 60.24, NNZs: 1274, Bias: -0.947002, T: 979, Avg. loss: 0.492157\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 66.12, NNZs: 1274, Bias: -0.870691, T: 1958, Avg. loss: 0.180181\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 69.98, NNZs: 1274, Bias: -0.790063, T: 2937, Avg. loss: 0.117866\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 72.59, NNZs: 1274, Bias: -0.824757, T: 3916, Avg. loss: 0.096366\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 74.59, NNZs: 1274, Bias: -0.809180, T: 4895, Avg. loss: 0.087001\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 76.16, NNZs: 1274, Bias: -0.817728, T: 5874, Avg. loss: 0.079830\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 77.44, NNZs: 1274, Bias: -0.832482, T: 6853, Avg. loss: 0.074749\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 78.54, NNZs: 1274, Bias: -0.825418, T: 7832, Avg. loss: 0.072441\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 79.49, NNZs: 1274, Bias: -0.826090, T: 8811, Avg. loss: 0.069457\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 80.32, NNZs: 1274, Bias: -0.829215, T: 9790, Avg. loss: 0.067140\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 81.06, NNZs: 1274, Bias: -0.832709, T: 10769, Avg. loss: 0.065426\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 81.73, NNZs: 1274, Bias: -0.832509, T: 11748, Avg. loss: 0.064146\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 82.33, NNZs: 1274, Bias: -0.830628, T: 12727, Avg. loss: 0.062973\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 82.88, NNZs: 1274, Bias: -0.834115, T: 13706, Avg. loss: 0.061446\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 83.39, NNZs: 1274, Bias: -0.836494, T: 14685, Avg. loss: 0.060572\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 83.86, NNZs: 1274, Bias: -0.837719, T: 15664, Avg. loss: 0.059763\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 84.29, NNZs: 1274, Bias: -0.839160, T: 16643, Avg. loss: 0.058954\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 84.70, NNZs: 1274, Bias: -0.837516, T: 17622, Avg. loss: 0.058578\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 85.08, NNZs: 1274, Bias: -0.838140, T: 18601, Avg. loss: 0.057660\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 19 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 76.02, NNZs: 1291, Bias: -0.103859, T: 979, Avg. loss: 0.502024\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 80.53, NNZs: 1291, Bias: -0.036440, T: 1958, Avg. loss: 0.148756\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 83.65, NNZs: 1291, Bias: -0.196763, T: 2937, Avg. loss: 0.089659\n",
            "Total training time: 0.00 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 4\n",
            "Norm: 85.95, NNZs: 1291, Bias: -0.264931, T: 3916, Avg. loss: 0.072030\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 87.69, NNZs: 1291, Bias: -0.195965, T: 4895, Avg. loss: 0.063493\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 89.08, NNZs: 1291, Bias: -0.208979, T: 5874, Avg. loss: 0.058357\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 90.25, NNZs: 1291, Bias: -0.213611, T: 6853, Avg. loss: 0.054286\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 91.25, NNZs: 1291, Bias: -0.224664, T: 7832, Avg. loss: 0.051709\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 92.11, NNZs: 1291, Bias: -0.249180, T: 8811, Avg. loss: 0.049561\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 92.87, NNZs: 1291, Bias: -0.252222, T: 9790, Avg. loss: 0.047958\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 93.55, NNZs: 1291, Bias: -0.252969, T: 10769, Avg. loss: 0.046592\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 94.16, NNZs: 1291, Bias: -0.251958, T: 11748, Avg. loss: 0.045398\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 94.71, NNZs: 1291, Bias: -0.244158, T: 12727, Avg. loss: 0.044350\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 95.22, NNZs: 1291, Bias: -0.248649, T: 13706, Avg. loss: 0.043555\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 95.70, NNZs: 1291, Bias: -0.256045, T: 14685, Avg. loss: 0.042807\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 96.13, NNZs: 1291, Bias: -0.256587, T: 15664, Avg. loss: 0.042097\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 96.54, NNZs: 1291, Bias: -0.261082, T: 16643, Avg. loss: 0.041543\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 96.92, NNZs: 1291, Bias: -0.267446, T: 17622, Avg. loss: 0.040998\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 18 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 143.59, NNZs: 1291, Bias: -0.671700, T: 979, Avg. loss: 0.928419\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 145.27, NNZs: 1291, Bias: -1.681881, T: 1958, Avg. loss: 0.285678\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 144.77, NNZs: 1291, Bias: -2.240679, T: 2937, Avg. loss: 0.119135\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 145.15, NNZs: 1291, Bias: -2.384400, T: 3916, Avg. loss: 0.055987\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 145.72, NNZs: 1291, Bias: -2.404580, T: 4895, Avg. loss: 0.040169\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 145.95, NNZs: 1291, Bias: -2.487372, T: 5874, Avg. loss: 0.037057\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 146.34, NNZs: 1291, Bias: -2.489246, T: 6853, Avg. loss: 0.030001\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 146.53, NNZs: 1291, Bias: -2.535582, T: 7832, Avg. loss: 0.030794\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 146.77, NNZs: 1291, Bias: -2.554179, T: 8811, Avg. loss: 0.027624\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 147.00, NNZs: 1291, Bias: -2.562792, T: 9790, Avg. loss: 0.025430\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 147.22, NNZs: 1291, Bias: -2.564970, T: 10769, Avg. loss: 0.024598\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 147.40, NNZs: 1291, Bias: -2.576132, T: 11748, Avg. loss: 0.024655\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 147.57, NNZs: 1291, Bias: -2.583343, T: 12727, Avg. loss: 0.023747\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 147.71, NNZs: 1291, Bias: -2.596056, T: 13706, Avg. loss: 0.023706\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 147.86, NNZs: 1291, Bias: -2.599510, T: 14685, Avg. loss: 0.022652\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 147.97, NNZs: 1291, Bias: -2.615803, T: 15664, Avg. loss: 0.023443\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 148.11, NNZs: 1291, Bias: -2.616802, T: 16643, Avg. loss: 0.021700\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 148.22, NNZs: 1291, Bias: -2.624397, T: 17622, Avg. loss: 0.022224\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 148.34, NNZs: 1291, Bias: -2.625725, T: 18601, Avg. loss: 0.021215\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 148.44, NNZs: 1291, Bias: -2.628756, T: 19580, Avg. loss: 0.021243\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 20 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 60.30, NNZs: 1291, Bias: -0.728830, T: 979, Avg. loss: 0.510404\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 65.95, NNZs: 1291, Bias: -0.728670, T: 1958, Avg. loss: 0.190440\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 69.63, NNZs: 1291, Bias: -0.684236, T: 2937, Avg. loss: 0.129785\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 72.21, NNZs: 1291, Bias: -0.709119, T: 3916, Avg. loss: 0.106958\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 74.27, NNZs: 1291, Bias: -0.674169, T: 4895, Avg. loss: 0.098167\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 75.85, NNZs: 1291, Bias: -0.693852, T: 5874, Avg. loss: 0.088745\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 77.18, NNZs: 1291, Bias: -0.687339, T: 6853, Avg. loss: 0.084875\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 78.31, NNZs: 1291, Bias: -0.680389, T: 7832, Avg. loss: 0.081175\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 79.27, NNZs: 1291, Bias: -0.687257, T: 8811, Avg. loss: 0.077450\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 80.14, NNZs: 1291, Bias: -0.682989, T: 9790, Avg. loss: 0.075718\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 80.91, NNZs: 1291, Bias: -0.684831, T: 10769, Avg. loss: 0.073296\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 81.59, NNZs: 1291, Bias: -0.690621, T: 11748, Avg. loss: 0.071111\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 82.22, NNZs: 1291, Bias: -0.681871, T: 12727, Avg. loss: 0.070920\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 82.80, NNZs: 1291, Bias: -0.685045, T: 13706, Avg. loss: 0.068636\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 83.33, NNZs: 1291, Bias: -0.687029, T: 14685, Avg. loss: 0.067692\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 83.82, NNZs: 1291, Bias: -0.684094, T: 15664, Avg. loss: 0.067095\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 84.28, NNZs: 1291, Bias: -0.684003, T: 16643, Avg. loss: 0.065954\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 84.71, NNZs: 1291, Bias: -0.685663, T: 17622, Avg. loss: 0.064876\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 85.11, NNZs: 1291, Bias: -0.683573, T: 18601, Avg. loss: 0.064537\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 85.49, NNZs: 1291, Bias: -0.682642, T: 19580, Avg. loss: 0.063807\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 85.84, NNZs: 1291, Bias: -0.683303, T: 20559, Avg. loss: 0.062890\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 86.18, NNZs: 1291, Bias: -0.682466, T: 21538, Avg. loss: 0.062528\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 86.49, NNZs: 1291, Bias: -0.687711, T: 22517, Avg. loss: 0.061047\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 86.80, NNZs: 1291, Bias: -0.684824, T: 23496, Avg. loss: 0.061709\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 87.09, NNZs: 1291, Bias: -0.685686, T: 24475, Avg. loss: 0.060653\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 87.37, NNZs: 1291, Bias: -0.688545, T: 25454, Avg. loss: 0.059881\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 87.63, NNZs: 1291, Bias: -0.687255, T: 26433, Avg. loss: 0.060166\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 87.88, NNZs: 1291, Bias: -0.687755, T: 27412, Avg. loss: 0.059437\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 28 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 76.53, NNZs: 1328, Bias: -0.116562, T: 979, Avg. loss: 0.498589\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 80.78, NNZs: 1328, Bias: -0.044003, T: 1958, Avg. loss: 0.159111\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 83.88, NNZs: 1328, Bias: -0.185535, T: 2937, Avg. loss: 0.094625\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 86.09, NNZs: 1328, Bias: -0.292293, T: 3916, Avg. loss: 0.076967\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 87.78, NNZs: 1328, Bias: -0.229375, T: 4895, Avg. loss: 0.069004\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 89.14, NNZs: 1328, Bias: -0.229470, T: 5874, Avg. loss: 0.062918\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 90.27, NNZs: 1328, Bias: -0.239844, T: 6853, Avg. loss: 0.058795\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 91.24, NNZs: 1328, Bias: -0.253648, T: 7832, Avg. loss: 0.056000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 92.08, NNZs: 1328, Bias: -0.267998, T: 8811, Avg. loss: 0.053746\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 92.83, NNZs: 1328, Bias: -0.267113, T: 9790, Avg. loss: 0.051981\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 93.49, NNZs: 1328, Bias: -0.267348, T: 10769, Avg. loss: 0.050609\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 94.09, NNZs: 1328, Bias: -0.265381, T: 11748, Avg. loss: 0.049316\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 94.62, NNZs: 1328, Bias: -0.265070, T: 12727, Avg. loss: 0.048281\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 95.12, NNZs: 1328, Bias: -0.270693, T: 13706, Avg. loss: 0.047409\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 95.58, NNZs: 1328, Bias: -0.276783, T: 14685, Avg. loss: 0.046633\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 96.01, NNZs: 1328, Bias: -0.276576, T: 15664, Avg. loss: 0.045857\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 96.40, NNZs: 1328, Bias: -0.281165, T: 16643, Avg. loss: 0.045287\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 96.78, NNZs: 1328, Bias: -0.285015, T: 17622, Avg. loss: 0.044706\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 18 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 144.06, NNZs: 1328, Bias: -0.971466, T: 979, Avg. loss: 0.905934\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 143.85, NNZs: 1328, Bias: -2.313567, T: 1958, Avg. loss: 0.317247\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 143.70, NNZs: 1328, Bias: -2.616055, T: 2937, Avg. loss: 0.124617\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 143.74, NNZs: 1328, Bias: -2.821854, T: 3916, Avg. loss: 0.068466\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 144.26, NNZs: 1328, Bias: -2.837813, T: 4895, Avg. loss: 0.047678\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 144.47, NNZs: 1328, Bias: -2.905480, T: 5874, Avg. loss: 0.043481\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 144.78, NNZs: 1328, Bias: -2.918325, T: 6853, Avg. loss: 0.037199\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 144.94, NNZs: 1328, Bias: -2.965970, T: 7832, Avg. loss: 0.036646\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 145.17, NNZs: 1328, Bias: -2.977193, T: 8811, Avg. loss: 0.033056\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 145.39, NNZs: 1328, Bias: -2.982530, T: 9790, Avg. loss: 0.030963\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 145.60, NNZs: 1328, Bias: -2.983822, T: 10769, Avg. loss: 0.030077\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 145.76, NNZs: 1328, Bias: -2.994286, T: 11748, Avg. loss: 0.030154\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 145.94, NNZs: 1328, Bias: -2.994230, T: 12727, Avg. loss: 0.028676\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 146.07, NNZs: 1328, Bias: -3.004432, T: 13706, Avg. loss: 0.028813\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 146.21, NNZs: 1328, Bias: -3.007533, T: 14685, Avg. loss: 0.027908\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 146.30, NNZs: 1328, Bias: -3.024176, T: 15664, Avg. loss: 0.028714\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 146.43, NNZs: 1328, Bias: -3.025285, T: 16643, Avg. loss: 0.026922\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 146.54, NNZs: 1328, Bias: -3.026838, T: 17622, Avg. loss: 0.026670\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 18 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 61.53, NNZs: 1328, Bias: -0.788064, T: 979, Avg. loss: 0.507760\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 67.25, NNZs: 1328, Bias: -0.737114, T: 1958, Avg. loss: 0.193608\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 71.00, NNZs: 1328, Bias: -0.713530, T: 2937, Avg. loss: 0.128048\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 73.59, NNZs: 1328, Bias: -0.729502, T: 3916, Avg. loss: 0.107230\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 75.65, NNZs: 1328, Bias: -0.691515, T: 4895, Avg. loss: 0.098141\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 77.23, NNZs: 1328, Bias: -0.712883, T: 5874, Avg. loss: 0.088838\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 78.55, NNZs: 1328, Bias: -0.701653, T: 6853, Avg. loss: 0.085092\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 79.69, NNZs: 1328, Bias: -0.693076, T: 7832, Avg. loss: 0.081273\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 80.66, NNZs: 1328, Bias: -0.702683, T: 8811, Avg. loss: 0.077520\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 81.52, NNZs: 1328, Bias: -0.700347, T: 9790, Avg. loss: 0.075690\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 82.28, NNZs: 1328, Bias: -0.707803, T: 10769, Avg. loss: 0.073001\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 82.96, NNZs: 1328, Bias: -0.714525, T: 11748, Avg. loss: 0.071170\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 83.59, NNZs: 1328, Bias: -0.709255, T: 12727, Avg. loss: 0.070836\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 84.17, NNZs: 1328, Bias: -0.711100, T: 13706, Avg. loss: 0.068971\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 84.70, NNZs: 1328, Bias: -0.711101, T: 14685, Avg. loss: 0.068014\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 85.19, NNZs: 1328, Bias: -0.710248, T: 15664, Avg. loss: 0.067077\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 85.64, NNZs: 1328, Bias: -0.709638, T: 16643, Avg. loss: 0.066194\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 86.07, NNZs: 1328, Bias: -0.709553, T: 17622, Avg. loss: 0.065298\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 86.47, NNZs: 1328, Bias: -0.708825, T: 18601, Avg. loss: 0.064611\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 19 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 77.39, NNZs: 1329, Bias: -0.170020, T: 979, Avg. loss: 0.514649\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 81.62, NNZs: 1329, Bias: -0.040098, T: 1958, Avg. loss: 0.155012\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 84.61, NNZs: 1329, Bias: -0.242083, T: 2937, Avg. loss: 0.095382\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 86.79, NNZs: 1329, Bias: -0.331866, T: 3916, Avg. loss: 0.075383\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 88.45, NNZs: 1329, Bias: -0.285861, T: 4895, Avg. loss: 0.066976\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 89.79, NNZs: 1329, Bias: -0.252963, T: 5874, Avg. loss: 0.060729\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 90.91, NNZs: 1329, Bias: -0.296136, T: 6853, Avg. loss: 0.057049\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 91.86, NNZs: 1329, Bias: -0.305251, T: 7832, Avg. loss: 0.054421\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 92.70, NNZs: 1329, Bias: -0.308352, T: 8811, Avg. loss: 0.052154\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 93.44, NNZs: 1329, Bias: -0.305621, T: 9790, Avg. loss: 0.050455\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 94.09, NNZs: 1329, Bias: -0.313238, T: 10769, Avg. loss: 0.049110\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 94.68, NNZs: 1329, Bias: -0.309421, T: 11748, Avg. loss: 0.047896\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 95.21, NNZs: 1329, Bias: -0.308776, T: 12727, Avg. loss: 0.046946\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 95.70, NNZs: 1329, Bias: -0.313324, T: 13706, Avg. loss: 0.046128\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 96.16, NNZs: 1329, Bias: -0.321339, T: 14685, Avg. loss: 0.045376\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 96.58, NNZs: 1329, Bias: -0.319265, T: 15664, Avg. loss: 0.044623\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 96.97, NNZs: 1329, Bias: -0.323022, T: 16643, Avg. loss: 0.044095\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 17 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 137.82, NNZs: 1329, Bias: -0.807076, T: 979, Avg. loss: 0.849392\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 138.71, NNZs: 1329, Bias: -1.960438, T: 1958, Avg. loss: 0.275117\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 139.20, NNZs: 1329, Bias: -2.230117, T: 2937, Avg. loss: 0.099147\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 139.64, NNZs: 1329, Bias: -2.371465, T: 3916, Avg. loss: 0.052724\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm: 140.12, NNZs: 1329, Bias: -2.413506, T: 4895, Avg. loss: 0.039327\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 140.43, NNZs: 1329, Bias: -2.464948, T: 5874, Avg. loss: 0.034940\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 140.74, NNZs: 1329, Bias: -2.489263, T: 6853, Avg. loss: 0.030699\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 140.95, NNZs: 1329, Bias: -2.532579, T: 7832, Avg. loss: 0.029893\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 141.21, NNZs: 1329, Bias: -2.540444, T: 8811, Avg. loss: 0.026765\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 141.42, NNZs: 1329, Bias: -2.552241, T: 9790, Avg. loss: 0.025704\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 141.65, NNZs: 1329, Bias: -2.551069, T: 10769, Avg. loss: 0.024239\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 141.80, NNZs: 1329, Bias: -2.570169, T: 11748, Avg. loss: 0.025202\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 141.99, NNZs: 1329, Bias: -2.567526, T: 12727, Avg. loss: 0.023084\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 142.13, NNZs: 1329, Bias: -2.579943, T: 13706, Avg. loss: 0.023745\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 142.28, NNZs: 1329, Bias: -2.583428, T: 14685, Avg. loss: 0.022735\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 142.39, NNZs: 1329, Bias: -2.597799, T: 15664, Avg. loss: 0.023404\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 142.51, NNZs: 1329, Bias: -2.603375, T: 16643, Avg. loss: 0.022360\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 142.64, NNZs: 1329, Bias: -2.602964, T: 17622, Avg. loss: 0.021481\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 18 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 61.63, NNZs: 1329, Bias: -0.891240, T: 979, Avg. loss: 0.515899\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 67.48, NNZs: 1329, Bias: -0.750594, T: 1958, Avg. loss: 0.196356\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 71.17, NNZs: 1329, Bias: -0.751198, T: 2937, Avg. loss: 0.128398\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 73.75, NNZs: 1329, Bias: -0.759195, T: 3916, Avg. loss: 0.108342\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 75.81, NNZs: 1329, Bias: -0.712820, T: 4895, Avg. loss: 0.099128\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 77.37, NNZs: 1329, Bias: -0.742434, T: 5874, Avg. loss: 0.089876\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 78.68, NNZs: 1329, Bias: -0.723762, T: 6853, Avg. loss: 0.086793\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 79.81, NNZs: 1329, Bias: -0.712813, T: 7832, Avg. loss: 0.082777\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 80.76, NNZs: 1329, Bias: -0.719918, T: 8811, Avg. loss: 0.079270\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 81.61, NNZs: 1329, Bias: -0.719709, T: 9790, Avg. loss: 0.077125\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 82.36, NNZs: 1329, Bias: -0.725585, T: 10769, Avg. loss: 0.074691\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 83.03, NNZs: 1329, Bias: -0.734300, T: 11748, Avg. loss: 0.072634\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 83.66, NNZs: 1329, Bias: -0.729593, T: 12727, Avg. loss: 0.072392\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 84.22, NNZs: 1329, Bias: -0.732797, T: 13706, Avg. loss: 0.070461\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 84.75, NNZs: 1329, Bias: -0.733819, T: 14685, Avg. loss: 0.069548\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 85.23, NNZs: 1329, Bias: -0.730086, T: 15664, Avg. loss: 0.069029\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 85.68, NNZs: 1329, Bias: -0.729377, T: 16643, Avg. loss: 0.067915\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 86.10, NNZs: 1329, Bias: -0.729602, T: 17622, Avg. loss: 0.066962\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 86.49, NNZs: 1329, Bias: -0.730661, T: 18601, Avg. loss: 0.066154\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 86.86, NNZs: 1329, Bias: -0.730764, T: 19580, Avg. loss: 0.065587\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 87.21, NNZs: 1329, Bias: -0.731776, T: 20559, Avg. loss: 0.064813\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 87.54, NNZs: 1329, Bias: -0.731801, T: 21538, Avg. loss: 0.064419\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 22 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 76.33, NNZs: 1327, Bias: -0.021162, T: 979, Avg. loss: 0.506984\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 81.53, NNZs: 1327, Bias: -0.104991, T: 1958, Avg. loss: 0.141350\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 84.73, NNZs: 1327, Bias: -0.187962, T: 2937, Avg. loss: 0.086238\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 86.99, NNZs: 1327, Bias: -0.278699, T: 3916, Avg. loss: 0.068920\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 88.72, NNZs: 1327, Bias: -0.241473, T: 4895, Avg. loss: 0.062430\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 90.10, NNZs: 1327, Bias: -0.213892, T: 5874, Avg. loss: 0.056988\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 91.23, NNZs: 1327, Bias: -0.238316, T: 6853, Avg. loss: 0.053845\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 92.19, NNZs: 1327, Bias: -0.246683, T: 7832, Avg. loss: 0.051360\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 93.03, NNZs: 1327, Bias: -0.250958, T: 8811, Avg. loss: 0.049312\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 93.77, NNZs: 1327, Bias: -0.242694, T: 9790, Avg. loss: 0.047822\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 94.42, NNZs: 1327, Bias: -0.260284, T: 10769, Avg. loss: 0.046590\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 95.00, NNZs: 1327, Bias: -0.255614, T: 11748, Avg. loss: 0.045553\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 95.53, NNZs: 1327, Bias: -0.253689, T: 12727, Avg. loss: 0.044633\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 96.02, NNZs: 1327, Bias: -0.253611, T: 13706, Avg. loss: 0.043874\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 96.48, NNZs: 1327, Bias: -0.262070, T: 14685, Avg. loss: 0.043187\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 96.89, NNZs: 1327, Bias: -0.258499, T: 15664, Avg. loss: 0.042523\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 97.28, NNZs: 1327, Bias: -0.259746, T: 16643, Avg. loss: 0.042018\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 17 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 138.26, NNZs: 1327, Bias: -0.722712, T: 979, Avg. loss: 0.885654\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 139.43, NNZs: 1327, Bias: -2.285185, T: 1958, Avg. loss: 0.303385\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 140.40, NNZs: 1327, Bias: -2.426522, T: 2937, Avg. loss: 0.103502\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 140.57, NNZs: 1327, Bias: -2.629556, T: 3916, Avg. loss: 0.057761\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 140.96, NNZs: 1327, Bias: -2.705219, T: 4895, Avg. loss: 0.038983\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 141.33, NNZs: 1327, Bias: -2.744596, T: 5874, Avg. loss: 0.033309\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 141.60, NNZs: 1327, Bias: -2.786113, T: 6853, Avg. loss: 0.030514\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 141.83, NNZs: 1327, Bias: -2.821953, T: 7832, Avg. loss: 0.028795\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 142.10, NNZs: 1327, Bias: -2.829469, T: 8811, Avg. loss: 0.026101\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 142.31, NNZs: 1327, Bias: -2.846168, T: 9790, Avg. loss: 0.025324\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 142.57, NNZs: 1327, Bias: -2.837945, T: 10769, Avg. loss: 0.023090\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 142.72, NNZs: 1327, Bias: -2.858497, T: 11748, Avg. loss: 0.024895\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 142.90, NNZs: 1327, Bias: -2.860157, T: 12727, Avg. loss: 0.022904\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 143.04, NNZs: 1327, Bias: -2.873432, T: 13706, Avg. loss: 0.023328\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 143.20, NNZs: 1327, Bias: -2.875240, T: 14685, Avg. loss: 0.022117\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 143.30, NNZs: 1327, Bias: -2.892957, T: 15664, Avg. loss: 0.023368\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 16 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 61.17, NNZs: 1327, Bias: -0.825013, T: 979, Avg. loss: 0.502949\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 66.94, NNZs: 1327, Bias: -0.757530, T: 1958, Avg. loss: 0.186933\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 70.78, NNZs: 1327, Bias: -0.735294, T: 2937, Avg. loss: 0.125370\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 73.39, NNZs: 1327, Bias: -0.725437, T: 3916, Avg. loss: 0.104991\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 75.45, NNZs: 1327, Bias: -0.685561, T: 4895, Avg. loss: 0.095286\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 77.02, NNZs: 1327, Bias: -0.713449, T: 5874, Avg. loss: 0.085968\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 78.34, NNZs: 1327, Bias: -0.708134, T: 6853, Avg. loss: 0.082329\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 79.48, NNZs: 1327, Bias: -0.691832, T: 7832, Avg. loss: 0.079106\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 80.45, NNZs: 1327, Bias: -0.696106, T: 8811, Avg. loss: 0.075328\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 81.31, NNZs: 1327, Bias: -0.694693, T: 9790, Avg. loss: 0.073268\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 82.07, NNZs: 1327, Bias: -0.696229, T: 10769, Avg. loss: 0.070997\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 82.76, NNZs: 1327, Bias: -0.701809, T: 11748, Avg. loss: 0.068983\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 83.38, NNZs: 1327, Bias: -0.697287, T: 12727, Avg. loss: 0.068435\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 83.96, NNZs: 1327, Bias: -0.699266, T: 13706, Avg. loss: 0.066593\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 84.49, NNZs: 1327, Bias: -0.700405, T: 14685, Avg. loss: 0.065609\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 84.98, NNZs: 1327, Bias: -0.697444, T: 15664, Avg. loss: 0.064974\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 85.44, NNZs: 1327, Bias: -0.693296, T: 16643, Avg. loss: 0.064254\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 85.87, NNZs: 1327, Bias: -0.693656, T: 17622, Avg. loss: 0.063008\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 86.26, NNZs: 1327, Bias: -0.693169, T: 18601, Avg. loss: 0.062349\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 86.64, NNZs: 1327, Bias: -0.694386, T: 19580, Avg. loss: 0.061456\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 86.99, NNZs: 1327, Bias: -0.694368, T: 20559, Avg. loss: 0.060975\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 87.33, NNZs: 1327, Bias: -0.693987, T: 21538, Avg. loss: 0.060496\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 87.64, NNZs: 1327, Bias: -0.697218, T: 22517, Avg. loss: 0.059397\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 87.95, NNZs: 1327, Bias: -0.695124, T: 23496, Avg. loss: 0.059620\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 88.23, NNZs: 1327, Bias: -0.695932, T: 24475, Avg. loss: 0.058744\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 88.50, NNZs: 1327, Bias: -0.697584, T: 25454, Avg. loss: 0.058159\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 88.77, NNZs: 1327, Bias: -0.696436, T: 26433, Avg. loss: 0.058225\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 89.02, NNZs: 1327, Bias: -0.697565, T: 27412, Avg. loss: 0.057424\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 28 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 76.56, NNZs: 1343, Bias: -0.045583, T: 979, Avg. loss: 0.504418\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 81.25, NNZs: 1343, Bias: -0.070918, T: 1958, Avg. loss: 0.154201\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 84.48, NNZs: 1343, Bias: -0.197368, T: 2937, Avg. loss: 0.092410\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 86.76, NNZs: 1343, Bias: -0.283557, T: 3916, Avg. loss: 0.074735\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 88.51, NNZs: 1343, Bias: -0.253498, T: 4895, Avg. loss: 0.065727\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 89.94, NNZs: 1343, Bias: -0.246539, T: 5874, Avg. loss: 0.059742\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 91.08, NNZs: 1343, Bias: -0.271696, T: 6853, Avg. loss: 0.056322\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 92.06, NNZs: 1343, Bias: -0.277412, T: 7832, Avg. loss: 0.053569\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 92.93, NNZs: 1343, Bias: -0.281765, T: 8811, Avg. loss: 0.051225\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 93.69, NNZs: 1343, Bias: -0.282360, T: 9790, Avg. loss: 0.049566\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 94.36, NNZs: 1343, Bias: -0.295340, T: 10769, Avg. loss: 0.048258\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 94.96, NNZs: 1343, Bias: -0.292040, T: 11748, Avg. loss: 0.047085\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 95.50, NNZs: 1343, Bias: -0.294594, T: 12727, Avg. loss: 0.046119\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 96.01, NNZs: 1343, Bias: -0.292630, T: 13706, Avg. loss: 0.045245\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 96.47, NNZs: 1343, Bias: -0.299378, T: 14685, Avg. loss: 0.044511\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 96.90, NNZs: 1343, Bias: -0.300654, T: 15664, Avg. loss: 0.043835\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 97.30, NNZs: 1343, Bias: -0.301106, T: 16643, Avg. loss: 0.043244\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 17 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 138.47, NNZs: 1343, Bias: -0.852542, T: 979, Avg. loss: 0.927863\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 138.50, NNZs: 1343, Bias: -2.262499, T: 1958, Avg. loss: 0.342848\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 139.30, NNZs: 1343, Bias: -2.443253, T: 2937, Avg. loss: 0.113564\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 139.33, NNZs: 1343, Bias: -2.662282, T: 3916, Avg. loss: 0.067900\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 139.78, NNZs: 1343, Bias: -2.717769, T: 4895, Avg. loss: 0.044959\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 140.10, NNZs: 1343, Bias: -2.772338, T: 5874, Avg. loss: 0.037400\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 140.48, NNZs: 1343, Bias: -2.778713, T: 6853, Avg. loss: 0.032221\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 140.67, NNZs: 1343, Bias: -2.830047, T: 7832, Avg. loss: 0.032553\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 140.93, NNZs: 1343, Bias: -2.839528, T: 8811, Avg. loss: 0.028870\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 141.18, NNZs: 1343, Bias: -2.848583, T: 9790, Avg. loss: 0.027298\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 141.42, NNZs: 1343, Bias: -2.848231, T: 10769, Avg. loss: 0.025951\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 141.56, NNZs: 1343, Bias: -2.871546, T: 11748, Avg. loss: 0.027208\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 141.76, NNZs: 1343, Bias: -2.870012, T: 12727, Avg. loss: 0.024728\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 141.90, NNZs: 1343, Bias: -2.885276, T: 13706, Avg. loss: 0.025526\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 142.08, NNZs: 1343, Bias: -2.883689, T: 14685, Avg. loss: 0.023634\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 142.19, NNZs: 1343, Bias: -2.898317, T: 15664, Avg. loss: 0.024884\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 142.30, NNZs: 1343, Bias: -2.909973, T: 16643, Avg. loss: 0.024401\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 142.44, NNZs: 1343, Bias: -2.908789, T: 17622, Avg. loss: 0.022798\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 142.57, NNZs: 1343, Bias: -2.908884, T: 18601, Avg. loss: 0.022475\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 142.68, NNZs: 1343, Bias: -2.912171, T: 19580, Avg. loss: 0.022771\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 20 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 61.90, NNZs: 1343, Bias: -0.871972, T: 979, Avg. loss: 0.515769\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 67.88, NNZs: 1343, Bias: -0.730542, T: 1958, Avg. loss: 0.193493\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 71.71, NNZs: 1343, Bias: -0.745829, T: 2937, Avg. loss: 0.127280\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 74.37, NNZs: 1343, Bias: -0.732985, T: 3916, Avg. loss: 0.107697\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 76.47, NNZs: 1343, Bias: -0.663287, T: 4895, Avg. loss: 0.098060\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 78.05, NNZs: 1343, Bias: -0.696513, T: 5874, Avg. loss: 0.088322\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 79.37, NNZs: 1343, Bias: -0.692757, T: 6853, Avg. loss: 0.084658\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 80.51, NNZs: 1343, Bias: -0.684464, T: 7832, Avg. loss: 0.081164\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 81.48, NNZs: 1343, Bias: -0.687249, T: 8811, Avg. loss: 0.077668\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 82.34, NNZs: 1343, Bias: -0.683576, T: 9790, Avg. loss: 0.075702\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 83.09, NNZs: 1343, Bias: -0.687461, T: 10769, Avg. loss: 0.073167\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 83.77, NNZs: 1343, Bias: -0.690552, T: 11748, Avg. loss: 0.071476\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 84.40, NNZs: 1343, Bias: -0.689831, T: 12727, Avg. loss: 0.070403\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 84.97, NNZs: 1343, Bias: -0.693931, T: 13706, Avg. loss: 0.068634\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 85.50, NNZs: 1343, Bias: -0.693211, T: 14685, Avg. loss: 0.068038\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 85.99, NNZs: 1343, Bias: -0.689179, T: 15664, Avg. loss: 0.067355\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 86.44, NNZs: 1343, Bias: -0.686008, T: 16643, Avg. loss: 0.066465\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 86.86, NNZs: 1343, Bias: -0.685161, T: 17622, Avg. loss: 0.065445\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 87.26, NNZs: 1343, Bias: -0.685030, T: 18601, Avg. loss: 0.064614\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 87.63, NNZs: 1343, Bias: -0.686055, T: 19580, Avg. loss: 0.063797\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 87.98, NNZs: 1343, Bias: -0.685799, T: 20559, Avg. loss: 0.063326\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 88.31, NNZs: 1343, Bias: -0.686251, T: 21538, Avg. loss: 0.062708\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 88.62, NNZs: 1343, Bias: -0.689015, T: 22517, Avg. loss: 0.061790\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 23 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 75.24, NNZs: 1325, Bias: -0.009448, T: 979, Avg. loss: 0.504505\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 79.66, NNZs: 1325, Bias: -0.099238, T: 1958, Avg. loss: 0.157003\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 82.76, NNZs: 1325, Bias: -0.225763, T: 2937, Avg. loss: 0.099963\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 84.98, NNZs: 1325, Bias: -0.301063, T: 3916, Avg. loss: 0.078420\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 86.73, NNZs: 1325, Bias: -0.254549, T: 4895, Avg. loss: 0.069550\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 88.15, NNZs: 1325, Bias: -0.264816, T: 5874, Avg. loss: 0.063430\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 89.30, NNZs: 1325, Bias: -0.295082, T: 6853, Avg. loss: 0.059827\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 90.29, NNZs: 1325, Bias: -0.304227, T: 7832, Avg. loss: 0.056651\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 91.15, NNZs: 1325, Bias: -0.304021, T: 8811, Avg. loss: 0.054560\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 91.91, NNZs: 1325, Bias: -0.309937, T: 9790, Avg. loss: 0.052694\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 92.57, NNZs: 1325, Bias: -0.325452, T: 10769, Avg. loss: 0.051305\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 93.17, NNZs: 1325, Bias: -0.324308, T: 11748, Avg. loss: 0.050024\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 93.72, NNZs: 1325, Bias: -0.328933, T: 12727, Avg. loss: 0.049000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 94.23, NNZs: 1325, Bias: -0.328528, T: 13706, Avg. loss: 0.048118\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 94.69, NNZs: 1325, Bias: -0.339277, T: 14685, Avg. loss: 0.047375\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 95.12, NNZs: 1325, Bias: -0.338800, T: 15664, Avg. loss: 0.046610\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm: 95.52, NNZs: 1325, Bias: -0.340273, T: 16643, Avg. loss: 0.046013\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 95.90, NNZs: 1325, Bias: -0.344312, T: 17622, Avg. loss: 0.045451\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 18 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 137.04, NNZs: 1325, Bias: -0.868464, T: 979, Avg. loss: 0.820197\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 137.03, NNZs: 1325, Bias: -2.369992, T: 1958, Avg. loss: 0.304354\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 138.17, NNZs: 1325, Bias: -2.335183, T: 2937, Avg. loss: 0.129061\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 137.94, NNZs: 1325, Bias: -2.647115, T: 3916, Avg. loss: 0.072217\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 138.48, NNZs: 1325, Bias: -2.653766, T: 4895, Avg. loss: 0.050707\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 138.74, NNZs: 1325, Bias: -2.710435, T: 5874, Avg. loss: 0.042972\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 139.03, NNZs: 1325, Bias: -2.739149, T: 6853, Avg. loss: 0.037485\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 139.18, NNZs: 1325, Bias: -2.797905, T: 7832, Avg. loss: 0.036327\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 139.45, NNZs: 1325, Bias: -2.804436, T: 8811, Avg. loss: 0.031750\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 139.65, NNZs: 1325, Bias: -2.824274, T: 9790, Avg. loss: 0.030517\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 139.86, NNZs: 1325, Bias: -2.829488, T: 10769, Avg. loss: 0.028953\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 140.02, NNZs: 1325, Bias: -2.846550, T: 11748, Avg. loss: 0.029288\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 140.21, NNZs: 1325, Bias: -2.844378, T: 12727, Avg. loss: 0.027219\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 140.35, NNZs: 1325, Bias: -2.857087, T: 13706, Avg. loss: 0.027714\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 140.51, NNZs: 1325, Bias: -2.857733, T: 14685, Avg. loss: 0.026328\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 140.62, NNZs: 1325, Bias: -2.871228, T: 15664, Avg. loss: 0.027210\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 140.74, NNZs: 1325, Bias: -2.877764, T: 16643, Avg. loss: 0.026245\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 140.87, NNZs: 1325, Bias: -2.877929, T: 17622, Avg. loss: 0.025323\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 18 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 60.24, NNZs: 1325, Bias: -0.764236, T: 979, Avg. loss: 0.498825\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 66.05, NNZs: 1325, Bias: -0.660977, T: 1958, Avg. loss: 0.180018\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 69.76, NNZs: 1325, Bias: -0.661084, T: 2937, Avg. loss: 0.126399\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 72.31, NNZs: 1325, Bias: -0.667870, T: 3916, Avg. loss: 0.105545\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 74.35, NNZs: 1325, Bias: -0.606717, T: 4895, Avg. loss: 0.096713\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 75.89, NNZs: 1325, Bias: -0.647071, T: 5874, Avg. loss: 0.086841\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 77.21, NNZs: 1325, Bias: -0.628294, T: 6853, Avg. loss: 0.084314\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 78.32, NNZs: 1325, Bias: -0.616686, T: 7832, Avg. loss: 0.080588\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 79.27, NNZs: 1325, Bias: -0.622926, T: 8811, Avg. loss: 0.076836\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 80.11, NNZs: 1325, Bias: -0.624221, T: 9790, Avg. loss: 0.074794\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 80.86, NNZs: 1325, Bias: -0.625439, T: 10769, Avg. loss: 0.072726\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 81.53, NNZs: 1325, Bias: -0.627676, T: 11748, Avg. loss: 0.070949\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 82.14, NNZs: 1325, Bias: -0.630092, T: 12727, Avg. loss: 0.069573\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 82.70, NNZs: 1325, Bias: -0.635302, T: 13706, Avg. loss: 0.068037\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 83.22, NNZs: 1325, Bias: -0.637558, T: 14685, Avg. loss: 0.067220\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 83.70, NNZs: 1325, Bias: -0.633335, T: 15664, Avg. loss: 0.066882\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 84.15, NNZs: 1325, Bias: -0.630094, T: 16643, Avg. loss: 0.065991\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 84.56, NNZs: 1325, Bias: -0.630694, T: 17622, Avg. loss: 0.064772\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 84.95, NNZs: 1325, Bias: -0.630999, T: 18601, Avg. loss: 0.064062\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 85.32, NNZs: 1325, Bias: -0.631806, T: 19580, Avg. loss: 0.063328\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 85.66, NNZs: 1325, Bias: -0.631869, T: 20559, Avg. loss: 0.062840\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 85.99, NNZs: 1325, Bias: -0.632262, T: 21538, Avg. loss: 0.062272\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 86.30, NNZs: 1325, Bias: -0.635126, T: 22517, Avg. loss: 0.061320\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 23 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 74.30, NNZs: 1320, Bias: -0.245965, T: 980, Avg. loss: 0.518532\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 79.66, NNZs: 1320, Bias: -0.178293, T: 1960, Avg. loss: 0.166265\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 82.92, NNZs: 1320, Bias: -0.285335, T: 2940, Avg. loss: 0.103580\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 85.30, NNZs: 1320, Bias: -0.283997, T: 3920, Avg. loss: 0.081777\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 87.14, NNZs: 1320, Bias: -0.309092, T: 4900, Avg. loss: 0.071516\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 88.61, NNZs: 1320, Bias: -0.294921, T: 5880, Avg. loss: 0.065300\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 89.85, NNZs: 1320, Bias: -0.335437, T: 6860, Avg. loss: 0.061442\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 90.90, NNZs: 1320, Bias: -0.341790, T: 7840, Avg. loss: 0.058367\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 91.80, NNZs: 1320, Bias: -0.347996, T: 8820, Avg. loss: 0.055852\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 92.60, NNZs: 1320, Bias: -0.357117, T: 9800, Avg. loss: 0.054099\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 93.32, NNZs: 1320, Bias: -0.366199, T: 10780, Avg. loss: 0.052566\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 93.95, NNZs: 1320, Bias: -0.358605, T: 11760, Avg. loss: 0.051175\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 94.54, NNZs: 1320, Bias: -0.364334, T: 12740, Avg. loss: 0.050188\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 95.07, NNZs: 1320, Bias: -0.371382, T: 13720, Avg. loss: 0.049151\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 95.57, NNZs: 1320, Bias: -0.371341, T: 14700, Avg. loss: 0.048313\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 96.02, NNZs: 1320, Bias: -0.374241, T: 15680, Avg. loss: 0.047597\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 96.45, NNZs: 1320, Bias: -0.385922, T: 16660, Avg. loss: 0.046947\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 96.84, NNZs: 1320, Bias: -0.386291, T: 17640, Avg. loss: 0.046325\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 97.21, NNZs: 1320, Bias: -0.384536, T: 18620, Avg. loss: 0.045753\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 19 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 147.01, NNZs: 1320, Bias: 0.011772, T: 980, Avg. loss: 1.004881\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 147.94, NNZs: 1320, Bias: -1.832814, T: 1960, Avg. loss: 0.323835\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 148.43, NNZs: 1320, Bias: -2.035509, T: 2940, Avg. loss: 0.114699\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 148.04, NNZs: 1320, Bias: -2.430220, T: 3920, Avg. loss: 0.079665\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 148.31, NNZs: 1320, Bias: -2.510629, T: 4900, Avg. loss: 0.047712\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 148.64, NNZs: 1320, Bias: -2.528716, T: 5880, Avg. loss: 0.041305\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 148.95, NNZs: 1320, Bias: -2.531845, T: 6860, Avg. loss: 0.037366\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 149.15, NNZs: 1320, Bias: -2.563613, T: 7840, Avg. loss: 0.036690\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 149.32, NNZs: 1320, Bias: -2.588044, T: 8820, Avg. loss: 0.033643\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 149.53, NNZs: 1320, Bias: -2.588546, T: 9800, Avg. loss: 0.031496\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 149.63, NNZs: 1320, Bias: -2.622188, T: 10780, Avg. loss: 0.032627\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 149.78, NNZs: 1320, Bias: -2.630656, T: 11760, Avg. loss: 0.030067\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 149.91, NNZs: 1320, Bias: -2.638277, T: 12740, Avg. loss: 0.029625\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 150.05, NNZs: 1320, Bias: -2.641118, T: 13720, Avg. loss: 0.028402\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 150.15, NNZs: 1320, Bias: -2.654098, T: 14700, Avg. loss: 0.029015\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 150.28, NNZs: 1320, Bias: -2.649143, T: 15680, Avg. loss: 0.026816\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 150.39, NNZs: 1320, Bias: -2.652363, T: 16660, Avg. loss: 0.027254\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 150.48, NNZs: 1320, Bias: -2.659915, T: 17640, Avg. loss: 0.027479\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 150.56, NNZs: 1320, Bias: -2.669147, T: 18620, Avg. loss: 0.027393\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 150.65, NNZs: 1320, Bias: -2.670538, T: 19600, Avg. loss: 0.026188\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 150.73, NNZs: 1320, Bias: -2.677518, T: 20580, Avg. loss: 0.026776\n",
            "Total training time: 0.24 seconds.\n",
            "Convergence after 21 epochs took 0.24 seconds\n",
            "-- Epoch 1\n",
            "Norm: 60.16, NNZs: 1320, Bias: -0.725641, T: 980, Avg. loss: 0.517717\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 66.34, NNZs: 1320, Bias: -0.635485, T: 1960, Avg. loss: 0.197051\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 70.20, NNZs: 1320, Bias: -0.556230, T: 2940, Avg. loss: 0.134614\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 72.86, NNZs: 1320, Bias: -0.583669, T: 3920, Avg. loss: 0.109910\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 74.91, NNZs: 1320, Bias: -0.531912, T: 4900, Avg. loss: 0.100069\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 76.51, NNZs: 1320, Bias: -0.559209, T: 5880, Avg. loss: 0.090838\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 77.83, NNZs: 1320, Bias: -0.568020, T: 6860, Avg. loss: 0.086215\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 78.99, NNZs: 1320, Bias: -0.563459, T: 7840, Avg. loss: 0.083029\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 79.97, NNZs: 1320, Bias: -0.567873, T: 8820, Avg. loss: 0.079658\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 80.84, NNZs: 1320, Bias: -0.571279, T: 9800, Avg. loss: 0.077312\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 81.63, NNZs: 1320, Bias: -0.559688, T: 10780, Avg. loss: 0.076486\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 82.31, NNZs: 1320, Bias: -0.567704, T: 11760, Avg. loss: 0.073249\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 82.96, NNZs: 1320, Bias: -0.556044, T: 12740, Avg. loss: 0.073305\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 83.55, NNZs: 1320, Bias: -0.556478, T: 13720, Avg. loss: 0.071243\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 84.08, NNZs: 1320, Bias: -0.558671, T: 14700, Avg. loss: 0.069903\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 84.58, NNZs: 1320, Bias: -0.556782, T: 15680, Avg. loss: 0.069270\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 85.03, NNZs: 1320, Bias: -0.562539, T: 16660, Avg. loss: 0.067592\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 85.46, NNZs: 1320, Bias: -0.559816, T: 17640, Avg. loss: 0.067634\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 85.86, NNZs: 1320, Bias: -0.561171, T: 18620, Avg. loss: 0.066508\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 86.24, NNZs: 1320, Bias: -0.563107, T: 19600, Avg. loss: 0.065651\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 86.59, NNZs: 1320, Bias: -0.563853, T: 20580, Avg. loss: 0.065154\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 86.93, NNZs: 1320, Bias: -0.566051, T: 21560, Avg. loss: 0.064383\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 87.25, NNZs: 1320, Bias: -0.565245, T: 22540, Avg. loss: 0.064215\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 87.55, NNZs: 1320, Bias: -0.565074, T: 23520, Avg. loss: 0.063641\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 24 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 75.87, NNZs: 1338, Bias: -0.243569, T: 980, Avg. loss: 0.497327\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 80.77, NNZs: 1338, Bias: -0.039122, T: 1960, Avg. loss: 0.160533\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 83.82, NNZs: 1338, Bias: -0.162060, T: 2940, Avg. loss: 0.100122\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 86.06, NNZs: 1338, Bias: -0.205815, T: 3920, Avg. loss: 0.079954\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 87.77, NNZs: 1338, Bias: -0.228610, T: 4900, Avg. loss: 0.069943\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 89.17, NNZs: 1338, Bias: -0.200655, T: 5880, Avg. loss: 0.063557\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 90.36, NNZs: 1338, Bias: -0.252969, T: 6860, Avg. loss: 0.060024\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 91.35, NNZs: 1338, Bias: -0.258280, T: 7840, Avg. loss: 0.057292\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 92.22, NNZs: 1338, Bias: -0.262494, T: 8820, Avg. loss: 0.054881\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 92.97, NNZs: 1338, Bias: -0.270765, T: 9800, Avg. loss: 0.053163\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 93.65, NNZs: 1338, Bias: -0.275799, T: 10780, Avg. loss: 0.051727\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 94.26, NNZs: 1338, Bias: -0.273373, T: 11760, Avg. loss: 0.050485\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 94.81, NNZs: 1338, Bias: -0.274590, T: 12740, Avg. loss: 0.049414\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 95.32, NNZs: 1338, Bias: -0.285205, T: 13720, Avg. loss: 0.048480\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 95.79, NNZs: 1338, Bias: -0.283085, T: 14700, Avg. loss: 0.047651\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 96.22, NNZs: 1338, Bias: -0.285450, T: 15680, Avg. loss: 0.047020\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 96.62, NNZs: 1338, Bias: -0.295426, T: 16660, Avg. loss: 0.046387\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 97.00, NNZs: 1338, Bias: -0.295688, T: 17640, Avg. loss: 0.045797\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 18 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 145.67, NNZs: 1338, Bias: -0.332268, T: 980, Avg. loss: 0.991032\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 145.48, NNZs: 1338, Bias: -2.227064, T: 1960, Avg. loss: 0.293896\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 146.31, NNZs: 1338, Bias: -2.300276, T: 2940, Avg. loss: 0.117884\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 146.02, NNZs: 1338, Bias: -2.642795, T: 3920, Avg. loss: 0.084651\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 146.11, NNZs: 1338, Bias: -2.796393, T: 4900, Avg. loss: 0.049976\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 146.49, NNZs: 1338, Bias: -2.816140, T: 5880, Avg. loss: 0.040496\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 146.86, NNZs: 1338, Bias: -2.813521, T: 6860, Avg. loss: 0.036514\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 147.10, NNZs: 1338, Bias: -2.838096, T: 7840, Avg. loss: 0.035351\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 147.30, NNZs: 1338, Bias: -2.861601, T: 8820, Avg. loss: 0.032572\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 147.54, NNZs: 1338, Bias: -2.862200, T: 9800, Avg. loss: 0.030481\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 147.65, NNZs: 1338, Bias: -2.897052, T: 10780, Avg. loss: 0.031677\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 147.82, NNZs: 1338, Bias: -2.904239, T: 11760, Avg. loss: 0.029001\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 147.98, NNZs: 1338, Bias: -2.911222, T: 12740, Avg. loss: 0.028554\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 148.13, NNZs: 1338, Bias: -2.914106, T: 13720, Avg. loss: 0.027441\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 148.25, NNZs: 1338, Bias: -2.926943, T: 14700, Avg. loss: 0.028029\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 148.40, NNZs: 1338, Bias: -2.921378, T: 15680, Avg. loss: 0.025788\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 148.52, NNZs: 1338, Bias: -2.924841, T: 16660, Avg. loss: 0.026387\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 148.62, NNZs: 1338, Bias: -2.932711, T: 17640, Avg. loss: 0.026627\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 148.71, NNZs: 1338, Bias: -2.941530, T: 18620, Avg. loss: 0.026477\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 148.82, NNZs: 1338, Bias: -2.942368, T: 19600, Avg. loss: 0.025204\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 148.90, NNZs: 1338, Bias: -2.949184, T: 20580, Avg. loss: 0.025897\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 21 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 60.58, NNZs: 1338, Bias: -0.642493, T: 980, Avg. loss: 0.501068\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 66.53, NNZs: 1338, Bias: -0.582244, T: 1960, Avg. loss: 0.187763\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 70.34, NNZs: 1338, Bias: -0.520227, T: 2940, Avg. loss: 0.128075\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 72.95, NNZs: 1338, Bias: -0.513892, T: 3920, Avg. loss: 0.106003\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 74.96, NNZs: 1338, Bias: -0.470093, T: 4900, Avg. loss: 0.095881\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 76.53, NNZs: 1338, Bias: -0.505494, T: 5880, Avg. loss: 0.086645\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 77.83, NNZs: 1338, Bias: -0.507595, T: 6860, Avg. loss: 0.082897\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 78.97, NNZs: 1338, Bias: -0.502877, T: 7840, Avg. loss: 0.079569\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 79.92, NNZs: 1338, Bias: -0.509418, T: 8820, Avg. loss: 0.076303\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 80.78, NNZs: 1338, Bias: -0.510102, T: 9800, Avg. loss: 0.074316\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 81.54, NNZs: 1338, Bias: -0.499290, T: 10780, Avg. loss: 0.073332\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 82.21, NNZs: 1338, Bias: -0.508482, T: 11760, Avg. loss: 0.070204\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 82.84, NNZs: 1338, Bias: -0.497111, T: 12740, Avg. loss: 0.070355\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 83.41, NNZs: 1338, Bias: -0.498396, T: 13720, Avg. loss: 0.068310\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 83.93, NNZs: 1338, Bias: -0.500835, T: 14700, Avg. loss: 0.067106\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 84.41, NNZs: 1338, Bias: -0.498664, T: 15680, Avg. loss: 0.066570\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 84.85, NNZs: 1338, Bias: -0.503455, T: 16660, Avg. loss: 0.065048\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 85.27, NNZs: 1338, Bias: -0.501002, T: 17640, Avg. loss: 0.064967\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 85.66, NNZs: 1338, Bias: -0.501857, T: 18620, Avg. loss: 0.063958\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 86.03, NNZs: 1338, Bias: -0.504462, T: 19600, Avg. loss: 0.063031\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 86.37, NNZs: 1338, Bias: -0.505850, T: 20580, Avg. loss: 0.062568\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 86.70, NNZs: 1338, Bias: -0.508021, T: 21560, Avg. loss: 0.061917\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 87.01, NNZs: 1338, Bias: -0.507158, T: 22540, Avg. loss: 0.061791\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 87.30, NNZs: 1338, Bias: -0.507315, T: 23520, Avg. loss: 0.061164\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 923.93, NNZs: 751, Bias: -1.115009, T: 979, Avg. loss: 189.388167\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 878.97, NNZs: 827, Bias: -0.156704, T: 1958, Avg. loss: 102.183475\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 875.26, NNZs: 808, Bias: -1.240783, T: 2937, Avg. loss: 7.596995\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 874.63, NNZs: 805, Bias: -1.330269, T: 3916, Avg. loss: 1.759983\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 874.35, NNZs: 795, Bias: -1.271183, T: 4895, Avg. loss: 0.991019\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 874.20, NNZs: 787, Bias: -1.229296, T: 5874, Avg. loss: 0.665485\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 874.10, NNZs: 784, Bias: -1.243638, T: 6853, Avg. loss: 0.515297\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 874.03, NNZs: 785, Bias: -1.246068, T: 7832, Avg. loss: 0.413424\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 873.98, NNZs: 780, Bias: -1.256640, T: 8811, Avg. loss: 0.353070\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 873.95, NNZs: 780, Bias: -1.263819, T: 9790, Avg. loss: 0.310173\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 873.92, NNZs: 776, Bias: -1.249941, T: 10769, Avg. loss: 0.272481\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 873.90, NNZs: 774, Bias: -1.252365, T: 11748, Avg. loss: 0.250418\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 873.88, NNZs: 771, Bias: -1.244474, T: 12727, Avg. loss: 0.229685\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 873.87, NNZs: 769, Bias: -1.243624, T: 13706, Avg. loss: 0.213791\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 873.85, NNZs: 767, Bias: -1.235141, T: 14685, Avg. loss: 0.199557\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 873.84, NNZs: 765, Bias: -1.234638, T: 15664, Avg. loss: 0.190912\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 873.83, NNZs: 765, Bias: -1.232265, T: 16643, Avg. loss: 0.182792\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 873.83, NNZs: 765, Bias: -1.233030, T: 17622, Avg. loss: 0.174690\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 873.82, NNZs: 764, Bias: -1.232078, T: 18601, Avg. loss: 0.166700\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 873.81, NNZs: 763, Bias: -1.232958, T: 19580, Avg. loss: 0.162764\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 873.81, NNZs: 760, Bias: -1.232314, T: 20559, Avg. loss: 0.156706\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 873.80, NNZs: 757, Bias: -1.228713, T: 21538, Avg. loss: 0.151997\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 873.80, NNZs: 751, Bias: -1.227392, T: 22517, Avg. loss: 0.149679\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 873.79, NNZs: 748, Bias: -1.227317, T: 23496, Avg. loss: 0.146230\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 873.79, NNZs: 748, Bias: -1.225585, T: 24475, Avg. loss: 0.141959\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 873.79, NNZs: 746, Bias: -1.225607, T: 25454, Avg. loss: 0.139577\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 873.78, NNZs: 745, Bias: -1.224875, T: 26433, Avg. loss: 0.136421\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 873.78, NNZs: 745, Bias: -1.223112, T: 27412, Avg. loss: 0.134245\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 873.78, NNZs: 744, Bias: -1.222602, T: 28391, Avg. loss: 0.131992\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 873.78, NNZs: 744, Bias: -1.222035, T: 29370, Avg. loss: 0.130111\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 873.77, NNZs: 742, Bias: -1.223158, T: 30349, Avg. loss: 0.127951\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 873.77, NNZs: 741, Bias: -1.222778, T: 31328, Avg. loss: 0.126240\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 873.77, NNZs: 736, Bias: -1.222636, T: 32307, Avg. loss: 0.124836\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 873.77, NNZs: 734, Bias: -1.221600, T: 33286, Avg. loss: 0.122820\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 873.76, NNZs: 736, Bias: -1.221377, T: 34265, Avg. loss: 0.121786\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 873.76, NNZs: 732, Bias: -1.220432, T: 35244, Avg. loss: 0.120058\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 873.76, NNZs: 732, Bias: -1.220363, T: 36223, Avg. loss: 0.119115\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 873.76, NNZs: 732, Bias: -1.220257, T: 37202, Avg. loss: 0.118007\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 873.76, NNZs: 730, Bias: -1.219369, T: 38181, Avg. loss: 0.116036\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 873.76, NNZs: 730, Bias: -1.218577, T: 39160, Avg. loss: 0.115265\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 873.76, NNZs: 730, Bias: -1.217629, T: 40139, Avg. loss: 0.114489\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 873.75, NNZs: 729, Bias: -1.217501, T: 41118, Avg. loss: 0.113559\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 873.75, NNZs: 727, Bias: -1.217236, T: 42097, Avg. loss: 0.112384\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 873.75, NNZs: 727, Bias: -1.216621, T: 43076, Avg. loss: 0.111289\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 873.75, NNZs: 727, Bias: -1.216166, T: 44055, Avg. loss: 0.110606\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 873.75, NNZs: 727, Bias: -1.215608, T: 45034, Avg. loss: 0.109613\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 873.75, NNZs: 726, Bias: -1.215462, T: 46013, Avg. loss: 0.109046\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 873.75, NNZs: 726, Bias: -1.215120, T: 46992, Avg. loss: 0.108445\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 873.75, NNZs: 725, Bias: -1.214809, T: 47971, Avg. loss: 0.107766\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 49 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 616429.61, NNZs: 703, Bias: -1348.228954, T: 979, Avg. loss: 59176449.722024\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 930512.31, NNZs: 908, Bias: -9126.747828, T: 1958, Avg. loss: 380962768.505254\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 917530.28, NNZs: 919, Bias: -8637.905277, T: 2937, Avg. loss: 41234938.004057\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 911833.32, NNZs: 926, Bias: -10402.597097, T: 3916, Avg. loss: 9026528.549260\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 911038.80, NNZs: 927, Bias: -10781.997302, T: 4895, Avg. loss: 971225.546377\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 910795.24, NNZs: 927, Bias: -10924.904074, T: 5874, Avg. loss: 227839.158168\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 910701.05, NNZs: 927, Bias: -10983.104966, T: 6853, Avg. loss: 84428.750207\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 910647.31, NNZs: 926, Bias: -11019.590849, T: 7832, Avg. loss: 37470.459727\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 910617.72, NNZs: 926, Bias: -11040.557199, T: 8811, Avg. loss: 18880.267418\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 910601.31, NNZs: 926, Bias: -11052.279227, T: 9790, Avg. loss: 10814.700895\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 910589.57, NNZs: 926, Bias: -11060.890106, T: 10769, Avg. loss: 6668.529109\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 910581.26, NNZs: 926, Bias: -11067.125792, T: 11748, Avg. loss: 4267.883220\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 910575.43, NNZs: 926, Bias: -11071.548091, T: 12727, Avg. loss: 2886.029780\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 910571.36, NNZs: 926, Bias: -11074.645389, T: 13706, Avg. loss: 2017.082579\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 910568.12, NNZs: 926, Bias: -11077.136508, T: 14685, Avg. loss: 1456.178775\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 910565.59, NNZs: 926, Bias: -11079.102165, T: 15664, Avg. loss: 1080.879263\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 910563.44, NNZs: 926, Bias: -11080.789110, T: 16643, Avg. loss: 819.735403\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 910561.79, NNZs: 926, Bias: -11082.083860, T: 17622, Avg. loss: 629.295249\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 910560.46, NNZs: 926, Bias: -11083.132092, T: 18601, Avg. loss: 490.369161\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 910559.28, NNZs: 926, Bias: -11084.063035, T: 19580, Avg. loss: 389.610364\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 910558.35, NNZs: 926, Bias: -11084.803635, T: 20559, Avg. loss: 310.965282\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 910557.57, NNZs: 926, Bias: -11085.422266, T: 21538, Avg. loss: 251.561125\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 910556.80, NNZs: 926, Bias: -11086.037006, T: 22517, Avg. loss: 207.318453\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 910556.18, NNZs: 926, Bias: -11086.531982, T: 23496, Avg. loss: 171.322786\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 910555.65, NNZs: 926, Bias: -11086.957134, T: 24475, Avg. loss: 142.792266\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 910555.17, NNZs: 926, Bias: -11087.344669, T: 25454, Avg. loss: 120.108825\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 910554.76, NNZs: 926, Bias: -11087.677431, T: 26433, Avg. loss: 101.897476\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 910554.39, NNZs: 926, Bias: -11087.972783, T: 27412, Avg. loss: 86.934452\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 910554.06, NNZs: 926, Bias: -11088.243893, T: 28391, Avg. loss: 74.655487\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 910553.77, NNZs: 926, Bias: -11088.473397, T: 29370, Avg. loss: 64.260088\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 910553.50, NNZs: 926, Bias: -11088.691586, T: 30349, Avg. loss: 55.840649\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 910553.26, NNZs: 926, Bias: -11088.888090, T: 31328, Avg. loss: 48.644422\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 910553.05, NNZs: 926, Bias: -11089.062250, T: 32307, Avg. loss: 42.553698\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 910552.85, NNZs: 926, Bias: -11089.223698, T: 33286, Avg. loss: 37.440343\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 910552.67, NNZs: 926, Bias: -11089.365164, T: 34265, Avg. loss: 33.038311\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 910552.51, NNZs: 926, Bias: -11089.492927, T: 35244, Avg. loss: 29.276596\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 910552.37, NNZs: 926, Bias: -11089.610525, T: 36223, Avg. loss: 26.021583\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 910552.23, NNZs: 926, Bias: -11089.721232, T: 37202, Avg. loss: 23.264936\n",
            "Total training time: 0.04 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 39\n",
            "Norm: 910552.11, NNZs: 926, Bias: -11089.821681, T: 38181, Avg. loss: 20.797761\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 910552.00, NNZs: 926, Bias: -11089.914239, T: 39160, Avg. loss: 18.654686\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 910551.89, NNZs: 926, Bias: -11090.001359, T: 40139, Avg. loss: 16.800858\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 910551.79, NNZs: 926, Bias: -11090.080019, T: 41118, Avg. loss: 15.152164\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 910551.70, NNZs: 926, Bias: -11090.153179, T: 42097, Avg. loss: 13.709435\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 910551.62, NNZs: 926, Bias: -11090.219669, T: 43076, Avg. loss: 12.437304\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 910551.55, NNZs: 926, Bias: -11090.281943, T: 44055, Avg. loss: 11.306136\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 910551.47, NNZs: 926, Bias: -11090.340160, T: 45034, Avg. loss: 10.293995\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 910551.41, NNZs: 926, Bias: -11090.394434, T: 46013, Avg. loss: 9.395653\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 910551.34, NNZs: 926, Bias: -11090.446035, T: 46992, Avg. loss: 8.598278\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 910551.29, NNZs: 926, Bias: -11090.493459, T: 47971, Avg. loss: 7.874192\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 910551.23, NNZs: 926, Bias: -11090.539317, T: 48950, Avg. loss: 7.237808\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 910551.18, NNZs: 926, Bias: -11090.580513, T: 49929, Avg. loss: 6.646907\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 910551.13, NNZs: 926, Bias: -11090.621276, T: 50908, Avg. loss: 6.137416\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 910551.08, NNZs: 926, Bias: -11090.659491, T: 51887, Avg. loss: 5.663245\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 910551.04, NNZs: 926, Bias: -11090.694073, T: 52866, Avg. loss: 5.227247\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 910551.00, NNZs: 926, Bias: -11090.727577, T: 53845, Avg. loss: 4.850375\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 910550.96, NNZs: 926, Bias: -11090.758882, T: 54824, Avg. loss: 4.496591\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 910550.93, NNZs: 926, Bias: -11090.788073, T: 55803, Avg. loss: 4.173553\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 910550.89, NNZs: 926, Bias: -11090.815908, T: 56782, Avg. loss: 3.883936\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 910550.86, NNZs: 926, Bias: -11090.841465, T: 57761, Avg. loss: 3.615077\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 910550.83, NNZs: 926, Bias: -11090.866463, T: 58740, Avg. loss: 3.371035\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 910550.80, NNZs: 926, Bias: -11090.890140, T: 59719, Avg. loss: 3.146856\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 910550.77, NNZs: 926, Bias: -11090.911836, T: 60698, Avg. loss: 2.935571\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 910550.75, NNZs: 926, Bias: -11090.933382, T: 61677, Avg. loss: 2.751123\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 910550.72, NNZs: 926, Bias: -11090.953639, T: 62656, Avg. loss: 2.575904\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 910550.70, NNZs: 926, Bias: -11090.973052, T: 63635, Avg. loss: 2.414727\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 910550.68, NNZs: 926, Bias: -11090.991234, T: 64614, Avg. loss: 2.265525\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 910550.66, NNZs: 926, Bias: -11091.008933, T: 65593, Avg. loss: 2.130114\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 910550.64, NNZs: 926, Bias: -11091.025136, T: 66572, Avg. loss: 2.000151\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 910550.62, NNZs: 926, Bias: -11091.040787, T: 67551, Avg. loss: 1.883211\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 910550.60, NNZs: 926, Bias: -11091.055957, T: 68530, Avg. loss: 1.774406\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 910550.58, NNZs: 926, Bias: -11091.070453, T: 69509, Avg. loss: 1.673117\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 910550.56, NNZs: 926, Bias: -11091.084135, T: 70488, Avg. loss: 1.578292\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 910550.55, NNZs: 926, Bias: -11091.097026, T: 71467, Avg. loss: 1.490286\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 910550.53, NNZs: 926, Bias: -11091.109307, T: 72446, Avg. loss: 1.407962\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 910550.52, NNZs: 926, Bias: -11091.121342, T: 73425, Avg. loss: 1.334067\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 910550.50, NNZs: 926, Bias: -11091.132541, T: 74404, Avg. loss: 1.261285\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 910550.49, NNZs: 926, Bias: -11091.143555, T: 75383, Avg. loss: 1.195805\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 910550.48, NNZs: 926, Bias: -11091.154098, T: 76362, Avg. loss: 1.134241\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 910550.47, NNZs: 926, Bias: -11091.164120, T: 77341, Avg. loss: 1.076158\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 910550.45, NNZs: 926, Bias: -11091.173813, T: 78320, Avg. loss: 1.021805\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 910550.44, NNZs: 926, Bias: -11091.183028, T: 79299, Avg. loss: 0.971084\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 910550.43, NNZs: 926, Bias: -11091.191848, T: 80278, Avg. loss: 0.922474\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 910550.42, NNZs: 926, Bias: -11091.200577, T: 81257, Avg. loss: 0.878831\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 910550.41, NNZs: 926, Bias: -11091.208603, T: 82236, Avg. loss: 0.835829\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 910550.40, NNZs: 926, Bias: -11091.216625, T: 83215, Avg. loss: 0.796827\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 910550.39, NNZs: 926, Bias: -11091.224179, T: 84194, Avg. loss: 0.759239\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 910550.38, NNZs: 926, Bias: -11091.231337, T: 85173, Avg. loss: 0.724177\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 910550.38, NNZs: 926, Bias: -11091.238331, T: 86152, Avg. loss: 0.691715\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 910550.37, NNZs: 926, Bias: -11091.245040, T: 87131, Avg. loss: 0.660903\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 910550.36, NNZs: 926, Bias: -11091.251305, T: 88110, Avg. loss: 0.631075\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 910550.35, NNZs: 926, Bias: -11091.257303, T: 89089, Avg. loss: 0.603248\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 910550.35, NNZs: 926, Bias: -11091.263089, T: 90068, Avg. loss: 0.577356\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 910550.34, NNZs: 926, Bias: -11091.268641, T: 91047, Avg. loss: 0.552706\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 910550.33, NNZs: 926, Bias: -11091.274136, T: 92026, Avg. loss: 0.529597\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 910550.33, NNZs: 926, Bias: -11091.279358, T: 93005, Avg. loss: 0.507379\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 910550.32, NNZs: 926, Bias: -11091.284328, T: 93984, Avg. loss: 0.486167\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 910550.31, NNZs: 926, Bias: -11091.289127, T: 94963, Avg. loss: 0.466267\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 910550.31, NNZs: 926, Bias: -11091.293852, T: 95942, Avg. loss: 0.447519\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 910550.30, NNZs: 926, Bias: -11091.298391, T: 96921, Avg. loss: 0.429874\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 910550.30, NNZs: 926, Bias: -11091.302670, T: 97900, Avg. loss: 0.412609\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 910550.29, NNZs: 926, Bias: -11091.306818, T: 98879, Avg. loss: 0.396803\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 910550.29, NNZs: 926, Bias: -11091.310922, T: 99858, Avg. loss: 0.381682\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 910550.28, NNZs: 926, Bias: -11091.314834, T: 100837, Avg. loss: 0.366803\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 910550.28, NNZs: 926, Bias: -11091.318624, T: 101816, Avg. loss: 0.353259\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 910550.27, NNZs: 926, Bias: -11091.322193, T: 102795, Avg. loss: 0.339650\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 910550.27, NNZs: 926, Bias: -11091.325801, T: 103774, Avg. loss: 0.327648\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 910550.26, NNZs: 926, Bias: -11091.329282, T: 104753, Avg. loss: 0.315761\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 910550.26, NNZs: 926, Bias: -11091.332593, T: 105732, Avg. loss: 0.304025\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 910550.26, NNZs: 926, Bias: -11091.335858, T: 106711, Avg. loss: 0.293632\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 910550.25, NNZs: 926, Bias: -11091.338981, T: 107690, Avg. loss: 0.282956\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 910550.25, NNZs: 926, Bias: -11091.341946, T: 108669, Avg. loss: 0.272908\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 910550.25, NNZs: 926, Bias: -11091.344939, T: 109648, Avg. loss: 0.263692\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 910550.24, NNZs: 926, Bias: -11091.347808, T: 110627, Avg. loss: 0.254697\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 910550.24, NNZs: 926, Bias: -11091.350526, T: 111606, Avg. loss: 0.245986\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 910550.24, NNZs: 926, Bias: -11091.353170, T: 112585, Avg. loss: 0.237846\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 910550.23, NNZs: 926, Bias: -11091.355691, T: 113564, Avg. loss: 0.229817\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 910550.23, NNZs: 926, Bias: -11091.358254, T: 114543, Avg. loss: 0.222525\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 910550.23, NNZs: 926, Bias: -11091.360599, T: 115522, Avg. loss: 0.215054\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 910550.22, NNZs: 926, Bias: -11091.362955, T: 116501, Avg. loss: 0.208299\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 910550.22, NNZs: 926, Bias: -11091.365223, T: 117480, Avg. loss: 0.201746\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 910550.22, NNZs: 926, Bias: -11091.367471, T: 118459, Avg. loss: 0.195432\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 910550.22, NNZs: 926, Bias: -11091.369593, T: 119438, Avg. loss: 0.189262\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 910550.21, NNZs: 926, Bias: -11091.371691, T: 120417, Avg. loss: 0.183571\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 910550.21, NNZs: 926, Bias: -11091.373734, T: 121396, Avg. loss: 0.177941\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 910550.21, NNZs: 926, Bias: -11091.375702, T: 122375, Avg. loss: 0.172636\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 910550.21, NNZs: 926, Bias: -11091.377587, T: 123354, Avg. loss: 0.167427\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 910550.20, NNZs: 926, Bias: -11091.379478, T: 124333, Avg. loss: 0.162764\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 910550.20, NNZs: 926, Bias: -11091.381270, T: 125312, Avg. loss: 0.157752\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 910550.20, NNZs: 926, Bias: -11091.383091, T: 126291, Avg. loss: 0.153414\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 910550.20, NNZs: 926, Bias: -11091.384821, T: 127270, Avg. loss: 0.148856\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 910550.19, NNZs: 926, Bias: -11091.386476, T: 128249, Avg. loss: 0.144681\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 910550.19, NNZs: 926, Bias: -11091.388037, T: 129228, Avg. loss: 0.140466\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 910550.19, NNZs: 926, Bias: -11091.389579, T: 130207, Avg. loss: 0.136702\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 910550.19, NNZs: 926, Bias: -11091.391094, T: 131186, Avg. loss: 0.132942\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 910550.19, NNZs: 926, Bias: -11091.392587, T: 132165, Avg. loss: 0.129420\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 910550.19, NNZs: 926, Bias: -11091.394039, T: 133144, Avg. loss: 0.125911\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 910550.18, NNZs: 926, Bias: -11091.395456, T: 134123, Avg. loss: 0.122601\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 910550.18, NNZs: 926, Bias: -11091.396865, T: 135102, Avg. loss: 0.119578\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 910550.18, NNZs: 926, Bias: -11091.398189, T: 136081, Avg. loss: 0.116233\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 910550.18, NNZs: 926, Bias: -11091.399524, T: 137060, Avg. loss: 0.113366\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 910550.18, NNZs: 926, Bias: -11091.400755, T: 138039, Avg. loss: 0.110356\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 910550.18, NNZs: 926, Bias: -11091.401929, T: 139018, Avg. loss: 0.107430\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 910550.17, NNZs: 926, Bias: -11091.403092, T: 139997, Avg. loss: 0.104775\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 910550.17, NNZs: 926, Bias: -11091.404180, T: 140976, Avg. loss: 0.102088\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 910550.17, NNZs: 926, Bias: -11091.405319, T: 141955, Avg. loss: 0.099844\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 910550.17, NNZs: 926, Bias: -11091.406395, T: 142934, Avg. loss: 0.097367\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 910550.17, NNZs: 926, Bias: -11091.407465, T: 143913, Avg. loss: 0.095076\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 910550.17, NNZs: 926, Bias: -11091.408454, T: 144892, Avg. loss: 0.092699\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 910550.17, NNZs: 926, Bias: -11091.409466, T: 145871, Avg. loss: 0.090698\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 910550.17, NNZs: 926, Bias: -11091.410415, T: 146850, Avg. loss: 0.088454\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 910550.16, NNZs: 926, Bias: -11091.411350, T: 147829, Avg. loss: 0.086486\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 910550.16, NNZs: 926, Bias: -11091.412278, T: 148808, Avg. loss: 0.084499\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 910550.16, NNZs: 926, Bias: -11091.413172, T: 149787, Avg. loss: 0.082614\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 910550.16, NNZs: 926, Bias: -11091.414052, T: 150766, Avg. loss: 0.080894\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 910550.16, NNZs: 926, Bias: -11091.414916, T: 151745, Avg. loss: 0.079064\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 910550.16, NNZs: 926, Bias: -11091.415764, T: 152724, Avg. loss: 0.077339\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 910550.16, NNZs: 926, Bias: -11091.416617, T: 153703, Avg. loss: 0.075751\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 910550.16, NNZs: 926, Bias: -11091.417434, T: 154682, Avg. loss: 0.074028\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 910550.16, NNZs: 926, Bias: -11091.418217, T: 155661, Avg. loss: 0.072489\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 910550.16, NNZs: 926, Bias: -11091.418964, T: 156640, Avg. loss: 0.070979\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 910550.15, NNZs: 926, Bias: -11091.419691, T: 157619, Avg. loss: 0.069506\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 910550.15, NNZs: 926, Bias: -11091.420386, T: 158598, Avg. loss: 0.068037\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 910550.15, NNZs: 926, Bias: -11091.421109, T: 159577, Avg. loss: 0.066766\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 910550.15, NNZs: 926, Bias: -11091.421796, T: 160556, Avg. loss: 0.065441\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 910550.15, NNZs: 926, Bias: -11091.422459, T: 161535, Avg. loss: 0.064126\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 910550.15, NNZs: 926, Bias: -11091.423089, T: 162514, Avg. loss: 0.062835\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 910550.15, NNZs: 926, Bias: -11091.423702, T: 163493, Avg. loss: 0.061654\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 910550.15, NNZs: 926, Bias: -11091.424290, T: 164472, Avg. loss: 0.060405\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 910550.15, NNZs: 926, Bias: -11091.424858, T: 165451, Avg. loss: 0.059225\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 910550.15, NNZs: 926, Bias: -11091.425442, T: 166430, Avg. loss: 0.058237\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 910550.15, NNZs: 926, Bias: -11091.426006, T: 167409, Avg. loss: 0.057171\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 910550.15, NNZs: 926, Bias: -11091.426553, T: 168388, Avg. loss: 0.056060\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 910550.15, NNZs: 926, Bias: -11091.427103, T: 169367, Avg. loss: 0.055113\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 910550.14, NNZs: 926, Bias: -11091.427612, T: 170346, Avg. loss: 0.054053\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 910550.14, NNZs: 926, Bias: -11091.428130, T: 171325, Avg. loss: 0.053221\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 910550.14, NNZs: 926, Bias: -11091.428628, T: 172304, Avg. loss: 0.052311\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 910550.14, NNZs: 926, Bias: -11091.429088, T: 173283, Avg. loss: 0.051365\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 910550.14, NNZs: 926, Bias: -11091.429530, T: 174262, Avg. loss: 0.050511\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 910550.14, NNZs: 926, Bias: -11091.429957, T: 175241, Avg. loss: 0.049653\n",
            "Total training time: 0.19 seconds.\n",
            "Convergence after 179 epochs took 0.19 seconds\n",
            "-- Epoch 1\n",
            "Norm: 216.24, NNZs: 639, Bias: -1.176486, T: 979, Avg. loss: 10.041642\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 213.09, NNZs: 570, Bias: -1.307528, T: 1958, Avg. loss: 2.333853\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 212.84, NNZs: 543, Bias: -1.010736, T: 2937, Avg. loss: 0.585172\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 212.70, NNZs: 518, Bias: -0.975088, T: 3916, Avg. loss: 0.374155\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 212.66, NNZs: 503, Bias: -0.927561, T: 4895, Avg. loss: 0.294646\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 212.64, NNZs: 493, Bias: -0.911654, T: 5874, Avg. loss: 0.249137\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 212.63, NNZs: 478, Bias: -0.891384, T: 6853, Avg. loss: 0.229321\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 212.63, NNZs: 472, Bias: -0.866634, T: 7832, Avg. loss: 0.214298\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 212.64, NNZs: 465, Bias: -0.853819, T: 8811, Avg. loss: 0.199950\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 212.64, NNZs: 459, Bias: -0.833858, T: 9790, Avg. loss: 0.194496\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 212.65, NNZs: 454, Bias: -0.827844, T: 10769, Avg. loss: 0.186380\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 212.66, NNZs: 448, Bias: -0.819088, T: 11748, Avg. loss: 0.181979\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 212.67, NNZs: 445, Bias: -0.802741, T: 12727, Avg. loss: 0.182734\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 212.68, NNZs: 444, Bias: -0.794250, T: 13706, Avg. loss: 0.176905\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 212.69, NNZs: 430, Bias: -0.786791, T: 14685, Avg. loss: 0.174263\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 212.70, NNZs: 429, Bias: -0.778295, T: 15664, Avg. loss: 0.172962\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 212.71, NNZs: 427, Bias: -0.774785, T: 16643, Avg. loss: 0.167581\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 212.72, NNZs: 426, Bias: -0.767301, T: 17622, Avg. loss: 0.170217\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 212.73, NNZs: 426, Bias: -0.760011, T: 18601, Avg. loss: 0.169546\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 212.73, NNZs: 423, Bias: -0.753507, T: 19580, Avg. loss: 0.168853\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 212.74, NNZs: 420, Bias: -0.747430, T: 20559, Avg. loss: 0.167425\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 212.75, NNZs: 415, Bias: -0.741522, T: 21538, Avg. loss: 0.167184\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 22 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 1176.73, NNZs: 745, Bias: -2.528698, T: 979, Avg. loss: 283.042437\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1114.99, NNZs: 813, Bias: 0.768256, T: 1958, Avg. loss: 170.344095\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1106.77, NNZs: 815, Bias: -0.909200, T: 2937, Avg. loss: 20.875302\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm: 1105.29, NNZs: 806, Bias: -1.173451, T: 3916, Avg. loss: 4.719774\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1104.57, NNZs: 806, Bias: -1.099916, T: 4895, Avg. loss: 2.757832\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1104.15, NNZs: 804, Bias: -1.082595, T: 5874, Avg. loss: 1.895800\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1103.87, NNZs: 802, Bias: -1.128957, T: 6853, Avg. loss: 1.441822\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1103.67, NNZs: 795, Bias: -1.181774, T: 7832, Avg. loss: 1.136727\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1103.53, NNZs: 793, Bias: -1.223131, T: 8811, Avg. loss: 0.938318\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1103.42, NNZs: 790, Bias: -1.234737, T: 9790, Avg. loss: 0.802611\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1103.34, NNZs: 789, Bias: -1.240444, T: 10769, Avg. loss: 0.699975\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1103.28, NNZs: 788, Bias: -1.259296, T: 11748, Avg. loss: 0.628813\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 1103.22, NNZs: 785, Bias: -1.260120, T: 12727, Avg. loss: 0.563567\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 1103.18, NNZs: 785, Bias: -1.270855, T: 13706, Avg. loss: 0.514755\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 1103.14, NNZs: 782, Bias: -1.275808, T: 14685, Avg. loss: 0.473457\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 1103.10, NNZs: 783, Bias: -1.284305, T: 15664, Avg. loss: 0.442141\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 1103.07, NNZs: 783, Bias: -1.289529, T: 16643, Avg. loss: 0.411710\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 1103.05, NNZs: 779, Bias: -1.297932, T: 17622, Avg. loss: 0.387593\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 1103.03, NNZs: 779, Bias: -1.302822, T: 18601, Avg. loss: 0.366039\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 1103.01, NNZs: 776, Bias: -1.309942, T: 19580, Avg. loss: 0.347792\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 1102.99, NNZs: 774, Bias: -1.315401, T: 20559, Avg. loss: 0.330546\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 1102.97, NNZs: 775, Bias: -1.318005, T: 21538, Avg. loss: 0.315262\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 1102.96, NNZs: 775, Bias: -1.321047, T: 22517, Avg. loss: 0.302135\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 1102.94, NNZs: 772, Bias: -1.323515, T: 23496, Avg. loss: 0.290353\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 1102.93, NNZs: 771, Bias: -1.324524, T: 24475, Avg. loss: 0.278932\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 1102.92, NNZs: 771, Bias: -1.327585, T: 25454, Avg. loss: 0.269913\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 1102.91, NNZs: 770, Bias: -1.330418, T: 26433, Avg. loss: 0.260409\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 1102.90, NNZs: 769, Bias: -1.331316, T: 27412, Avg. loss: 0.252372\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 1102.89, NNZs: 768, Bias: -1.333671, T: 28391, Avg. loss: 0.245162\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 1102.89, NNZs: 768, Bias: -1.336029, T: 29370, Avg. loss: 0.238206\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 1102.88, NNZs: 765, Bias: -1.338570, T: 30349, Avg. loss: 0.231770\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 1102.87, NNZs: 762, Bias: -1.340120, T: 31328, Avg. loss: 0.226421\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 1102.87, NNZs: 762, Bias: -1.342354, T: 32307, Avg. loss: 0.220544\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 1102.86, NNZs: 761, Bias: -1.343311, T: 33286, Avg. loss: 0.215489\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 1102.85, NNZs: 761, Bias: -1.345271, T: 34265, Avg. loss: 0.211009\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 1102.85, NNZs: 759, Bias: -1.345818, T: 35244, Avg. loss: 0.206075\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 1102.84, NNZs: 759, Bias: -1.347602, T: 36223, Avg. loss: 0.202048\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 1102.84, NNZs: 758, Bias: -1.349067, T: 37202, Avg. loss: 0.197887\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 1102.83, NNZs: 757, Bias: -1.350279, T: 38181, Avg. loss: 0.194039\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 1102.83, NNZs: 757, Bias: -1.351855, T: 39160, Avg. loss: 0.190953\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 1102.83, NNZs: 756, Bias: -1.352886, T: 40139, Avg. loss: 0.187260\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 1102.82, NNZs: 755, Bias: -1.354107, T: 41118, Avg. loss: 0.184251\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 1102.82, NNZs: 754, Bias: -1.355329, T: 42097, Avg. loss: 0.181190\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 1102.82, NNZs: 754, Bias: -1.356633, T: 43076, Avg. loss: 0.178394\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 1102.81, NNZs: 754, Bias: -1.357669, T: 44055, Avg. loss: 0.175889\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 1102.81, NNZs: 753, Bias: -1.358978, T: 45034, Avg. loss: 0.173446\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 1102.81, NNZs: 753, Bias: -1.360077, T: 46013, Avg. loss: 0.170955\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 1102.80, NNZs: 751, Bias: -1.361154, T: 46992, Avg. loss: 0.168517\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 1102.80, NNZs: 750, Bias: -1.362020, T: 47971, Avg. loss: 0.165865\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 1102.80, NNZs: 750, Bias: -1.362686, T: 48950, Avg. loss: 0.164403\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 1102.80, NNZs: 750, Bias: -1.363400, T: 49929, Avg. loss: 0.162373\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 1102.79, NNZs: 750, Bias: -1.364170, T: 50908, Avg. loss: 0.160099\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 1102.79, NNZs: 750, Bias: -1.365196, T: 51887, Avg. loss: 0.158395\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 1102.79, NNZs: 749, Bias: -1.365979, T: 52866, Avg. loss: 0.156782\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 1102.79, NNZs: 749, Bias: -1.366591, T: 53845, Avg. loss: 0.154789\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 1102.78, NNZs: 748, Bias: -1.367608, T: 54824, Avg. loss: 0.153268\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 1102.78, NNZs: 748, Bias: -1.368234, T: 55803, Avg. loss: 0.151458\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 1102.78, NNZs: 748, Bias: -1.368647, T: 56782, Avg. loss: 0.150000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 1102.78, NNZs: 746, Bias: -1.369532, T: 57761, Avg. loss: 0.148380\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 1102.78, NNZs: 746, Bias: -1.370347, T: 58740, Avg. loss: 0.146924\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 1102.78, NNZs: 746, Bias: -1.371030, T: 59719, Avg. loss: 0.145676\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 1102.77, NNZs: 746, Bias: -1.371925, T: 60698, Avg. loss: 0.144317\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 1102.77, NNZs: 746, Bias: -1.372489, T: 61677, Avg. loss: 0.142792\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 1102.77, NNZs: 745, Bias: -1.373043, T: 62656, Avg. loss: 0.141605\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 1102.77, NNZs: 744, Bias: -1.373686, T: 63635, Avg. loss: 0.140475\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 1102.77, NNZs: 744, Bias: -1.374283, T: 64614, Avg. loss: 0.139433\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 1102.77, NNZs: 744, Bias: -1.374828, T: 65593, Avg. loss: 0.138107\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 1102.77, NNZs: 744, Bias: -1.375581, T: 66572, Avg. loss: 0.137171\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 1102.76, NNZs: 745, Bias: -1.375996, T: 67551, Avg. loss: 0.135972\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 1102.76, NNZs: 745, Bias: -1.376468, T: 68530, Avg. loss: 0.135019\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 1102.76, NNZs: 745, Bias: -1.376914, T: 69509, Avg. loss: 0.133813\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 1102.76, NNZs: 744, Bias: -1.377394, T: 70488, Avg. loss: 0.133076\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 1102.76, NNZs: 744, Bias: -1.377872, T: 71467, Avg. loss: 0.132154\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 1102.76, NNZs: 744, Bias: -1.378194, T: 72446, Avg. loss: 0.130926\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 1102.76, NNZs: 744, Bias: -1.378661, T: 73425, Avg. loss: 0.130138\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 1102.76, NNZs: 744, Bias: -1.378982, T: 74404, Avg. loss: 0.129346\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 1102.76, NNZs: 743, Bias: -1.379394, T: 75383, Avg. loss: 0.128629\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 1102.75, NNZs: 743, Bias: -1.379705, T: 76362, Avg. loss: 0.127562\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 1102.75, NNZs: 741, Bias: -1.379912, T: 77341, Avg. loss: 0.126732\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 1102.75, NNZs: 741, Bias: -1.380148, T: 78320, Avg. loss: 0.126112\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 1102.75, NNZs: 741, Bias: -1.380405, T: 79299, Avg. loss: 0.125108\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 1102.75, NNZs: 741, Bias: -1.380706, T: 80278, Avg. loss: 0.124536\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 1102.75, NNZs: 739, Bias: -1.380888, T: 81257, Avg. loss: 0.123853\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 1102.75, NNZs: 739, Bias: -1.381108, T: 82236, Avg. loss: 0.123080\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 1102.75, NNZs: 737, Bias: -1.381189, T: 83215, Avg. loss: 0.122340\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 1102.75, NNZs: 737, Bias: -1.381397, T: 84194, Avg. loss: 0.121747\n",
            "Total training time: 0.08 seconds.\n",
            "Convergence after 86 epochs took 0.08 seconds\n",
            "-- Epoch 1\n",
            "Norm: 802111.39, NNZs: 756, Bias: -2497.530864, T: 979, Avg. loss: 89560151.286223\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 956968.22, NNZs: 885, Bias: -6941.507036, T: 1958, Avg. loss: 298795580.862175\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 944157.86, NNZs: 896, Bias: -8530.693285, T: 2937, Avg. loss: 30715372.773712\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 941426.99, NNZs: 897, Bias: -9810.172843, T: 3916, Avg. loss: 3603081.577317\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 940962.70, NNZs: 897, Bias: -10110.933934, T: 4895, Avg. loss: 408027.250937\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 940810.05, NNZs: 897, Bias: -10226.907277, T: 5874, Avg. loss: 87966.169241\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 940756.93, NNZs: 897, Bias: -10268.607090, T: 6853, Avg. loss: 27497.852752\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 940725.50, NNZs: 897, Bias: -10294.754164, T: 7832, Avg. loss: 11420.323762\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 940707.58, NNZs: 897, Bias: -10310.016747, T: 8811, Avg. loss: 5520.497421\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 940697.83, NNZs: 897, Bias: -10318.356740, T: 9790, Avg. loss: 3066.680153\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 940691.72, NNZs: 897, Bias: -10323.605780, T: 10769, Avg. loss: 1845.468053\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 940687.45, NNZs: 897, Bias: -10327.303843, T: 11748, Avg. loss: 1241.261261\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 940683.75, NNZs: 897, Bias: -10330.543612, T: 12727, Avg. loss: 895.785352\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 940681.37, NNZs: 897, Bias: -10332.623922, T: 13706, Avg. loss: 627.948767\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 940679.26, NNZs: 897, Bias: -10334.480987, T: 14685, Avg. loss: 493.646454\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 940677.74, NNZs: 897, Bias: -10335.820263, T: 15664, Avg. loss: 383.345564\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 940676.64, NNZs: 897, Bias: -10336.786798, T: 16643, Avg. loss: 296.343585\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 940675.62, NNZs: 897, Bias: -10337.692205, T: 17622, Avg. loss: 252.046462\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 940674.77, NNZs: 897, Bias: -10338.438886, T: 18601, Avg. loss: 208.310092\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 940674.09, NNZs: 897, Bias: -10339.042364, T: 19580, Avg. loss: 174.671985\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 940673.49, NNZs: 897, Bias: -10339.573939, T: 20559, Avg. loss: 149.289105\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 940673.06, NNZs: 897, Bias: -10339.954728, T: 21538, Avg. loss: 123.489093\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 940672.68, NNZs: 897, Bias: -10340.287775, T: 22517, Avg. loss: 109.096844\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 940672.33, NNZs: 897, Bias: -10340.597781, T: 23496, Avg. loss: 96.946795\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 940671.97, NNZs: 897, Bias: -10340.925555, T: 24475, Avg. loss: 89.245187\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 940671.69, NNZs: 897, Bias: -10341.167506, T: 25454, Avg. loss: 76.685644\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 940671.42, NNZs: 897, Bias: -10341.414500, T: 26433, Avg. loss: 71.524534\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 940671.21, NNZs: 897, Bias: -10341.599088, T: 27412, Avg. loss: 62.165314\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 940671.01, NNZs: 897, Bias: -10341.774202, T: 28391, Avg. loss: 56.937478\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 940670.84, NNZs: 897, Bias: -10341.930567, T: 29370, Avg. loss: 52.122248\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 940670.67, NNZs: 897, Bias: -10342.078772, T: 30349, Avg. loss: 47.975695\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 940670.49, NNZs: 897, Bias: -10342.234606, T: 31328, Avg. loss: 45.534463\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 940670.34, NNZs: 897, Bias: -10342.373870, T: 32307, Avg. loss: 41.799090\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 940670.20, NNZs: 897, Bias: -10342.494127, T: 33286, Avg. loss: 38.512578\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 940670.08, NNZs: 897, Bias: -10342.606309, T: 34265, Avg. loss: 35.754822\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 940669.97, NNZs: 897, Bias: -10342.706123, T: 35244, Avg. loss: 33.233629\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 940669.86, NNZs: 897, Bias: -10342.798413, T: 36223, Avg. loss: 31.013125\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 940669.77, NNZs: 897, Bias: -10342.880156, T: 37202, Avg. loss: 28.925785\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 940669.70, NNZs: 897, Bias: -10342.946278, T: 38181, Avg. loss: 26.333531\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 940669.63, NNZs: 897, Bias: -10343.008970, T: 39160, Avg. loss: 24.903975\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 940669.56, NNZs: 897, Bias: -10343.067973, T: 40139, Avg. loss: 23.537628\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 940669.50, NNZs: 897, Bias: -10343.126271, T: 41118, Avg. loss: 22.342767\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 940669.42, NNZs: 897, Bias: -10343.190663, T: 42097, Avg. loss: 21.788592\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 940669.36, NNZs: 897, Bias: -10343.249130, T: 43076, Avg. loss: 20.560539\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 940669.30, NNZs: 897, Bias: -10343.303734, T: 44055, Avg. loss: 19.472373\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 940669.25, NNZs: 897, Bias: -10343.347103, T: 45034, Avg. loss: 17.977938\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 940669.20, NNZs: 897, Bias: -10343.395144, T: 46013, Avg. loss: 17.601197\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 940669.15, NNZs: 897, Bias: -10343.433668, T: 46992, Avg. loss: 16.305974\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 940669.10, NNZs: 897, Bias: -10343.476395, T: 47971, Avg. loss: 15.992165\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 940669.06, NNZs: 897, Bias: -10343.516369, T: 48950, Avg. loss: 15.258727\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 940669.02, NNZs: 897, Bias: -10343.548318, T: 49929, Avg. loss: 14.203285\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 940668.98, NNZs: 897, Bias: -10343.584745, T: 50908, Avg. loss: 13.984446\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 940668.95, NNZs: 897, Bias: -10343.618453, T: 51887, Avg. loss: 13.367682\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 940668.91, NNZs: 897, Bias: -10343.649478, T: 52866, Avg. loss: 12.813650\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 940668.88, NNZs: 897, Bias: -10343.675518, T: 53845, Avg. loss: 12.003105\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 940668.85, NNZs: 897, Bias: -10343.703916, T: 54824, Avg. loss: 11.831636\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 940668.82, NNZs: 897, Bias: -10343.726906, T: 55803, Avg. loss: 11.109831\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 940668.80, NNZs: 897, Bias: -10343.753364, T: 56782, Avg. loss: 10.965839\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 940668.77, NNZs: 897, Bias: -10343.774302, T: 57761, Avg. loss: 10.308569\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 940668.74, NNZs: 897, Bias: -10343.798490, T: 58740, Avg. loss: 10.190014\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 940668.72, NNZs: 897, Bias: -10343.818626, T: 59719, Avg. loss: 9.609357\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 940668.70, NNZs: 897, Bias: -10343.840702, T: 60698, Avg. loss: 9.487317\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 940668.68, NNZs: 897, Bias: -10343.859142, T: 61677, Avg. loss: 8.963161\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 940668.65, NNZs: 897, Bias: -10343.879780, T: 62656, Avg. loss: 8.861176\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 940668.63, NNZs: 897, Bias: -10343.898995, T: 63635, Avg. loss: 8.552088\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 940668.61, NNZs: 897, Bias: -10343.917394, T: 64614, Avg. loss: 8.272591\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 940668.59, NNZs: 897, Bias: -10343.935108, T: 65593, Avg. loss: 7.999073\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 940668.58, NNZs: 897, Bias: -10343.949962, T: 66572, Avg. loss: 7.594326\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 940668.56, NNZs: 897, Bias: -10343.966315, T: 67551, Avg. loss: 7.518826\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 940668.54, NNZs: 897, Bias: -10343.980334, T: 68530, Avg. loss: 7.147378\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 940668.53, NNZs: 897, Bias: -10343.993961, T: 69509, Avg. loss: 6.946840\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 940668.51, NNZs: 897, Bias: -10344.007124, T: 70488, Avg. loss: 6.753518\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 940668.50, NNZs: 897, Bias: -10344.020022, T: 71467, Avg. loss: 6.566916\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 940668.48, NNZs: 897, Bias: -10344.032388, T: 72446, Avg. loss: 6.390926\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 940668.47, NNZs: 897, Bias: -10344.044310, T: 73425, Avg. loss: 6.214215\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 940668.46, NNZs: 897, Bias: -10344.057779, T: 74404, Avg. loss: 6.158674\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 940668.44, NNZs: 897, Bias: -10344.070829, T: 75383, Avg. loss: 5.987936\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 940668.43, NNZs: 897, Bias: -10344.083214, T: 76362, Avg. loss: 5.819418\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 940668.41, NNZs: 897, Bias: -10344.095101, T: 77341, Avg. loss: 5.664122\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 940668.40, NNZs: 897, Bias: -10344.106581, T: 78320, Avg. loss: 5.512796\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 940668.39, NNZs: 897, Bias: -10344.117418, T: 79299, Avg. loss: 5.366486\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 940668.38, NNZs: 897, Bias: -10344.127928, T: 80278, Avg. loss: 5.229257\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 940668.37, NNZs: 897, Bias: -10344.138283, T: 81257, Avg. loss: 5.097433\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 940668.36, NNZs: 897, Bias: -10344.146928, T: 82236, Avg. loss: 4.885630\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 940668.35, NNZs: 897, Bias: -10344.156592, T: 83215, Avg. loss: 4.852104\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 940668.33, NNZs: 897, Bias: -10344.165933, T: 84194, Avg. loss: 4.733699\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 940668.33, NNZs: 897, Bias: -10344.174007, T: 85173, Avg. loss: 4.548541\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 940668.32, NNZs: 895, Bias: -10344.182941, T: 86152, Avg. loss: 4.519149\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 940668.31, NNZs: 895, Bias: -10344.191598, T: 87131, Avg. loss: 4.412613\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 940668.30, NNZs: 895, Bias: -10344.199031, T: 88110, Avg. loss: 4.242049\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 940668.29, NNZs: 895, Bias: -10344.207235, T: 89089, Avg. loss: 4.215824\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 940668.28, NNZs: 895, Bias: -10344.214348, T: 90068, Avg. loss: 4.057579\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 940668.27, NNZs: 895, Bias: -10344.222108, T: 91047, Avg. loss: 4.031095\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 940668.26, NNZs: 895, Bias: -10344.228930, T: 92026, Avg. loss: 3.885524\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 940668.26, NNZs: 895, Bias: -10344.235601, T: 93005, Avg. loss: 3.807119\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 940668.25, NNZs: 895, Bias: -10344.242851, T: 93984, Avg. loss: 3.780818\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 940668.24, NNZs: 895, Bias: -10344.249928, T: 94963, Avg. loss: 3.700909\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 940668.23, NNZs: 895, Bias: -10344.256102, T: 95942, Avg. loss: 3.571511\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 940668.23, NNZs: 895, Bias: -10344.262858, T: 96921, Avg. loss: 3.551200\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 940668.22, NNZs: 895, Bias: -10344.268677, T: 97900, Avg. loss: 3.427852\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 940668.21, NNZs: 895, Bias: -10344.274384, T: 98879, Avg. loss: 3.362948\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 940668.21, NNZs: 895, Bias: -10344.280692, T: 99858, Avg. loss: 3.343865\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 940668.20, NNZs: 895, Bias: -10344.286808, T: 100837, Avg. loss: 3.277255\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 940668.19, NNZs: 895, Bias: -10344.292781, T: 101816, Avg. loss: 3.212260\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 940668.19, NNZs: 895, Bias: -10344.298497, T: 102795, Avg. loss: 3.147511\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 940668.18, NNZs: 895, Bias: -10344.304155, T: 103774, Avg. loss: 3.088270\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 940668.18, NNZs: 895, Bias: -10344.309114, T: 104753, Avg. loss: 2.988681\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 940668.17, NNZs: 895, Bias: -10344.313985, T: 105732, Avg. loss: 2.936079\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 940668.17, NNZs: 895, Bias: -10344.318715, T: 106711, Avg. loss: 2.882961\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 940668.16, NNZs: 895, Bias: -10344.323957, T: 107690, Avg. loss: 2.870106\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 940668.15, NNZs: 895, Bias: -10344.328589, T: 108669, Avg. loss: 2.781257\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 940668.15, NNZs: 895, Bias: -10344.333108, T: 109648, Avg. loss: 2.731500\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 940668.14, NNZs: 895, Bias: -10344.337967, T: 110627, Avg. loss: 2.718596\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 940668.14, NNZs: 895, Bias: -10344.342731, T: 111606, Avg. loss: 2.670422\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 940668.13, NNZs: 895, Bias: -10344.347368, T: 112585, Avg. loss: 2.622378\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 940668.13, NNZs: 893, Bias: -10344.351914, T: 113564, Avg. loss: 2.576260\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 940668.12, NNZs: 893, Bias: -10344.355970, T: 114543, Avg. loss: 2.502169\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 940668.12, NNZs: 893, Bias: -10344.360319, T: 115522, Avg. loss: 2.489613\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 940668.11, NNZs: 893, Bias: -10344.364205, T: 116501, Avg. loss: 2.418428\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 940668.11, NNZs: 893, Bias: -10344.368007, T: 117480, Avg. loss: 2.379373\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 940668.11, NNZs: 893, Bias: -10344.371719, T: 118459, Avg. loss: 2.341031\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 940668.10, NNZs: 893, Bias: -10344.375800, T: 119438, Avg. loss: 2.330979\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 940668.10, NNZs: 893, Bias: -10344.379442, T: 120417, Avg. loss: 2.267000\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 940668.09, NNZs: 893, Bias: -10344.382999, T: 121396, Avg. loss: 2.230355\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 940668.09, NNZs: 893, Bias: -10344.386841, T: 122375, Avg. loss: 2.221195\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 940668.09, NNZs: 893, Bias: -10344.390589, T: 123354, Avg. loss: 2.184789\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 940668.08, NNZs: 893, Bias: -10344.393923, T: 124333, Avg. loss: 2.126295\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 940668.08, NNZs: 893, Bias: -10344.397514, T: 125312, Avg. loss: 2.117031\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 940668.07, NNZs: 893, Bias: -10344.401121, T: 126291, Avg. loss: 2.085347\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 940668.07, NNZs: 893, Bias: -10344.404313, T: 127270, Avg. loss: 2.029988\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 940668.07, NNZs: 893, Bias: -10344.407444, T: 128249, Avg. loss: 1.999582\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 940668.06, NNZs: 893, Bias: -10344.410464, T: 129228, Avg. loss: 1.970259\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 940668.06, NNZs: 893, Bias: -10344.413465, T: 130207, Avg. loss: 1.941764\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 940668.06, NNZs: 893, Bias: -10344.416691, T: 131186, Avg. loss: 1.934030\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 940668.05, NNZs: 893, Bias: -10344.419567, T: 132165, Avg. loss: 1.885492\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 940668.05, NNZs: 893, Bias: -10344.422678, T: 133144, Avg. loss: 1.878297\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 940668.05, NNZs: 893, Bias: -10344.425446, T: 134123, Avg. loss: 1.831865\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 940668.04, NNZs: 893, Bias: -10344.428158, T: 135102, Avg. loss: 1.805750\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 940668.04, NNZs: 893, Bias: -10344.431055, T: 136081, Avg. loss: 1.799126\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 940668.04, NNZs: 893, Bias: -10344.433667, T: 137060, Avg. loss: 1.755681\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 940668.03, NNZs: 893, Bias: -10344.436216, T: 138039, Avg. loss: 1.731579\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 940668.03, NNZs: 893, Bias: -10344.438689, T: 139018, Avg. loss: 1.707560\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 940668.03, NNZs: 893, Bias: -10344.441137, T: 139997, Avg. loss: 1.685129\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 940668.03, NNZs: 893, Bias: -10344.443524, T: 140976, Avg. loss: 1.662311\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 940668.02, NNZs: 893, Bias: -10344.445899, T: 141955, Avg. loss: 1.640208\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 940668.02, NNZs: 893, Bias: -10344.448202, T: 142934, Avg. loss: 1.618247\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 940668.02, NNZs: 893, Bias: -10344.450718, T: 143913, Avg. loss: 1.612689\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 940668.02, NNZs: 893, Bias: -10344.452922, T: 144892, Avg. loss: 1.575478\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 940668.01, NNZs: 893, Bias: -10344.455384, T: 145871, Avg. loss: 1.570194\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 940668.01, NNZs: 893, Bias: -10344.457538, T: 146850, Avg. loss: 1.534502\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 940668.01, NNZs: 893, Bias: -10344.459683, T: 147829, Avg. loss: 1.514729\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 940668.01, NNZs: 893, Bias: -10344.461789, T: 148808, Avg. loss: 1.495504\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 940668.00, NNZs: 893, Bias: -10344.463857, T: 149787, Avg. loss: 1.476981\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 940668.00, NNZs: 893, Bias: -10344.465886, T: 150766, Avg. loss: 1.458146\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 940668.00, NNZs: 893, Bias: -10344.468087, T: 151745, Avg. loss: 1.453290\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 940668.00, NNZs: 893, Bias: -10344.470062, T: 152724, Avg. loss: 1.421829\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 940667.99, NNZs: 893, Bias: -10344.472199, T: 153703, Avg. loss: 1.417020\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 940667.99, NNZs: 893, Bias: -10344.474097, T: 154682, Avg. loss: 1.386384\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 940667.99, NNZs: 893, Bias: -10344.476168, T: 155661, Avg. loss: 1.381885\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 940667.99, NNZs: 892, Bias: -10344.478175, T: 156640, Avg. loss: 1.364443\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 940667.99, NNZs: 892, Bias: -10344.480150, T: 157619, Avg. loss: 1.347768\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 940667.98, NNZs: 892, Bias: -10344.482092, T: 158598, Avg. loss: 1.331449\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 940667.98, NNZs: 892, Bias: -10344.484002, T: 159577, Avg. loss: 1.315201\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 940667.98, NNZs: 892, Bias: -10344.485722, T: 160556, Avg. loss: 1.287969\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 940667.98, NNZs: 892, Bias: -10344.487572, T: 161535, Avg. loss: 1.284069\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 940667.98, NNZs: 892, Bias: -10344.489368, T: 162514, Avg. loss: 1.268420\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 940667.97, NNZs: 892, Bias: -10344.491139, T: 163493, Avg. loss: 1.253459\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 940667.97, NNZs: 892, Bias: -10344.492746, T: 164472, Avg. loss: 1.228431\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 940667.97, NNZs: 892, Bias: -10344.494488, T: 165451, Avg. loss: 1.224604\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 940667.97, NNZs: 892, Bias: -10344.496203, T: 166430, Avg. loss: 1.210575\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 940667.97, NNZs: 892, Bias: -10344.497881, T: 167409, Avg. loss: 1.196623\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 940667.96, NNZs: 892, Bias: -10344.499393, T: 168388, Avg. loss: 1.172788\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 940667.96, NNZs: 892, Bias: -10344.500890, T: 169367, Avg. loss: 1.159938\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 940667.96, NNZs: 892, Bias: -10344.502507, T: 170346, Avg. loss: 1.156766\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 940667.96, NNZs: 892, Bias: -10344.504111, T: 171325, Avg. loss: 1.143879\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 940667.96, NNZs: 892, Bias: -10344.505558, T: 172304, Avg. loss: 1.121841\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 940667.96, NNZs: 892, Bias: -10344.506976, T: 173283, Avg. loss: 1.109782\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 940667.95, NNZs: 892, Bias: -10344.508386, T: 174262, Avg. loss: 1.098010\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 940667.95, NNZs: 892, Bias: -10344.509905, T: 175241, Avg. loss: 1.094989\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 940667.95, NNZs: 892, Bias: -10344.511277, T: 176220, Avg. loss: 1.074546\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 940667.95, NNZs: 892, Bias: -10344.512635, T: 177199, Avg. loss: 1.063176\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 940667.95, NNZs: 892, Bias: -10344.514087, T: 178178, Avg. loss: 1.060257\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 940667.95, NNZs: 892, Bias: -10344.515424, T: 179157, Avg. loss: 1.040920\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 940667.95, NNZs: 892, Bias: -10344.516734, T: 180136, Avg. loss: 1.030075\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 940667.94, NNZs: 892, Bias: -10344.518035, T: 181115, Avg. loss: 1.019528\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 940667.94, NNZs: 892, Bias: -10344.519333, T: 182094, Avg. loss: 1.009108\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 940667.94, NNZs: 892, Bias: -10344.520613, T: 183073, Avg. loss: 0.998779\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 940667.94, NNZs: 892, Bias: -10344.521871, T: 184052, Avg. loss: 0.988718\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 940667.94, NNZs: 892, Bias: -10344.523241, T: 185031, Avg. loss: 0.986230\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 940667.94, NNZs: 892, Bias: -10344.524482, T: 186010, Avg. loss: 0.968703\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 940667.94, NNZs: 892, Bias: -10344.525800, T: 186989, Avg. loss: 0.966004\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 940667.93, NNZs: 892, Bias: -10344.527096, T: 187968, Avg. loss: 0.956295\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 940667.93, NNZs: 892, Bias: -10344.528280, T: 188947, Avg. loss: 0.939514\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 940667.93, NNZs: 892, Bias: -10344.529461, T: 189926, Avg. loss: 0.930239\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 940667.93, NNZs: 892, Bias: -10344.530712, T: 190905, Avg. loss: 0.928009\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 940667.93, NNZs: 892, Bias: -10344.531942, T: 191884, Avg. loss: 0.918803\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 940667.93, NNZs: 892, Bias: -10344.533055, T: 192863, Avg. loss: 0.902849\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 940667.93, NNZs: 892, Bias: -10344.534259, T: 193842, Avg. loss: 0.901057\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 940667.92, NNZs: 892, Bias: -10344.535456, T: 194821, Avg. loss: 0.892024\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 940667.92, NNZs: 892, Bias: -10344.536540, T: 195800, Avg. loss: 0.877020\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 940667.92, NNZs: 892, Bias: -10344.537695, T: 196779, Avg. loss: 0.874693\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 940667.92, NNZs: 892, Bias: -10344.538775, T: 197758, Avg. loss: 0.860523\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 940667.92, NNZs: 892, Bias: -10344.539831, T: 198737, Avg. loss: 0.852218\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 940667.92, NNZs: 892, Bias: -10344.540967, T: 199716, Avg. loss: 0.850539\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 940667.92, NNZs: 892, Bias: -10344.542101, T: 200695, Avg. loss: 0.842537\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 940667.92, NNZs: 892, Bias: -10344.543196, T: 201674, Avg. loss: 0.834278\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 940667.91, NNZs: 892, Bias: -10344.544200, T: 202653, Avg. loss: 0.820785\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 940667.91, NNZs: 892, Bias: -10344.545195, T: 203632, Avg. loss: 0.813216\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 940667.91, NNZs: 892, Bias: -10344.546255, T: 204611, Avg. loss: 0.811338\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 940667.91, NNZs: 892, Bias: -10344.547305, T: 205590, Avg. loss: 0.804023\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 940667.91, NNZs: 892, Bias: -10344.548328, T: 206569, Avg. loss: 0.796564\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 940667.91, NNZs: 892, Bias: -10344.549345, T: 207548, Avg. loss: 0.789169\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 940667.91, NNZs: 892, Bias: -10344.550276, T: 208527, Avg. loss: 0.776769\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 214\n",
            "Norm: 940667.91, NNZs: 892, Bias: -10344.551283, T: 209506, Avg. loss: 0.775223\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 215\n",
            "Norm: 940667.91, NNZs: 892, Bias: -10344.552278, T: 210485, Avg. loss: 0.768277\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 216\n",
            "Norm: 940667.90, NNZs: 892, Bias: -10344.553185, T: 211464, Avg. loss: 0.756198\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 217\n",
            "Norm: 940667.90, NNZs: 892, Bias: -10344.554138, T: 212443, Avg. loss: 0.754451\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 218\n",
            "Norm: 940667.90, NNZs: 892, Bias: -10344.555034, T: 213422, Avg. loss: 0.742979\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 219\n",
            "Norm: 940667.90, NNZs: 892, Bias: -10344.555909, T: 214401, Avg. loss: 0.736532\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 220\n",
            "Norm: 940667.90, NNZs: 892, Bias: -10344.556844, T: 215380, Avg. loss: 0.734945\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 221\n",
            "Norm: 940667.90, NNZs: 892, Bias: -10344.557708, T: 216359, Avg. loss: 0.724005\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 222\n",
            "Norm: 940667.90, NNZs: 892, Bias: -10344.558638, T: 217338, Avg. loss: 0.722493\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 223\n",
            "Norm: 940667.90, NNZs: 892, Bias: -10344.559547, T: 218317, Avg. loss: 0.716065\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 224\n",
            "Norm: 940667.90, NNZs: 892, Bias: -10344.560394, T: 219296, Avg. loss: 0.705508\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 225\n",
            "Norm: 940667.90, NNZs: 892, Bias: -10344.561272, T: 220275, Avg. loss: 0.703886\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 226\n",
            "Norm: 940667.89, NNZs: 892, Bias: -10344.562154, T: 221254, Avg. loss: 0.697942\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 227\n",
            "Norm: 940667.89, NNZs: 892, Bias: -10344.562971, T: 222233, Avg. loss: 0.687705\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 228\n",
            "Norm: 940667.89, NNZs: 892, Bias: -10344.563839, T: 223212, Avg. loss: 0.686285\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 229\n",
            "Norm: 940667.89, NNZs: 892, Bias: -10344.564699, T: 224191, Avg. loss: 0.680618\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 230\n",
            "Norm: 940667.89, NNZs: 892, Bias: -10344.565552, T: 225170, Avg. loss: 0.674966\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 231\n",
            "Norm: 940667.89, NNZs: 892, Bias: -10344.566389, T: 226149, Avg. loss: 0.669305\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 232\n",
            "Norm: 940667.89, NNZs: 892, Bias: -10344.567221, T: 227128, Avg. loss: 0.663665\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 233\n",
            "Norm: 940667.89, NNZs: 892, Bias: -10344.567982, T: 228107, Avg. loss: 0.654121\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 234\n",
            "Norm: 940667.89, NNZs: 892, Bias: -10344.568737, T: 229086, Avg. loss: 0.648770\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 235\n",
            "Norm: 940667.89, NNZs: 892, Bias: -10344.569539, T: 230065, Avg. loss: 0.647676\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 236\n",
            "Norm: 940667.89, NNZs: 892, Bias: -10344.570274, T: 231044, Avg. loss: 0.638410\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 237\n",
            "Norm: 940667.89, NNZs: 892, Bias: -10344.571013, T: 232023, Avg. loss: 0.633462\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 238\n",
            "Norm: 940667.88, NNZs: 892, Bias: -10344.571794, T: 233002, Avg. loss: 0.632208\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 239\n",
            "Norm: 940667.88, NNZs: 892, Bias: -10344.572526, T: 233981, Avg. loss: 0.623455\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 240\n",
            "Norm: 940667.88, NNZs: 892, Bias: -10344.573248, T: 234960, Avg. loss: 0.618486\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 241\n",
            "Norm: 940667.88, NNZs: 892, Bias: -10344.573961, T: 235939, Avg. loss: 0.613627\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 242\n",
            "Norm: 940667.88, NNZs: 892, Bias: -10344.574719, T: 236918, Avg. loss: 0.612554\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 243\n",
            "Norm: 940667.88, NNZs: 892, Bias: -10344.575410, T: 237897, Avg. loss: 0.604058\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 244\n",
            "Norm: 940667.88, NNZs: 892, Bias: -10344.576100, T: 238876, Avg. loss: 0.599546\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 245\n",
            "Norm: 940667.88, NNZs: 892, Bias: -10344.576790, T: 239855, Avg. loss: 0.594938\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 246\n",
            "Norm: 940667.88, NNZs: 892, Bias: -10344.577475, T: 240834, Avg. loss: 0.590381\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 247\n",
            "Norm: 940667.88, NNZs: 892, Bias: -10344.578201, T: 241813, Avg. loss: 0.589342\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 248\n",
            "Norm: 940667.88, NNZs: 892, Bias: -10344.578863, T: 242792, Avg. loss: 0.581345\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 249\n",
            "Norm: 940667.88, NNZs: 892, Bias: -10344.579540, T: 243771, Avg. loss: 0.577037\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 250\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.580206, T: 244750, Avg. loss: 0.572752\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 251\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.580868, T: 245729, Avg. loss: 0.568395\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 252\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.581551, T: 246708, Avg. loss: 0.567328\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 253\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.582197, T: 247687, Avg. loss: 0.559956\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 254\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.582869, T: 248666, Avg. loss: 0.558821\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 255\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.583498, T: 249645, Avg. loss: 0.551615\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 256\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.584170, T: 250624, Avg. loss: 0.550584\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 257\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.584827, T: 251603, Avg. loss: 0.546549\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 258\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.585483, T: 252582, Avg. loss: 0.542483\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 259\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.586090, T: 253561, Avg. loss: 0.535453\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 260\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.586687, T: 254540, Avg. loss: 0.531606\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 261\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.587283, T: 255519, Avg. loss: 0.527790\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 262\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.587869, T: 256498, Avg. loss: 0.524012\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 263\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.588500, T: 257477, Avg. loss: 0.523311\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 264\n",
            "Norm: 940667.87, NNZs: 892, Bias: -10344.589088, T: 258456, Avg. loss: 0.516683\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 265\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.589667, T: 259435, Avg. loss: 0.512957\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 266\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.590282, T: 260414, Avg. loss: 0.512147\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 267\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.590851, T: 261393, Avg. loss: 0.505701\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 268\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.591423, T: 262372, Avg. loss: 0.502209\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 269\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.592019, T: 263351, Avg. loss: 0.501371\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 270\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.592573, T: 264330, Avg. loss: 0.495177\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 271\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.593121, T: 265309, Avg. loss: 0.491737\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 272\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.593699, T: 266288, Avg. loss: 0.490857\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 273\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.594248, T: 267267, Avg. loss: 0.485098\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 274\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.594813, T: 268246, Avg. loss: 0.484229\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 275\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.595374, T: 269225, Avg. loss: 0.480743\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 276\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.595900, T: 270204, Avg. loss: 0.475028\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 277\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.596431, T: 271183, Avg. loss: 0.471871\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 278\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.596946, T: 272162, Avg. loss: 0.468546\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 279\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.597469, T: 273141, Avg. loss: 0.465565\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 280\n",
            "Norm: 940667.86, NNZs: 892, Bias: -10344.598016, T: 274120, Avg. loss: 0.464842\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 281\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.598527, T: 275099, Avg. loss: 0.459323\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 282\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.599064, T: 276078, Avg. loss: 0.458614\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 283\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.599559, T: 277057, Avg. loss: 0.453197\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 284\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.600083, T: 278036, Avg. loss: 0.452520\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 285\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.600607, T: 279015, Avg. loss: 0.449542\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 286\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.601093, T: 279994, Avg. loss: 0.444314\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 287\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.601570, T: 280973, Avg. loss: 0.441382\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 288\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.602048, T: 281952, Avg. loss: 0.438435\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 289\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.602558, T: 282931, Avg. loss: 0.437985\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 290\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.603036, T: 283910, Avg. loss: 0.432959\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 291\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.603534, T: 284889, Avg. loss: 0.432243\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 292\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.604025, T: 285868, Avg. loss: 0.429456\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 293\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.604488, T: 286847, Avg. loss: 0.424647\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 294\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.604975, T: 287826, Avg. loss: 0.423985\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 295\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.605455, T: 288805, Avg. loss: 0.421222\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 296\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.605904, T: 289784, Avg. loss: 0.416492\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 297\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.606381, T: 290763, Avg. loss: 0.415940\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 298\n",
            "Norm: 940667.85, NNZs: 892, Bias: -10344.606820, T: 291742, Avg. loss: 0.411291\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 299\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.607259, T: 292721, Avg. loss: 0.408839\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 300\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.607687, T: 293700, Avg. loss: 0.406235\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 301\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.608114, T: 294679, Avg. loss: 0.403740\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 302\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.608563, T: 295658, Avg. loss: 0.403093\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 303\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.609006, T: 296637, Avg. loss: 0.400669\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 304\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.609437, T: 297616, Avg. loss: 0.398127\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 305\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.609874, T: 298595, Avg. loss: 0.395759\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 306\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.610283, T: 299574, Avg. loss: 0.391473\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 307\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.610682, T: 300553, Avg. loss: 0.389003\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 308\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.611080, T: 301532, Avg. loss: 0.386800\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 309\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.611476, T: 302511, Avg. loss: 0.384486\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 310\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.611887, T: 303490, Avg. loss: 0.383880\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 311\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.612298, T: 304469, Avg. loss: 0.381642\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 312\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.612674, T: 305448, Avg. loss: 0.377474\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 313\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.613057, T: 306427, Avg. loss: 0.375411\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 314\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.613447, T: 307406, Avg. loss: 0.374769\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 315\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.613839, T: 308385, Avg. loss: 0.372615\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 316\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.614206, T: 309364, Avg. loss: 0.368781\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 317\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.614569, T: 310343, Avg. loss: 0.366620\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 318\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.614947, T: 311322, Avg. loss: 0.366069\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 319\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.615307, T: 312301, Avg. loss: 0.362411\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 320\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.615654, T: 313280, Avg. loss: 0.360221\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 321\n",
            "Norm: 940667.84, NNZs: 892, Bias: -10344.616004, T: 314259, Avg. loss: 0.358153\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 322\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.616373, T: 315238, Avg. loss: 0.357720\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 323\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.616741, T: 316217, Avg. loss: 0.355667\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 324\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.617078, T: 317196, Avg. loss: 0.352092\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 325\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.617441, T: 318175, Avg. loss: 0.351635\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 326\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.617773, T: 319154, Avg. loss: 0.348072\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 327\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.618101, T: 320133, Avg. loss: 0.346058\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 328\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.618450, T: 321112, Avg. loss: 0.345664\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 329\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.618776, T: 322091, Avg. loss: 0.342230\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 330\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.619098, T: 323070, Avg. loss: 0.340344\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 331\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.619440, T: 324049, Avg. loss: 0.339849\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 332\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.619762, T: 325028, Avg. loss: 0.336594\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 333\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.620091, T: 326007, Avg. loss: 0.336110\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 334\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.620402, T: 326986, Avg. loss: 0.332881\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 335\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.620712, T: 327965, Avg. loss: 0.331026\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 336\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.621026, T: 328944, Avg. loss: 0.329275\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 337\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.621350, T: 329923, Avg. loss: 0.328828\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 338\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.621671, T: 330902, Avg. loss: 0.327034\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 339\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.621972, T: 331881, Avg. loss: 0.323898\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 340\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.622289, T: 332860, Avg. loss: 0.323492\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 341\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.622584, T: 333839, Avg. loss: 0.320335\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 342\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.622896, T: 334818, Avg. loss: 0.319968\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 343\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.623205, T: 335797, Avg. loss: 0.318277\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 344\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.623495, T: 336776, Avg. loss: 0.315269\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 345\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.623802, T: 337755, Avg. loss: 0.314880\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 346\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.624085, T: 338734, Avg. loss: 0.311931\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 347\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.624386, T: 339713, Avg. loss: 0.311544\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 348\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.624685, T: 340692, Avg. loss: 0.309926\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 349\n",
            "Norm: 940667.83, NNZs: 892, Bias: -10344.624962, T: 341671, Avg. loss: 0.306983\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 350\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.625252, T: 342650, Avg. loss: 0.306559\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 351\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.625541, T: 343629, Avg. loss: 0.305047\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 352\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.625805, T: 344608, Avg. loss: 0.302108\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 353\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.626094, T: 345587, Avg. loss: 0.301860\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 354\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.626363, T: 346566, Avg. loss: 0.299105\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 355\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.626644, T: 347545, Avg. loss: 0.298696\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 356\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.626907, T: 348524, Avg. loss: 0.296015\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 357\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.627169, T: 349503, Avg. loss: 0.294487\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 358\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.627444, T: 350482, Avg. loss: 0.294168\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 359\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.627703, T: 351461, Avg. loss: 0.291506\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 360\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.627958, T: 352440, Avg. loss: 0.289988\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 361\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.628210, T: 353419, Avg. loss: 0.288572\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 362\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.628481, T: 354398, Avg. loss: 0.288296\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 363\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.628733, T: 355377, Avg. loss: 0.285695\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 364\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.628979, T: 356356, Avg. loss: 0.284193\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 365\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.629241, T: 357335, Avg. loss: 0.283925\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 366\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.629505, T: 358314, Avg. loss: 0.282517\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 367\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.629768, T: 359293, Avg. loss: 0.281075\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 368\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.630024, T: 360272, Avg. loss: 0.279703\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 369\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.630265, T: 361251, Avg. loss: 0.277237\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 370\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.630506, T: 362230, Avg. loss: 0.275808\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 371\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.630754, T: 363209, Avg. loss: 0.275531\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 372\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.630990, T: 364188, Avg. loss: 0.273164\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 373\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.631237, T: 365167, Avg. loss: 0.272876\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 374\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.631469, T: 366146, Avg. loss: 0.270528\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 375\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.631717, T: 367125, Avg. loss: 0.270242\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 376\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.631959, T: 368104, Avg. loss: 0.268935\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 377\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.632200, T: 369083, Avg. loss: 0.267569\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 378\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.632426, T: 370062, Avg. loss: 0.265347\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 379\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.632665, T: 371041, Avg. loss: 0.265053\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 380\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.632896, T: 372020, Avg. loss: 0.263729\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 381\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.633131, T: 372999, Avg. loss: 0.262521\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 382\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.633349, T: 373978, Avg. loss: 0.260292\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 383\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.633569, T: 374957, Avg. loss: 0.259058\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 384\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.633786, T: 375936, Avg. loss: 0.257814\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 385\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.634015, T: 376915, Avg. loss: 0.257590\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 386\n",
            "Norm: 940667.82, NNZs: 892, Bias: -10344.634238, T: 377894, Avg. loss: 0.256335\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 387\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.634465, T: 378873, Avg. loss: 0.255180\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 388\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.634692, T: 379852, Avg. loss: 0.253990\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 389\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.634917, T: 380831, Avg. loss: 0.252807\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 390\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.635139, T: 381810, Avg. loss: 0.251568\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 391\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.635359, T: 382789, Avg. loss: 0.250445\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 392\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.635565, T: 383768, Avg. loss: 0.248362\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 393\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.635772, T: 384747, Avg. loss: 0.247195\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 394\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.635979, T: 385726, Avg. loss: 0.246146\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 395\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.636181, T: 386705, Avg. loss: 0.244979\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 396\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.636397, T: 387684, Avg. loss: 0.244746\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 397\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.636612, T: 388663, Avg. loss: 0.243687\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 398\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.636810, T: 389642, Avg. loss: 0.241649\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 399\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.637018, T: 390621, Avg. loss: 0.241381\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 400\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.637219, T: 391600, Avg. loss: 0.239532\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 401\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.637415, T: 392579, Avg. loss: 0.238406\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 402\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.637626, T: 393558, Avg. loss: 0.238206\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 403\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.637819, T: 394537, Avg. loss: 0.236303\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 404\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.638021, T: 395516, Avg. loss: 0.236034\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 405\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.638220, T: 396495, Avg. loss: 0.234973\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 406\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.638420, T: 397474, Avg. loss: 0.233942\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 407\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.638610, T: 398453, Avg. loss: 0.232149\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 408\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.638809, T: 399432, Avg. loss: 0.231883\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 409\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.639006, T: 400411, Avg. loss: 0.230887\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 410\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.639204, T: 401390, Avg. loss: 0.229872\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 411\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.639398, T: 402369, Avg. loss: 0.228826\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 412\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.639578, T: 403348, Avg. loss: 0.227021\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 413\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.639763, T: 404327, Avg. loss: 0.226108\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 414\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.639944, T: 405306, Avg. loss: 0.225090\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 415\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.640125, T: 406285, Avg. loss: 0.224172\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 416\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.640304, T: 407264, Avg. loss: 0.223147\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 417\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.640483, T: 408243, Avg. loss: 0.222223\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 418\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.640658, T: 409222, Avg. loss: 0.221266\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 419\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.640846, T: 410201, Avg. loss: 0.221110\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 420\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.641022, T: 411180, Avg. loss: 0.219420\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 421\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.641198, T: 412159, Avg. loss: 0.218455\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 422\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.641372, T: 413138, Avg. loss: 0.217568\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 423\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.641557, T: 414117, Avg. loss: 0.217393\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 424\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.641726, T: 415096, Avg. loss: 0.215716\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 425\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.641898, T: 416075, Avg. loss: 0.214826\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 426\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.642070, T: 417054, Avg. loss: 0.213959\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 427\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.642250, T: 418033, Avg. loss: 0.213737\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 428\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.642420, T: 419012, Avg. loss: 0.212143\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 429\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.642587, T: 419991, Avg. loss: 0.211284\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 430\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.642763, T: 420970, Avg. loss: 0.211073\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 431\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.642938, T: 421949, Avg. loss: 0.210226\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 432\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.643111, T: 422928, Avg. loss: 0.209279\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 433\n",
            "Norm: 940667.81, NNZs: 892, Bias: -10344.643283, T: 423907, Avg. loss: 0.208449\n",
            "Total training time: 0.47 seconds.\n",
            "Convergence after 433 epochs took 0.47 seconds\n",
            "-- Epoch 1\n",
            "Norm: 177.54, NNZs: 610, Bias: -1.415271, T: 979, Avg. loss: 6.361905\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 175.70, NNZs: 518, Bias: -1.001616, T: 1958, Avg. loss: 1.510639\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 175.39, NNZs: 474, Bias: -0.969312, T: 2937, Avg. loss: 0.475693\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 175.32, NNZs: 442, Bias: -0.924665, T: 3916, Avg. loss: 0.311158\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 175.32, NNZs: 423, Bias: -0.881377, T: 4895, Avg. loss: 0.264736\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 175.34, NNZs: 407, Bias: -0.849876, T: 5874, Avg. loss: 0.236140\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 175.37, NNZs: 399, Bias: -0.817714, T: 6853, Avg. loss: 0.224797\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 175.39, NNZs: 391, Bias: -0.799228, T: 7832, Avg. loss: 0.216244\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 175.42, NNZs: 386, Bias: -0.785927, T: 8811, Avg. loss: 0.208863\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 175.45, NNZs: 380, Bias: -0.769400, T: 9790, Avg. loss: 0.207414\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 175.47, NNZs: 370, Bias: -0.761824, T: 10769, Avg. loss: 0.202641\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 175.50, NNZs: 370, Bias: -0.749122, T: 11748, Avg. loss: 0.203011\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 175.52, NNZs: 356, Bias: -0.739431, T: 12727, Avg. loss: 0.200048\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 175.54, NNZs: 356, Bias: -0.733767, T: 13706, Avg. loss: 0.198333\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 175.57, NNZs: 351, Bias: -0.724832, T: 14685, Avg. loss: 0.198977\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 175.59, NNZs: 347, Bias: -0.717841, T: 15664, Avg. loss: 0.198342\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 175.61, NNZs: 344, Bias: -0.713822, T: 16643, Avg. loss: 0.195995\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 175.63, NNZs: 341, Bias: -0.706404, T: 17622, Avg. loss: 0.197548\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 175.65, NNZs: 339, Bias: -0.701603, T: 18601, Avg. loss: 0.196658\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 175.67, NNZs: 335, Bias: -0.695226, T: 19580, Avg. loss: 0.197454\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 175.69, NNZs: 332, Bias: -0.689386, T: 20559, Avg. loss: 0.197461\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 175.70, NNZs: 328, Bias: -0.685034, T: 21538, Avg. loss: 0.196615\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 22 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 1196.39, NNZs: 727, Bias: -2.034398, T: 979, Avg. loss: 404.439741\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1094.90, NNZs: 805, Bias: 2.171681, T: 1958, Avg. loss: 285.992048\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1085.01, NNZs: 814, Bias: -1.550657, T: 2937, Avg. loss: 25.678955\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1082.62, NNZs: 806, Bias: -1.317203, T: 3916, Avg. loss: 7.176813\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1081.53, NNZs: 797, Bias: -1.195300, T: 4895, Avg. loss: 3.900678\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1080.90, NNZs: 795, Bias: -1.223666, T: 5874, Avg. loss: 2.662631\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1080.48, NNZs: 793, Bias: -1.195911, T: 6853, Avg. loss: 2.046425\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1080.17, NNZs: 792, Bias: -1.217775, T: 7832, Avg. loss: 1.663361\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1079.93, NNZs: 786, Bias: -1.235437, T: 8811, Avg. loss: 1.415646\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1079.76, NNZs: 780, Bias: -1.245024, T: 9790, Avg. loss: 1.219181\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1079.61, NNZs: 774, Bias: -1.248612, T: 10769, Avg. loss: 1.088447\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1079.50, NNZs: 772, Bias: -1.238573, T: 11748, Avg. loss: 0.979465\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 1079.40, NNZs: 766, Bias: -1.227668, T: 12727, Avg. loss: 0.896994\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 1079.32, NNZs: 763, Bias: -1.233404, T: 13706, Avg. loss: 0.817268\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 1079.24, NNZs: 760, Bias: -1.244767, T: 14685, Avg. loss: 0.761962\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 1079.18, NNZs: 755, Bias: -1.247022, T: 15664, Avg. loss: 0.715725\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 1079.12, NNZs: 753, Bias: -1.246129, T: 16643, Avg. loss: 0.681294\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 1079.07, NNZs: 751, Bias: -1.246649, T: 17622, Avg. loss: 0.639883\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 1079.03, NNZs: 749, Bias: -1.251225, T: 18601, Avg. loss: 0.611044\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 1078.99, NNZs: 748, Bias: -1.255559, T: 19580, Avg. loss: 0.585709\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 1078.95, NNZs: 744, Bias: -1.254496, T: 20559, Avg. loss: 0.561257\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 1078.91, NNZs: 742, Bias: -1.255568, T: 21538, Avg. loss: 0.540660\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 1078.88, NNZs: 742, Bias: -1.255798, T: 22517, Avg. loss: 0.520171\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 1078.85, NNZs: 742, Bias: -1.257946, T: 23496, Avg. loss: 0.504428\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 1078.83, NNZs: 740, Bias: -1.257962, T: 24475, Avg. loss: 0.486725\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 1078.80, NNZs: 739, Bias: -1.259815, T: 25454, Avg. loss: 0.473226\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 1078.78, NNZs: 738, Bias: -1.262366, T: 26433, Avg. loss: 0.458777\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 1078.76, NNZs: 738, Bias: -1.263599, T: 27412, Avg. loss: 0.447301\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 1078.74, NNZs: 738, Bias: -1.265424, T: 28391, Avg. loss: 0.435633\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 1078.72, NNZs: 737, Bias: -1.265954, T: 29370, Avg. loss: 0.425121\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 1078.70, NNZs: 736, Bias: -1.267577, T: 30349, Avg. loss: 0.415908\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 1078.69, NNZs: 734, Bias: -1.267999, T: 31328, Avg. loss: 0.405975\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 1078.67, NNZs: 734, Bias: -1.269350, T: 32307, Avg. loss: 0.396851\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 1078.66, NNZs: 734, Bias: -1.269535, T: 33286, Avg. loss: 0.388631\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 1078.64, NNZs: 731, Bias: -1.270871, T: 34265, Avg. loss: 0.381407\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 1078.63, NNZs: 730, Bias: -1.271144, T: 35244, Avg. loss: 0.373464\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 1078.62, NNZs: 730, Bias: -1.271498, T: 36223, Avg. loss: 0.366245\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 1078.60, NNZs: 730, Bias: -1.272051, T: 37202, Avg. loss: 0.360051\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 1078.59, NNZs: 730, Bias: -1.274037, T: 38181, Avg. loss: 0.353676\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 1078.58, NNZs: 729, Bias: -1.274169, T: 39160, Avg. loss: 0.347358\n",
            "Total training time: 0.03 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 41\n",
            "Norm: 1078.57, NNZs: 729, Bias: -1.275154, T: 40139, Avg. loss: 0.342013\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 1078.56, NNZs: 729, Bias: -1.276028, T: 41118, Avg. loss: 0.336874\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 1078.55, NNZs: 729, Bias: -1.276847, T: 42097, Avg. loss: 0.331231\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 1078.54, NNZs: 729, Bias: -1.277692, T: 43076, Avg. loss: 0.326665\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 1078.53, NNZs: 729, Bias: -1.278296, T: 44055, Avg. loss: 0.321710\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 1078.52, NNZs: 729, Bias: -1.279235, T: 45034, Avg. loss: 0.317548\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 1078.52, NNZs: 727, Bias: -1.280000, T: 46013, Avg. loss: 0.313057\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 1078.51, NNZs: 727, Bias: -1.279846, T: 46992, Avg. loss: 0.308593\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 1078.50, NNZs: 727, Bias: -1.280209, T: 47971, Avg. loss: 0.304579\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 1078.49, NNZs: 727, Bias: -1.280436, T: 48950, Avg. loss: 0.301229\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 1078.49, NNZs: 726, Bias: -1.280655, T: 49929, Avg. loss: 0.297370\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 1078.48, NNZs: 726, Bias: -1.280795, T: 50908, Avg. loss: 0.293599\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 1078.47, NNZs: 724, Bias: -1.281320, T: 51887, Avg. loss: 0.290454\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 1078.47, NNZs: 724, Bias: -1.281565, T: 52866, Avg. loss: 0.287251\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 1078.46, NNZs: 723, Bias: -1.281825, T: 53845, Avg. loss: 0.283789\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 1078.45, NNZs: 723, Bias: -1.282521, T: 54824, Avg. loss: 0.280848\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 1078.45, NNZs: 722, Bias: -1.282471, T: 55803, Avg. loss: 0.277530\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 1078.44, NNZs: 722, Bias: -1.282361, T: 56782, Avg. loss: 0.274879\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 1078.44, NNZs: 722, Bias: -1.282951, T: 57761, Avg. loss: 0.272292\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 1078.43, NNZs: 722, Bias: -1.283458, T: 58740, Avg. loss: 0.269423\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 1078.43, NNZs: 721, Bias: -1.283516, T: 59719, Avg. loss: 0.266474\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 1078.42, NNZs: 721, Bias: -1.283814, T: 60698, Avg. loss: 0.264009\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 1078.42, NNZs: 721, Bias: -1.284099, T: 61677, Avg. loss: 0.261917\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 1078.41, NNZs: 721, Bias: -1.284280, T: 62656, Avg. loss: 0.259570\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 1078.41, NNZs: 721, Bias: -1.284606, T: 63635, Avg. loss: 0.257014\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 1078.40, NNZs: 721, Bias: -1.285041, T: 64614, Avg. loss: 0.254835\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 1078.40, NNZs: 721, Bias: -1.285121, T: 65593, Avg. loss: 0.252523\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 1078.39, NNZs: 721, Bias: -1.285510, T: 66572, Avg. loss: 0.250417\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 1078.39, NNZs: 721, Bias: -1.285832, T: 67551, Avg. loss: 0.248387\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 1078.39, NNZs: 721, Bias: -1.285879, T: 68530, Avg. loss: 0.246400\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 1078.38, NNZs: 721, Bias: -1.286058, T: 69509, Avg. loss: 0.244245\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 1078.38, NNZs: 719, Bias: -1.286281, T: 70488, Avg. loss: 0.242515\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 1078.38, NNZs: 718, Bias: -1.286481, T: 71467, Avg. loss: 0.240543\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 1078.37, NNZs: 718, Bias: -1.286618, T: 72446, Avg. loss: 0.238619\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 1078.37, NNZs: 718, Bias: -1.286862, T: 73425, Avg. loss: 0.237006\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 1078.36, NNZs: 717, Bias: -1.287017, T: 74404, Avg. loss: 0.235212\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 1078.36, NNZs: 717, Bias: -1.287143, T: 75383, Avg. loss: 0.233572\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 1078.36, NNZs: 717, Bias: -1.287312, T: 76362, Avg. loss: 0.232057\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 1078.35, NNZs: 717, Bias: -1.287449, T: 77341, Avg. loss: 0.230273\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 1078.35, NNZs: 717, Bias: -1.287383, T: 78320, Avg. loss: 0.228896\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 1078.35, NNZs: 717, Bias: -1.287432, T: 79299, Avg. loss: 0.227186\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 1078.35, NNZs: 713, Bias: -1.287689, T: 80278, Avg. loss: 0.225860\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 1078.34, NNZs: 711, Bias: -1.287918, T: 81257, Avg. loss: 0.224324\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 1078.34, NNZs: 711, Bias: -1.288133, T: 82236, Avg. loss: 0.222762\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 1078.34, NNZs: 713, Bias: -1.288239, T: 83215, Avg. loss: 0.221389\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 1078.33, NNZs: 710, Bias: -1.288346, T: 84194, Avg. loss: 0.219929\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 1078.33, NNZs: 710, Bias: -1.288645, T: 85173, Avg. loss: 0.218864\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 1078.33, NNZs: 709, Bias: -1.288706, T: 86152, Avg. loss: 0.217364\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 1078.33, NNZs: 709, Bias: -1.288933, T: 87131, Avg. loss: 0.216092\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 1078.32, NNZs: 708, Bias: -1.289150, T: 88110, Avg. loss: 0.214873\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 1078.32, NNZs: 708, Bias: -1.289261, T: 89089, Avg. loss: 0.213704\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 1078.32, NNZs: 708, Bias: -1.289432, T: 90068, Avg. loss: 0.212342\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 1078.32, NNZs: 708, Bias: -1.289418, T: 91047, Avg. loss: 0.211126\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 1078.31, NNZs: 708, Bias: -1.289565, T: 92026, Avg. loss: 0.209858\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 1078.31, NNZs: 707, Bias: -1.289718, T: 93005, Avg. loss: 0.208773\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 1078.31, NNZs: 707, Bias: -1.289847, T: 93984, Avg. loss: 0.207623\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 1078.31, NNZs: 707, Bias: -1.289864, T: 94963, Avg. loss: 0.206445\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 1078.31, NNZs: 707, Bias: -1.289985, T: 95942, Avg. loss: 0.205583\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 1078.30, NNZs: 707, Bias: -1.290129, T: 96921, Avg. loss: 0.204612\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 1078.30, NNZs: 707, Bias: -1.290247, T: 97900, Avg. loss: 0.203499\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 1078.30, NNZs: 707, Bias: -1.290305, T: 98879, Avg. loss: 0.202364\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 1078.30, NNZs: 707, Bias: -1.290377, T: 99858, Avg. loss: 0.201451\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 1078.30, NNZs: 707, Bias: -1.290384, T: 100837, Avg. loss: 0.200316\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 1078.29, NNZs: 707, Bias: -1.290441, T: 101816, Avg. loss: 0.199377\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 1078.29, NNZs: 707, Bias: -1.290447, T: 102795, Avg. loss: 0.198465\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 1078.29, NNZs: 707, Bias: -1.290517, T: 103774, Avg. loss: 0.197364\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 1078.29, NNZs: 707, Bias: -1.290567, T: 104753, Avg. loss: 0.196510\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 1078.29, NNZs: 707, Bias: -1.290723, T: 105732, Avg. loss: 0.195766\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 1078.28, NNZs: 707, Bias: -1.290892, T: 106711, Avg. loss: 0.194779\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 1078.28, NNZs: 707, Bias: -1.290958, T: 107690, Avg. loss: 0.193878\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 1078.28, NNZs: 707, Bias: -1.291037, T: 108669, Avg. loss: 0.193009\n",
            "Total training time: 0.10 seconds.\n",
            "Convergence after 111 epochs took 0.10 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2708851.22, NNZs: 734, Bias: -5892.295699, T: 979, Avg. loss: 896990095.880469\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 3486357.22, NNZs: 909, Bias: -29667.963313, T: 1958, Avg. loss: 5328178837.293420\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 3428344.55, NNZs: 932, Bias: -33076.889378, T: 2937, Avg. loss: 527099366.444040\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 3420886.68, NNZs: 932, Bias: -34585.457168, T: 3916, Avg. loss: 64932779.086236\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 3416093.85, NNZs: 932, Bias: -36413.056964, T: 4895, Avg. loss: 29370166.027742\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 3414941.18, NNZs: 932, Bias: -36996.508754, T: 5874, Avg. loss: 6160131.338963\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 3414486.91, NNZs: 932, Bias: -37260.327558, T: 6853, Avg. loss: 2149208.992376\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 3414284.91, NNZs: 933, Bias: -37388.477488, T: 7832, Avg. loss: 957859.606588\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 3414156.82, NNZs: 933, Bias: -37477.494549, T: 8811, Avg. loss: 528294.815304\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 3414090.21, NNZs: 933, Bias: -37522.097589, T: 9790, Avg. loss: 321596.630246\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 3414038.10, NNZs: 933, Bias: -37559.891124, T: 10769, Avg. loss: 204962.049697\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 3414005.64, NNZs: 933, Bias: -37583.466988, T: 11748, Avg. loss: 134656.654194\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 3413975.45, NNZs: 933, Bias: -37607.181824, T: 12727, Avg. loss: 95180.482066\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 3413955.00, NNZs: 933, Bias: -37623.217243, T: 13706, Avg. loss: 69174.619490\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 3413940.35, NNZs: 933, Bias: -37634.723668, T: 14685, Avg. loss: 51281.082158\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 3413926.47, NNZs: 933, Bias: -37645.986455, T: 15664, Avg. loss: 41017.816101\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 3413915.81, NNZs: 933, Bias: -37654.655317, T: 16643, Avg. loss: 32531.911039\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 3413908.17, NNZs: 933, Bias: -37660.808265, T: 17622, Avg. loss: 26298.611609\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 3413902.70, NNZs: 933, Bias: -37665.153772, T: 18601, Avg. loss: 21129.129484\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 3413896.59, NNZs: 932, Bias: -37670.188592, T: 19580, Avg. loss: 18728.111793\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 3413891.66, NNZs: 932, Bias: -37674.244515, T: 20559, Avg. loss: 16052.769802\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 3413888.53, NNZs: 932, Bias: -37676.732928, T: 21538, Avg. loss: 13353.029093\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 3413885.23, NNZs: 932, Bias: -37679.435935, T: 22517, Avg. loss: 11977.118426\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 3413881.50, NNZs: 931, Bias: -37682.562532, T: 23496, Avg. loss: 11244.897075\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 3413878.48, NNZs: 931, Bias: -37685.073969, T: 24475, Avg. loss: 10114.038290\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 3413875.98, NNZs: 931, Bias: -37687.148772, T: 25454, Avg. loss: 9165.060949\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 3413873.76, NNZs: 931, Bias: -37688.982704, T: 26433, Avg. loss: 8412.881537\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 3413872.48, NNZs: 931, Bias: -37689.988859, T: 27412, Avg. loss: 7390.158251\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 3413870.66, NNZs: 931, Bias: -37691.493326, T: 28391, Avg. loss: 7330.693165\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 3413869.10, NNZs: 931, Bias: -37692.778345, T: 29370, Avg. loss: 6842.845924\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 3413867.64, NNZs: 931, Bias: -37693.984188, T: 30349, Avg. loss: 6446.647561\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 3413866.70, NNZs: 931, Bias: -37694.720237, T: 31328, Avg. loss: 5779.419148\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 3413865.75, NNZs: 931, Bias: -37695.483412, T: 32307, Avg. loss: 5564.658648\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 3413864.81, NNZs: 931, Bias: -37696.240488, T: 33286, Avg. loss: 5366.709917\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 3413863.97, NNZs: 931, Bias: -37696.910663, T: 34265, Avg. loss: 5169.312036\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 3413863.15, NNZs: 931, Bias: -37697.570811, T: 35244, Avg. loss: 4993.599482\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 3413862.36, NNZs: 931, Bias: -37698.213258, T: 36223, Avg. loss: 4823.807594\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 3413861.36, NNZs: 930, Bias: -37699.045559, T: 37202, Avg. loss: 4879.309218\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 3413860.51, NNZs: 930, Bias: -37699.748568, T: 38181, Avg. loss: 4684.085055\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 3413859.98, NNZs: 930, Bias: -37700.162107, T: 39160, Avg. loss: 4317.237158\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 3413859.46, NNZs: 930, Bias: -37700.569896, T: 40139, Avg. loss: 4217.265268\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 3413858.96, NNZs: 930, Bias: -37700.964826, T: 41118, Avg. loss: 4119.945177\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 3413858.46, NNZs: 930, Bias: -37701.363646, T: 42097, Avg. loss: 4028.029376\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 3413858.01, NNZs: 930, Bias: -37701.714431, T: 43076, Avg. loss: 3937.004130\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 3413857.38, NNZs: 928, Bias: -37702.233303, T: 44055, Avg. loss: 4008.626782\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 3413856.99, NNZs: 928, Bias: -37702.538979, T: 45034, Avg. loss: 3740.336716\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 3413856.46, NNZs: 928, Bias: -37702.970467, T: 46013, Avg. loss: 3823.505187\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 3413856.12, NNZs: 928, Bias: -37703.233141, T: 46992, Avg. loss: 3578.879123\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 3413855.65, NNZs: 928, Bias: -37703.618014, T: 47971, Avg. loss: 3659.446527\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 3413855.22, NNZs: 928, Bias: -37703.963652, T: 48950, Avg. loss: 3574.076185\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 3413854.97, NNZs: 928, Bias: -37704.147889, T: 49929, Avg. loss: 3359.903395\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 3413854.71, NNZs: 928, Bias: -37704.347925, T: 50908, Avg. loss: 3315.486206\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 3413854.33, NNZs: 928, Bias: -37704.655845, T: 51887, Avg. loss: 3396.344386\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 3413854.10, NNZs: 928, Bias: -37704.823682, T: 52866, Avg. loss: 3203.959195\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 3413853.87, NNZs: 928, Bias: -37704.997084, T: 53845, Avg. loss: 3165.055912\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 3413853.66, NNZs: 928, Bias: -37705.155615, T: 54824, Avg. loss: 3124.636955\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 3413853.33, NNZs: 928, Bias: -37705.414190, T: 55803, Avg. loss: 3197.512222\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 3413853.04, NNZs: 928, Bias: -37705.650295, T: 56782, Avg. loss: 3137.785557\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 3413852.76, NNZs: 928, Bias: -37705.870173, T: 57761, Avg. loss: 3086.167559\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 3413852.51, NNZs: 928, Bias: -37706.070175, T: 58740, Avg. loss: 3034.641237\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 3413852.37, NNZs: 928, Bias: -37706.166798, T: 59719, Avg. loss: 2884.815761\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 3413852.22, NNZs: 928, Bias: -37706.274069, T: 60698, Avg. loss: 2857.493340\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 3413852.07, NNZs: 928, Bias: -37706.380032, T: 61677, Avg. loss: 2830.013225\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 3413851.92, NNZs: 928, Bias: -37706.491973, T: 62656, Avg. loss: 2803.154046\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 3413851.77, NNZs: 928, Bias: -37706.600619, T: 63635, Avg. loss: 2775.342106\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 3413851.62, NNZs: 928, Bias: -37706.711948, T: 64614, Avg. loss: 2747.349812\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 3413851.40, NNZs: 928, Bias: -37706.880070, T: 65593, Avg. loss: 2807.244152\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 3413851.27, NNZs: 928, Bias: -37706.980291, T: 66572, Avg. loss: 2682.476977\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 3413851.13, NNZs: 928, Bias: -37707.081452, T: 67551, Avg. loss: 2658.806651\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 3413850.94, NNZs: 928, Bias: -37707.230890, T: 68530, Avg. loss: 2715.286090\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 3413850.76, NNZs: 928, Bias: -37707.370129, T: 69509, Avg. loss: 2678.956250\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 3413850.59, NNZs: 928, Bias: -37707.499302, T: 70488, Avg. loss: 2645.660293\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 3413850.44, NNZs: 928, Bias: -37707.618739, T: 71467, Avg. loss: 2613.349438\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 3413850.29, NNZs: 928, Bias: -37707.732450, T: 72446, Avg. loss: 2584.302297\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 3413850.20, NNZs: 927, Bias: -37707.794743, T: 73425, Avg. loss: 2480.724158\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 3413850.11, NNZs: 927, Bias: -37707.854420, T: 74404, Avg. loss: 2463.173463\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 3413849.97, NNZs: 927, Bias: -37707.960721, T: 75383, Avg. loss: 2515.250484\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 3413849.84, NNZs: 927, Bias: -37708.062003, T: 76362, Avg. loss: 2489.187538\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 3413849.76, NNZs: 927, Bias: -37708.117330, T: 77341, Avg. loss: 2395.213318\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 3413849.67, NNZs: 927, Bias: -37708.172600, T: 78320, Avg. loss: 2378.567024\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 3413849.55, NNZs: 927, Bias: -37708.265617, T: 79299, Avg. loss: 2427.351401\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 3413849.47, NNZs: 927, Bias: -37708.318317, T: 80278, Avg. loss: 2338.700084\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 3413849.36, NNZs: 927, Bias: -37708.406778, T: 81257, Avg. loss: 2385.792933\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 3413849.28, NNZs: 927, Bias: -37708.457173, T: 82236, Avg. loss: 2300.890354\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 3413849.17, NNZs: 927, Bias: -37708.542549, T: 83215, Avg. loss: 2346.312247\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 3413849.10, NNZs: 927, Bias: -37708.588308, T: 84194, Avg. loss: 2264.906664\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 3413849.04, NNZs: 927, Bias: -37708.631812, T: 85173, Avg. loss: 2250.518009\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 3413848.97, NNZs: 927, Bias: -37708.677945, T: 86152, Avg. loss: 2236.306762\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 3413848.87, NNZs: 927, Bias: -37708.755099, T: 87131, Avg. loss: 2277.837484\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 3413848.77, NNZs: 927, Bias: -37708.828567, T: 88110, Avg. loss: 2256.994244\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 3413848.71, NNZs: 927, Bias: -37708.868190, T: 89089, Avg. loss: 2183.111485\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 3413848.65, NNZs: 927, Bias: -37708.906621, T: 90068, Avg. loss: 2170.004571\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 3413848.56, NNZs: 927, Bias: -37708.974930, T: 91047, Avg. loss: 2208.530639\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 3413848.47, NNZs: 927, Bias: -37709.039315, T: 92026, Avg. loss: 2189.867144\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 3413848.42, NNZs: 927, Bias: -37709.072741, T: 93005, Avg. loss: 2120.990530\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 3413848.36, NNZs: 927, Bias: -37709.109686, T: 93984, Avg. loss: 2108.947821\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 3413848.28, NNZs: 927, Bias: -37709.170791, T: 94963, Avg. loss: 2145.365864\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 3413848.23, NNZs: 927, Bias: -37709.201518, T: 95942, Avg. loss: 2080.051081\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 3413848.18, NNZs: 927, Bias: -37709.236197, T: 96921, Avg. loss: 2068.325745\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 3413848.13, NNZs: 927, Bias: -37709.270496, T: 97900, Avg. loss: 2056.497129\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 3413848.05, NNZs: 927, Bias: -37709.327808, T: 98879, Avg. loss: 2090.361859\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 3413847.98, NNZs: 927, Bias: -37709.382582, T: 99858, Avg. loss: 2074.200124\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 3413847.91, NNZs: 927, Bias: -37709.435179, T: 100837, Avg. loss: 2059.068741\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 3413847.86, NNZs: 927, Bias: -37709.461717, T: 101816, Avg. loss: 2000.258936\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 3413847.82, NNZs: 927, Bias: -37709.491414, T: 102795, Avg. loss: 1989.600347\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 3413847.75, NNZs: 927, Bias: -37709.541916, T: 103774, Avg. loss: 2021.688038\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 3413847.68, NNZs: 927, Bias: -37709.589087, T: 104753, Avg. loss: 2006.925803\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 3413847.62, NNZs: 927, Bias: -37709.633684, T: 105732, Avg. loss: 1992.843957\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 3413847.58, NNZs: 927, Bias: -37709.658197, T: 106711, Avg. loss: 1938.840463\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 3413847.55, NNZs: 927, Bias: -37709.681985, T: 107690, Avg. loss: 1929.274743\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 3413847.51, NNZs: 927, Bias: -37709.705642, T: 108669, Avg. loss: 1919.733412\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 3413847.45, NNZs: 927, Bias: -37709.750013, T: 109648, Avg. loss: 1948.953403\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 3413847.41, NNZs: 927, Bias: -37709.773123, T: 110627, Avg. loss: 1897.789925\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 3413847.35, NNZs: 927, Bias: -37709.814076, T: 111606, Avg. loss: 1926.073944\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 3413847.30, NNZs: 927, Bias: -37709.853735, T: 112585, Avg. loss: 1913.966488\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 3413847.25, NNZs: 927, Bias: -37709.891762, T: 113564, Avg. loss: 1901.861370\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 3413847.19, NNZs: 927, Bias: -37709.928165, T: 114543, Avg. loss: 1889.865118\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 3413847.16, NNZs: 927, Bias: -37709.946444, T: 115522, Avg. loss: 1842.214713\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 3413847.11, NNZs: 927, Bias: -37709.982066, T: 116501, Avg. loss: 1869.416464\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 3413847.06, NNZs: 927, Bias: -37710.016328, T: 117480, Avg. loss: 1858.029032\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 3413847.03, NNZs: 927, Bias: -37710.034712, T: 118459, Avg. loss: 1812.507613\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 3413846.99, NNZs: 927, Bias: -37710.068622, T: 119438, Avg. loss: 1838.917397\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 3413846.94, NNZs: 927, Bias: -37710.101153, T: 120417, Avg. loss: 1828.183303\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 3413846.91, NNZs: 927, Bias: -37710.117784, T: 121396, Avg. loss: 1784.538277\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 3413846.87, NNZs: 927, Bias: -37710.149249, T: 122375, Avg. loss: 1809.777154\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 3413846.83, NNZs: 927, Bias: -37710.179622, T: 123354, Avg. loss: 1799.628147\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 3413846.80, NNZs: 927, Bias: -37710.194542, T: 124333, Avg. loss: 1757.534798\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 3413846.76, NNZs: 927, Bias: -37710.223799, T: 125312, Avg. loss: 1781.722931\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 3413846.73, NNZs: 927, Bias: -37710.238402, T: 126291, Avg. loss: 1740.871166\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 3413846.71, NNZs: 927, Bias: -37710.253840, T: 127270, Avg. loss: 1733.742029\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 3413846.67, NNZs: 927, Bias: -37710.282275, T: 128249, Avg. loss: 1756.956851\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 3413846.64, NNZs: 927, Bias: -37710.297743, T: 129228, Avg. loss: 1717.522143\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 3413846.61, NNZs: 927, Bias: -37710.313166, T: 130207, Avg. loss: 1710.576282\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 3413846.57, NNZs: 927, Bias: -37710.341023, T: 131186, Avg. loss: 1732.932058\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 3413846.55, NNZs: 927, Bias: -37710.355579, T: 132165, Avg. loss: 1694.887078\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 3413846.52, NNZs: 927, Bias: -37710.370532, T: 133144, Avg. loss: 1687.989896\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 3413846.49, NNZs: 927, Bias: -37710.397594, T: 134123, Avg. loss: 1709.411268\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 3413846.46, NNZs: 927, Bias: -37710.412674, T: 135102, Avg. loss: 1672.775045\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 3413846.44, NNZs: 927, Bias: -37710.427777, T: 136081, Avg. loss: 1666.134588\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 3413846.40, NNZs: 927, Bias: -37710.453956, T: 137060, Avg. loss: 1686.917165\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 3413846.36, NNZs: 927, Bias: -37710.479249, T: 138039, Avg. loss: 1678.354765\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 3413846.33, NNZs: 927, Bias: -37710.503678, T: 139018, Avg. loss: 1670.164051\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 3413846.30, NNZs: 927, Bias: -37710.527039, T: 139997, Avg. loss: 1661.855171\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 3413846.26, NNZs: 927, Bias: -37710.549898, T: 140976, Avg. loss: 1653.905366\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 3413846.24, NNZs: 927, Bias: -37710.562252, T: 141955, Avg. loss: 1619.926190\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 3413846.21, NNZs: 927, Bias: -37710.584332, T: 142934, Avg. loss: 1639.616562\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 3413846.19, NNZs: 927, Bias: -37710.596360, T: 143913, Avg. loss: 1606.558669\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 3413846.17, NNZs: 927, Bias: -37710.608107, T: 144892, Avg. loss: 1600.690088\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 3413846.15, NNZs: 927, Bias: -37710.620496, T: 145871, Avg. loss: 1594.703425\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 3413846.13, NNZs: 927, Bias: -37710.633312, T: 146850, Avg. loss: 1588.932319\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 3413846.10, NNZs: 927, Bias: -37710.646234, T: 147829, Avg. loss: 1583.068033\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 3413846.08, NNZs: 927, Bias: -37710.659021, T: 148808, Avg. loss: 1577.125209\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 3413846.06, NNZs: 927, Bias: -37710.671447, T: 149787, Avg. loss: 1571.325726\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 3413846.04, NNZs: 927, Bias: -37710.684551, T: 150766, Avg. loss: 1565.569268\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 3413846.01, NNZs: 927, Bias: -37710.706228, T: 151745, Avg. loss: 1583.017589\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 3413845.99, NNZs: 927, Bias: -37710.718838, T: 152724, Avg. loss: 1552.815430\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 3413845.96, NNZs: 927, Bias: -37710.739566, T: 153703, Avg. loss: 1569.898388\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 3413845.94, NNZs: 927, Bias: -37710.751447, T: 154682, Avg. loss: 1540.320381\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 3413845.92, NNZs: 927, Bias: -37710.763562, T: 155661, Avg. loss: 1534.863330\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 3413845.90, NNZs: 927, Bias: -37710.775808, T: 156640, Avg. loss: 1529.446971\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 3413845.88, NNZs: 927, Bias: -37710.787581, T: 157619, Avg. loss: 1524.030679\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 3413845.86, NNZs: 927, Bias: -37710.799766, T: 158598, Avg. loss: 1518.570057\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 3413845.83, NNZs: 927, Bias: -37710.819579, T: 159577, Avg. loss: 1534.562544\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 3413845.80, NNZs: 927, Bias: -37710.838875, T: 160556, Avg. loss: 1527.932257\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 3413845.78, NNZs: 927, Bias: -37710.857536, T: 161535, Avg. loss: 1521.287196\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 3413845.76, NNZs: 927, Bias: -37710.868394, T: 162514, Avg. loss: 1494.186433\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 3413845.73, NNZs: 927, Bias: -37710.886409, T: 163493, Avg. loss: 1509.560220\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 3413845.71, NNZs: 927, Bias: -37710.903957, T: 164472, Avg. loss: 1503.394735\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 3413845.68, NNZs: 927, Bias: -37710.921008, T: 165451, Avg. loss: 1497.249776\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 3413845.66, NNZs: 927, Bias: -37710.937754, T: 166430, Avg. loss: 1491.174028\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 3413845.63, NNZs: 927, Bias: -37710.954211, T: 167409, Avg. loss: 1485.167351\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 3413845.61, NNZs: 927, Bias: -37710.970107, T: 168388, Avg. loss: 1479.302811\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 3413845.59, NNZs: 927, Bias: -37710.985531, T: 169367, Avg. loss: 1473.445211\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 3413845.57, NNZs: 927, Bias: -37710.994170, T: 170346, Avg. loss: 1448.444826\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 3413845.56, NNZs: 927, Bias: -37711.002724, T: 171325, Avg. loss: 1443.895600\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 3413845.54, NNZs: 927, Bias: -37711.017794, T: 172304, Avg. loss: 1458.374296\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 3413845.52, NNZs: 927, Bias: -37711.026404, T: 173283, Avg. loss: 1433.900766\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 3413845.50, NNZs: 927, Bias: -37711.041004, T: 174262, Avg. loss: 1448.081236\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 3413845.48, NNZs: 927, Bias: -37711.055542, T: 175241, Avg. loss: 1442.581864\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 3413845.46, NNZs: 927, Bias: -37711.069628, T: 176220, Avg. loss: 1437.135785\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 3413845.44, NNZs: 927, Bias: -37711.083552, T: 177199, Avg. loss: 1431.873960\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 3413845.42, NNZs: 927, Bias: -37711.090975, T: 178178, Avg. loss: 1408.542777\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 3413845.40, NNZs: 927, Bias: -37711.104598, T: 179157, Avg. loss: 1422.132350\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 3413845.39, NNZs: 927, Bias: -37711.112003, T: 180136, Avg. loss: 1399.312442\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 3413845.38, NNZs: 927, Bias: -37711.119857, T: 181115, Avg. loss: 1395.213001\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 3413845.36, NNZs: 927, Bias: -37711.127649, T: 182094, Avg. loss: 1391.044194\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 3413845.35, NNZs: 927, Bias: -37711.135596, T: 183073, Avg. loss: 1386.930841\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 3413845.33, NNZs: 927, Bias: -37711.143482, T: 184052, Avg. loss: 1382.859613\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 3413845.31, NNZs: 927, Bias: -37711.156944, T: 185031, Avg. loss: 1395.711475\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 3413845.29, NNZs: 927, Bias: -37711.170287, T: 186010, Avg. loss: 1390.772425\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 3413845.28, NNZs: 927, Bias: -37711.177956, T: 186989, Avg. loss: 1369.173650\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 3413845.26, NNZs: 927, Bias: -37711.190823, T: 187968, Avg. loss: 1381.692611\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 3413845.24, NNZs: 927, Bias: -37711.203414, T: 188947, Avg. loss: 1376.912088\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 3413845.22, NNZs: 927, Bias: -37711.215749, T: 189926, Avg. loss: 1372.133377\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 3413845.21, NNZs: 927, Bias: -37711.222517, T: 190905, Avg. loss: 1351.312684\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 3413845.19, NNZs: 927, Bias: -37711.234593, T: 191884, Avg. loss: 1363.500649\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 3413845.18, NNZs: 927, Bias: -37711.246584, T: 192863, Avg. loss: 1358.947462\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 3413845.16, NNZs: 927, Bias: -37711.253166, T: 193842, Avg. loss: 1338.615398\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 3413845.15, NNZs: 927, Bias: -37711.264694, T: 194821, Avg. loss: 1350.477136\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 3413845.13, NNZs: 927, Bias: -37711.276174, T: 195800, Avg. loss: 1346.045808\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 3413845.11, NNZs: 927, Bias: -37711.287393, T: 196779, Avg. loss: 1341.618515\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 3413845.10, NNZs: 927, Bias: -37711.293737, T: 197758, Avg. loss: 1321.923077\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 3413845.09, NNZs: 927, Bias: -37711.304728, T: 198737, Avg. loss: 1333.492756\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 3413845.07, NNZs: 927, Bias: -37711.310820, T: 199716, Avg. loss: 1314.198109\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 3413845.06, NNZs: 927, Bias: -37711.317023, T: 200695, Avg. loss: 1310.634121\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 3413845.05, NNZs: 927, Bias: -37711.327870, T: 201674, Avg. loss: 1321.876278\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 3413845.04, NNZs: 927, Bias: -37711.334027, T: 202653, Avg. loss: 1302.986899\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 3413845.02, NNZs: 927, Bias: -37711.344692, T: 203632, Avg. loss: 1314.027404\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 3413845.00, NNZs: 927, Bias: -37711.355157, T: 204611, Avg. loss: 1309.886703\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 3413844.99, NNZs: 927, Bias: -37711.365455, T: 205590, Avg. loss: 1305.783938\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 3413844.98, NNZs: 927, Bias: -37711.371541, T: 206569, Avg. loss: 1287.411348\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 3413844.97, NNZs: 927, Bias: -37711.377220, T: 207548, Avg. loss: 1284.044835\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 3413844.95, NNZs: 927, Bias: -37711.387561, T: 208527, Avg. loss: 1294.659187\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 214\n",
            "Norm: 3413844.94, NNZs: 927, Bias: -37711.397567, T: 209506, Avg. loss: 1290.673480\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 215\n",
            "Norm: 3413844.92, NNZs: 927, Bias: -37711.407366, T: 210485, Avg. loss: 1286.747295\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 216\n",
            "Norm: 3413844.91, NNZs: 927, Bias: -37711.412646, T: 211464, Avg. loss: 1269.136094\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 217\n",
            "Norm: 3413844.90, NNZs: 927, Bias: -37711.418468, T: 212443, Avg. loss: 1265.902923\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 218\n",
            "Norm: 3413844.89, NNZs: 927, Bias: -37711.424282, T: 213422, Avg. loss: 1262.678646\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 219\n",
            "Norm: 3413844.88, NNZs: 927, Bias: -37711.434016, T: 214401, Avg. loss: 1272.901229\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 220\n",
            "Norm: 3413844.87, NNZs: 927, Bias: -37711.439458, T: 215380, Avg. loss: 1255.745981\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 221\n",
            "Norm: 3413844.85, NNZs: 927, Bias: -37711.449084, T: 216359, Avg. loss: 1265.743837\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 222\n",
            "Norm: 3413844.84, NNZs: 927, Bias: -37711.458527, T: 217338, Avg. loss: 1262.023781\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 223\n",
            "Norm: 3413844.82, NNZs: 927, Bias: -37711.467760, T: 218317, Avg. loss: 1258.274038\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 224\n",
            "Norm: 3413844.81, NNZs: 927, Bias: -37711.472828, T: 219296, Avg. loss: 1241.683479\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 225\n",
            "Norm: 3413844.80, NNZs: 927, Bias: -37711.482003, T: 220275, Avg. loss: 1251.426265\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 226\n",
            "Norm: 3413844.79, NNZs: 927, Bias: -37711.491010, T: 221254, Avg. loss: 1247.834497\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 227\n",
            "Norm: 3413844.77, NNZs: 927, Bias: -37711.499761, T: 222233, Avg. loss: 1244.290424\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 228\n",
            "Norm: 3413844.76, NNZs: 927, Bias: -37711.504796, T: 223212, Avg. loss: 1228.120394\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 229\n",
            "Norm: 3413844.75, NNZs: 927, Bias: -37711.513521, T: 224191, Avg. loss: 1237.619594\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 230\n",
            "Norm: 3413844.74, NNZs: 927, Bias: -37711.518708, T: 225170, Avg. loss: 1221.724553\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 231\n",
            "Norm: 3413844.73, NNZs: 927, Bias: -37711.523879, T: 226149, Avg. loss: 1218.800359\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 232\n",
            "Norm: 3413844.72, NNZs: 927, Bias: -37711.532492, T: 227128, Avg. loss: 1228.119196\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 233\n",
            "Norm: 3413844.71, NNZs: 927, Bias: -37711.540985, T: 228107, Avg. loss: 1224.665485\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 234\n",
            "Norm: 3413844.69, NNZs: 927, Bias: -37711.549287, T: 229086, Avg. loss: 1221.195166\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 235\n",
            "Norm: 3413844.68, NNZs: 927, Bias: -37711.554192, T: 230065, Avg. loss: 1205.857662\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 236\n",
            "Norm: 3413844.67, NNZs: 927, Bias: -37711.559225, T: 231044, Avg. loss: 1202.999683\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 237\n",
            "Norm: 3413844.67, NNZs: 927, Bias: -37711.564268, T: 232023, Avg. loss: 1200.211460\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 238\n",
            "Norm: 3413844.66, NNZs: 927, Bias: -37711.569199, T: 233002, Avg. loss: 1197.396265\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 239\n",
            "Norm: 3413844.65, NNZs: 927, Bias: -37711.574146, T: 233981, Avg. loss: 1194.555614\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 240\n",
            "Norm: 3413844.64, NNZs: 927, Bias: -37711.579178, T: 234960, Avg. loss: 1191.753362\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 241\n",
            "Norm: 3413844.63, NNZs: 927, Bias: -37711.584399, T: 235939, Avg. loss: 1188.962163\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 242\n",
            "Norm: 3413844.62, NNZs: 927, Bias: -37711.589634, T: 236918, Avg. loss: 1186.168534\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 243\n",
            "Norm: 3413844.61, NNZs: 927, Bias: -37711.594910, T: 237897, Avg. loss: 1183.423586\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 244\n",
            "Norm: 3413844.60, NNZs: 927, Bias: -37711.603158, T: 238876, Avg. loss: 1191.920118\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 245\n",
            "Norm: 3413844.59, NNZs: 927, Bias: -37711.608336, T: 239855, Avg. loss: 1177.534474\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 246\n",
            "Norm: 3413844.58, NNZs: 927, Bias: -37711.616485, T: 240834, Avg. loss: 1185.922444\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 247\n",
            "Norm: 3413844.57, NNZs: 927, Bias: -37711.621590, T: 241813, Avg. loss: 1171.706917\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 248\n",
            "Norm: 3413844.56, NNZs: 927, Bias: -37711.629558, T: 242792, Avg. loss: 1180.000417\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 249\n",
            "Norm: 3413844.55, NNZs: 927, Bias: -37711.637472, T: 243771, Avg. loss: 1176.907044\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 250\n",
            "Norm: 3413844.53, NNZs: 927, Bias: -37711.645167, T: 244750, Avg. loss: 1173.837723\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 251\n",
            "Norm: 3413844.52, NNZs: 927, Bias: -37711.650042, T: 245729, Avg. loss: 1159.984374\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 252\n",
            "Norm: 3413844.52, NNZs: 927, Bias: -37711.654907, T: 246708, Avg. loss: 1157.380355\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 253\n",
            "Norm: 3413844.50, NNZs: 927, Bias: -37711.662496, T: 247687, Avg. loss: 1165.410283\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 254\n",
            "Norm: 3413844.49, NNZs: 927, Bias: -37711.669952, T: 248666, Avg. loss: 1162.399282\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 255\n",
            "Norm: 3413844.49, NNZs: 927, Bias: -37711.674300, T: 249645, Avg. loss: 1148.935458\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 256\n",
            "Norm: 3413844.48, NNZs: 927, Bias: -37711.678871, T: 250624, Avg. loss: 1146.368574\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 257\n",
            "Norm: 3413844.47, NNZs: 927, Bias: -37711.686244, T: 251603, Avg. loss: 1154.225234\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 258\n",
            "Norm: 3413844.46, NNZs: 927, Bias: -37711.690690, T: 252582, Avg. loss: 1140.971084\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 259\n",
            "Norm: 3413844.45, NNZs: 927, Bias: -37711.697945, T: 253561, Avg. loss: 1148.722557\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 260\n",
            "Norm: 3413844.44, NNZs: 927, Bias: -37711.705135, T: 254540, Avg. loss: 1145.853427\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 261\n",
            "Norm: 3413844.43, NNZs: 927, Bias: -37711.712205, T: 255519, Avg. loss: 1142.981068\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 262\n",
            "Norm: 3413844.42, NNZs: 927, Bias: -37711.719190, T: 256498, Avg. loss: 1140.170569\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 263\n",
            "Norm: 3413844.40, NNZs: 927, Bias: -37711.725991, T: 257477, Avg. loss: 1137.369972\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 264\n",
            "Norm: 3413844.40, NNZs: 927, Bias: -37711.730243, T: 258456, Avg. loss: 1124.609971\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 265\n",
            "Norm: 3413844.39, NNZs: 927, Bias: -37711.734487, T: 259435, Avg. loss: 1122.219716\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 266\n",
            "Norm: 3413844.38, NNZs: 927, Bias: -37711.738605, T: 260414, Avg. loss: 1119.833181\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 267\n",
            "Norm: 3413844.37, NNZs: 927, Bias: -37711.742712, T: 261393, Avg. loss: 1117.481972\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 268\n",
            "Norm: 3413844.37, NNZs: 927, Bias: -37711.747076, T: 262372, Avg. loss: 1115.098532\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 269\n",
            "Norm: 3413844.36, NNZs: 927, Bias: -37711.753813, T: 263351, Avg. loss: 1122.388390\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 270\n",
            "Norm: 3413844.35, NNZs: 927, Bias: -37711.758031, T: 264330, Avg. loss: 1110.095056\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 271\n",
            "Norm: 3413844.34, NNZs: 927, Bias: -37711.762356, T: 265309, Avg. loss: 1107.773110\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 272\n",
            "Norm: 3413844.33, NNZs: 927, Bias: -37711.769013, T: 266288, Avg. loss: 1114.941706\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 273\n",
            "Norm: 3413844.32, NNZs: 927, Bias: -37711.775628, T: 267267, Avg. loss: 1112.276783\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 274\n",
            "Norm: 3413844.31, NNZs: 927, Bias: -37711.779836, T: 268246, Avg. loss: 1100.262554\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 275\n",
            "Norm: 3413844.31, NNZs: 927, Bias: -37711.783939, T: 269225, Avg. loss: 1097.991335\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 276\n",
            "Norm: 3413844.30, NNZs: 927, Bias: -37711.790423, T: 270204, Avg. loss: 1104.980476\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 277\n",
            "Norm: 3413844.29, NNZs: 927, Bias: -37711.794471, T: 271183, Avg. loss: 1093.160916\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 278\n",
            "Norm: 3413844.28, NNZs: 927, Bias: -37711.800936, T: 272162, Avg. loss: 1100.085834\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 279\n",
            "Norm: 3413844.27, NNZs: 927, Bias: -37711.805057, T: 273141, Avg. loss: 1088.422907\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 280\n",
            "Norm: 3413844.27, NNZs: 927, Bias: -37711.808911, T: 274120, Avg. loss: 1086.228935\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 281\n",
            "Norm: 3413844.26, NNZs: 927, Bias: -37711.813023, T: 275099, Avg. loss: 1084.002014\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 282\n",
            "Norm: 3413844.25, NNZs: 927, Bias: -37711.819408, T: 276078, Avg. loss: 1090.753149\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 283\n",
            "Norm: 3413844.24, NNZs: 927, Bias: -37711.823506, T: 277057, Avg. loss: 1079.342598\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 284\n",
            "Norm: 3413844.23, NNZs: 927, Bias: -37711.829688, T: 278036, Avg. loss: 1086.037250\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 285\n",
            "Norm: 3413844.23, NNZs: 927, Bias: -37711.833638, T: 279015, Avg. loss: 1074.772218\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 286\n",
            "Norm: 3413844.22, NNZs: 927, Bias: -37711.837676, T: 279994, Avg. loss: 1072.610282\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 287\n",
            "Norm: 3413844.21, NNZs: 927, Bias: -37711.841703, T: 280973, Avg. loss: 1070.484259\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 288\n",
            "Norm: 3413844.20, NNZs: 927, Bias: -37711.847843, T: 281952, Avg. loss: 1076.988810\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 289\n",
            "Norm: 3413844.20, NNZs: 927, Bias: -37711.851616, T: 282931, Avg. loss: 1065.971262\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 290\n",
            "Norm: 3413844.19, NNZs: 927, Bias: -37711.857685, T: 283910, Avg. loss: 1072.420364\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 291\n",
            "Norm: 3413844.18, NNZs: 927, Bias: -37711.863679, T: 284889, Avg. loss: 1070.031840\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 292\n",
            "Norm: 3413844.17, NNZs: 927, Bias: -37711.869527, T: 285868, Avg. loss: 1067.670008\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 293\n",
            "Norm: 3413844.16, NNZs: 927, Bias: -37711.875245, T: 286847, Avg. loss: 1065.301508\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 294\n",
            "Norm: 3413844.15, NNZs: 927, Bias: -37711.879044, T: 287826, Avg. loss: 1054.577048\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 295\n",
            "Norm: 3413844.15, NNZs: 927, Bias: -37711.882791, T: 288805, Avg. loss: 1052.535261\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 296\n",
            "Norm: 3413844.14, NNZs: 927, Bias: -37711.888472, T: 289784, Avg. loss: 1058.794756\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 297\n",
            "Norm: 3413844.13, NNZs: 927, Bias: -37711.894110, T: 290763, Avg. loss: 1056.481658\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 298\n",
            "Norm: 3413844.12, NNZs: 927, Bias: -37711.899719, T: 291742, Avg. loss: 1054.215775\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 299\n",
            "Norm: 3413844.11, NNZs: 927, Bias: -37711.905259, T: 292721, Avg. loss: 1051.947321\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 300\n",
            "Norm: 3413844.11, NNZs: 927, Bias: -37711.908607, T: 293700, Avg. loss: 1041.586631\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 301\n",
            "Norm: 3413844.10, NNZs: 927, Bias: -37711.914118, T: 294679, Avg. loss: 1047.683557\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 302\n",
            "Norm: 3413844.09, NNZs: 927, Bias: -37711.917537, T: 295658, Avg. loss: 1037.428697\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 303\n",
            "Norm: 3413844.08, NNZs: 927, Bias: -37711.921070, T: 296637, Avg. loss: 1035.483339\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 304\n",
            "Norm: 3413844.08, NNZs: 927, Bias: -37711.926412, T: 297616, Avg. loss: 1041.475271\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 305\n",
            "Norm: 3413844.07, NNZs: 927, Bias: -37711.929810, T: 298595, Avg. loss: 1031.384839\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 306\n",
            "Norm: 3413844.06, NNZs: 927, Bias: -37711.933319, T: 299574, Avg. loss: 1029.468807\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 307\n",
            "Norm: 3413844.06, NNZs: 927, Bias: -37711.938667, T: 300553, Avg. loss: 1035.388406\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 308\n",
            "Norm: 3413844.05, NNZs: 927, Bias: -37711.943874, T: 301532, Avg. loss: 1033.244719\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 309\n",
            "Norm: 3413844.04, NNZs: 927, Bias: -37711.947064, T: 302511, Avg. loss: 1023.337811\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 310\n",
            "Norm: 3413844.03, NNZs: 927, Bias: -37711.952165, T: 303490, Avg. loss: 1029.151646\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 311\n",
            "Norm: 3413844.03, NNZs: 927, Bias: -37711.957256, T: 304469, Avg. loss: 1027.046300\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 312\n",
            "Norm: 3413844.02, NNZs: 927, Bias: -37711.960413, T: 305448, Avg. loss: 1017.284003\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 313\n",
            "Norm: 3413844.01, NNZs: 927, Bias: -37711.965451, T: 306427, Avg. loss: 1023.001993\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 314\n",
            "Norm: 3413844.01, NNZs: 927, Bias: -37711.968673, T: 307406, Avg. loss: 1013.385082\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 315\n",
            "Norm: 3413844.00, NNZs: 927, Bias: -37711.973811, T: 308385, Avg. loss: 1019.069410\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 316\n",
            "Norm: 3413843.99, NNZs: 927, Bias: -37711.978799, T: 309364, Avg. loss: 1017.003007\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 317\n",
            "Norm: 3413843.98, NNZs: 927, Bias: -37711.983672, T: 310343, Avg. loss: 1014.944207\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 318\n",
            "Norm: 3413843.98, NNZs: 927, Bias: -37711.986741, T: 311322, Avg. loss: 1005.518092\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 319\n",
            "Norm: 3413843.97, NNZs: 927, Bias: -37711.991619, T: 312301, Avg. loss: 1011.096720\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 320\n",
            "Norm: 3413843.97, NNZs: 927, Bias: -37711.994674, T: 313280, Avg. loss: 1001.757851\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 321\n",
            "Norm: 3413843.96, NNZs: 927, Bias: -37711.999542, T: 314259, Avg. loss: 1007.284039\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 322\n",
            "Norm: 3413843.95, NNZs: 927, Bias: -37712.002687, T: 315238, Avg. loss: 998.019570\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 323\n",
            "Norm: 3413843.95, NNZs: 927, Bias: -37712.005656, T: 316217, Avg. loss: 996.265822\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 324\n",
            "Norm: 3413843.94, NNZs: 927, Bias: -37712.008835, T: 317196, Avg. loss: 994.515436\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 325\n",
            "Norm: 3413843.94, NNZs: 927, Bias: -37712.012035, T: 318175, Avg. loss: 992.764983\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 326\n",
            "Norm: 3413843.93, NNZs: 927, Bias: -37712.015057, T: 319154, Avg. loss: 991.034814\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 327\n",
            "Norm: 3413843.92, NNZs: 927, Bias: -37712.018064, T: 320133, Avg. loss: 989.290884\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 328\n",
            "Norm: 3413843.92, NNZs: 927, Bias: -37712.022790, T: 321112, Avg. loss: 994.578482\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 329\n",
            "Norm: 3413843.91, NNZs: 927, Bias: -37712.027434, T: 322091, Avg. loss: 992.645323\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 330\n",
            "Norm: 3413843.90, NNZs: 927, Bias: -37712.032064, T: 323070, Avg. loss: 990.711442\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 331\n",
            "Norm: 3413843.90, NNZs: 927, Bias: -37712.035188, T: 324049, Avg. loss: 981.872737\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 332\n",
            "Norm: 3413843.89, NNZs: 927, Bias: -37712.038176, T: 325028, Avg. loss: 980.178230\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 333\n",
            "Norm: 3413843.89, NNZs: 927, Bias: -37712.041150, T: 326007, Avg. loss: 978.493211\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 334\n",
            "Norm: 3413843.88, NNZs: 927, Bias: -37712.044204, T: 326986, Avg. loss: 976.804678\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 335\n",
            "Norm: 3413843.88, NNZs: 927, Bias: -37712.047228, T: 327965, Avg. loss: 975.130006\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 336\n",
            "Norm: 3413843.87, NNZs: 927, Bias: -37712.050319, T: 328944, Avg. loss: 973.459096\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 337\n",
            "Norm: 3413843.86, NNZs: 927, Bias: -37712.054926, T: 329923, Avg. loss: 978.545137\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 338\n",
            "Norm: 3413843.86, NNZs: 927, Bias: -37712.059498, T: 330902, Avg. loss: 976.677799\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 339\n",
            "Norm: 3413843.85, NNZs: 927, Bias: -37712.062499, T: 331881, Avg. loss: 968.148604\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 340\n",
            "Norm: 3413843.85, NNZs: 927, Bias: -37712.065565, T: 332860, Avg. loss: 966.519694\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 341\n",
            "Norm: 3413843.84, NNZs: 927, Bias: -37712.070026, T: 333839, Avg. loss: 971.486638\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 342\n",
            "Norm: 3413843.83, NNZs: 927, Bias: -37712.073073, T: 334818, Avg. loss: 963.097683\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 343\n",
            "Norm: 3413843.83, NNZs: 927, Bias: -37712.077553, T: 335797, Avg. loss: 968.033992\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 344\n",
            "Norm: 3413843.82, NNZs: 927, Bias: -37712.081992, T: 336776, Avg. loss: 966.239867\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 345\n",
            "Norm: 3413843.81, NNZs: 927, Bias: -37712.086300, T: 337755, Avg. loss: 964.452380\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 346\n",
            "Norm: 3413843.81, NNZs: 927, Bias: -37712.089122, T: 338734, Avg. loss: 956.206099\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 347\n",
            "Norm: 3413843.80, NNZs: 927, Bias: -37712.093396, T: 339713, Avg. loss: 961.075779\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 348\n",
            "Norm: 3413843.80, NNZs: 927, Bias: -37712.097633, T: 340692, Avg. loss: 959.312106\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 349\n",
            "Norm: 3413843.79, NNZs: 927, Bias: -37712.101917, T: 341671, Avg. loss: 957.574787\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 350\n",
            "Norm: 3413843.78, NNZs: 927, Bias: -37712.104773, T: 342650, Avg. loss: 949.489204\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 351\n",
            "Norm: 3413843.78, NNZs: 927, Bias: -37712.107541, T: 343629, Avg. loss: 947.946413\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 352\n",
            "Norm: 3413843.77, NNZs: 927, Bias: -37712.110346, T: 344608, Avg. loss: 946.403199\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 353\n",
            "Norm: 3413843.77, NNZs: 927, Bias: -37712.113192, T: 345587, Avg. loss: 944.862376\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 354\n",
            "Norm: 3413843.76, NNZs: 927, Bias: -37712.117347, T: 346566, Avg. loss: 949.565723\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 355\n",
            "Norm: 3413843.76, NNZs: 927, Bias: -37712.120102, T: 347545, Avg. loss: 941.646716\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 356\n",
            "Norm: 3413843.75, NNZs: 927, Bias: -37712.122953, T: 348524, Avg. loss: 940.118588\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 357\n",
            "Norm: 3413843.75, NNZs: 927, Bias: -37712.125815, T: 349503, Avg. loss: 938.620049\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 358\n",
            "Norm: 3413843.74, NNZs: 927, Bias: -37712.128594, T: 350482, Avg. loss: 937.113427\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 359\n",
            "Norm: 3413843.74, NNZs: 927, Bias: -37712.132717, T: 351461, Avg. loss: 941.690587\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 360\n",
            "Norm: 3413843.73, NNZs: 927, Bias: -37712.136766, T: 352440, Avg. loss: 940.035176\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 361\n",
            "Norm: 3413843.73, NNZs: 927, Bias: -37712.139543, T: 353419, Avg. loss: 932.332019\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 362\n",
            "Norm: 3413843.72, NNZs: 927, Bias: -37712.143530, T: 354398, Avg. loss: 936.855369\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 363\n",
            "Norm: 3413843.72, NNZs: 927, Bias: -37712.146316, T: 355377, Avg. loss: 929.220129\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 364\n",
            "Norm: 3413843.71, NNZs: 927, Bias: -37712.150263, T: 356356, Avg. loss: 933.727211\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 365\n",
            "Norm: 3413843.70, NNZs: 927, Bias: -37712.154256, T: 357335, Avg. loss: 932.080709\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 366\n",
            "Norm: 3413843.70, NNZs: 927, Bias: -37712.156980, T: 358314, Avg. loss: 924.555237\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 367\n",
            "Norm: 3413843.69, NNZs: 927, Bias: -37712.159709, T: 359293, Avg. loss: 923.110469\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 368\n",
            "Norm: 3413843.69, NNZs: 927, Bias: -37712.163626, T: 360272, Avg. loss: 927.539673\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 369\n",
            "Norm: 3413843.68, NNZs: 927, Bias: -37712.167548, T: 361251, Avg. loss: 925.938075\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 370\n",
            "Norm: 3413843.68, NNZs: 927, Bias: -37712.171332, T: 362230, Avg. loss: 924.345397\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 371\n",
            "Norm: 3413843.67, NNZs: 927, Bias: -37712.175084, T: 363209, Avg. loss: 922.771813\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 372\n",
            "Norm: 3413843.66, NNZs: 927, Bias: -37712.177682, T: 364188, Avg. loss: 915.433137\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 373\n",
            "Norm: 3413843.66, NNZs: 927, Bias: -37712.181476, T: 365167, Avg. loss: 919.772870\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 374\n",
            "Norm: 3413843.65, NNZs: 927, Bias: -37712.183919, T: 366146, Avg. loss: 912.496346\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 375\n",
            "Norm: 3413843.65, NNZs: 927, Bias: -37712.187645, T: 367125, Avg. loss: 916.792141\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 376\n",
            "Norm: 3413843.64, NNZs: 927, Bias: -37712.191282, T: 368104, Avg. loss: 915.242039\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 377\n",
            "Norm: 3413843.64, NNZs: 927, Bias: -37712.193819, T: 369083, Avg. loss: 908.071998\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 378\n",
            "Norm: 3413843.63, NNZs: 927, Bias: -37712.196293, T: 370062, Avg. loss: 906.692686\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 379\n",
            "Norm: 3413843.63, NNZs: 927, Bias: -37712.200018, T: 371041, Avg. loss: 910.931000\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 380\n",
            "Norm: 3413843.62, NNZs: 927, Bias: -37712.203620, T: 372020, Avg. loss: 909.427205\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 381\n",
            "Norm: 3413843.62, NNZs: 927, Bias: -37712.207174, T: 372999, Avg. loss: 907.909901\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 382\n",
            "Norm: 3413843.61, NNZs: 927, Bias: -37712.209649, T: 373978, Avg. loss: 900.877892\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 383\n",
            "Norm: 3413843.61, NNZs: 927, Bias: -37712.212070, T: 374957, Avg. loss: 899.535649\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 384\n",
            "Norm: 3413843.60, NNZs: 927, Bias: -37712.215626, T: 375936, Avg. loss: 903.674660\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 385\n",
            "Norm: 3413843.60, NNZs: 927, Bias: -37712.219116, T: 376915, Avg. loss: 902.186151\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 386\n",
            "Norm: 3413843.59, NNZs: 927, Bias: -37712.221490, T: 377894, Avg. loss: 895.277788\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 387\n",
            "Norm: 3413843.59, NNZs: 927, Bias: -37712.223923, T: 378873, Avg. loss: 893.948724\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 388\n",
            "Norm: 3413843.59, NNZs: 927, Bias: -37712.226395, T: 379852, Avg. loss: 892.628148\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 389\n",
            "Norm: 3413843.58, NNZs: 927, Bias: -37712.229963, T: 380831, Avg. loss: 896.673868\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 390\n",
            "Norm: 3413843.57, NNZs: 927, Bias: -37712.233454, T: 381810, Avg. loss: 895.217458\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 391\n",
            "Norm: 3413843.57, NNZs: 927, Bias: -37712.236940, T: 382789, Avg. loss: 893.777365\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 392\n",
            "Norm: 3413843.56, NNZs: 927, Bias: -37712.240333, T: 383768, Avg. loss: 892.345957\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 393\n",
            "Norm: 3413843.56, NNZs: 927, Bias: -37712.243739, T: 384747, Avg. loss: 890.921249\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 394\n",
            "Norm: 3413843.55, NNZs: 927, Bias: -37712.246003, T: 385726, Avg. loss: 884.222073\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 395\n",
            "Norm: 3413843.55, NNZs: 927, Bias: -37712.248355, T: 386705, Avg. loss: 882.946862\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 396\n",
            "Norm: 3413843.54, NNZs: 927, Bias: -37712.251683, T: 387684, Avg. loss: 886.895113\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 397\n",
            "Norm: 3413843.54, NNZs: 927, Bias: -37712.254985, T: 388663, Avg. loss: 885.475919\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 398\n",
            "Norm: 3413843.54, NNZs: 927, Bias: -37712.257281, T: 389642, Avg. loss: 878.905252\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 399\n",
            "Norm: 3413843.53, NNZs: 927, Bias: -37712.260614, T: 390621, Avg. loss: 882.806407\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 400\n",
            "Norm: 3413843.53, NNZs: 927, Bias: -37712.262780, T: 391600, Avg. loss: 876.281899\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 401\n",
            "Norm: 3413843.52, NNZs: 927, Bias: -37712.266016, T: 392579, Avg. loss: 880.131770\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 402\n",
            "Norm: 3413843.52, NNZs: 927, Bias: -37712.269346, T: 393558, Avg. loss: 878.764301\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 403\n",
            "Norm: 3413843.51, NNZs: 927, Bias: -37712.271594, T: 394537, Avg. loss: 872.320983\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 404\n",
            "Norm: 3413843.51, NNZs: 927, Bias: -37712.273863, T: 395516, Avg. loss: 871.087300\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 405\n",
            "Norm: 3413843.50, NNZs: 927, Bias: -37712.277097, T: 396495, Avg. loss: 874.893725\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 406\n",
            "Norm: 3413843.50, NNZs: 927, Bias: -37712.280293, T: 397474, Avg. loss: 873.543736\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 407\n",
            "Norm: 3413843.49, NNZs: 927, Bias: -37712.283425, T: 398453, Avg. loss: 872.191691\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 408\n",
            "Norm: 3413843.49, NNZs: 927, Bias: -37712.285582, T: 399432, Avg. loss: 865.868370\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 409\n",
            "Norm: 3413843.48, NNZs: 927, Bias: -37712.288690, T: 400411, Avg. loss: 869.615146\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 410\n",
            "Norm: 3413843.48, NNZs: 927, Bias: -37712.291845, T: 401390, Avg. loss: 868.275266\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 411\n",
            "Norm: 3413843.48, NNZs: 927, Bias: -37712.293961, T: 402369, Avg. loss: 862.035889\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 412\n",
            "Norm: 3413843.47, NNZs: 927, Bias: -37712.297135, T: 403348, Avg. loss: 865.736102\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 413\n",
            "Norm: 3413843.47, NNZs: 927, Bias: -37712.299246, T: 404327, Avg. loss: 859.541760\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 414\n",
            "Norm: 3413843.46, NNZs: 927, Bias: -37712.301357, T: 405306, Avg. loss: 858.352897\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 415\n",
            "Norm: 3413843.46, NNZs: 927, Bias: -37712.304411, T: 406285, Avg. loss: 862.009431\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 416\n",
            "Norm: 3413843.46, NNZs: 927, Bias: -37712.306561, T: 407264, Avg. loss: 855.880379\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 417\n",
            "Norm: 3413843.45, NNZs: 927, Bias: -37712.308669, T: 408243, Avg. loss: 854.704222\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 418\n",
            "Norm: 3413843.45, NNZs: 927, Bias: -37712.311700, T: 409222, Avg. loss: 858.312788\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 419\n",
            "Norm: 3413843.44, NNZs: 927, Bias: -37712.313843, T: 410201, Avg. loss: 852.262119\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 420\n",
            "Norm: 3413843.44, NNZs: 927, Bias: -37712.316020, T: 411180, Avg. loss: 851.095727\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 421\n",
            "Norm: 3413843.43, NNZs: 927, Bias: -37712.319065, T: 412159, Avg. loss: 854.676208\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 422\n",
            "Norm: 3413843.43, NNZs: 927, Bias: -37712.321209, T: 413138, Avg. loss: 848.680698\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 423\n",
            "Norm: 3413843.43, NNZs: 927, Bias: -37712.324205, T: 414117, Avg. loss: 852.230385\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 424\n",
            "Norm: 3413843.42, NNZs: 927, Bias: -37712.327276, T: 415096, Avg. loss: 850.976587\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 425\n",
            "Norm: 3413843.42, NNZs: 927, Bias: -37712.330224, T: 416075, Avg. loss: 849.719150\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 426\n",
            "Norm: 3413843.41, NNZs: 927, Bias: -37712.332274, T: 417054, Avg. loss: 843.825960\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 427\n",
            "Norm: 3413843.41, NNZs: 927, Bias: -37712.335199, T: 418033, Avg. loss: 847.323276\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 428\n",
            "Norm: 3413843.40, NNZs: 927, Bias: -37712.337236, T: 419012, Avg. loss: 841.467281\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 429\n",
            "Norm: 3413843.40, NNZs: 927, Bias: -37712.340135, T: 419991, Avg. loss: 844.934531\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 430\n",
            "Norm: 3413843.40, NNZs: 927, Bias: -37712.343090, T: 420970, Avg. loss: 843.715319\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 431\n",
            "Norm: 3413843.39, NNZs: 927, Bias: -37712.345096, T: 421949, Avg. loss: 837.932519\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 432\n",
            "Norm: 3413843.39, NNZs: 927, Bias: -37712.347973, T: 422928, Avg. loss: 841.365575\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 433\n",
            "Norm: 3413843.38, NNZs: 927, Bias: -37712.350824, T: 423907, Avg. loss: 840.152149\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 434\n",
            "Norm: 3413843.38, NNZs: 927, Bias: -37712.353666, T: 424886, Avg. loss: 838.943811\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 435\n",
            "Norm: 3413843.38, NNZs: 927, Bias: -37712.355641, T: 425865, Avg. loss: 833.239770\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 436\n",
            "Norm: 3413843.37, NNZs: 927, Bias: -37712.358432, T: 426844, Avg. loss: 836.633177\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 437\n",
            "Norm: 3413843.37, NNZs: 927, Bias: -37712.361306, T: 427823, Avg. loss: 835.430287\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 438\n",
            "Norm: 3413843.36, NNZs: 927, Bias: -37712.363254, T: 428802, Avg. loss: 829.795944\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 439\n",
            "Norm: 3413843.36, NNZs: 927, Bias: -37712.365182, T: 429781, Avg. loss: 828.713644\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 440\n",
            "Norm: 3413843.36, NNZs: 927, Bias: -37712.367186, T: 430760, Avg. loss: 827.637012\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 441\n",
            "Norm: 3413843.35, NNZs: 927, Bias: -37712.369144, T: 431739, Avg. loss: 826.562248\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 442\n",
            "Norm: 3413843.35, NNZs: 927, Bias: -37712.371162, T: 432718, Avg. loss: 825.479295\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 443\n",
            "Norm: 3413843.35, NNZs: 927, Bias: -37712.373074, T: 433697, Avg. loss: 824.413443\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 444\n",
            "Norm: 3413843.34, NNZs: 927, Bias: -37712.375060, T: 434676, Avg. loss: 823.339786\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 445\n",
            "Norm: 3413843.34, NNZs: 927, Bias: -37712.377872, T: 435655, Avg. loss: 826.608094\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 446\n",
            "Norm: 3413843.33, NNZs: 927, Bias: -37712.379850, T: 436634, Avg. loss: 821.129700\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 447\n",
            "Norm: 3413843.33, NNZs: 927, Bias: -37712.382599, T: 437613, Avg. loss: 824.377429\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 448\n",
            "Norm: 3413843.33, NNZs: 927, Bias: -37712.384587, T: 438592, Avg. loss: 818.933167\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 449\n",
            "Norm: 3413843.32, NNZs: 927, Bias: -37712.387306, T: 439571, Avg. loss: 822.155755\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 450\n",
            "Norm: 3413843.32, NNZs: 927, Bias: -37712.389254, T: 440550, Avg. loss: 816.754939\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 451\n",
            "Norm: 3413843.32, NNZs: 927, Bias: -37712.391231, T: 441529, Avg. loss: 815.711384\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 452\n",
            "Norm: 3413843.31, NNZs: 927, Bias: -37712.393158, T: 442508, Avg. loss: 814.665150\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 453\n",
            "Norm: 3413843.31, NNZs: 927, Bias: -37712.395851, T: 443487, Avg. loss: 817.844639\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 454\n",
            "Norm: 3413843.30, NNZs: 927, Bias: -37712.398603, T: 444466, Avg. loss: 816.715204\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 455\n",
            "Norm: 3413843.30, NNZs: 927, Bias: -37712.401306, T: 445445, Avg. loss: 815.595162\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 456\n",
            "Norm: 3413843.30, NNZs: 927, Bias: -37712.403219, T: 446424, Avg. loss: 810.310896\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 457\n",
            "Norm: 3413843.29, NNZs: 927, Bias: -37712.405165, T: 447403, Avg. loss: 809.283022\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 458\n",
            "Norm: 3413843.29, NNZs: 927, Bias: -37712.407782, T: 448382, Avg. loss: 812.409474\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 459\n",
            "Norm: 3413843.28, NNZs: 927, Bias: -37712.410483, T: 449361, Avg. loss: 811.302071\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 460\n",
            "Norm: 3413843.28, NNZs: 927, Bias: -37712.413120, T: 450340, Avg. loss: 810.201230\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 461\n",
            "Norm: 3413843.28, NNZs: 927, Bias: -37712.415024, T: 451319, Avg. loss: 805.009038\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 462\n",
            "Norm: 3413843.27, NNZs: 927, Bias: -37712.417610, T: 452298, Avg. loss: 808.089013\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 463\n",
            "Norm: 3413843.27, NNZs: 927, Bias: -37712.419460, T: 453277, Avg. loss: 802.927311\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 464\n",
            "Norm: 3413843.27, NNZs: 927, Bias: -37712.422092, T: 454256, Avg. loss: 805.981989\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 465\n",
            "Norm: 3413843.26, NNZs: 927, Bias: -37712.423885, T: 455235, Avg. loss: 800.861224\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 466\n",
            "Norm: 3413843.26, NNZs: 927, Bias: -37712.426505, T: 456214, Avg. loss: 803.903120\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 467\n",
            "Norm: 3413843.25, NNZs: 927, Bias: -37712.429059, T: 457193, Avg. loss: 802.827112\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 468\n",
            "Norm: 3413843.25, NNZs: 927, Bias: -37712.431569, T: 458172, Avg. loss: 801.760210\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 469\n",
            "Norm: 3413843.25, NNZs: 927, Bias: -37712.434061, T: 459151, Avg. loss: 800.693182\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 470\n",
            "Norm: 3413843.24, NNZs: 927, Bias: -37712.435889, T: 460130, Avg. loss: 795.658000\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 471\n",
            "Norm: 3413843.24, NNZs: 927, Bias: -37712.438378, T: 461109, Avg. loss: 798.649344\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 472\n",
            "Norm: 3413843.24, NNZs: 927, Bias: -37712.440904, T: 462088, Avg. loss: 797.593346\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 473\n",
            "Norm: 3413843.23, NNZs: 927, Bias: -37712.443342, T: 463067, Avg. loss: 796.545120\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 474\n",
            "Norm: 3413843.23, NNZs: 927, Bias: -37712.445816, T: 464046, Avg. loss: 795.502118\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 475\n",
            "Norm: 3413843.22, NNZs: 927, Bias: -37712.448271, T: 465025, Avg. loss: 794.464484\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 476\n",
            "Norm: 3413843.22, NNZs: 927, Bias: -37712.449997, T: 466004, Avg. loss: 789.535168\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 477\n",
            "Norm: 3413843.22, NNZs: 927, Bias: -37712.452418, T: 466983, Avg. loss: 792.465522\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 478\n",
            "Norm: 3413843.21, NNZs: 927, Bias: -37712.454176, T: 467962, Avg. loss: 787.569394\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 479\n",
            "Norm: 3413843.21, NNZs: 927, Bias: -37712.456571, T: 468941, Avg. loss: 790.487080\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 480\n",
            "Norm: 3413843.21, NNZs: 927, Bias: -37712.458998, T: 469920, Avg. loss: 789.459987\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 481\n",
            "Norm: 3413843.20, NNZs: 927, Bias: -37712.460731, T: 470899, Avg. loss: 784.609600\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 482\n",
            "Norm: 3413843.20, NNZs: 927, Bias: -37712.462433, T: 471878, Avg. loss: 783.676594\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 483\n",
            "Norm: 3413843.20, NNZs: 927, Bias: -37712.464822, T: 472857, Avg. loss: 786.551248\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 484\n",
            "Norm: 3413843.19, NNZs: 927, Bias: -37712.466546, T: 473836, Avg. loss: 781.750950\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 485\n",
            "Norm: 3413843.19, NNZs: 927, Bias: -37712.468193, T: 474815, Avg. loss: 780.823864\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 486\n",
            "Norm: 3413843.19, NNZs: 927, Bias: -37712.470602, T: 475794, Avg. loss: 783.664969\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 487\n",
            "Norm: 3413843.18, NNZs: 927, Bias: -37712.472343, T: 476773, Avg. loss: 778.912936\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 488\n",
            "Norm: 3413843.18, NNZs: 927, Bias: -37712.474677, T: 477752, Avg. loss: 781.737215\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 489\n",
            "Norm: 3413843.18, NNZs: 927, Bias: -37712.477056, T: 478731, Avg. loss: 780.739602\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 490\n",
            "Norm: 3413843.17, NNZs: 927, Bias: -37712.479393, T: 479710, Avg. loss: 779.761046\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 491\n",
            "Norm: 3413843.17, NNZs: 927, Bias: -37712.481693, T: 480689, Avg. loss: 778.772634\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 492\n",
            "Norm: 3413843.17, NNZs: 927, Bias: -37712.483383, T: 481668, Avg. loss: 774.092353\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 493\n",
            "Norm: 3413843.16, NNZs: 927, Bias: -37712.485694, T: 482647, Avg. loss: 776.877807\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 494\n",
            "Norm: 3413843.16, NNZs: 927, Bias: -37712.487348, T: 483626, Avg. loss: 772.234194\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 495\n",
            "Norm: 3413843.16, NNZs: 927, Bias: -37712.489619, T: 484605, Avg. loss: 774.995812\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 496\n",
            "Norm: 3413843.15, NNZs: 927, Bias: -37712.491261, T: 485584, Avg. loss: 770.381556\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 497\n",
            "Norm: 3413843.15, NNZs: 927, Bias: -37712.492949, T: 486563, Avg. loss: 769.489680\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 498\n",
            "Norm: 3413843.15, NNZs: 927, Bias: -37712.495193, T: 487542, Avg. loss: 772.224305\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 499\n",
            "Norm: 3413843.14, NNZs: 927, Bias: -37712.497435, T: 488521, Avg. loss: 771.270048\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 500\n",
            "Norm: 3413843.14, NNZs: 927, Bias: -37712.499719, T: 489500, Avg. loss: 770.313669\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 501\n",
            "Norm: 3413843.14, NNZs: 927, Bias: -37712.501369, T: 490479, Avg. loss: 765.761654\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 502\n",
            "Norm: 3413843.13, NNZs: 927, Bias: -37712.502982, T: 491458, Avg. loss: 764.888946\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 503\n",
            "Norm: 3413843.13, NNZs: 927, Bias: -37712.504618, T: 492437, Avg. loss: 764.011427\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 504\n",
            "Norm: 3413843.13, NNZs: 927, Bias: -37712.506835, T: 493416, Avg. loss: 766.690261\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 505\n",
            "Norm: 3413843.12, NNZs: 927, Bias: -37712.509059, T: 494395, Avg. loss: 765.750551\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 506\n",
            "Norm: 3413843.12, NNZs: 927, Bias: -37712.510630, T: 495374, Avg. loss: 761.274892\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 507\n",
            "Norm: 3413843.12, NNZs: 927, Bias: -37712.512272, T: 496353, Avg. loss: 760.417035\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 508\n",
            "Norm: 3413843.12, NNZs: 927, Bias: -37712.513891, T: 497332, Avg. loss: 759.553410\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 509\n",
            "Norm: 3413843.11, NNZs: 927, Bias: -37712.515542, T: 498311, Avg. loss: 758.696293\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 510\n",
            "Norm: 3413843.11, NNZs: 927, Bias: -37712.517159, T: 499290, Avg. loss: 757.837137\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 511\n",
            "Norm: 3413843.11, NNZs: 927, Bias: -37712.518811, T: 500269, Avg. loss: 756.979534\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 512\n",
            "Norm: 3413843.10, NNZs: 927, Bias: -37712.520463, T: 501248, Avg. loss: 756.126155\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 513\n",
            "Norm: 3413843.10, NNZs: 927, Bias: -37712.522059, T: 502227, Avg. loss: 755.277100\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 514\n",
            "Norm: 3413843.10, NNZs: 927, Bias: -37712.523712, T: 503206, Avg. loss: 754.422388\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 515\n",
            "Norm: 3413843.10, NNZs: 927, Bias: -37712.525901, T: 504185, Avg. loss: 757.009835\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 516\n",
            "Norm: 3413843.09, NNZs: 927, Bias: -37712.528076, T: 505164, Avg. loss: 756.100304\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 517\n",
            "Norm: 3413843.09, NNZs: 927, Bias: -37712.530280, T: 506143, Avg. loss: 755.190268\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 518\n",
            "Norm: 3413843.09, NNZs: 927, Bias: -37712.531908, T: 507122, Avg. loss: 750.882610\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 519\n",
            "Norm: 3413843.08, NNZs: 927, Bias: -37712.534080, T: 508101, Avg. loss: 753.436100\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 520\n",
            "Norm: 3413843.08, NNZs: 927, Bias: -37712.536259, T: 509080, Avg. loss: 752.540852\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 521\n",
            "Norm: 3413843.08, NNZs: 927, Bias: -37712.537851, T: 510059, Avg. loss: 748.273765\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 522\n",
            "Norm: 3413843.07, NNZs: 927, Bias: -37712.539975, T: 511038, Avg. loss: 750.810688\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 523\n",
            "Norm: 3413843.07, NNZs: 927, Bias: -37712.542102, T: 512017, Avg. loss: 749.922528\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 524\n",
            "Norm: 3413843.07, NNZs: 927, Bias: -37712.544204, T: 512996, Avg. loss: 749.031131\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 525\n",
            "Norm: 3413843.06, NNZs: 927, Bias: -37712.546318, T: 513975, Avg. loss: 748.149804\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 526\n",
            "Norm: 3413843.06, NNZs: 927, Bias: -37712.547850, T: 514954, Avg. loss: 743.947127\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 527\n",
            "Norm: 3413843.06, NNZs: 927, Bias: -37712.549946, T: 515933, Avg. loss: 746.445143\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 528\n",
            "Norm: 3413843.05, NNZs: 927, Bias: -37712.551493, T: 516912, Avg. loss: 742.264920\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 529\n",
            "Norm: 3413843.05, NNZs: 927, Bias: -37712.553062, T: 517891, Avg. loss: 741.452397\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 530\n",
            "Norm: 3413843.05, NNZs: 927, Bias: -37712.554605, T: 518870, Avg. loss: 740.644009\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 531\n",
            "Norm: 3413843.05, NNZs: 927, Bias: -37712.556174, T: 519849, Avg. loss: 739.836822\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 532\n",
            "Norm: 3413843.04, NNZs: 927, Bias: -37712.558267, T: 520828, Avg. loss: 742.291334\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 533\n",
            "Norm: 3413843.04, NNZs: 927, Bias: -37712.559836, T: 521807, Avg. loss: 738.179401\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 534\n",
            "Norm: 3413843.04, NNZs: 927, Bias: -37712.561405, T: 522786, Avg. loss: 737.383047\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 535\n",
            "Norm: 3413843.03, NNZs: 927, Bias: -37712.563472, T: 523765, Avg. loss: 739.809578\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 536\n",
            "Norm: 3413843.03, NNZs: 927, Bias: -37712.565514, T: 524744, Avg. loss: 738.957743\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 537\n",
            "Norm: 3413843.03, NNZs: 927, Bias: -37712.567017, T: 525723, Avg. loss: 734.891643\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 538\n",
            "Norm: 3413843.03, NNZs: 927, Bias: -37712.569041, T: 526702, Avg. loss: 737.299608\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 539\n",
            "Norm: 3413843.02, NNZs: 927, Bias: -37712.571086, T: 527681, Avg. loss: 736.454355\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 540\n",
            "Norm: 3413843.02, NNZs: 927, Bias: -37712.573084, T: 528660, Avg. loss: 735.610426\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 541\n",
            "Norm: 3413843.02, NNZs: 927, Bias: -37712.575071, T: 529639, Avg. loss: 734.761594\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 542\n",
            "Norm: 3413843.01, NNZs: 927, Bias: -37712.576582, T: 530618, Avg. loss: 730.757141\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 543\n",
            "Norm: 3413843.01, NNZs: 927, Bias: -37712.578099, T: 531597, Avg. loss: 729.981788\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 544\n",
            "Norm: 3413843.01, NNZs: 927, Bias: -37712.580075, T: 532576, Avg. loss: 732.353247\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 545\n",
            "Norm: 3413843.00, NNZs: 927, Bias: -37712.582045, T: 533555, Avg. loss: 731.518840\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 546\n",
            "Norm: 3413843.00, NNZs: 927, Bias: -37712.583534, T: 534534, Avg. loss: 727.561996\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 547\n",
            "Norm: 3413843.00, NNZs: 927, Bias: -37712.585515, T: 535513, Avg. loss: 729.919403\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 548\n",
            "Norm: 3413843.00, NNZs: 927, Bias: -37712.587005, T: 536492, Avg. loss: 725.980442\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 549\n",
            "Norm: 3413842.99, NNZs: 927, Bias: -37712.588493, T: 537471, Avg. loss: 725.220515\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 550\n",
            "Norm: 3413842.99, NNZs: 927, Bias: -37712.589989, T: 538450, Avg. loss: 724.461277\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 551\n",
            "Norm: 3413842.99, NNZs: 927, Bias: -37712.591952, T: 539429, Avg. loss: 726.786848\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 552\n",
            "Norm: 3413842.99, NNZs: 927, Bias: -37712.593936, T: 540408, Avg. loss: 725.973669\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 553\n",
            "Norm: 3413842.98, NNZs: 927, Bias: -37712.595856, T: 541387, Avg. loss: 725.164997\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 554\n",
            "Norm: 3413842.98, NNZs: 927, Bias: -37712.597773, T: 542366, Avg. loss: 724.349003\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 555\n",
            "Norm: 3413842.98, NNZs: 927, Bias: -37712.599700, T: 543345, Avg. loss: 723.546545\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 556\n",
            "Norm: 3413842.97, NNZs: 927, Bias: -37712.601091, T: 544324, Avg. loss: 719.701924\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 557\n",
            "Norm: 3413842.97, NNZs: 927, Bias: -37712.602544, T: 545303, Avg. loss: 718.952078\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 558\n",
            "Norm: 3413842.97, NNZs: 927, Bias: -37712.603990, T: 546282, Avg. loss: 718.208131\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 559\n",
            "Norm: 3413842.97, NNZs: 927, Bias: -37712.605881, T: 547261, Avg. loss: 720.484040\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 560\n",
            "Norm: 3413842.96, NNZs: 927, Bias: -37712.607765, T: 548240, Avg. loss: 719.686906\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 561\n",
            "Norm: 3413842.96, NNZs: 927, Bias: -37712.609213, T: 549219, Avg. loss: 715.893348\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 562\n",
            "Norm: 3413842.96, NNZs: 927, Bias: -37712.611095, T: 550198, Avg. loss: 718.152358\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 563\n",
            "Norm: 3413842.95, NNZs: 927, Bias: -37712.612972, T: 551177, Avg. loss: 717.360154\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 564\n",
            "Norm: 3413842.95, NNZs: 927, Bias: -37712.614837, T: 552156, Avg. loss: 716.574555\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 565\n",
            "Norm: 3413842.95, NNZs: 927, Bias: -37712.616738, T: 553135, Avg. loss: 715.786288\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 566\n",
            "Norm: 3413842.95, NNZs: 927, Bias: -37712.618623, T: 554114, Avg. loss: 715.008952\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 567\n",
            "Norm: 3413842.94, NNZs: 927, Bias: -37712.620028, T: 555093, Avg. loss: 711.279293\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 568\n",
            "Norm: 3413842.94, NNZs: 927, Bias: -37712.621878, T: 556072, Avg. loss: 713.506002\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 569\n",
            "Norm: 3413842.94, NNZs: 927, Bias: -37712.623263, T: 557051, Avg. loss: 709.798314\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 570\n",
            "Norm: 3413842.94, NNZs: 927, Bias: -37712.624651, T: 558030, Avg. loss: 709.074569\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 571\n",
            "Norm: 3413842.93, NNZs: 927, Bias: -37712.625993, T: 559009, Avg. loss: 708.358802\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 572\n",
            "Norm: 3413842.93, NNZs: 927, Bias: -37712.627371, T: 559988, Avg. loss: 707.643106\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 573\n",
            "Norm: 3413842.93, NNZs: 927, Bias: -37712.629196, T: 560967, Avg. loss: 709.824677\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 574\n",
            "Norm: 3413842.93, NNZs: 927, Bias: -37712.631012, T: 561946, Avg. loss: 709.067584\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 575\n",
            "Norm: 3413842.92, NNZs: 927, Bias: -37712.632815, T: 562925, Avg. loss: 708.301270\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 576\n",
            "Norm: 3413842.92, NNZs: 927, Bias: -37712.634174, T: 563904, Avg. loss: 704.671143\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 577\n",
            "Norm: 3413842.92, NNZs: 927, Bias: -37712.635971, T: 564883, Avg. loss: 706.828335\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 578\n",
            "Norm: 3413842.91, NNZs: 927, Bias: -37712.637781, T: 565862, Avg. loss: 706.076990\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 579\n",
            "Norm: 3413842.91, NNZs: 927, Bias: -37712.639097, T: 566841, Avg. loss: 702.473795\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 580\n",
            "Norm: 3413842.91, NNZs: 927, Bias: -37712.640880, T: 567820, Avg. loss: 704.620825\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 581\n",
            "Norm: 3413842.91, NNZs: 927, Bias: -37712.642680, T: 568799, Avg. loss: 703.870480\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 582\n",
            "Norm: 3413842.90, NNZs: 927, Bias: -37712.644438, T: 569778, Avg. loss: 703.127884\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 583\n",
            "Norm: 3413842.90, NNZs: 927, Bias: -37712.646188, T: 570757, Avg. loss: 702.383882\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 584\n",
            "Norm: 3413842.90, NNZs: 927, Bias: -37712.647534, T: 571736, Avg. loss: 698.827644\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 585\n",
            "Norm: 3413842.90, NNZs: 927, Bias: -37712.648879, T: 572715, Avg. loss: 698.139390\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 586\n",
            "Norm: 3413842.89, NNZs: 927, Bias: -37712.650653, T: 573694, Avg. loss: 700.248473\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 587\n",
            "Norm: 3413842.89, NNZs: 927, Bias: -37712.651964, T: 574673, Avg. loss: 696.724846\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 588\n",
            "Norm: 3413842.89, NNZs: 927, Bias: -37712.653694, T: 575652, Avg. loss: 698.820540\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 589\n",
            "Norm: 3413842.89, NNZs: 927, Bias: -37712.655022, T: 576631, Avg. loss: 695.317657\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 590\n",
            "Norm: 3413842.88, NNZs: 927, Bias: -37712.656303, T: 577610, Avg. loss: 694.635291\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 591\n",
            "Norm: 3413842.88, NNZs: 927, Bias: -37712.657638, T: 578589, Avg. loss: 693.952340\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 592\n",
            "Norm: 3413842.88, NNZs: 927, Bias: -37712.658927, T: 579568, Avg. loss: 693.275388\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 593\n",
            "Norm: 3413842.88, NNZs: 927, Bias: -37712.660273, T: 580547, Avg. loss: 692.599143\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 594\n",
            "Norm: 3413842.87, NNZs: 927, Bias: -37712.661997, T: 581526, Avg. loss: 694.659814\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 595\n",
            "Norm: 3413842.87, NNZs: 927, Bias: -37712.663284, T: 582505, Avg. loss: 691.210942\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 596\n",
            "Norm: 3413842.87, NNZs: 927, Bias: -37712.665021, T: 583484, Avg. loss: 693.264294\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 597\n",
            "Norm: 3413842.87, NNZs: 927, Bias: -37712.666739, T: 584463, Avg. loss: 692.550862\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 598\n",
            "Norm: 3413842.86, NNZs: 927, Bias: -37712.668470, T: 585442, Avg. loss: 691.836952\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 599\n",
            "Norm: 3413842.86, NNZs: 927, Bias: -37712.669789, T: 586421, Avg. loss: 688.420477\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 600\n",
            "Norm: 3413842.86, NNZs: 927, Bias: -37712.671479, T: 587400, Avg. loss: 690.454947\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 601\n",
            "Norm: 3413842.86, NNZs: 927, Bias: -37712.673212, T: 588379, Avg. loss: 689.750134\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 602\n",
            "Norm: 3413842.85, NNZs: 927, Bias: -37712.674471, T: 589358, Avg. loss: 686.362859\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 603\n",
            "Norm: 3413842.85, NNZs: 927, Bias: -37712.675741, T: 590337, Avg. loss: 685.704681\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 604\n",
            "Norm: 3413842.85, NNZs: 927, Bias: -37712.677043, T: 591316, Avg. loss: 685.048390\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 605\n",
            "Norm: 3413842.85, NNZs: 927, Bias: -37712.678735, T: 592295, Avg. loss: 687.053498\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 606\n",
            "Norm: 3413842.85, NNZs: 927, Bias: -37712.680427, T: 593274, Avg. loss: 686.355013\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 607\n",
            "Norm: 3413842.84, NNZs: 927, Bias: -37712.681725, T: 594253, Avg. loss: 683.015018\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 608\n",
            "Norm: 3413842.84, NNZs: 927, Bias: -37712.683009, T: 595232, Avg. loss: 682.361162\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 609\n",
            "Norm: 3413842.84, NNZs: 927, Bias: -37712.684667, T: 596211, Avg. loss: 684.347057\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 610\n",
            "Norm: 3413842.84, NNZs: 927, Bias: -37712.686325, T: 597190, Avg. loss: 683.653057\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 611\n",
            "Norm: 3413842.83, NNZs: 927, Bias: -37712.688020, T: 598169, Avg. loss: 682.968130\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 612\n",
            "Norm: 3413842.83, NNZs: 927, Bias: -37712.689245, T: 599148, Avg. loss: 679.667529\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 613\n",
            "Norm: 3413842.83, NNZs: 927, Bias: -37712.690526, T: 600127, Avg. loss: 679.023423\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 614\n",
            "Norm: 3413842.83, NNZs: 927, Bias: -37712.692174, T: 601106, Avg. loss: 680.984285\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 615\n",
            "Norm: 3413842.82, NNZs: 927, Bias: -37712.693452, T: 602085, Avg. loss: 677.711765\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 616\n",
            "Norm: 3413842.82, NNZs: 927, Bias: -37712.694701, T: 603064, Avg. loss: 677.073774\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 617\n",
            "Norm: 3413842.82, NNZs: 927, Bias: -37712.695939, T: 604043, Avg. loss: 676.437672\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 618\n",
            "Norm: 3413842.82, NNZs: 927, Bias: -37712.697609, T: 605022, Avg. loss: 678.369105\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 619\n",
            "Norm: 3413842.81, NNZs: 927, Bias: -37712.698862, T: 606001, Avg. loss: 675.132716\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 620\n",
            "Norm: 3413842.81, NNZs: 927, Bias: -37712.700100, T: 606980, Avg. loss: 674.502627\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 621\n",
            "Norm: 3413842.81, NNZs: 927, Bias: -37712.701755, T: 607959, Avg. loss: 676.421500\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 622\n",
            "Norm: 3413842.81, NNZs: 927, Bias: -37712.702975, T: 608938, Avg. loss: 673.206827\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 623\n",
            "Norm: 3413842.81, NNZs: 927, Bias: -37712.704194, T: 609917, Avg. loss: 672.578964\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 624\n",
            "Norm: 3413842.80, NNZs: 927, Bias: -37712.705440, T: 610896, Avg. loss: 671.952504\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 625\n",
            "Norm: 3413842.80, NNZs: 927, Bias: -37712.706666, T: 611875, Avg. loss: 671.328296\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 626\n",
            "Norm: 3413842.80, NNZs: 927, Bias: -37712.707915, T: 612854, Avg. loss: 670.704836\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 627\n",
            "Norm: 3413842.80, NNZs: 927, Bias: -37712.709520, T: 613833, Avg. loss: 672.596557\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 628\n",
            "Norm: 3413842.79, NNZs: 927, Bias: -37712.711136, T: 614812, Avg. loss: 671.934751\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 629\n",
            "Norm: 3413842.79, NNZs: 927, Bias: -37712.712373, T: 615791, Avg. loss: 668.774045\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 630\n",
            "Norm: 3413842.79, NNZs: 927, Bias: -37712.713975, T: 616770, Avg. loss: 670.649578\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 631\n",
            "Norm: 3413842.79, NNZs: 927, Bias: -37712.715225, T: 617749, Avg. loss: 667.509362\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 632\n",
            "Norm: 3413842.79, NNZs: 927, Bias: -37712.716418, T: 618728, Avg. loss: 666.899059\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 633\n",
            "Norm: 3413842.78, NNZs: 927, Bias: -37712.718004, T: 619707, Avg. loss: 668.758809\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 634\n",
            "Norm: 3413842.78, NNZs: 927, Bias: -37712.719582, T: 620686, Avg. loss: 668.108555\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 635\n",
            "Norm: 3413842.78, NNZs: 927, Bias: -37712.721162, T: 621665, Avg. loss: 667.458364\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 636\n",
            "Norm: 3413842.78, NNZs: 927, Bias: -37712.722386, T: 622644, Avg. loss: 664.355929\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 637\n",
            "Norm: 3413842.77, NNZs: 927, Bias: -37712.723945, T: 623623, Avg. loss: 666.205260\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 638\n",
            "Norm: 3413842.77, NNZs: 927, Bias: -37712.725498, T: 624602, Avg. loss: 665.559284\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 639\n",
            "Norm: 3413842.77, NNZs: 927, Bias: -37712.726718, T: 625581, Avg. loss: 662.481294\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 640\n",
            "Norm: 3413842.77, NNZs: 927, Bias: -37712.728264, T: 626560, Avg. loss: 664.310588\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 641\n",
            "Norm: 3413842.77, NNZs: 927, Bias: -37712.729433, T: 627539, Avg. loss: 661.249793\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 642\n",
            "Norm: 3413842.76, NNZs: 927, Bias: -37712.730973, T: 628518, Avg. loss: 663.070067\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 643\n",
            "Norm: 3413842.76, NNZs: 927, Bias: -37712.732501, T: 629497, Avg. loss: 662.435864\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 644\n",
            "Norm: 3413842.76, NNZs: 927, Bias: -37712.734027, T: 630476, Avg. loss: 661.806380\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 645\n",
            "Norm: 3413842.76, NNZs: 927, Bias: -37712.735202, T: 631455, Avg. loss: 658.772403\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 646\n",
            "Norm: 3413842.75, NNZs: 927, Bias: -37712.736343, T: 632434, Avg. loss: 658.180471\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 647\n",
            "Norm: 3413842.75, NNZs: 927, Bias: -37712.737855, T: 633413, Avg. loss: 659.979026\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 648\n",
            "Norm: 3413842.75, NNZs: 927, Bias: -37712.739362, T: 634392, Avg. loss: 659.349379\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 649\n",
            "Norm: 3413842.75, NNZs: 927, Bias: -37712.740908, T: 635371, Avg. loss: 658.726850\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 650\n",
            "Norm: 3413842.74, NNZs: 927, Bias: -37712.742446, T: 636350, Avg. loss: 658.107143\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 651\n",
            "Norm: 3413842.74, NNZs: 927, Bias: -37712.743941, T: 637329, Avg. loss: 657.485740\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 652\n",
            "Norm: 3413842.74, NNZs: 927, Bias: -37712.745112, T: 638308, Avg. loss: 654.508487\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 653\n",
            "Norm: 3413842.74, NNZs: 927, Bias: -37712.746597, T: 639287, Avg. loss: 656.282480\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 654\n",
            "Norm: 3413842.74, NNZs: 927, Bias: -37712.748078, T: 640266, Avg. loss: 655.669941\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 655\n",
            "Norm: 3413842.73, NNZs: 927, Bias: -37712.749216, T: 641245, Avg. loss: 652.710014\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 656\n",
            "Norm: 3413842.73, NNZs: 927, Bias: -37712.750678, T: 642224, Avg. loss: 654.467734\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 657\n",
            "Norm: 3413842.73, NNZs: 927, Bias: -37712.751817, T: 643203, Avg. loss: 651.526520\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 658\n",
            "Norm: 3413842.73, NNZs: 927, Bias: -37712.753274, T: 644182, Avg. loss: 653.282156\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 659\n",
            "Norm: 3413842.72, NNZs: 927, Bias: -37712.754761, T: 645161, Avg. loss: 652.671127\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 660\n",
            "Norm: 3413842.72, NNZs: 927, Bias: -37712.755891, T: 646140, Avg. loss: 649.750057\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 661\n",
            "Norm: 3413842.72, NNZs: 927, Bias: -37712.757009, T: 647119, Avg. loss: 649.181020\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 662\n",
            "Norm: 3413842.72, NNZs: 927, Bias: -37712.758157, T: 648098, Avg. loss: 648.611203\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 663\n",
            "Norm: 3413842.72, NNZs: 927, Bias: -37712.759280, T: 649077, Avg. loss: 648.045885\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 664\n",
            "Norm: 3413842.72, NNZs: 927, Bias: -37712.760398, T: 650056, Avg. loss: 647.479194\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 665\n",
            "Norm: 3413842.71, NNZs: 927, Bias: -37712.761888, T: 651035, Avg. loss: 649.202454\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 666\n",
            "Norm: 3413842.71, NNZs: 927, Bias: -37712.763003, T: 652014, Avg. loss: 646.322549\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 667\n",
            "Norm: 3413842.71, NNZs: 927, Bias: -37712.764440, T: 652993, Avg. loss: 648.036676\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 668\n",
            "Norm: 3413842.71, NNZs: 927, Bias: -37712.765869, T: 653972, Avg. loss: 647.438667\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 669\n",
            "Norm: 3413842.70, NNZs: 927, Bias: -37712.767340, T: 654951, Avg. loss: 646.850351\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 670\n",
            "Norm: 3413842.70, NNZs: 927, Bias: -37712.768752, T: 655930, Avg. loss: 646.260255\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 671\n",
            "Norm: 3413842.70, NNZs: 927, Bias: -37712.770175, T: 656909, Avg. loss: 645.669029\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 672\n",
            "Norm: 3413842.70, NNZs: 927, Bias: -37712.771273, T: 657888, Avg. loss: 642.830394\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 673\n",
            "Norm: 3413842.70, NNZs: 927, Bias: -37712.772665, T: 658867, Avg. loss: 644.519249\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 674\n",
            "Norm: 3413842.69, NNZs: 927, Bias: -37712.774065, T: 659846, Avg. loss: 643.933507\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 675\n",
            "Norm: 3413842.69, NNZs: 927, Bias: -37712.775155, T: 660825, Avg. loss: 641.115493\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 676\n",
            "Norm: 3413842.69, NNZs: 927, Bias: -37712.776232, T: 661804, Avg. loss: 640.566685\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 677\n",
            "Norm: 3413842.69, NNZs: 927, Bias: -37712.777631, T: 662783, Avg. loss: 642.240734\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 678\n",
            "Norm: 3413842.69, NNZs: 927, Bias: -37712.778675, T: 663762, Avg. loss: 639.441351\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 679\n",
            "Norm: 3413842.68, NNZs: 927, Bias: -37712.779769, T: 664741, Avg. loss: 638.895114\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 680\n",
            "Norm: 3413842.68, NNZs: 927, Bias: -37712.781157, T: 665720, Avg. loss: 640.555982\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 681\n",
            "Norm: 3413842.68, NNZs: 927, Bias: -37712.782545, T: 666699, Avg. loss: 639.980149\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 682\n",
            "Norm: 3413842.68, NNZs: 927, Bias: -37712.783939, T: 667678, Avg. loss: 639.405189\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 683\n",
            "Norm: 3413842.68, NNZs: 927, Bias: -37712.785327, T: 668657, Avg. loss: 638.834859\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 684\n",
            "Norm: 3413842.67, NNZs: 927, Bias: -37712.786392, T: 669636, Avg. loss: 636.071631\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 685\n",
            "Norm: 3413842.67, NNZs: 927, Bias: -37712.787456, T: 670615, Avg. loss: 635.534546\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 686\n",
            "Norm: 3413842.67, NNZs: 927, Bias: -37712.788821, T: 671594, Avg. loss: 637.174934\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 687\n",
            "Norm: 3413842.67, NNZs: 927, Bias: -37712.789881, T: 672573, Avg. loss: 634.432646\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 688\n",
            "Norm: 3413842.67, NNZs: 927, Bias: -37712.790940, T: 673552, Avg. loss: 633.897368\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 689\n",
            "Norm: 3413842.66, NNZs: 927, Bias: -37712.792011, T: 674531, Avg. loss: 633.365092\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 690\n",
            "Norm: 3413842.66, NNZs: 927, Bias: -37712.793029, T: 675510, Avg. loss: 632.829779\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 691\n",
            "Norm: 3413842.66, NNZs: 927, Bias: -37712.794095, T: 676489, Avg. loss: 632.299538\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 692\n",
            "Norm: 3413842.66, NNZs: 927, Bias: -37712.795164, T: 677468, Avg. loss: 631.770430\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 693\n",
            "Norm: 3413842.66, NNZs: 927, Bias: -37712.796517, T: 678447, Avg. loss: 633.381279\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 694\n",
            "Norm: 3413842.65, NNZs: 927, Bias: -37712.797564, T: 679426, Avg. loss: 630.686462\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 695\n",
            "Norm: 3413842.65, NNZs: 927, Bias: -37712.798936, T: 680405, Avg. loss: 632.292383\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 696\n",
            "Norm: 3413842.65, NNZs: 927, Bias: -37712.800304, T: 681384, Avg. loss: 631.737825\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 697\n",
            "Norm: 3413842.65, NNZs: 927, Bias: -37712.801633, T: 682363, Avg. loss: 631.183133\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 698\n",
            "Norm: 3413842.65, NNZs: 927, Bias: -37712.802682, T: 683342, Avg. loss: 628.509229\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 699\n",
            "Norm: 3413842.64, NNZs: 927, Bias: -37712.804038, T: 684321, Avg. loss: 630.102714\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 700\n",
            "Norm: 3413842.64, NNZs: 927, Bias: -37712.805378, T: 685300, Avg. loss: 629.550342\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 701\n",
            "Norm: 3413842.64, NNZs: 927, Bias: -37712.806411, T: 686279, Avg. loss: 626.897830\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 702\n",
            "Norm: 3413842.64, NNZs: 927, Bias: -37712.807710, T: 687258, Avg. loss: 628.479908\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 703\n",
            "Norm: 3413842.64, NNZs: 927, Bias: -37712.808741, T: 688237, Avg. loss: 625.836485\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 704\n",
            "Norm: 3413842.64, NNZs: 927, Bias: -37712.809762, T: 689216, Avg. loss: 625.320727\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 705\n",
            "Norm: 3413842.63, NNZs: 927, Bias: -37712.811053, T: 690195, Avg. loss: 626.890071\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 706\n",
            "Norm: 3413842.63, NNZs: 927, Bias: -37712.812367, T: 691174, Avg. loss: 626.344417\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 707\n",
            "Norm: 3413842.63, NNZs: 927, Bias: -37712.813368, T: 692153, Avg. loss: 623.725547\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 708\n",
            "Norm: 3413842.63, NNZs: 927, Bias: -37712.814662, T: 693132, Avg. loss: 625.282033\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 709\n",
            "Norm: 3413842.63, NNZs: 927, Bias: -37712.815667, T: 694111, Avg. loss: 622.678229\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 710\n",
            "Norm: 3413842.62, NNZs: 927, Bias: -37712.816644, T: 695090, Avg. loss: 622.168513\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 711\n",
            "Norm: 3413842.62, NNZs: 927, Bias: -37712.817964, T: 696069, Avg. loss: 623.716092\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 712\n",
            "Norm: 3413842.62, NNZs: 927, Bias: -37712.818975, T: 697048, Avg. loss: 621.128260\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 713\n",
            "Norm: 3413842.62, NNZs: 927, Bias: -37712.819975, T: 698027, Avg. loss: 620.622915\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 714\n",
            "Norm: 3413842.62, NNZs: 927, Bias: -37712.821246, T: 699006, Avg. loss: 622.160593\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 715\n",
            "Norm: 3413842.61, NNZs: 927, Bias: -37712.822509, T: 699985, Avg. loss: 621.626190\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 716\n",
            "Norm: 3413842.61, NNZs: 927, Bias: -37712.823484, T: 700964, Avg. loss: 619.061034\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 717\n",
            "Norm: 3413842.61, NNZs: 927, Bias: -37712.824745, T: 701943, Avg. loss: 620.587970\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 718\n",
            "Norm: 3413842.61, NNZs: 927, Bias: -37712.825996, T: 702922, Avg. loss: 620.057553\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 719\n",
            "Norm: 3413842.61, NNZs: 927, Bias: -37712.826990, T: 703901, Avg. loss: 617.510039\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 720\n",
            "Norm: 3413842.61, NNZs: 927, Bias: -37712.828280, T: 704880, Avg. loss: 619.029185\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 721\n",
            "Norm: 3413842.60, NNZs: 927, Bias: -37712.829540, T: 705859, Avg. loss: 618.503714\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 722\n",
            "Norm: 3413842.60, NNZs: 927, Bias: -37712.830792, T: 706838, Avg. loss: 617.981667\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 723\n",
            "Norm: 3413842.60, NNZs: 927, Bias: -37712.832065, T: 707817, Avg. loss: 617.459453\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 724\n",
            "Norm: 3413842.60, NNZs: 927, Bias: -37712.833324, T: 708796, Avg. loss: 616.938993\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 725\n",
            "Norm: 3413842.60, NNZs: 927, Bias: -37712.834291, T: 709775, Avg. loss: 614.424024\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 726\n",
            "Norm: 3413842.59, NNZs: 927, Bias: -37712.835512, T: 710754, Avg. loss: 615.924388\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 727\n",
            "Norm: 3413842.59, NNZs: 927, Bias: -37712.836757, T: 711733, Avg. loss: 615.403388\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 728\n",
            "Norm: 3413842.59, NNZs: 927, Bias: -37712.837969, T: 712712, Avg. loss: 614.885460\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 729\n",
            "Norm: 3413842.59, NNZs: 927, Bias: -37712.838913, T: 713691, Avg. loss: 612.394515\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 730\n",
            "Norm: 3413842.59, NNZs: 927, Bias: -37712.840145, T: 714670, Avg. loss: 613.881683\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 731\n",
            "Norm: 3413842.58, NNZs: 927, Bias: -37712.841348, T: 715649, Avg. loss: 613.366392\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 732\n",
            "Norm: 3413842.58, NNZs: 927, Bias: -37712.842283, T: 716628, Avg. loss: 610.892385\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 733\n",
            "Norm: 3413842.58, NNZs: 927, Bias: -37712.843214, T: 717607, Avg. loss: 610.406438\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 734\n",
            "Norm: 3413842.58, NNZs: 927, Bias: -37712.844175, T: 718586, Avg. loss: 609.924231\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 735\n",
            "Norm: 3413842.58, NNZs: 927, Bias: -37712.845385, T: 719565, Avg. loss: 611.395648\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 736\n",
            "Norm: 3413842.58, NNZs: 927, Bias: -37712.846345, T: 720544, Avg. loss: 608.939200\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 737\n",
            "Norm: 3413842.57, NNZs: 927, Bias: -37712.847554, T: 721523, Avg. loss: 610.403836\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 738\n",
            "Norm: 3413842.57, NNZs: 927, Bias: -37712.848487, T: 722502, Avg. loss: 607.959796\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 739\n",
            "Norm: 3413842.57, NNZs: 927, Bias: -37712.849429, T: 723481, Avg. loss: 607.481618\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 740\n",
            "Norm: 3413842.57, NNZs: 927, Bias: -37712.850385, T: 724460, Avg. loss: 607.004803\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 741\n",
            "Norm: 3413842.57, NNZs: 927, Bias: -37712.851584, T: 725439, Avg. loss: 608.453922\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 742\n",
            "Norm: 3413842.57, NNZs: 927, Bias: -37712.852535, T: 726418, Avg. loss: 606.031567\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 743\n",
            "Norm: 3413842.56, NNZs: 927, Bias: -37712.853743, T: 727397, Avg. loss: 607.476916\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 744\n",
            "Norm: 3413842.56, NNZs: 927, Bias: -37712.854946, T: 728376, Avg. loss: 606.980402\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 745\n",
            "Norm: 3413842.56, NNZs: 927, Bias: -37712.855893, T: 729355, Avg. loss: 604.571894\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 746\n",
            "Norm: 3413842.56, NNZs: 927, Bias: -37712.857067, T: 730334, Avg. loss: 606.004816\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 747\n",
            "Norm: 3413842.56, NNZs: 927, Bias: -37712.857974, T: 731313, Avg. loss: 603.606873\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 748\n",
            "Norm: 3413842.56, NNZs: 927, Bias: -37712.859144, T: 732292, Avg. loss: 605.036831\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 749\n",
            "Norm: 3413842.55, NNZs: 927, Bias: -37712.860342, T: 733271, Avg. loss: 604.542342\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 750\n",
            "Norm: 3413842.55, NNZs: 927, Bias: -37712.861511, T: 734250, Avg. loss: 604.051509\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 751\n",
            "Norm: 3413842.55, NNZs: 927, Bias: -37712.862686, T: 735229, Avg. loss: 603.558767\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 752\n",
            "Norm: 3413842.55, NNZs: 927, Bias: -37712.863621, T: 736208, Avg. loss: 601.188583\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 753\n",
            "Norm: 3413842.55, NNZs: 927, Bias: -37712.864549, T: 737187, Avg. loss: 600.722339\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 754\n",
            "Norm: 3413842.54, NNZs: 927, Bias: -37712.865730, T: 738166, Avg. loss: 602.135106\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 755\n",
            "Norm: 3413842.54, NNZs: 927, Bias: -37712.866628, T: 739145, Avg. loss: 599.778801\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 756\n",
            "Norm: 3413842.54, NNZs: 927, Bias: -37712.867775, T: 740124, Avg. loss: 601.185531\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 757\n",
            "Norm: 3413842.54, NNZs: 927, Bias: -37712.868925, T: 741103, Avg. loss: 600.698337\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 758\n",
            "Norm: 3413842.54, NNZs: 927, Bias: -37712.869825, T: 742082, Avg. loss: 598.357626\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 759\n",
            "Norm: 3413842.54, NNZs: 927, Bias: -37712.870965, T: 743061, Avg. loss: 599.753830\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 760\n",
            "Norm: 3413842.53, NNZs: 927, Bias: -37712.871880, T: 744040, Avg. loss: 597.422597\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 761\n",
            "Norm: 3413842.53, NNZs: 927, Bias: -37712.873022, T: 745019, Avg. loss: 598.812854\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 762\n",
            "Norm: 3413842.53, NNZs: 927, Bias: -37712.874180, T: 745998, Avg. loss: 598.331660\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 763\n",
            "Norm: 3413842.53, NNZs: 927, Bias: -37712.875059, T: 746977, Avg. loss: 596.014979\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 764\n",
            "Norm: 3413842.53, NNZs: 927, Bias: -37712.876194, T: 747956, Avg. loss: 597.395274\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 765\n",
            "Norm: 3413842.53, NNZs: 927, Bias: -37712.877098, T: 748935, Avg. loss: 595.089658\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 766\n",
            "Norm: 3413842.52, NNZs: 927, Bias: -37712.877980, T: 749914, Avg. loss: 594.637900\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 767\n",
            "Norm: 3413842.52, NNZs: 927, Bias: -37712.878889, T: 750893, Avg. loss: 594.185629\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 768\n",
            "Norm: 3413842.52, NNZs: 927, Bias: -37712.879786, T: 751872, Avg. loss: 593.737341\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 769\n",
            "Norm: 3413842.52, NNZs: 927, Bias: -37712.880910, T: 752851, Avg. loss: 595.104207\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 770\n",
            "Norm: 3413842.52, NNZs: 927, Bias: -37712.882066, T: 753830, Avg. loss: 594.631418\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 771\n",
            "Norm: 3413842.52, NNZs: 927, Bias: -37712.882961, T: 754809, Avg. loss: 592.351967\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 772\n",
            "Norm: 3413842.51, NNZs: 927, Bias: -37712.884076, T: 755788, Avg. loss: 593.709118\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 773\n",
            "Norm: 3413842.51, NNZs: 927, Bias: -37712.885194, T: 756767, Avg. loss: 593.241862\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 774\n",
            "Norm: 3413842.51, NNZs: 927, Bias: -37712.886095, T: 757746, Avg. loss: 590.977157\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 775\n",
            "Norm: 3413842.51, NNZs: 927, Bias: -37712.886994, T: 758725, Avg. loss: 590.532942\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 776\n",
            "Norm: 3413842.51, NNZs: 927, Bias: -37712.888113, T: 759704, Avg. loss: 591.880331\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 777\n",
            "Norm: 3413842.51, NNZs: 927, Bias: -37712.889240, T: 760683, Avg. loss: 591.417289\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 778\n",
            "Norm: 3413842.50, NNZs: 927, Bias: -37712.890338, T: 761662, Avg. loss: 590.953544\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 779\n",
            "Norm: 3413842.50, NNZs: 927, Bias: -37712.891228, T: 762641, Avg. loss: 588.711274\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 780\n",
            "Norm: 3413842.50, NNZs: 927, Bias: -37712.892328, T: 763620, Avg. loss: 590.050054\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 781\n",
            "Norm: 3413842.50, NNZs: 927, Bias: -37712.893454, T: 764599, Avg. loss: 589.587745\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 782\n",
            "Norm: 3413842.50, NNZs: 927, Bias: -37712.894326, T: 765578, Avg. loss: 587.359952\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 783\n",
            "Norm: 3413842.50, NNZs: 927, Bias: -37712.895187, T: 766557, Avg. loss: 586.925204\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 784\n",
            "Norm: 3413842.50, NNZs: 927, Bias: -37712.896278, T: 767536, Avg. loss: 588.249302\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 785\n",
            "Norm: 3413842.49, NNZs: 927, Bias: -37712.897134, T: 768515, Avg. loss: 586.035696\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 786\n",
            "Norm: 3413842.49, NNZs: 927, Bias: -37712.898231, T: 769494, Avg. loss: 587.355061\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 787\n",
            "Norm: 3413842.49, NNZs: 927, Bias: -37712.899105, T: 770473, Avg. loss: 585.150689\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 788\n",
            "Norm: 3413842.49, NNZs: 927, Bias: -37712.899987, T: 771452, Avg. loss: 584.718867\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 789\n",
            "Norm: 3413842.49, NNZs: 927, Bias: -37712.901102, T: 772431, Avg. loss: 586.032265\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 790\n",
            "Norm: 3413842.49, NNZs: 927, Bias: -37712.902211, T: 773410, Avg. loss: 585.581215\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 791\n",
            "Norm: 3413842.48, NNZs: 927, Bias: -37712.903080, T: 774389, Avg. loss: 583.392778\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 792\n",
            "Norm: 3413842.48, NNZs: 927, Bias: -37712.903932, T: 775368, Avg. loss: 582.963302\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 793\n",
            "Norm: 3413842.48, NNZs: 927, Bias: -37712.904805, T: 776347, Avg. loss: 582.536672\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 794\n",
            "Norm: 3413842.48, NNZs: 927, Bias: -37712.905891, T: 777326, Avg. loss: 583.832895\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 795\n",
            "Norm: 3413842.48, NNZs: 927, Bias: -37712.906750, T: 778305, Avg. loss: 581.663806\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 796\n",
            "Norm: 3413842.48, NNZs: 927, Bias: -37712.907597, T: 779284, Avg. loss: 581.238075\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 797\n",
            "Norm: 3413842.48, NNZs: 927, Bias: -37712.908664, T: 780263, Avg. loss: 582.528254\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 798\n",
            "Norm: 3413842.47, NNZs: 927, Bias: -37712.909524, T: 781242, Avg. loss: 580.371452\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 799\n",
            "Norm: 3413842.47, NNZs: 927, Bias: -37712.910588, T: 782221, Avg. loss: 581.656665\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 800\n",
            "Norm: 3413842.47, NNZs: 927, Bias: -37712.911435, T: 783200, Avg. loss: 579.506633\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 801\n",
            "Norm: 3413842.47, NNZs: 927, Bias: -37712.912303, T: 784179, Avg. loss: 579.085682\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 802\n",
            "Norm: 3413842.47, NNZs: 927, Bias: -37712.913162, T: 785158, Avg. loss: 578.665094\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 803\n",
            "Norm: 3413842.47, NNZs: 927, Bias: -37712.914008, T: 786137, Avg. loss: 578.245806\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 804\n",
            "Norm: 3413842.46, NNZs: 927, Bias: -37712.914851, T: 787116, Avg. loss: 577.825519\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 805\n",
            "Norm: 3413842.46, NNZs: 927, Bias: -37712.915718, T: 788095, Avg. loss: 577.407178\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 806\n",
            "Norm: 3413842.46, NNZs: 927, Bias: -37712.916803, T: 789074, Avg. loss: 578.674518\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 807\n",
            "Norm: 3413842.46, NNZs: 927, Bias: -37712.917866, T: 790053, Avg. loss: 578.239320\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 808\n",
            "Norm: 3413842.46, NNZs: 927, Bias: -37712.918725, T: 791032, Avg. loss: 576.124584\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 809\n",
            "Norm: 3413842.46, NNZs: 927, Bias: -37712.919587, T: 792011, Avg. loss: 575.707537\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 810\n",
            "Norm: 3413842.46, NNZs: 927, Bias: -37712.920637, T: 792990, Avg. loss: 576.966187\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 811\n",
            "Norm: 3413842.45, NNZs: 927, Bias: -37712.921478, T: 793969, Avg. loss: 574.863009\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 812\n",
            "Norm: 3413842.45, NNZs: 927, Bias: -37712.922556, T: 794948, Avg. loss: 576.116306\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 813\n",
            "Norm: 3413842.45, NNZs: 927, Bias: -37712.923598, T: 795927, Avg. loss: 575.685424\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 814\n",
            "Norm: 3413842.45, NNZs: 927, Bias: -37712.924451, T: 796906, Avg. loss: 573.594398\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 815\n",
            "Norm: 3413842.45, NNZs: 927, Bias: -37712.925293, T: 797885, Avg. loss: 573.184228\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 816\n",
            "Norm: 3413842.45, NNZs: 927, Bias: -37712.926328, T: 798864, Avg. loss: 574.422923\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 817\n",
            "Norm: 3413842.44, NNZs: 927, Bias: -37712.927374, T: 799843, Avg. loss: 573.996663\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 818\n",
            "Norm: 3413842.44, NNZs: 927, Bias: -37712.928220, T: 800822, Avg. loss: 571.921447\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 819\n",
            "Norm: 3413842.44, NNZs: 927, Bias: -37712.929260, T: 801801, Avg. loss: 573.156945\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 820\n",
            "Norm: 3413842.44, NNZs: 927, Bias: -37712.930101, T: 802780, Avg. loss: 571.092518\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 821\n",
            "Norm: 3413842.44, NNZs: 927, Bias: -37712.931128, T: 803759, Avg. loss: 572.321788\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 822\n",
            "Norm: 3413842.44, NNZs: 927, Bias: -37712.931967, T: 804738, Avg. loss: 570.264153\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 823\n",
            "Norm: 3413842.44, NNZs: 927, Bias: -37712.933005, T: 805717, Avg. loss: 571.488651\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 824\n",
            "Norm: 3413842.43, NNZs: 927, Bias: -37712.934040, T: 806696, Avg. loss: 571.063996\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 825\n",
            "Norm: 3413842.43, NNZs: 927, Bias: -37712.935058, T: 807675, Avg. loss: 570.641139\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 826\n",
            "Norm: 3413842.43, NNZs: 927, Bias: -37712.936094, T: 808654, Avg. loss: 570.221646\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 827\n",
            "Norm: 3413842.43, NNZs: 927, Bias: -37712.937109, T: 809633, Avg. loss: 569.801051\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 828\n",
            "Norm: 3413842.43, NNZs: 927, Bias: -37712.937936, T: 810612, Avg. loss: 567.768602\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 829\n",
            "Norm: 3413842.43, NNZs: 927, Bias: -37712.938964, T: 811591, Avg. loss: 568.980655\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 830\n",
            "Norm: 3413842.42, NNZs: 927, Bias: -37712.939970, T: 812570, Avg. loss: 568.564517\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 831\n",
            "Norm: 3413842.42, NNZs: 927, Bias: -37712.940766, T: 813549, Avg. loss: 566.541879\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 832\n",
            "Norm: 3413842.42, NNZs: 927, Bias: -37712.941766, T: 814528, Avg. loss: 567.744880\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 833\n",
            "Norm: 3413842.42, NNZs: 927, Bias: -37712.942588, T: 815507, Avg. loss: 565.732217\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 834\n",
            "Norm: 3413842.42, NNZs: 927, Bias: -37712.943381, T: 816486, Avg. loss: 565.337046\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 835\n",
            "Norm: 3413842.42, NNZs: 927, Bias: -37712.944378, T: 817465, Avg. loss: 566.532532\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 836\n",
            "Norm: 3413842.42, NNZs: 927, Bias: -37712.945179, T: 818444, Avg. loss: 564.531841\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 837\n",
            "Norm: 3413842.41, NNZs: 927, Bias: -37712.945988, T: 819423, Avg. loss: 564.139208\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 838\n",
            "Norm: 3413842.41, NNZs: 927, Bias: -37712.946798, T: 820402, Avg. loss: 563.746315\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 839\n",
            "Norm: 3413842.41, NNZs: 927, Bias: -37712.947597, T: 821381, Avg. loss: 563.355259\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 840\n",
            "Norm: 3413842.41, NNZs: 927, Bias: -37712.948591, T: 822360, Avg. loss: 564.541515\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 841\n",
            "Norm: 3413842.41, NNZs: 927, Bias: -37712.949607, T: 823339, Avg. loss: 564.132461\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 842\n",
            "Norm: 3413842.41, NNZs: 927, Bias: -37712.950605, T: 824318, Avg. loss: 563.725245\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 843\n",
            "Norm: 3413842.41, NNZs: 927, Bias: -37712.951597, T: 825297, Avg. loss: 563.318118\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 844\n",
            "Norm: 3413842.40, NNZs: 927, Bias: -37712.952585, T: 826276, Avg. loss: 562.909281\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 845\n",
            "Norm: 3413842.40, NNZs: 927, Bias: -37712.953579, T: 827255, Avg. loss: 562.504533\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 846\n",
            "Norm: 3413842.40, NNZs: 927, Bias: -37712.954361, T: 828234, Avg. loss: 560.540797\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 847\n",
            "Norm: 3413842.40, NNZs: 927, Bias: -37712.955166, T: 829213, Avg. loss: 560.154291\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 848\n",
            "Norm: 3413842.40, NNZs: 927, Bias: -37712.955956, T: 830192, Avg. loss: 559.771105\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 849\n",
            "Norm: 3413842.40, NNZs: 927, Bias: -37712.956939, T: 831171, Avg. loss: 560.937437\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 850\n",
            "Norm: 3413842.40, NNZs: 927, Bias: -37712.957915, T: 832150, Avg. loss: 560.535160\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 851\n",
            "Norm: 3413842.39, NNZs: 927, Bias: -37712.958699, T: 833129, Avg. loss: 558.588794\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 852\n",
            "Norm: 3413842.39, NNZs: 927, Bias: -37712.959665, T: 834108, Avg. loss: 559.750227\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 853\n",
            "Norm: 3413842.39, NNZs: 927, Bias: -37712.960642, T: 835087, Avg. loss: 559.349483\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 854\n",
            "Norm: 3413842.39, NNZs: 927, Bias: -37712.961438, T: 836066, Avg. loss: 557.414863\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 855\n",
            "Norm: 3413842.39, NNZs: 927, Bias: -37712.962229, T: 837045, Avg. loss: 557.035068\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 856\n",
            "Norm: 3413842.39, NNZs: 927, Bias: -37712.963202, T: 838024, Avg. loss: 558.186378\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 857\n",
            "Norm: 3413842.39, NNZs: 927, Bias: -37712.963983, T: 839003, Avg. loss: 556.260846\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 858\n",
            "Norm: 3413842.38, NNZs: 927, Bias: -37712.964950, T: 839982, Avg. loss: 557.410125\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 859\n",
            "Norm: 3413842.38, NNZs: 927, Bias: -37712.965738, T: 840961, Avg. loss: 555.492952\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 860\n",
            "Norm: 3413842.38, NNZs: 927, Bias: -37712.966525, T: 841940, Avg. loss: 555.115109\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 861\n",
            "Norm: 3413842.38, NNZs: 927, Bias: -37712.967292, T: 842919, Avg. loss: 554.738776\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 862\n",
            "Norm: 3413842.38, NNZs: 927, Bias: -37712.968083, T: 843898, Avg. loss: 554.363456\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 863\n",
            "Norm: 3413842.38, NNZs: 927, Bias: -37712.968868, T: 844877, Avg. loss: 553.988375\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 864\n",
            "Norm: 3413842.38, NNZs: 927, Bias: -37712.969826, T: 845856, Avg. loss: 555.122980\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 865\n",
            "Norm: 3413842.37, NNZs: 927, Bias: -37712.970608, T: 846835, Avg. loss: 553.226704\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 866\n",
            "Norm: 3413842.37, NNZs: 927, Bias: -37712.971397, T: 847814, Avg. loss: 552.852406\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 867\n",
            "Norm: 3413842.37, NNZs: 927, Bias: -37712.972188, T: 848793, Avg. loss: 552.480657\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 868\n",
            "Norm: 3413842.37, NNZs: 927, Bias: -37712.972977, T: 849772, Avg. loss: 552.110222\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 869\n",
            "Norm: 3413842.37, NNZs: 927, Bias: -37712.973926, T: 850751, Avg. loss: 553.231359\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 870\n",
            "Norm: 3413842.37, NNZs: 927, Bias: -37712.974873, T: 851730, Avg. loss: 552.846587\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 871\n",
            "Norm: 3413842.37, NNZs: 927, Bias: -37712.975844, T: 852709, Avg. loss: 552.458586\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 872\n",
            "Norm: 3413842.37, NNZs: 927, Bias: -37712.976809, T: 853688, Avg. loss: 552.074247\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 873\n",
            "Norm: 3413842.36, NNZs: 927, Bias: -37712.977748, T: 854667, Avg. loss: 551.689511\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 874\n",
            "Norm: 3413842.36, NNZs: 927, Bias: -37712.978684, T: 855646, Avg. loss: 551.304544\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 875\n",
            "Norm: 3413842.36, NNZs: 927, Bias: -37712.979625, T: 856625, Avg. loss: 550.920781\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 876\n",
            "Norm: 3413842.36, NNZs: 927, Bias: -37712.980564, T: 857604, Avg. loss: 550.538719\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 877\n",
            "Norm: 3413842.36, NNZs: 927, Bias: -37712.981504, T: 858583, Avg. loss: 550.155757\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 878\n",
            "Norm: 3413842.36, NNZs: 927, Bias: -37712.982273, T: 859562, Avg. loss: 548.305262\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 879\n",
            "Norm: 3413842.36, NNZs: 927, Bias: -37712.983044, T: 860541, Avg. loss: 547.942264\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 880\n",
            "Norm: 3413842.35, NNZs: 927, Bias: -37712.983786, T: 861520, Avg. loss: 547.579748\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 881\n",
            "Norm: 3413842.35, NNZs: 927, Bias: -37712.984552, T: 862499, Avg. loss: 547.215769\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 882\n",
            "Norm: 3413842.35, NNZs: 927, Bias: -37712.985302, T: 863478, Avg. loss: 546.853979\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 883\n",
            "Norm: 3413842.35, NNZs: 927, Bias: -37712.986225, T: 864457, Avg. loss: 547.950022\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 884\n",
            "Norm: 3413842.35, NNZs: 927, Bias: -37712.987145, T: 865436, Avg. loss: 547.570938\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 885\n",
            "Norm: 3413842.35, NNZs: 927, Bias: -37712.987885, T: 866415, Avg. loss: 545.743672\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 886\n",
            "Norm: 3413842.35, NNZs: 927, Bias: -37712.988647, T: 867394, Avg. loss: 545.384402\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 887\n",
            "Norm: 3413842.34, NNZs: 927, Bias: -37712.989581, T: 868373, Avg. loss: 546.472293\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 888\n",
            "Norm: 3413842.34, NNZs: 927, Bias: -37712.990507, T: 869352, Avg. loss: 546.099671\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 889\n",
            "Norm: 3413842.34, NNZs: 927, Bias: -37712.991248, T: 870331, Avg. loss: 544.283729\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 890\n",
            "Norm: 3413842.34, NNZs: 927, Bias: -37712.992168, T: 871310, Avg. loss: 545.364821\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 891\n",
            "Norm: 3413842.34, NNZs: 927, Bias: -37712.993091, T: 872289, Avg. loss: 544.993169\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 892\n",
            "Norm: 3413842.34, NNZs: 927, Bias: -37712.993832, T: 873268, Avg. loss: 543.187274\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 893\n",
            "Norm: 3413842.34, NNZs: 927, Bias: -37712.994589, T: 874247, Avg. loss: 542.831109\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 894\n",
            "Norm: 3413842.34, NNZs: 927, Bias: -37712.995344, T: 875226, Avg. loss: 542.478114\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 895\n",
            "Norm: 3413842.33, NNZs: 927, Bias: -37712.996257, T: 876205, Avg. loss: 543.551027\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 896\n",
            "Norm: 3413842.33, NNZs: 927, Bias: -37712.997165, T: 877184, Avg. loss: 543.181032\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 897\n",
            "Norm: 3413842.33, NNZs: 927, Bias: -37712.998064, T: 878163, Avg. loss: 542.811520\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 898\n",
            "Norm: 3413842.33, NNZs: 927, Bias: -37712.998806, T: 879142, Avg. loss: 541.025784\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 899\n",
            "Norm: 3413842.33, NNZs: 927, Bias: -37712.999711, T: 880121, Avg. loss: 542.092498\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 900\n",
            "Norm: 3413842.33, NNZs: 927, Bias: -37713.000610, T: 881100, Avg. loss: 541.725702\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 901\n",
            "Norm: 3413842.33, NNZs: 927, Bias: -37713.001520, T: 882079, Avg. loss: 541.361930\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 902\n",
            "Norm: 3413842.32, NNZs: 927, Bias: -37713.002240, T: 883058, Avg. loss: 539.587098\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 903\n",
            "Norm: 3413842.32, NNZs: 927, Bias: -37713.002981, T: 884037, Avg. loss: 539.237253\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 904\n",
            "Norm: 3413842.32, NNZs: 927, Bias: -37713.003889, T: 885016, Avg. loss: 540.293944\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 905\n",
            "Norm: 3413842.32, NNZs: 927, Bias: -37713.004628, T: 885995, Avg. loss: 538.530247\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 906\n",
            "Norm: 3413842.32, NNZs: 927, Bias: -37713.005513, T: 886974, Avg. loss: 539.580322\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 907\n",
            "Norm: 3413842.32, NNZs: 927, Bias: -37713.006252, T: 887953, Avg. loss: 537.823363\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 908\n",
            "Norm: 3413842.32, NNZs: 927, Bias: -37713.006993, T: 888932, Avg. loss: 537.478933\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 909\n",
            "Norm: 3413842.32, NNZs: 927, Bias: -37713.007874, T: 889911, Avg. loss: 538.524089\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 910\n",
            "Norm: 3413842.31, NNZs: 927, Bias: -37713.008613, T: 890890, Avg. loss: 536.775554\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 911\n",
            "Norm: 3413842.31, NNZs: 927, Bias: -37713.009494, T: 891869, Avg. loss: 537.818973\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 912\n",
            "Norm: 3413842.31, NNZs: 927, Bias: -37713.010205, T: 892848, Avg. loss: 536.075412\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 913\n",
            "Norm: 3413842.31, NNZs: 927, Bias: -37713.010926, T: 893827, Avg. loss: 535.732400\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 914\n",
            "Norm: 3413842.31, NNZs: 927, Bias: -37713.011637, T: 894806, Avg. loss: 535.390673\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 915\n",
            "Norm: 3413842.31, NNZs: 927, Bias: -37713.012354, T: 895785, Avg. loss: 535.048022\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 916\n",
            "Norm: 3413842.31, NNZs: 927, Bias: -37713.013078, T: 896764, Avg. loss: 534.706698\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 917\n",
            "Norm: 3413842.31, NNZs: 927, Bias: -37713.013979, T: 897743, Avg. loss: 535.738733\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 918\n",
            "Norm: 3413842.30, NNZs: 927, Bias: -37713.014872, T: 898722, Avg. loss: 535.384094\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 919\n",
            "Norm: 3413842.30, NNZs: 927, Bias: -37713.015765, T: 899701, Avg. loss: 535.028197\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 920\n",
            "Norm: 3413842.30, NNZs: 927, Bias: -37713.016494, T: 900680, Avg. loss: 533.311581\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 921\n",
            "Norm: 3413842.30, NNZs: 927, Bias: -37713.017219, T: 901659, Avg. loss: 532.973690\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 922\n",
            "Norm: 3413842.30, NNZs: 927, Bias: -37713.017934, T: 902638, Avg. loss: 532.634758\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 923\n",
            "Norm: 3413842.30, NNZs: 927, Bias: -37713.018803, T: 903617, Avg. loss: 533.656473\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 924\n",
            "Norm: 3413842.30, NNZs: 927, Bias: -37713.019521, T: 904596, Avg. loss: 531.949312\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 925\n",
            "Norm: 3413842.30, NNZs: 927, Bias: -37713.020392, T: 905575, Avg. loss: 532.965306\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 926\n",
            "Norm: 3413842.29, NNZs: 927, Bias: -37713.021097, T: 906554, Avg. loss: 531.265413\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 927\n",
            "Norm: 3413842.29, NNZs: 927, Bias: -37713.021970, T: 907533, Avg. loss: 532.280026\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 928\n",
            "Norm: 3413842.29, NNZs: 927, Bias: -37713.022688, T: 908512, Avg. loss: 530.585783\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 929\n",
            "Norm: 3413842.29, NNZs: 927, Bias: -37713.023563, T: 909491, Avg. loss: 531.595091\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 930\n",
            "Norm: 3413842.29, NNZs: 927, Bias: -37713.024429, T: 910470, Avg. loss: 531.247322\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 931\n",
            "Norm: 3413842.29, NNZs: 927, Bias: -37713.025141, T: 911449, Avg. loss: 529.562067\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 932\n",
            "Norm: 3413842.29, NNZs: 927, Bias: -37713.025847, T: 912428, Avg. loss: 529.230485\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 933\n",
            "Norm: 3413842.28, NNZs: 927, Bias: -37713.026701, T: 913407, Avg. loss: 530.232447\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 934\n",
            "Norm: 3413842.28, NNZs: 927, Bias: -37713.027416, T: 914386, Avg. loss: 528.556171\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 935\n",
            "Norm: 3413842.28, NNZs: 927, Bias: -37713.028111, T: 915365, Avg. loss: 528.225669\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 936\n",
            "Norm: 3413842.28, NNZs: 927, Bias: -37713.028804, T: 916344, Avg. loss: 527.896100\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 937\n",
            "Norm: 3413842.28, NNZs: 927, Bias: -37713.029519, T: 917323, Avg. loss: 527.566551\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 938\n",
            "Norm: 3413842.28, NNZs: 927, Bias: -37713.030371, T: 918302, Avg. loss: 528.560128\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 939\n",
            "Norm: 3413842.28, NNZs: 927, Bias: -37713.031088, T: 919281, Avg. loss: 526.898056\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 940\n",
            "Norm: 3413842.28, NNZs: 927, Bias: -37713.031957, T: 920260, Avg. loss: 527.889061\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 941\n",
            "Norm: 3413842.28, NNZs: 927, Bias: -37713.032667, T: 921239, Avg. loss: 526.231346\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 942\n",
            "Norm: 3413842.27, NNZs: 927, Bias: -37713.033380, T: 922218, Avg. loss: 525.904946\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 943\n",
            "Norm: 3413842.27, NNZs: 927, Bias: -37713.034088, T: 923197, Avg. loss: 525.579053\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 944\n",
            "Norm: 3413842.27, NNZs: 927, Bias: -37713.034932, T: 924176, Avg. loss: 526.563504\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 945\n",
            "Norm: 3413842.27, NNZs: 927, Bias: -37713.035626, T: 925155, Avg. loss: 524.916160\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 946\n",
            "Norm: 3413842.27, NNZs: 927, Bias: -37713.036468, T: 926134, Avg. loss: 525.895308\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 947\n",
            "Norm: 3413842.27, NNZs: 927, Bias: -37713.037170, T: 927113, Avg. loss: 524.254819\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 948\n",
            "Norm: 3413842.27, NNZs: 927, Bias: -37713.038027, T: 928092, Avg. loss: 525.233723\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 949\n",
            "Norm: 3413842.27, NNZs: 927, Bias: -37713.038867, T: 929071, Avg. loss: 524.896828\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 950\n",
            "Norm: 3413842.26, NNZs: 927, Bias: -37713.039572, T: 930050, Avg. loss: 523.264069\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 951\n",
            "Norm: 3413842.26, NNZs: 927, Bias: -37713.040418, T: 931029, Avg. loss: 524.236496\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 952\n",
            "Norm: 3413842.26, NNZs: 927, Bias: -37713.041122, T: 932008, Avg. loss: 522.609235\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 953\n",
            "Norm: 3413842.26, NNZs: 927, Bias: -37713.041961, T: 932987, Avg. loss: 523.579344\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 954\n",
            "Norm: 3413842.26, NNZs: 927, Bias: -37713.042652, T: 933966, Avg. loss: 521.957855\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 955\n",
            "Norm: 3413842.26, NNZs: 927, Bias: -37713.043497, T: 934945, Avg. loss: 522.925519\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 956\n",
            "Norm: 3413842.26, NNZs: 927, Bias: -37713.044189, T: 935924, Avg. loss: 521.308091\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 957\n",
            "Norm: 3413842.26, NNZs: 927, Bias: -37713.045016, T: 936903, Avg. loss: 522.270997\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 958\n",
            "Norm: 3413842.25, NNZs: 927, Bias: -37713.045712, T: 937882, Avg. loss: 520.660237\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 959\n",
            "Norm: 3413842.25, NNZs: 927, Bias: -37713.046538, T: 938861, Avg. loss: 521.622202\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 960\n",
            "Norm: 3413842.25, NNZs: 927, Bias: -37713.047370, T: 939840, Avg. loss: 521.290885\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 961\n",
            "Norm: 3413842.25, NNZs: 927, Bias: -37713.048191, T: 940819, Avg. loss: 520.961500\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 962\n",
            "Norm: 3413842.25, NNZs: 927, Bias: -37713.049008, T: 941798, Avg. loss: 520.631147\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 963\n",
            "Norm: 3413842.25, NNZs: 927, Bias: -37713.049834, T: 942777, Avg. loss: 520.303027\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 964\n",
            "Norm: 3413842.25, NNZs: 927, Bias: -37713.050646, T: 943756, Avg. loss: 519.975144\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 965\n",
            "Norm: 3413842.25, NNZs: 927, Bias: -37713.051463, T: 944735, Avg. loss: 519.646677\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 966\n",
            "Norm: 3413842.24, NNZs: 927, Bias: -37713.052147, T: 945714, Avg. loss: 518.058238\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 967\n",
            "Norm: 3413842.24, NNZs: 927, Bias: -37713.052955, T: 946693, Avg. loss: 519.006669\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 968\n",
            "Norm: 3413842.24, NNZs: 927, Bias: -37713.053628, T: 947672, Avg. loss: 517.421761\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 969\n",
            "Norm: 3413842.24, NNZs: 927, Bias: -37713.054311, T: 948651, Avg. loss: 517.110740\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 970\n",
            "Norm: 3413842.24, NNZs: 927, Bias: -37713.055130, T: 949630, Avg. loss: 518.052384\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 971\n",
            "Norm: 3413842.24, NNZs: 927, Bias: -37713.055806, T: 950609, Avg. loss: 516.477182\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 972\n",
            "Norm: 3413842.24, NNZs: 927, Bias: -37713.056486, T: 951588, Avg. loss: 516.166218\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 973\n",
            "Norm: 3413842.24, NNZs: 927, Bias: -37713.057304, T: 952567, Avg. loss: 517.105501\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 974\n",
            "Norm: 3413842.24, NNZs: 927, Bias: -37713.057966, T: 953546, Avg. loss: 515.536493\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 975\n",
            "Norm: 3413842.23, NNZs: 927, Bias: -37713.058644, T: 954525, Avg. loss: 515.227704\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 976\n",
            "Norm: 3413842.23, NNZs: 927, Bias: -37713.059457, T: 955504, Avg. loss: 516.162236\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 977\n",
            "Norm: 3413842.23, NNZs: 927, Bias: -37713.060125, T: 956483, Avg. loss: 514.600206\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 978\n",
            "Norm: 3413842.23, NNZs: 927, Bias: -37713.060778, T: 957462, Avg. loss: 514.292770\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 979\n",
            "Norm: 3413842.23, NNZs: 927, Bias: -37713.061455, T: 958441, Avg. loss: 513.986195\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 980\n",
            "Norm: 3413842.23, NNZs: 927, Bias: -37713.062112, T: 959420, Avg. loss: 513.678878\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 981\n",
            "Norm: 3413842.23, NNZs: 927, Bias: -37713.062780, T: 960399, Avg. loss: 513.372071\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 982\n",
            "Norm: 3413842.23, NNZs: 927, Bias: -37713.063596, T: 961378, Avg. loss: 514.297023\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 983\n",
            "Norm: 3413842.22, NNZs: 927, Bias: -37713.064403, T: 962357, Avg. loss: 513.979236\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 984\n",
            "Norm: 3413842.22, NNZs: 927, Bias: -37713.065061, T: 963336, Avg. loss: 512.436462\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 985\n",
            "Norm: 3413842.22, NNZs: 927, Bias: -37713.065860, T: 964315, Avg. loss: 513.356187\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 986\n",
            "Norm: 3413842.22, NNZs: 927, Bias: -37713.066667, T: 965294, Avg. loss: 513.040247\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 987\n",
            "Norm: 3413842.22, NNZs: 927, Bias: -37713.067473, T: 966273, Avg. loss: 512.724203\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 988\n",
            "Norm: 3413842.22, NNZs: 927, Bias: -37713.068258, T: 967252, Avg. loss: 512.407530\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 989\n",
            "Norm: 3413842.22, NNZs: 927, Bias: -37713.069044, T: 968231, Avg. loss: 512.094907\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 990\n",
            "Norm: 3413842.22, NNZs: 927, Bias: -37713.069832, T: 969210, Avg. loss: 511.779519\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 991\n",
            "Norm: 3413842.22, NNZs: 927, Bias: -37713.070483, T: 970189, Avg. loss: 510.254375\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 992\n",
            "Norm: 3413842.21, NNZs: 927, Bias: -37713.071271, T: 971168, Avg. loss: 511.163448\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 993\n",
            "Norm: 3413842.21, NNZs: 927, Bias: -37713.071931, T: 972147, Avg. loss: 509.642926\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 994\n",
            "Norm: 3413842.21, NNZs: 927, Bias: -37713.072715, T: 973126, Avg. loss: 510.548816\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 995\n",
            "Norm: 3413842.21, NNZs: 927, Bias: -37713.073500, T: 974105, Avg. loss: 510.239014\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 996\n",
            "Norm: 3413842.21, NNZs: 927, Bias: -37713.074277, T: 975084, Avg. loss: 509.928174\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 997\n",
            "Norm: 3413842.21, NNZs: 927, Bias: -37713.074925, T: 976063, Avg. loss: 508.416669\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 998\n",
            "Norm: 3413842.21, NNZs: 927, Bias: -37713.075570, T: 977042, Avg. loss: 508.119081\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 999\n",
            "Norm: 3413842.21, NNZs: 927, Bias: -37713.076217, T: 978021, Avg. loss: 507.821951\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 1000\n",
            "Norm: 3413842.21, NNZs: 927, Bias: -37713.076873, T: 979000, Avg. loss: 507.524206\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 1\n",
            "Norm: 333.92, NNZs: 632, Bias: -2.617568, T: 979, Avg. loss: 26.783182\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 327.87, NNZs: 618, Bias: -2.225194, T: 1958, Avg. loss: 5.916937\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 327.23, NNZs: 600, Bias: -1.956313, T: 2937, Avg. loss: 1.178999\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 326.94, NNZs: 571, Bias: -1.906220, T: 3916, Avg. loss: 0.678317\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 326.81, NNZs: 558, Bias: -1.837442, T: 4895, Avg. loss: 0.499158\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 326.70, NNZs: 555, Bias: -1.850305, T: 5874, Avg. loss: 0.385177\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 326.65, NNZs: 541, Bias: -1.814313, T: 6853, Avg. loss: 0.348838\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 326.61, NNZs: 535, Bias: -1.782840, T: 7832, Avg. loss: 0.319544\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 326.57, NNZs: 529, Bias: -1.772592, T: 8811, Avg. loss: 0.288356\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 326.55, NNZs: 527, Bias: -1.754037, T: 9790, Avg. loss: 0.276287\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 326.53, NNZs: 521, Bias: -1.739835, T: 10769, Avg. loss: 0.263262\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 326.51, NNZs: 517, Bias: -1.730787, T: 11748, Avg. loss: 0.248894\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 326.50, NNZs: 515, Bias: -1.712594, T: 12727, Avg. loss: 0.248098\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 326.49, NNZs: 510, Bias: -1.697949, T: 13706, Avg. loss: 0.239674\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 326.48, NNZs: 508, Bias: -1.687028, T: 14685, Avg. loss: 0.232446\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 326.47, NNZs: 504, Bias: -1.676039, T: 15664, Avg. loss: 0.227249\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 326.47, NNZs: 502, Bias: -1.667340, T: 16643, Avg. loss: 0.221515\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 326.46, NNZs: 500, Bias: -1.659441, T: 17622, Avg. loss: 0.218968\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 326.45, NNZs: 500, Bias: -1.651769, T: 18601, Avg. loss: 0.215652\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 326.45, NNZs: 495, Bias: -1.643467, T: 19580, Avg. loss: 0.214469\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 326.45, NNZs: 495, Bias: -1.636751, T: 20559, Avg. loss: 0.210836\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 326.44, NNZs: 494, Bias: -1.629537, T: 21538, Avg. loss: 0.208656\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 326.44, NNZs: 491, Bias: -1.625410, T: 22517, Avg. loss: 0.203852\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 326.43, NNZs: 491, Bias: -1.617668, T: 23496, Avg. loss: 0.206859\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 326.43, NNZs: 491, Bias: -1.612805, T: 24475, Avg. loss: 0.201534\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 326.43, NNZs: 490, Bias: -1.608378, T: 25454, Avg. loss: 0.200090\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 326.43, NNZs: 488, Bias: -1.601377, T: 26433, Avg. loss: 0.201980\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 326.42, NNZs: 486, Bias: -1.596632, T: 27412, Avg. loss: 0.198680\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 326.42, NNZs: 479, Bias: -1.590927, T: 28391, Avg. loss: 0.199157\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 326.42, NNZs: 477, Bias: -1.586000, T: 29370, Avg. loss: 0.196570\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 326.42, NNZs: 476, Bias: -1.581271, T: 30349, Avg. loss: 0.196014\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 326.42, NNZs: 476, Bias: -1.576018, T: 31328, Avg. loss: 0.195983\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 326.42, NNZs: 474, Bias: -1.571554, T: 32307, Avg. loss: 0.194326\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 326.41, NNZs: 474, Bias: -1.567549, T: 33286, Avg. loss: 0.193017\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 326.41, NNZs: 474, Bias: -1.563344, T: 34265, Avg. loss: 0.193066\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 326.41, NNZs: 473, Bias: -1.559178, T: 35244, Avg. loss: 0.192239\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 326.41, NNZs: 472, Bias: -1.554445, T: 36223, Avg. loss: 0.192805\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 326.41, NNZs: 469, Bias: -1.550339, T: 37202, Avg. loss: 0.191117\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 326.41, NNZs: 470, Bias: -1.546689, T: 38181, Avg. loss: 0.189906\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 326.41, NNZs: 470, Bias: -1.542747, T: 39160, Avg. loss: 0.190570\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 326.41, NNZs: 470, Bias: -1.539518, T: 40139, Avg. loss: 0.188626\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 326.41, NNZs: 468, Bias: -1.535649, T: 41118, Avg. loss: 0.189790\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 326.41, NNZs: 466, Bias: -1.532143, T: 42097, Avg. loss: 0.188472\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 326.40, NNZs: 465, Bias: -1.529012, T: 43076, Avg. loss: 0.187495\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 326.40, NNZs: 465, Bias: -1.525871, T: 44055, Avg. loss: 0.187048\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 326.40, NNZs: 465, Bias: -1.522732, T: 45034, Avg. loss: 0.186820\n",
            "Total training time: 0.06 seconds.\n",
            "Convergence after 46 epochs took 0.06 seconds\n",
            "-- Epoch 1\n",
            "Norm: 1311.71, NNZs: 752, Bias: -1.258647, T: 979, Avg. loss: 535.759889\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1173.97, NNZs: 849, Bias: 1.978550, T: 1958, Avg. loss: 411.041341\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1165.07, NNZs: 847, Bias: -1.256076, T: 2937, Avg. loss: 24.784564\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1162.33, NNZs: 848, Bias: -1.141046, T: 3916, Avg. loss: 8.764421\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1161.09, NNZs: 847, Bias: -0.995245, T: 4895, Avg. loss: 4.806514\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1160.32, NNZs: 843, Bias: -0.938455, T: 5874, Avg. loss: 3.380785\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1159.81, NNZs: 839, Bias: -0.906397, T: 6853, Avg. loss: 2.580357\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1159.43, NNZs: 837, Bias: -0.895152, T: 7832, Avg. loss: 2.138752\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1159.15, NNZs: 836, Bias: -0.901543, T: 8811, Avg. loss: 1.814347\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1158.93, NNZs: 832, Bias: -0.908341, T: 9790, Avg. loss: 1.580798\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1158.75, NNZs: 832, Bias: -0.906469, T: 10769, Avg. loss: 1.413932\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1158.60, NNZs: 827, Bias: -0.885445, T: 11748, Avg. loss: 1.276173\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 1158.48, NNZs: 826, Bias: -0.877211, T: 12727, Avg. loss: 1.168662\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 1158.37, NNZs: 822, Bias: -0.877496, T: 13706, Avg. loss: 1.084259\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 1158.27, NNZs: 821, Bias: -0.888986, T: 14685, Avg. loss: 1.013670\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 1158.19, NNZs: 820, Bias: -0.884730, T: 15664, Avg. loss: 0.948231\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 1158.12, NNZs: 820, Bias: -0.885528, T: 16643, Avg. loss: 0.893256\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 1158.05, NNZs: 818, Bias: -0.885070, T: 17622, Avg. loss: 0.846986\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 1158.00, NNZs: 815, Bias: -0.886682, T: 18601, Avg. loss: 0.807427\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 1157.94, NNZs: 815, Bias: -0.891377, T: 19580, Avg. loss: 0.768988\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 1157.90, NNZs: 813, Bias: -0.889038, T: 20559, Avg. loss: 0.734699\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 1157.85, NNZs: 811, Bias: -0.888651, T: 21538, Avg. loss: 0.706089\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 1157.81, NNZs: 811, Bias: -0.889089, T: 22517, Avg. loss: 0.678006\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 1157.78, NNZs: 805, Bias: -0.890737, T: 23496, Avg. loss: 0.655900\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 1157.74, NNZs: 803, Bias: -0.888871, T: 24475, Avg. loss: 0.631116\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 1157.71, NNZs: 801, Bias: -0.886797, T: 25454, Avg. loss: 0.611170\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 1157.68, NNZs: 801, Bias: -0.888815, T: 26433, Avg. loss: 0.592841\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 1157.65, NNZs: 795, Bias: -0.891664, T: 27412, Avg. loss: 0.576592\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 1157.63, NNZs: 795, Bias: -0.891237, T: 28391, Avg. loss: 0.559316\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 1157.61, NNZs: 792, Bias: -0.891181, T: 29370, Avg. loss: 0.544667\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 1157.58, NNZs: 798, Bias: -0.891271, T: 30349, Avg. loss: 0.531274\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 1157.56, NNZs: 791, Bias: -0.890458, T: 31328, Avg. loss: 0.517581\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 1157.54, NNZs: 788, Bias: -0.891200, T: 32307, Avg. loss: 0.506215\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 1157.52, NNZs: 787, Bias: -0.891371, T: 33286, Avg. loss: 0.494217\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 1157.51, NNZs: 787, Bias: -0.891098, T: 34265, Avg. loss: 0.483772\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 1157.49, NNZs: 784, Bias: -0.891638, T: 35244, Avg. loss: 0.473782\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 1157.47, NNZs: 784, Bias: -0.891461, T: 36223, Avg. loss: 0.462889\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 1157.46, NNZs: 782, Bias: -0.891632, T: 37202, Avg. loss: 0.454651\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 1157.44, NNZs: 782, Bias: -0.893429, T: 38181, Avg. loss: 0.446457\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 1157.43, NNZs: 780, Bias: -0.893759, T: 39160, Avg. loss: 0.437633\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 1157.42, NNZs: 780, Bias: -0.894396, T: 40139, Avg. loss: 0.429648\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 1157.40, NNZs: 780, Bias: -0.894541, T: 41118, Avg. loss: 0.422580\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 1157.39, NNZs: 777, Bias: -0.894680, T: 42097, Avg. loss: 0.415283\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 1157.38, NNZs: 777, Bias: -0.895438, T: 43076, Avg. loss: 0.409193\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 1157.37, NNZs: 777, Bias: -0.895928, T: 44055, Avg. loss: 0.402386\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 1157.36, NNZs: 777, Bias: -0.896667, T: 45034, Avg. loss: 0.396517\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 1157.35, NNZs: 777, Bias: -0.896569, T: 46013, Avg. loss: 0.390239\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 1157.34, NNZs: 774, Bias: -0.896288, T: 46992, Avg. loss: 0.383950\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 1157.33, NNZs: 774, Bias: -0.896689, T: 47971, Avg. loss: 0.378976\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 1157.32, NNZs: 772, Bias: -0.897108, T: 48950, Avg. loss: 0.374043\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 1157.31, NNZs: 772, Bias: -0.897373, T: 49929, Avg. loss: 0.368798\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 1157.30, NNZs: 772, Bias: -0.897349, T: 50908, Avg. loss: 0.363899\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 1157.30, NNZs: 772, Bias: -0.897577, T: 51887, Avg. loss: 0.359498\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 1157.29, NNZs: 772, Bias: -0.897847, T: 52866, Avg. loss: 0.354898\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 1157.28, NNZs: 772, Bias: -0.897963, T: 53845, Avg. loss: 0.350608\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 1157.27, NNZs: 769, Bias: -0.898767, T: 54824, Avg. loss: 0.346372\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 1157.27, NNZs: 768, Bias: -0.898245, T: 55803, Avg. loss: 0.342005\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 1157.26, NNZs: 769, Bias: -0.897848, T: 56782, Avg. loss: 0.337909\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 1157.25, NNZs: 766, Bias: -0.898289, T: 57761, Avg. loss: 0.334770\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 1157.25, NNZs: 767, Bias: -0.898711, T: 58740, Avg. loss: 0.331076\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 1157.24, NNZs: 765, Bias: -0.898588, T: 59719, Avg. loss: 0.327210\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 1157.23, NNZs: 765, Bias: -0.898510, T: 60698, Avg. loss: 0.323662\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 1157.23, NNZs: 764, Bias: -0.898760, T: 61677, Avg. loss: 0.320394\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 1157.22, NNZs: 763, Bias: -0.898841, T: 62656, Avg. loss: 0.317228\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 1157.22, NNZs: 763, Bias: -0.899138, T: 63635, Avg. loss: 0.314255\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 1157.21, NNZs: 761, Bias: -0.899641, T: 64614, Avg. loss: 0.311277\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 1157.21, NNZs: 759, Bias: -0.899723, T: 65593, Avg. loss: 0.308092\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 1157.20, NNZs: 758, Bias: -0.900058, T: 66572, Avg. loss: 0.305120\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 1157.20, NNZs: 758, Bias: -0.900323, T: 67551, Avg. loss: 0.302319\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 1157.19, NNZs: 758, Bias: -0.900244, T: 68530, Avg. loss: 0.299755\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 1157.19, NNZs: 758, Bias: -0.900232, T: 69509, Avg. loss: 0.296930\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 1157.18, NNZs: 758, Bias: -0.900392, T: 70488, Avg. loss: 0.294443\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 1157.18, NNZs: 757, Bias: -0.900570, T: 71467, Avg. loss: 0.291749\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 1157.17, NNZs: 756, Bias: -0.900700, T: 72446, Avg. loss: 0.289371\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 1157.17, NNZs: 755, Bias: -0.900859, T: 73425, Avg. loss: 0.287201\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 1157.16, NNZs: 755, Bias: -0.900966, T: 74404, Avg. loss: 0.284730\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 1157.16, NNZs: 755, Bias: -0.900859, T: 75383, Avg. loss: 0.282161\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 1157.16, NNZs: 755, Bias: -0.900811, T: 76362, Avg. loss: 0.280121\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 1157.15, NNZs: 755, Bias: -0.900911, T: 77341, Avg. loss: 0.278033\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 1157.15, NNZs: 755, Bias: -0.900882, T: 78320, Avg. loss: 0.275907\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 1157.14, NNZs: 755, Bias: -0.900769, T: 79299, Avg. loss: 0.273827\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 1157.14, NNZs: 755, Bias: -0.901036, T: 80278, Avg. loss: 0.271855\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 1157.14, NNZs: 753, Bias: -0.901132, T: 81257, Avg. loss: 0.270031\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 1157.13, NNZs: 752, Bias: -0.901273, T: 82236, Avg. loss: 0.267888\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 1157.13, NNZs: 752, Bias: -0.901362, T: 83215, Avg. loss: 0.265966\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 1157.13, NNZs: 751, Bias: -0.901370, T: 84194, Avg. loss: 0.264173\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 1157.12, NNZs: 750, Bias: -0.901563, T: 85173, Avg. loss: 0.262490\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 1157.12, NNZs: 750, Bias: -0.901628, T: 86152, Avg. loss: 0.260663\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 1157.12, NNZs: 750, Bias: -0.901759, T: 87131, Avg. loss: 0.258987\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 1157.11, NNZs: 750, Bias: -0.901890, T: 88110, Avg. loss: 0.257325\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 1157.11, NNZs: 750, Bias: -0.901996, T: 89089, Avg. loss: 0.255482\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 1157.11, NNZs: 750, Bias: -0.902110, T: 90068, Avg. loss: 0.253906\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 1157.11, NNZs: 750, Bias: -0.902024, T: 91047, Avg. loss: 0.252238\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 1157.10, NNZs: 750, Bias: -0.902120, T: 92026, Avg. loss: 0.250664\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 1157.10, NNZs: 750, Bias: -0.902175, T: 93005, Avg. loss: 0.249159\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 1157.10, NNZs: 750, Bias: -0.902261, T: 93984, Avg. loss: 0.247699\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 1157.10, NNZs: 750, Bias: -0.902170, T: 94963, Avg. loss: 0.246063\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 1157.09, NNZs: 749, Bias: -0.902200, T: 95942, Avg. loss: 0.244897\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 1157.09, NNZs: 749, Bias: -0.902318, T: 96921, Avg. loss: 0.243599\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 1157.09, NNZs: 749, Bias: -0.902415, T: 97900, Avg. loss: 0.242073\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 1157.09, NNZs: 749, Bias: -0.902496, T: 98879, Avg. loss: 0.240722\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 1157.08, NNZs: 748, Bias: -0.902477, T: 99858, Avg. loss: 0.239304\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 1157.08, NNZs: 748, Bias: -0.902435, T: 100837, Avg. loss: 0.237884\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.1s finished\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\"Maximum number of iteration reached before \"\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm: 1157.08, NNZs: 748, Bias: -0.902407, T: 101816, Avg. loss: 0.236594\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 1157.08, NNZs: 748, Bias: -0.902439, T: 102795, Avg. loss: 0.235467\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 1157.07, NNZs: 748, Bias: -0.902479, T: 103774, Avg. loss: 0.234099\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 1157.07, NNZs: 747, Bias: -0.902547, T: 104753, Avg. loss: 0.232823\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 1157.07, NNZs: 747, Bias: -0.902683, T: 105732, Avg. loss: 0.231863\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 1157.07, NNZs: 747, Bias: -0.902868, T: 106711, Avg. loss: 0.230722\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 1157.07, NNZs: 747, Bias: -0.902944, T: 107690, Avg. loss: 0.229458\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 1157.06, NNZs: 747, Bias: -0.902944, T: 108669, Avg. loss: 0.228343\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 1157.06, NNZs: 747, Bias: -0.903024, T: 109648, Avg. loss: 0.227232\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 1157.06, NNZs: 744, Bias: -0.903152, T: 110627, Avg. loss: 0.226044\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 1157.06, NNZs: 744, Bias: -0.903306, T: 111606, Avg. loss: 0.225208\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 1157.06, NNZs: 744, Bias: -0.903358, T: 112585, Avg. loss: 0.223901\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 1157.05, NNZs: 744, Bias: -0.903410, T: 113564, Avg. loss: 0.222863\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 1157.05, NNZs: 744, Bias: -0.903508, T: 114543, Avg. loss: 0.221805\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 1157.05, NNZs: 744, Bias: -0.903640, T: 115522, Avg. loss: 0.220838\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 1157.05, NNZs: 744, Bias: -0.903738, T: 116501, Avg. loss: 0.219791\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 1157.05, NNZs: 743, Bias: -0.903741, T: 117480, Avg. loss: 0.218940\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 1157.04, NNZs: 743, Bias: -0.903777, T: 118459, Avg. loss: 0.217914\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 1157.04, NNZs: 743, Bias: -0.903858, T: 119438, Avg. loss: 0.217066\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 1157.04, NNZs: 743, Bias: -0.903926, T: 120417, Avg. loss: 0.215961\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 1157.04, NNZs: 742, Bias: -0.903962, T: 121396, Avg. loss: 0.215179\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 1157.04, NNZs: 742, Bias: -0.904055, T: 122375, Avg. loss: 0.214316\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 1157.04, NNZs: 741, Bias: -0.904106, T: 123354, Avg. loss: 0.213382\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 1157.03, NNZs: 741, Bias: -0.904172, T: 124333, Avg. loss: 0.212320\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 1157.03, NNZs: 741, Bias: -0.904264, T: 125312, Avg. loss: 0.211656\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 1157.03, NNZs: 740, Bias: -0.904307, T: 126291, Avg. loss: 0.210714\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 1157.03, NNZs: 740, Bias: -0.904351, T: 127270, Avg. loss: 0.209931\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 1157.03, NNZs: 740, Bias: -0.904437, T: 128249, Avg. loss: 0.209119\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 1157.03, NNZs: 740, Bias: -0.904482, T: 129228, Avg. loss: 0.208171\n",
            "Total training time: 0.17 seconds.\n",
            "Convergence after 132 epochs took 0.17 seconds\n",
            "-- Epoch 1\n",
            "Norm: 1421809.95, NNZs: 776, Bias: -3098.407266, T: 979, Avg. loss: 243963046.738914\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1769569.93, NNZs: 923, Bias: -20181.017783, T: 1958, Avg. loss: 1059206301.296573\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1722052.25, NNZs: 942, Bias: -20329.646664, T: 2937, Avg. loss: 294073884.483897\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1712856.83, NNZs: 947, Bias: -21932.496659, T: 3916, Avg. loss: 34540797.143503\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1709767.06, NNZs: 942, Bias: -22992.418800, T: 4895, Avg. loss: 8605753.214057\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1708961.27, NNZs: 941, Bias: -23295.562245, T: 5874, Avg. loss: 2309731.638275\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1708553.03, NNZs: 941, Bias: -23493.899101, T: 6853, Avg. loss: 885791.150253\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1708378.91, NNZs: 941, Bias: -23582.275389, T: 7832, Avg. loss: 399352.198024\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1708286.89, NNZs: 938, Bias: -23630.360021, T: 8811, Avg. loss: 215718.613159\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1708226.02, NNZs: 938, Bias: -23663.714185, T: 9790, Avg. loss: 132999.387318\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1708174.60, NNZs: 938, Bias: -23694.325268, T: 10769, Avg. loss: 88433.091139\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1708148.15, NNZs: 938, Bias: -23709.254508, T: 11748, Avg. loss: 57830.291205\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 1708121.15, NNZs: 938, Bias: -23725.982776, T: 12727, Avg. loss: 44316.933334\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 1708103.67, NNZs: 938, Bias: -23736.581831, T: 13706, Avg. loss: 33670.440301\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 1708093.39, NNZs: 936, Bias: -23742.488741, T: 14685, Avg. loss: 25302.647066\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 1708081.29, NNZs: 936, Bias: -23750.013180, T: 15664, Avg. loss: 22662.531665\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 1708071.97, NNZs: 935, Bias: -23755.771879, T: 16643, Avg. loss: 19086.411924\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 1708064.89, NNZs: 935, Bias: -23760.069999, T: 17622, Avg. loss: 16504.229908\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 1708061.18, NNZs: 935, Bias: -23762.074691, T: 18601, Avg. loss: 13445.133065\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 1708055.16, NNZs: 935, Bias: -23765.826052, T: 19580, Avg. loss: 13372.107323\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 1708050.49, NNZs: 935, Bias: -23768.674274, T: 20559, Avg. loss: 12106.633185\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 1708048.16, NNZs: 935, Bias: -23769.894459, T: 21538, Avg. loss: 10245.427027\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 1708045.57, NNZs: 935, Bias: -23771.363070, T: 22517, Avg. loss: 9765.815081\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 1708041.97, NNZs: 935, Bias: -23773.592067, T: 23496, Avg. loss: 9922.913493\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 1708039.08, NNZs: 935, Bias: -23775.334880, T: 24475, Avg. loss: 9277.608633\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 1708036.61, NNZs: 935, Bias: -23776.806605, T: 25454, Avg. loss: 8731.721171\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 1708034.52, NNZs: 935, Bias: -23778.037209, T: 26433, Avg. loss: 8275.704218\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 1708033.55, NNZs: 934, Bias: -23778.471472, T: 27412, Avg. loss: 7363.423158\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 1708031.67, NNZs: 934, Bias: -23779.580955, T: 28391, Avg. loss: 7709.021113\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 1708030.11, NNZs: 934, Bias: -23780.477487, T: 29370, Avg. loss: 7376.786634\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 1708028.68, NNZs: 934, Bias: -23781.286391, T: 30349, Avg. loss: 7111.669428\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 1708028.05, NNZs: 934, Bias: -23781.541090, T: 31328, Avg. loss: 6417.010189\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 1708027.27, NNZs: 934, Bias: -23781.912433, T: 32307, Avg. loss: 6326.883711\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 1708026.45, NNZs: 934, Bias: -23782.328084, T: 33286, Avg. loss: 6231.880666\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 1708025.67, NNZs: 934, Bias: -23782.713245, T: 34265, Avg. loss: 6119.612234\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 1708024.87, NNZs: 934, Bias: -23783.128015, T: 35244, Avg. loss: 5999.950148\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 1708024.10, NNZs: 934, Bias: -23783.535061, T: 36223, Avg. loss: 5881.470885\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 1708022.99, NNZs: 934, Bias: -23784.185783, T: 37202, Avg. loss: 6062.570687\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 1708022.04, NNZs: 934, Bias: -23784.730341, T: 38181, Avg. loss: 5878.299505\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 1708021.54, NNZs: 934, Bias: -23784.960768, T: 39160, Avg. loss: 5436.166817\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 1708021.04, NNZs: 934, Bias: -23785.192768, T: 40139, Avg. loss: 5364.068814\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 1708020.55, NNZs: 934, Bias: -23785.420788, T: 41118, Avg. loss: 5286.568337\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 1708020.06, NNZs: 934, Bias: -23785.656954, T: 42097, Avg. loss: 5208.591160\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 1708019.60, NNZs: 934, Bias: -23785.874008, T: 43076, Avg. loss: 5130.672519\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 1708018.88, NNZs: 934, Bias: -23786.285521, T: 44055, Avg. loss: 5272.641014\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 1708018.49, NNZs: 934, Bias: -23786.463369, T: 45034, Avg. loss: 4929.731008\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 1708017.86, NNZs: 934, Bias: -23786.818197, T: 46013, Avg. loss: 5082.425273\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 1708017.52, NNZs: 934, Bias: -23786.958554, T: 46992, Avg. loss: 4764.409134\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 1708016.99, NNZs: 933, Bias: -23787.249763, T: 47971, Avg. loss: 4907.000226\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 1708016.49, NNZs: 933, Bias: -23787.522334, T: 48950, Avg. loss: 4811.878838\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 1708016.25, NNZs: 933, Bias: -23787.605706, T: 49929, Avg. loss: 4529.887022\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 1708016.00, NNZs: 933, Bias: -23787.700252, T: 50908, Avg. loss: 4486.128153\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 1708015.54, NNZs: 933, Bias: -23787.949292, T: 51887, Avg. loss: 4619.416200\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 1708015.31, NNZs: 933, Bias: -23788.034286, T: 52866, Avg. loss: 4363.963852\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 1708015.07, NNZs: 933, Bias: -23788.129490, T: 53845, Avg. loss: 4322.276410\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 1708014.84, NNZs: 933, Bias: -23788.220395, T: 54824, Avg. loss: 4280.445170\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 1708014.45, NNZs: 933, Bias: -23788.427263, T: 55803, Avg. loss: 4395.850289\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 1708014.09, NNZs: 933, Bias: -23788.619002, T: 56782, Avg. loss: 4322.937345\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 1708013.77, NNZs: 933, Bias: -23788.781677, T: 57761, Avg. loss: 4259.333952\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 1708013.46, NNZs: 933, Bias: -23788.937966, T: 58740, Avg. loss: 4196.303728\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 1708013.32, NNZs: 933, Bias: -23788.971457, T: 59719, Avg. loss: 3991.962953\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 1708013.16, NNZs: 933, Bias: -23789.024278, T: 60698, Avg. loss: 3961.576421\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 1708013.00, NNZs: 933, Bias: -23789.080150, T: 61677, Avg. loss: 3929.471463\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 1708012.82, NNZs: 933, Bias: -23789.146241, T: 62656, Avg. loss: 3897.896742\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 1708012.65, NNZs: 933, Bias: -23789.211054, T: 63635, Avg. loss: 3865.440338\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 1708012.47, NNZs: 933, Bias: -23789.281580, T: 64614, Avg. loss: 3831.396166\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 1708012.20, NNZs: 933, Bias: -23789.423193, T: 65593, Avg. loss: 3922.571079\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 1708012.04, NNZs: 933, Bias: -23789.484571, T: 66572, Avg. loss: 3749.583740\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 1708011.88, NNZs: 933, Bias: -23789.547475, T: 67551, Avg. loss: 3720.217381\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 1708011.63, NNZs: 933, Bias: -23789.675005, T: 68530, Avg. loss: 3805.766426\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 1708011.40, NNZs: 933, Bias: -23789.789717, T: 69509, Avg. loss: 3758.108094\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 1708011.19, NNZs: 933, Bias: -23789.894940, T: 70488, Avg. loss: 3714.591018\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 1708010.99, NNZs: 933, Bias: -23789.990580, T: 71467, Avg. loss: 3672.536893\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 1708010.79, NNZs: 933, Bias: -23790.083317, T: 72446, Avg. loss: 3634.136373\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 1708010.69, NNZs: 933, Bias: -23790.110889, T: 73425, Avg. loss: 3489.513877\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 1708010.59, NNZs: 933, Bias: -23790.141733, T: 74404, Avg. loss: 3467.493686\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 1708010.41, NNZs: 933, Bias: -23790.227035, T: 75383, Avg. loss: 3544.364795\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 1708010.23, NNZs: 933, Bias: -23790.311460, T: 76362, Avg. loss: 3509.813478\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 1708010.13, NNZs: 933, Bias: -23790.337510, T: 77341, Avg. loss: 3378.094525\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 1708010.04, NNZs: 933, Bias: -23790.367382, T: 78320, Avg. loss: 3356.869962\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 1708009.87, NNZs: 933, Bias: -23790.444016, T: 79299, Avg. loss: 3428.631905\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 1708009.78, NNZs: 933, Bias: -23790.469908, T: 80278, Avg. loss: 3304.055371\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 1708009.63, NNZs: 933, Bias: -23790.543454, T: 81257, Avg. loss: 3373.177091\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 1708009.54, NNZs: 933, Bias: -23790.568167, T: 82236, Avg. loss: 3253.797224\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 1708009.38, NNZs: 933, Bias: -23790.640927, T: 83215, Avg. loss: 3320.251647\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 1708009.30, NNZs: 933, Bias: -23790.663560, T: 84194, Avg. loss: 3205.695400\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 1708009.22, NNZs: 933, Bias: -23790.686998, T: 85173, Avg. loss: 3186.856313\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 1708009.13, NNZs: 933, Bias: -23790.715786, T: 86152, Avg. loss: 3168.036345\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 1708008.99, NNZs: 933, Bias: -23790.782316, T: 87131, Avg. loss: 3228.467890\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 1708008.85, NNZs: 933, Bias: -23790.847992, T: 88110, Avg. loss: 3199.970267\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 1708008.77, NNZs: 933, Bias: -23790.871325, T: 89089, Avg. loss: 3095.742036\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 1708008.70, NNZs: 933, Bias: -23790.893366, T: 90068, Avg. loss: 3077.930784\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 1708008.57, NNZs: 933, Bias: -23790.952818, T: 91047, Avg. loss: 3134.225797\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 1708008.45, NNZs: 933, Bias: -23791.007736, T: 92026, Avg. loss: 3108.575771\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 1708008.38, NNZs: 933, Bias: -23791.025311, T: 93005, Avg. loss: 3011.112244\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 1708008.31, NNZs: 933, Bias: -23791.045808, T: 93984, Avg. loss: 2994.751291\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 1708008.19, NNZs: 933, Bias: -23791.099429, T: 94963, Avg. loss: 3047.805535\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 1708008.13, NNZs: 933, Bias: -23791.117130, T: 95942, Avg. loss: 2955.253763\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 1708008.05, NNZs: 933, Bias: -23791.139457, T: 96921, Avg. loss: 2939.148739\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 1708007.98, NNZs: 933, Bias: -23791.161761, T: 97900, Avg. loss: 2923.132774\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 1708007.87, NNZs: 933, Bias: -23791.212941, T: 98879, Avg. loss: 2972.192659\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 1708007.76, NNZs: 933, Bias: -23791.262840, T: 99858, Avg. loss: 2949.680384\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 1708007.66, NNZs: 933, Bias: -23791.310265, T: 100837, Avg. loss: 2928.693649\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 1708007.60, NNZs: 933, Bias: -23791.324967, T: 101816, Avg. loss: 2845.235501\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 1708007.54, NNZs: 933, Bias: -23791.343290, T: 102795, Avg. loss: 2830.507378\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 1708007.44, NNZs: 933, Bias: -23791.388372, T: 103774, Avg. loss: 2877.047105\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 1708007.35, NNZs: 933, Bias: -23791.429130, T: 104753, Avg. loss: 2856.601424\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 1708007.26, NNZs: 933, Bias: -23791.467628, T: 105732, Avg. loss: 2836.970794\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 1708007.21, NNZs: 933, Bias: -23791.481949, T: 106711, Avg. loss: 2760.184448\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 1708007.15, NNZs: 933, Bias: -23791.495182, T: 107690, Avg. loss: 2747.017890\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 1708007.10, NNZs: 933, Bias: -23791.509288, T: 108669, Avg. loss: 2733.883115\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 1708007.01, NNZs: 933, Bias: -23791.549139, T: 109648, Avg. loss: 2776.006399\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 1708006.96, NNZs: 933, Bias: -23791.564573, T: 110627, Avg. loss: 2703.287762\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 1708006.87, NNZs: 933, Bias: -23791.601282, T: 111606, Avg. loss: 2744.181179\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 1708006.79, NNZs: 933, Bias: -23791.637823, T: 112585, Avg. loss: 2727.180567\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 1708006.71, NNZs: 933, Bias: -23791.671207, T: 113564, Avg. loss: 2710.290129\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 1708006.64, NNZs: 933, Bias: -23791.703079, T: 114543, Avg. loss: 2693.452041\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 1708006.59, NNZs: 933, Bias: -23791.713140, T: 115522, Avg. loss: 2625.723005\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 1708006.52, NNZs: 933, Bias: -23791.745532, T: 116501, Avg. loss: 2664.820881\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 1708006.45, NNZs: 933, Bias: -23791.775783, T: 117480, Avg. loss: 2648.864483\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 1708006.40, NNZs: 933, Bias: -23791.786595, T: 118459, Avg. loss: 2584.109210\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 1708006.33, NNZs: 933, Bias: -23791.817093, T: 119438, Avg. loss: 2622.088791\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 1708006.26, NNZs: 933, Bias: -23791.845980, T: 120417, Avg. loss: 2607.048348\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 1708006.22, NNZs: 933, Bias: -23791.855506, T: 121396, Avg. loss: 2544.939163\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 1708006.16, NNZs: 933, Bias: -23791.883153, T: 122375, Avg. loss: 2581.280141\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 1708006.09, NNZs: 933, Bias: -23791.909474, T: 123354, Avg. loss: 2567.033056\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 1708006.05, NNZs: 933, Bias: -23791.918271, T: 124333, Avg. loss: 2507.099813\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 1708005.99, NNZs: 933, Bias: -23791.943854, T: 125312, Avg. loss: 2541.855703\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 1708005.95, NNZs: 933, Bias: -23791.950707, T: 126291, Avg. loss: 2483.644333\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 1708005.92, NNZs: 933, Bias: -23791.958366, T: 127270, Avg. loss: 2473.695630\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 1708005.85, NNZs: 933, Bias: -23791.985622, T: 128249, Avg. loss: 2507.089812\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 1708005.81, NNZs: 933, Bias: -23791.995966, T: 129228, Avg. loss: 2450.906285\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 1708005.77, NNZs: 933, Bias: -23792.006203, T: 130207, Avg. loss: 2441.164946\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 1708005.71, NNZs: 933, Bias: -23792.032960, T: 131186, Avg. loss: 2473.247442\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 1708005.67, NNZs: 933, Bias: -23792.042096, T: 132165, Avg. loss: 2418.998261\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 1708005.64, NNZs: 933, Bias: -23792.052932, T: 133144, Avg. loss: 2409.344435\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 1708005.58, NNZs: 933, Bias: -23792.078466, T: 134123, Avg. loss: 2440.099066\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 1708005.54, NNZs: 933, Bias: -23792.089487, T: 135102, Avg. loss: 2387.845389\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 1708005.50, NNZs: 933, Bias: -23792.100478, T: 136081, Avg. loss: 2378.524302\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 1708005.44, NNZs: 933, Bias: -23792.125757, T: 137060, Avg. loss: 2408.358307\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 1708005.38, NNZs: 933, Bias: -23792.149634, T: 138039, Avg. loss: 2396.319751\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 1708005.33, NNZs: 933, Bias: -23792.172070, T: 139018, Avg. loss: 2384.672456\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 1708005.28, NNZs: 933, Bias: -23792.194586, T: 139997, Avg. loss: 2372.957668\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 1708005.23, NNZs: 933, Bias: -23792.215466, T: 140976, Avg. loss: 2361.670807\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 1708005.19, NNZs: 933, Bias: -23792.224051, T: 141955, Avg. loss: 2313.232068\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 1708005.14, NNZs: 933, Bias: -23792.245182, T: 142934, Avg. loss: 2341.467610\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 1708005.11, NNZs: 933, Bias: -23792.253790, T: 143913, Avg. loss: 2294.316841\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 1708005.08, NNZs: 933, Bias: -23792.261130, T: 144892, Avg. loss: 2286.034103\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 1708005.04, NNZs: 933, Bias: -23792.270605, T: 145871, Avg. loss: 2277.593256\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 1708005.01, NNZs: 933, Bias: -23792.279932, T: 146850, Avg. loss: 2269.445220\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 1708004.98, NNZs: 933, Bias: -23792.288627, T: 147829, Avg. loss: 2261.134762\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 1708004.94, NNZs: 933, Bias: -23792.298437, T: 148808, Avg. loss: 2252.743553\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 1708004.91, NNZs: 933, Bias: -23792.308466, T: 149787, Avg. loss: 2244.529504\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 1708004.87, NNZs: 933, Bias: -23792.319348, T: 150766, Avg. loss: 2236.362075\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 1708004.82, NNZs: 933, Bias: -23792.340456, T: 151745, Avg. loss: 2261.428716\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 1708004.79, NNZs: 933, Bias: -23792.350514, T: 152724, Avg. loss: 2218.337767\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 1708004.74, NNZs: 933, Bias: -23792.370737, T: 153703, Avg. loss: 2242.830197\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 1708004.71, NNZs: 933, Bias: -23792.380529, T: 154682, Avg. loss: 2200.625275\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 1708004.68, NNZs: 933, Bias: -23792.390709, T: 155661, Avg. loss: 2192.896040\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 1708004.64, NNZs: 933, Bias: -23792.401059, T: 156640, Avg. loss: 2185.186795\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 1708004.61, NNZs: 933, Bias: -23792.410374, T: 157619, Avg. loss: 2177.541619\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 1708004.58, NNZs: 933, Bias: -23792.420591, T: 158598, Avg. loss: 2169.779898\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 1708004.53, NNZs: 933, Bias: -23792.440527, T: 159577, Avg. loss: 2192.704933\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 1708004.49, NNZs: 933, Bias: -23792.459742, T: 160556, Avg. loss: 2183.286163\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 1708004.45, NNZs: 933, Bias: -23792.478522, T: 161535, Avg. loss: 2173.846636\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 1708004.42, NNZs: 933, Bias: -23792.487167, T: 162514, Avg. loss: 2135.124026\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 1708004.37, NNZs: 933, Bias: -23792.504736, T: 163493, Avg. loss: 2157.195311\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 1708004.33, NNZs: 933, Bias: -23792.522469, T: 164472, Avg. loss: 2148.420895\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 1708004.29, NNZs: 933, Bias: -23792.538955, T: 165451, Avg. loss: 2139.705548\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 1708004.25, NNZs: 933, Bias: -23792.555402, T: 166430, Avg. loss: 2131.070135\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 1708004.21, NNZs: 933, Bias: -23792.571138, T: 167409, Avg. loss: 2122.528505\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 1708004.18, NNZs: 933, Bias: -23792.586678, T: 168388, Avg. loss: 2114.190136\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 1708004.14, NNZs: 933, Bias: -23792.601770, T: 169367, Avg. loss: 2105.856661\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 1708004.11, NNZs: 933, Bias: -23792.608430, T: 170346, Avg. loss: 2070.143012\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 1708004.09, NNZs: 933, Bias: -23792.615202, T: 171325, Avg. loss: 2063.706102\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 1708004.05, NNZs: 933, Bias: -23792.629917, T: 172304, Avg. loss: 2084.454022\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 1708004.03, NNZs: 933, Bias: -23792.636253, T: 173283, Avg. loss: 2049.476757\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 1708003.99, NNZs: 933, Bias: -23792.650746, T: 174262, Avg. loss: 2069.816985\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 1708003.95, NNZs: 933, Bias: -23792.664979, T: 175241, Avg. loss: 2061.990218\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 1708003.92, NNZs: 933, Bias: -23792.679018, T: 176220, Avg. loss: 2054.256873\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 1708003.89, NNZs: 933, Bias: -23792.692334, T: 177199, Avg. loss: 2046.759515\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 1708003.86, NNZs: 933, Bias: -23792.698054, T: 178178, Avg. loss: 2013.441181\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 1708003.83, NNZs: 933, Bias: -23792.711343, T: 179157, Avg. loss: 2032.911478\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 1708003.81, NNZs: 933, Bias: -23792.717412, T: 180136, Avg. loss: 2000.297716\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 1708003.78, NNZs: 933, Bias: -23792.723415, T: 181115, Avg. loss: 1994.463957\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 1708003.76, NNZs: 933, Bias: -23792.729898, T: 182094, Avg. loss: 1988.548707\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 1708003.74, NNZs: 933, Bias: -23792.736621, T: 183073, Avg. loss: 1982.712744\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 1708003.71, NNZs: 933, Bias: -23792.743007, T: 184052, Avg. loss: 1976.906705\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 1708003.68, NNZs: 933, Bias: -23792.756909, T: 185031, Avg. loss: 1995.318729\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 1708003.65, NNZs: 933, Bias: -23792.770265, T: 186010, Avg. loss: 1988.283396\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 1708003.62, NNZs: 933, Bias: -23792.776447, T: 186989, Avg. loss: 1957.425640\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 1708003.59, NNZs: 933, Bias: -23792.789298, T: 187968, Avg. loss: 1975.367621\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 1708003.56, NNZs: 933, Bias: -23792.802134, T: 188947, Avg. loss: 1968.557590\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 1708003.53, NNZs: 933, Bias: -23792.814514, T: 189926, Avg. loss: 1961.752256\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 1708003.51, NNZs: 933, Bias: -23792.820344, T: 190905, Avg. loss: 1931.997712\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 1708003.48, NNZs: 933, Bias: -23792.832325, T: 191884, Avg. loss: 1949.452089\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 1708003.45, NNZs: 933, Bias: -23792.844141, T: 192863, Avg. loss: 1942.959559\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 1708003.43, NNZs: 933, Bias: -23792.849696, T: 193842, Avg. loss: 1913.897020\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 1708003.40, NNZs: 933, Bias: -23792.861105, T: 194821, Avg. loss: 1930.882982\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 1708003.37, NNZs: 933, Bias: -23792.872649, T: 195800, Avg. loss: 1924.576411\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 1708003.34, NNZs: 933, Bias: -23792.883639, T: 196779, Avg. loss: 1918.266566\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 1708003.32, NNZs: 933, Bias: -23792.888940, T: 197758, Avg. loss: 1890.129422\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 1708003.29, NNZs: 933, Bias: -23792.899813, T: 198737, Avg. loss: 1906.689469\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 1708003.27, NNZs: 933, Bias: -23792.904632, T: 199716, Avg. loss: 1879.119186\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 1708003.25, NNZs: 933, Bias: -23792.910099, T: 200695, Avg. loss: 1874.042142\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 1708003.23, NNZs: 933, Bias: -23792.920937, T: 201674, Avg. loss: 1890.158407\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 1708003.21, NNZs: 933, Bias: -23792.925960, T: 202653, Avg. loss: 1863.154305\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 1708003.18, NNZs: 933, Bias: -23792.936654, T: 203632, Avg. loss: 1878.969666\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 1708003.15, NNZs: 933, Bias: -23792.947418, T: 204611, Avg. loss: 1873.053100\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 1708003.13, NNZs: 933, Bias: -23792.957662, T: 205590, Avg. loss: 1867.211663\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 1708003.11, NNZs: 933, Bias: -23792.962340, T: 206569, Avg. loss: 1840.947011\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 1708003.09, NNZs: 933, Bias: -23792.967293, T: 207548, Avg. loss: 1836.144514\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 1708003.06, NNZs: 933, Bias: -23792.977807, T: 208527, Avg. loss: 1851.352297\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 214\n",
            "Norm: 1708003.04, NNZs: 933, Bias: -23792.987861, T: 209506, Avg. loss: 1845.676565\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 215\n",
            "Norm: 1708003.01, NNZs: 933, Bias: -23792.997983, T: 210485, Avg. loss: 1840.066296\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 216\n",
            "Norm: 1708002.99, NNZs: 933, Bias: -23793.002677, T: 211464, Avg. loss: 1814.901417\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 217\n",
            "Norm: 1708002.97, NNZs: 933, Bias: -23793.007583, T: 212443, Avg. loss: 1810.291691\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 218\n",
            "Norm: 1708002.96, NNZs: 933, Bias: -23793.012784, T: 213422, Avg. loss: 1805.698055\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 219\n",
            "Norm: 1708002.93, NNZs: 933, Bias: -23793.022708, T: 214401, Avg. loss: 1820.329772\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 220\n",
            "Norm: 1708002.91, NNZs: 933, Bias: -23793.027231, T: 215380, Avg. loss: 1795.803456\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 221\n",
            "Norm: 1708002.89, NNZs: 933, Bias: -23793.037028, T: 216359, Avg. loss: 1810.115639\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 222\n",
            "Norm: 1708002.86, NNZs: 933, Bias: -23793.046601, T: 217338, Avg. loss: 1804.820058\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 223\n",
            "Norm: 1708002.84, NNZs: 933, Bias: -23793.056026, T: 218317, Avg. loss: 1799.469119\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 224\n",
            "Norm: 1708002.82, NNZs: 933, Bias: -23793.060328, T: 219296, Avg. loss: 1775.744612\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 225\n",
            "Norm: 1708002.80, NNZs: 933, Bias: -23793.069802, T: 220275, Avg. loss: 1789.693081\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 226\n",
            "Norm: 1708002.78, NNZs: 933, Bias: -23793.079202, T: 221254, Avg. loss: 1784.565522\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 227\n",
            "Norm: 1708002.75, NNZs: 933, Bias: -23793.088189, T: 222233, Avg. loss: 1779.516909\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 228\n",
            "Norm: 1708002.74, NNZs: 933, Bias: -23793.092195, T: 223212, Avg. loss: 1756.394095\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 229\n",
            "Norm: 1708002.71, NNZs: 933, Bias: -23793.101143, T: 224191, Avg. loss: 1770.000466\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 230\n",
            "Norm: 1708002.70, NNZs: 933, Bias: -23793.105853, T: 225170, Avg. loss: 1747.279526\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 231\n",
            "Norm: 1708002.68, NNZs: 933, Bias: -23793.110058, T: 226149, Avg. loss: 1743.099125\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 232\n",
            "Norm: 1708002.66, NNZs: 933, Bias: -23793.118935, T: 227128, Avg. loss: 1756.455977\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 233\n",
            "Norm: 1708002.63, NNZs: 933, Bias: -23793.127636, T: 228107, Avg. loss: 1751.525230\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 234\n",
            "Norm: 1708002.61, NNZs: 933, Bias: -23793.136197, T: 229086, Avg. loss: 1746.569259\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 235\n",
            "Norm: 1708002.60, NNZs: 933, Bias: -23793.140650, T: 230065, Avg. loss: 1724.635613\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 236\n",
            "Norm: 1708002.58, NNZs: 933, Bias: -23793.145226, T: 231044, Avg. loss: 1720.563023\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 237\n",
            "Norm: 1708002.56, NNZs: 933, Bias: -23793.149905, T: 232023, Avg. loss: 1716.584852\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 238\n",
            "Norm: 1708002.55, NNZs: 933, Bias: -23793.154155, T: 233002, Avg. loss: 1712.573789\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 239\n",
            "Norm: 1708002.53, NNZs: 933, Bias: -23793.158768, T: 233981, Avg. loss: 1708.517050\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 240\n",
            "Norm: 1708002.52, NNZs: 933, Bias: -23793.163115, T: 234960, Avg. loss: 1704.527091\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 241\n",
            "Norm: 1708002.50, NNZs: 933, Bias: -23793.167994, T: 235939, Avg. loss: 1700.534678\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 242\n",
            "Norm: 1708002.48, NNZs: 933, Bias: -23793.173020, T: 236918, Avg. loss: 1696.550506\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 243\n",
            "Norm: 1708002.47, NNZs: 933, Bias: -23793.178107, T: 237897, Avg. loss: 1692.634353\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 244\n",
            "Norm: 1708002.44, NNZs: 933, Bias: -23793.186786, T: 238876, Avg. loss: 1704.805515\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 245\n",
            "Norm: 1708002.43, NNZs: 933, Bias: -23793.191487, T: 239855, Avg. loss: 1684.219560\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 246\n",
            "Norm: 1708002.41, NNZs: 933, Bias: -23793.200065, T: 240834, Avg. loss: 1696.233985\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 247\n",
            "Norm: 1708002.39, NNZs: 933, Bias: -23793.205004, T: 241813, Avg. loss: 1675.904247\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 248\n",
            "Norm: 1708002.37, NNZs: 933, Bias: -23793.213443, T: 242792, Avg. loss: 1687.778552\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 249\n",
            "Norm: 1708002.35, NNZs: 933, Bias: -23793.221890, T: 243771, Avg. loss: 1683.365963\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 250\n",
            "Norm: 1708002.33, NNZs: 933, Bias: -23793.230080, T: 244750, Avg. loss: 1678.980983\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 251\n",
            "Norm: 1708002.31, NNZs: 933, Bias: -23793.234590, T: 245729, Avg. loss: 1659.170442\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 252\n",
            "Norm: 1708002.30, NNZs: 933, Bias: -23793.239185, T: 246708, Avg. loss: 1655.453269\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 253\n",
            "Norm: 1708002.28, NNZs: 933, Bias: -23793.247260, T: 247687, Avg. loss: 1666.940140\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 254\n",
            "Norm: 1708002.26, NNZs: 933, Bias: -23793.255093, T: 248666, Avg. loss: 1662.647372\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 255\n",
            "Norm: 1708002.24, NNZs: 933, Bias: -23793.259160, T: 249645, Avg. loss: 1643.395822\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 256\n",
            "Norm: 1708002.23, NNZs: 933, Bias: -23793.263359, T: 250624, Avg. loss: 1639.726537\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 257\n",
            "Norm: 1708002.21, NNZs: 933, Bias: -23793.271119, T: 251603, Avg. loss: 1650.975006\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 258\n",
            "Norm: 1708002.20, NNZs: 933, Bias: -23793.275464, T: 252582, Avg. loss: 1632.021762\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 259\n",
            "Norm: 1708002.18, NNZs: 933, Bias: -23793.283126, T: 253561, Avg. loss: 1643.120738\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 260\n",
            "Norm: 1708002.16, NNZs: 933, Bias: -23793.290727, T: 254540, Avg. loss: 1639.023054\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 261\n",
            "Norm: 1708002.14, NNZs: 933, Bias: -23793.298401, T: 255519, Avg. loss: 1634.914014\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 262\n",
            "Norm: 1708002.12, NNZs: 933, Bias: -23793.305747, T: 256498, Avg. loss: 1630.901293\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 263\n",
            "Norm: 1708002.10, NNZs: 933, Bias: -23793.313108, T: 257477, Avg. loss: 1626.900831\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 264\n",
            "Norm: 1708002.09, NNZs: 933, Bias: -23793.317139, T: 258456, Avg. loss: 1608.650766\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 265\n",
            "Norm: 1708002.07, NNZs: 933, Bias: -23793.321243, T: 259435, Avg. loss: 1605.238333\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 266\n",
            "Norm: 1708002.06, NNZs: 933, Bias: -23793.325464, T: 260414, Avg. loss: 1601.828217\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 267\n",
            "Norm: 1708002.04, NNZs: 933, Bias: -23793.329346, T: 261393, Avg. loss: 1598.471878\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 268\n",
            "Norm: 1708002.03, NNZs: 933, Bias: -23793.333618, T: 262372, Avg. loss: 1595.066708\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 269\n",
            "Norm: 1708002.01, NNZs: 933, Bias: -23793.340788, T: 263351, Avg. loss: 1605.501838\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 270\n",
            "Norm: 1708002.00, NNZs: 933, Bias: -23793.344881, T: 264330, Avg. loss: 1587.920825\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 271\n",
            "Norm: 1708001.99, NNZs: 933, Bias: -23793.348896, T: 265309, Avg. loss: 1584.603737\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 272\n",
            "Norm: 1708001.97, NNZs: 933, Bias: -23793.356126, T: 266288, Avg. loss: 1594.869474\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 273\n",
            "Norm: 1708001.95, NNZs: 933, Bias: -23793.363208, T: 267267, Avg. loss: 1591.059697\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 274\n",
            "Norm: 1708001.94, NNZs: 933, Bias: -23793.367396, T: 268246, Avg. loss: 1573.876423\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 275\n",
            "Norm: 1708001.92, NNZs: 933, Bias: -23793.371295, T: 269225, Avg. loss: 1570.637966\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 276\n",
            "Norm: 1708001.91, NNZs: 933, Bias: -23793.378218, T: 270204, Avg. loss: 1580.643156\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 277\n",
            "Norm: 1708001.89, NNZs: 933, Bias: -23793.382038, T: 271183, Avg. loss: 1563.734631\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 278\n",
            "Norm: 1708001.87, NNZs: 933, Bias: -23793.389018, T: 272162, Avg. loss: 1573.646815\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 279\n",
            "Norm: 1708001.86, NNZs: 933, Bias: -23793.393146, T: 273141, Avg. loss: 1556.965649\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 280\n",
            "Norm: 1708001.85, NNZs: 933, Bias: -23793.396852, T: 274120, Avg. loss: 1553.834954\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 281\n",
            "Norm: 1708001.84, NNZs: 933, Bias: -23793.400870, T: 275099, Avg. loss: 1550.647964\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 282\n",
            "Norm: 1708001.82, NNZs: 933, Bias: -23793.407729, T: 276078, Avg. loss: 1560.312927\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 283\n",
            "Norm: 1708001.81, NNZs: 933, Bias: -23793.411849, T: 277057, Avg. loss: 1543.993646\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 284\n",
            "Norm: 1708001.79, NNZs: 933, Bias: -23793.418574, T: 278036, Avg. loss: 1553.573716\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 285\n",
            "Norm: 1708001.78, NNZs: 933, Bias: -23793.422543, T: 279015, Avg. loss: 1537.462162\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 286\n",
            "Norm: 1708001.76, NNZs: 933, Bias: -23793.426642, T: 279994, Avg. loss: 1534.373899\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 287\n",
            "Norm: 1708001.75, NNZs: 933, Bias: -23793.430703, T: 280973, Avg. loss: 1531.342953\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 288\n",
            "Norm: 1708001.73, NNZs: 933, Bias: -23793.437483, T: 281952, Avg. loss: 1540.647054\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 289\n",
            "Norm: 1708001.72, NNZs: 933, Bias: -23793.441195, T: 282931, Avg. loss: 1524.889989\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 290\n",
            "Norm: 1708001.70, NNZs: 933, Bias: -23793.447798, T: 283910, Avg. loss: 1534.117914\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 291\n",
            "Norm: 1708001.69, NNZs: 933, Bias: -23793.454250, T: 284889, Avg. loss: 1530.705868\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 292\n",
            "Norm: 1708001.67, NNZs: 933, Bias: -23793.460529, T: 285868, Avg. loss: 1527.330974\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 293\n",
            "Norm: 1708001.66, NNZs: 933, Bias: -23793.466667, T: 286847, Avg. loss: 1523.945558\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 294\n",
            "Norm: 1708001.64, NNZs: 933, Bias: -23793.470349, T: 287826, Avg. loss: 1508.602299\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 295\n",
            "Norm: 1708001.63, NNZs: 933, Bias: -23793.474171, T: 288805, Avg. loss: 1505.691910\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 296\n",
            "Norm: 1708001.62, NNZs: 933, Bias: -23793.480318, T: 289784, Avg. loss: 1514.651930\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 297\n",
            "Norm: 1708001.60, NNZs: 933, Bias: -23793.486389, T: 290763, Avg. loss: 1511.348137\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 298\n",
            "Norm: 1708001.59, NNZs: 933, Bias: -23793.492405, T: 291742, Avg. loss: 1508.102951\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 299\n",
            "Norm: 1708001.57, NNZs: 933, Bias: -23793.498302, T: 292721, Avg. loss: 1504.868384\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 300\n",
            "Norm: 1708001.56, NNZs: 933, Bias: -23793.501618, T: 293700, Avg. loss: 1490.046791\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 301\n",
            "Norm: 1708001.54, NNZs: 933, Bias: -23793.507556, T: 294679, Avg. loss: 1498.772851\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 302\n",
            "Norm: 1708001.53, NNZs: 933, Bias: -23793.511100, T: 295658, Avg. loss: 1484.105855\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 303\n",
            "Norm: 1708001.52, NNZs: 933, Bias: -23793.514671, T: 296637, Avg. loss: 1481.324543\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 304\n",
            "Norm: 1708001.51, NNZs: 933, Bias: -23793.520435, T: 297616, Avg. loss: 1489.900154\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 305\n",
            "Norm: 1708001.50, NNZs: 933, Bias: -23793.523868, T: 298595, Avg. loss: 1475.468056\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 306\n",
            "Norm: 1708001.48, NNZs: 933, Bias: -23793.527435, T: 299574, Avg. loss: 1472.731173\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 307\n",
            "Norm: 1708001.47, NNZs: 933, Bias: -23793.533346, T: 300553, Avg. loss: 1481.200146\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 308\n",
            "Norm: 1708001.45, NNZs: 933, Bias: -23793.539025, T: 301532, Avg. loss: 1478.138083\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 309\n",
            "Norm: 1708001.44, NNZs: 933, Bias: -23793.542226, T: 302511, Avg. loss: 1463.970789\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 310\n",
            "Norm: 1708001.43, NNZs: 933, Bias: -23793.547809, T: 303490, Avg. loss: 1472.283905\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 311\n",
            "Norm: 1708001.42, NNZs: 933, Bias: -23793.553308, T: 304469, Avg. loss: 1469.280709\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 312\n",
            "Norm: 1708001.40, NNZs: 933, Bias: -23793.556587, T: 305448, Avg. loss: 1455.312346\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 313\n",
            "Norm: 1708001.39, NNZs: 933, Bias: -23793.562019, T: 306427, Avg. loss: 1463.497763\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 314\n",
            "Norm: 1708001.38, NNZs: 933, Bias: -23793.565410, T: 307406, Avg. loss: 1449.739470\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 315\n",
            "Norm: 1708001.37, NNZs: 933, Bias: -23793.570943, T: 308385, Avg. loss: 1457.875537\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 316\n",
            "Norm: 1708001.35, NNZs: 933, Bias: -23793.576328, T: 309364, Avg. loss: 1454.919943\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 317\n",
            "Norm: 1708001.34, NNZs: 933, Bias: -23793.581628, T: 310343, Avg. loss: 1451.982708\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 318\n",
            "Norm: 1708001.33, NNZs: 933, Bias: -23793.584657, T: 311322, Avg. loss: 1438.495878\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 319\n",
            "Norm: 1708001.31, NNZs: 933, Bias: -23793.589919, T: 312301, Avg. loss: 1446.483927\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 320\n",
            "Norm: 1708001.30, NNZs: 933, Bias: -23793.593159, T: 313280, Avg. loss: 1433.123780\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 321\n",
            "Norm: 1708001.29, NNZs: 933, Bias: -23793.598458, T: 314259, Avg. loss: 1441.030232\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 322\n",
            "Norm: 1708001.28, NNZs: 933, Bias: -23793.601647, T: 315238, Avg. loss: 1427.776236\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 323\n",
            "Norm: 1708001.27, NNZs: 933, Bias: -23793.604688, T: 316217, Avg. loss: 1425.274679\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 324\n",
            "Norm: 1708001.26, NNZs: 933, Bias: -23793.607989, T: 317196, Avg. loss: 1422.769520\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 325\n",
            "Norm: 1708001.25, NNZs: 933, Bias: -23793.611308, T: 318175, Avg. loss: 1420.270192\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 326\n",
            "Norm: 1708001.24, NNZs: 933, Bias: -23793.614353, T: 319154, Avg. loss: 1417.797535\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 327\n",
            "Norm: 1708001.23, NNZs: 933, Bias: -23793.617547, T: 320133, Avg. loss: 1415.305817\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 328\n",
            "Norm: 1708001.21, NNZs: 933, Bias: -23793.622724, T: 321112, Avg. loss: 1422.870422\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 329\n",
            "Norm: 1708001.20, NNZs: 933, Bias: -23793.627806, T: 322091, Avg. loss: 1420.106815\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 330\n",
            "Norm: 1708001.19, NNZs: 933, Bias: -23793.632847, T: 323070, Avg. loss: 1417.342449\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 331\n",
            "Norm: 1708001.18, NNZs: 933, Bias: -23793.636090, T: 324049, Avg. loss: 1404.697179\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 332\n",
            "Norm: 1708001.17, NNZs: 933, Bias: -23793.639142, T: 325028, Avg. loss: 1402.278955\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 333\n",
            "Norm: 1708001.16, NNZs: 933, Bias: -23793.642209, T: 326007, Avg. loss: 1399.870241\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 334\n",
            "Norm: 1708001.15, NNZs: 933, Bias: -23793.645394, T: 326986, Avg. loss: 1397.458739\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 335\n",
            "Norm: 1708001.14, NNZs: 933, Bias: -23793.648564, T: 327965, Avg. loss: 1395.062080\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 336\n",
            "Norm: 1708001.13, NNZs: 933, Bias: -23793.651710, T: 328944, Avg. loss: 1392.672153\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 337\n",
            "Norm: 1708001.12, NNZs: 933, Bias: -23793.656773, T: 329923, Avg. loss: 1399.956064\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 338\n",
            "Norm: 1708001.10, NNZs: 933, Bias: -23793.661811, T: 330902, Avg. loss: 1397.284973\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 339\n",
            "Norm: 1708001.09, NNZs: 933, Bias: -23793.664946, T: 331881, Avg. loss: 1385.080200\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 340\n",
            "Norm: 1708001.08, NNZs: 933, Bias: -23793.668171, T: 332860, Avg. loss: 1382.756029\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 341\n",
            "Norm: 1708001.07, NNZs: 933, Bias: -23793.673119, T: 333839, Avg. loss: 1389.863747\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 342\n",
            "Norm: 1708001.06, NNZs: 933, Bias: -23793.676262, T: 334818, Avg. loss: 1377.864983\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 343\n",
            "Norm: 1708001.05, NNZs: 933, Bias: -23793.681151, T: 335797, Avg. loss: 1384.930284\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 344\n",
            "Norm: 1708001.04, NNZs: 933, Bias: -23793.686013, T: 336776, Avg. loss: 1382.362794\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 345\n",
            "Norm: 1708001.02, NNZs: 933, Bias: -23793.690768, T: 337755, Avg. loss: 1379.811919\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 346\n",
            "Norm: 1708001.01, NNZs: 933, Bias: -23793.693743, T: 338734, Avg. loss: 1368.013242\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 347\n",
            "Norm: 1708001.00, NNZs: 933, Bias: -23793.698466, T: 339713, Avg. loss: 1374.983789\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 348\n",
            "Norm: 1708000.99, NNZs: 933, Bias: -23793.703166, T: 340692, Avg. loss: 1372.461514\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 349\n",
            "Norm: 1708000.98, NNZs: 933, Bias: -23793.707939, T: 341671, Avg. loss: 1369.977700\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 350\n",
            "Norm: 1708000.97, NNZs: 933, Bias: -23793.710933, T: 342650, Avg. loss: 1358.410619\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 351\n",
            "Norm: 1708000.96, NNZs: 933, Bias: -23793.713795, T: 343629, Avg. loss: 1356.202137\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 352\n",
            "Norm: 1708000.95, NNZs: 933, Bias: -23793.716802, T: 344608, Avg. loss: 1354.001989\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 353\n",
            "Norm: 1708000.94, NNZs: 933, Bias: -23793.719743, T: 345587, Avg. loss: 1351.797810\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 354\n",
            "Norm: 1708000.93, NNZs: 933, Bias: -23793.724337, T: 346566, Avg. loss: 1358.529731\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 355\n",
            "Norm: 1708000.92, NNZs: 933, Bias: -23793.727335, T: 347545, Avg. loss: 1347.202153\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 356\n",
            "Norm: 1708000.91, NNZs: 933, Bias: -23793.730355, T: 348524, Avg. loss: 1345.012016\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 357\n",
            "Norm: 1708000.90, NNZs: 933, Bias: -23793.733373, T: 349503, Avg. loss: 1342.869873\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 358\n",
            "Norm: 1708000.89, NNZs: 933, Bias: -23793.736296, T: 350482, Avg. loss: 1340.718190\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 359\n",
            "Norm: 1708000.88, NNZs: 933, Bias: -23793.740921, T: 351461, Avg. loss: 1347.270755\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 360\n",
            "Norm: 1708000.87, NNZs: 933, Bias: -23793.745451, T: 352440, Avg. loss: 1344.902729\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 361\n",
            "Norm: 1708000.86, NNZs: 933, Bias: -23793.748389, T: 353419, Avg. loss: 1333.882996\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 362\n",
            "Norm: 1708000.85, NNZs: 933, Bias: -23793.752805, T: 354398, Avg. loss: 1340.357766\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 363\n",
            "Norm: 1708000.84, NNZs: 933, Bias: -23793.755760, T: 355377, Avg. loss: 1329.432711\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 364\n",
            "Norm: 1708000.83, NNZs: 933, Bias: -23793.760130, T: 356356, Avg. loss: 1335.884744\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 365\n",
            "Norm: 1708000.82, NNZs: 933, Bias: -23793.764471, T: 357335, Avg. loss: 1333.530794\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 366\n",
            "Norm: 1708000.81, NNZs: 933, Bias: -23793.767360, T: 358314, Avg. loss: 1322.764486\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 367\n",
            "Norm: 1708000.80, NNZs: 933, Bias: -23793.770175, T: 359293, Avg. loss: 1320.697401\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 368\n",
            "Norm: 1708000.79, NNZs: 933, Bias: -23793.774519, T: 360272, Avg. loss: 1327.038666\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 369\n",
            "Norm: 1708000.78, NNZs: 933, Bias: -23793.778858, T: 361251, Avg. loss: 1324.749035\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 370\n",
            "Norm: 1708000.77, NNZs: 933, Bias: -23793.783066, T: 362230, Avg. loss: 1322.473145\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 371\n",
            "Norm: 1708000.76, NNZs: 933, Bias: -23793.787326, T: 363209, Avg. loss: 1320.221695\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 372\n",
            "Norm: 1708000.75, NNZs: 933, Bias: -23793.790086, T: 364188, Avg. loss: 1309.721817\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 373\n",
            "Norm: 1708000.74, NNZs: 933, Bias: -23793.794221, T: 365167, Avg. loss: 1315.934523\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 374\n",
            "Norm: 1708000.73, NNZs: 933, Bias: -23793.796850, T: 366146, Avg. loss: 1305.526302\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 375\n",
            "Norm: 1708000.72, NNZs: 933, Bias: -23793.801010, T: 367125, Avg. loss: 1311.672323\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 376\n",
            "Norm: 1708000.71, NNZs: 933, Bias: -23793.805071, T: 368104, Avg. loss: 1309.455879\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 377\n",
            "Norm: 1708000.70, NNZs: 933, Bias: -23793.807772, T: 369083, Avg. loss: 1299.195638\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 378\n",
            "Norm: 1708000.69, NNZs: 933, Bias: -23793.810402, T: 370062, Avg. loss: 1297.224296\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 379\n",
            "Norm: 1708000.68, NNZs: 933, Bias: -23793.814557, T: 371041, Avg. loss: 1303.291683\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 380\n",
            "Norm: 1708000.67, NNZs: 933, Bias: -23793.818520, T: 372020, Avg. loss: 1301.143202\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 381\n",
            "Norm: 1708000.66, NNZs: 933, Bias: -23793.822472, T: 372999, Avg. loss: 1298.973674\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 382\n",
            "Norm: 1708000.65, NNZs: 933, Bias: -23793.825053, T: 373978, Avg. loss: 1288.909541\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 383\n",
            "Norm: 1708000.64, NNZs: 933, Bias: -23793.827622, T: 374957, Avg. loss: 1286.994127\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 384\n",
            "Norm: 1708000.63, NNZs: 933, Bias: -23793.831505, T: 375936, Avg. loss: 1292.917749\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 385\n",
            "Norm: 1708000.62, NNZs: 933, Bias: -23793.835375, T: 376915, Avg. loss: 1290.792031\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 386\n",
            "Norm: 1708000.62, NNZs: 933, Bias: -23793.837940, T: 377894, Avg. loss: 1280.908533\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 387\n",
            "Norm: 1708000.61, NNZs: 933, Bias: -23793.840560, T: 378873, Avg. loss: 1279.006427\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 388\n",
            "Norm: 1708000.60, NNZs: 933, Bias: -23793.843189, T: 379852, Avg. loss: 1277.115732\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 389\n",
            "Norm: 1708000.59, NNZs: 933, Bias: -23793.847190, T: 380831, Avg. loss: 1282.907441\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 390\n",
            "Norm: 1708000.58, NNZs: 933, Bias: -23793.851033, T: 381810, Avg. loss: 1280.827295\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 391\n",
            "Norm: 1708000.57, NNZs: 933, Bias: -23793.855004, T: 382789, Avg. loss: 1278.765707\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 392\n",
            "Norm: 1708000.56, NNZs: 933, Bias: -23793.858749, T: 383768, Avg. loss: 1276.720889\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 393\n",
            "Norm: 1708000.55, NNZs: 933, Bias: -23793.862461, T: 384747, Avg. loss: 1274.684382\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 394\n",
            "Norm: 1708000.54, NNZs: 933, Bias: -23793.864885, T: 385726, Avg. loss: 1265.098643\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 395\n",
            "Norm: 1708000.53, NNZs: 933, Bias: -23793.867403, T: 386705, Avg. loss: 1263.274845\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 396\n",
            "Norm: 1708000.53, NNZs: 933, Bias: -23793.871123, T: 387684, Avg. loss: 1268.927028\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 397\n",
            "Norm: 1708000.52, NNZs: 933, Bias: -23793.874805, T: 388663, Avg. loss: 1266.895118\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 398\n",
            "Norm: 1708000.51, NNZs: 933, Bias: -23793.877274, T: 389642, Avg. loss: 1257.495289\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 399\n",
            "Norm: 1708000.50, NNZs: 933, Bias: -23793.880983, T: 390621, Avg. loss: 1263.077945\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 400\n",
            "Norm: 1708000.49, NNZs: 933, Bias: -23793.883290, T: 391600, Avg. loss: 1253.745423\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 401\n",
            "Norm: 1708000.48, NNZs: 933, Bias: -23793.886944, T: 392579, Avg. loss: 1259.253284\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 402\n",
            "Norm: 1708000.47, NNZs: 933, Bias: -23793.890610, T: 393558, Avg. loss: 1257.298541\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 403\n",
            "Norm: 1708000.46, NNZs: 933, Bias: -23793.893013, T: 394537, Avg. loss: 1248.078895\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 404\n",
            "Norm: 1708000.46, NNZs: 933, Bias: -23793.895369, T: 395516, Avg. loss: 1246.315999\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 405\n",
            "Norm: 1708000.45, NNZs: 933, Bias: -23793.899000, T: 396495, Avg. loss: 1251.764204\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 406\n",
            "Norm: 1708000.44, NNZs: 933, Bias: -23793.902528, T: 397474, Avg. loss: 1249.837300\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 407\n",
            "Norm: 1708000.43, NNZs: 933, Bias: -23793.906033, T: 398453, Avg. loss: 1247.902449\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 408\n",
            "Norm: 1708000.42, NNZs: 933, Bias: -23793.908285, T: 399432, Avg. loss: 1238.854576\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 409\n",
            "Norm: 1708000.41, NNZs: 933, Bias: -23793.911785, T: 400411, Avg. loss: 1244.218825\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 410\n",
            "Norm: 1708000.40, NNZs: 933, Bias: -23793.915318, T: 401390, Avg. loss: 1242.301276\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 411\n",
            "Norm: 1708000.40, NNZs: 933, Bias: -23793.917630, T: 402369, Avg. loss: 1233.375016\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 412\n",
            "Norm: 1708000.39, NNZs: 933, Bias: -23793.921115, T: 403348, Avg. loss: 1238.670909\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 413\n",
            "Norm: 1708000.38, NNZs: 933, Bias: -23793.923335, T: 404327, Avg. loss: 1229.807431\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 414\n",
            "Norm: 1708000.37, NNZs: 933, Bias: -23793.925591, T: 405306, Avg. loss: 1228.107870\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 415\n",
            "Norm: 1708000.36, NNZs: 933, Bias: -23793.929017, T: 406285, Avg. loss: 1233.344094\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 416\n",
            "Norm: 1708000.36, NNZs: 933, Bias: -23793.931371, T: 407264, Avg. loss: 1224.570965\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 417\n",
            "Norm: 1708000.35, NNZs: 933, Bias: -23793.933709, T: 408243, Avg. loss: 1222.890569\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 418\n",
            "Norm: 1708000.34, NNZs: 933, Bias: -23793.937097, T: 409222, Avg. loss: 1228.055278\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 419\n",
            "Norm: 1708000.33, NNZs: 933, Bias: -23793.939371, T: 410201, Avg. loss: 1219.395794\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 420\n",
            "Norm: 1708000.33, NNZs: 933, Bias: -23793.941662, T: 411180, Avg. loss: 1217.728213\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 421\n",
            "Norm: 1708000.32, NNZs: 933, Bias: -23793.945112, T: 412159, Avg. loss: 1222.856485\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 422\n",
            "Norm: 1708000.31, NNZs: 933, Bias: -23793.947445, T: 413138, Avg. loss: 1214.275443\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 423\n",
            "Norm: 1708000.30, NNZs: 933, Bias: -23793.950876, T: 414117, Avg. loss: 1219.358025\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 424\n",
            "Norm: 1708000.29, NNZs: 933, Bias: -23793.954331, T: 415096, Avg. loss: 1217.563163\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 425\n",
            "Norm: 1708000.29, NNZs: 933, Bias: -23793.957641, T: 416075, Avg. loss: 1215.764525\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 426\n",
            "Norm: 1708000.28, NNZs: 933, Bias: -23793.959902, T: 417054, Avg. loss: 1207.333065\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 427\n",
            "Norm: 1708000.27, NNZs: 933, Bias: -23793.963204, T: 418033, Avg. loss: 1212.338443\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 428\n",
            "Norm: 1708000.26, NNZs: 933, Bias: -23793.965479, T: 419012, Avg. loss: 1203.960328\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 429\n",
            "Norm: 1708000.25, NNZs: 933, Bias: -23793.968718, T: 419991, Avg. loss: 1208.920935\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 430\n",
            "Norm: 1708000.25, NNZs: 933, Bias: -23793.972062, T: 420970, Avg. loss: 1207.178342\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 431\n",
            "Norm: 1708000.24, NNZs: 933, Bias: -23793.974277, T: 421949, Avg. loss: 1198.905551\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 432\n",
            "Norm: 1708000.23, NNZs: 933, Bias: -23793.977504, T: 422928, Avg. loss: 1203.818895\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 433\n",
            "Norm: 1708000.22, NNZs: 933, Bias: -23793.980715, T: 423907, Avg. loss: 1202.083390\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 434\n",
            "Norm: 1708000.21, NNZs: 933, Bias: -23793.983939, T: 424886, Avg. loss: 1200.355359\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 435\n",
            "Norm: 1708000.21, NNZs: 933, Bias: -23793.986132, T: 425865, Avg. loss: 1192.192565\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 436\n",
            "Norm: 1708000.20, NNZs: 933, Bias: -23793.989247, T: 426844, Avg. loss: 1197.051679\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 437\n",
            "Norm: 1708000.19, NNZs: 933, Bias: -23793.992466, T: 427823, Avg. loss: 1195.329290\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 438\n",
            "Norm: 1708000.18, NNZs: 933, Bias: -23793.994546, T: 428802, Avg. loss: 1187.267199\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 439\n",
            "Norm: 1708000.18, NNZs: 933, Bias: -23793.996679, T: 429781, Avg. loss: 1185.721812\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 440\n",
            "Norm: 1708000.17, NNZs: 933, Bias: -23793.998851, T: 430760, Avg. loss: 1184.180611\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 441\n",
            "Norm: 1708000.16, NNZs: 933, Bias: -23794.000900, T: 431739, Avg. loss: 1182.643555\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 442\n",
            "Norm: 1708000.16, NNZs: 933, Bias: -23794.003077, T: 432718, Avg. loss: 1181.093720\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 443\n",
            "Norm: 1708000.15, NNZs: 933, Bias: -23794.005116, T: 433697, Avg. loss: 1179.571822\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 444\n",
            "Norm: 1708000.15, NNZs: 933, Bias: -23794.007233, T: 434676, Avg. loss: 1178.034311\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 445\n",
            "Norm: 1708000.14, NNZs: 933, Bias: -23794.010382, T: 435655, Avg. loss: 1182.714730\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 446\n",
            "Norm: 1708000.13, NNZs: 933, Bias: -23794.012465, T: 436634, Avg. loss: 1174.876406\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 447\n",
            "Norm: 1708000.12, NNZs: 933, Bias: -23794.015532, T: 437613, Avg. loss: 1179.525743\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 448\n",
            "Norm: 1708000.12, NNZs: 933, Bias: -23794.017701, T: 438592, Avg. loss: 1171.734872\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 449\n",
            "Norm: 1708000.11, NNZs: 933, Bias: -23794.020750, T: 439571, Avg. loss: 1176.348490\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 450\n",
            "Norm: 1708000.10, NNZs: 933, Bias: -23794.022837, T: 440550, Avg. loss: 1168.618707\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 451\n",
            "Norm: 1708000.10, NNZs: 933, Bias: -23794.024940, T: 441529, Avg. loss: 1167.126818\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 452\n",
            "Norm: 1708000.09, NNZs: 933, Bias: -23794.026985, T: 442508, Avg. loss: 1165.631471\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 453\n",
            "Norm: 1708000.08, NNZs: 933, Bias: -23794.030068, T: 443487, Avg. loss: 1170.183449\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 454\n",
            "Norm: 1708000.07, NNZs: 933, Bias: -23794.033179, T: 444466, Avg. loss: 1168.566519\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 455\n",
            "Norm: 1708000.07, NNZs: 933, Bias: -23794.036244, T: 445445, Avg. loss: 1166.965233\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 456\n",
            "Norm: 1708000.06, NNZs: 933, Bias: -23794.038253, T: 446424, Avg. loss: 1159.403771\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 457\n",
            "Norm: 1708000.05, NNZs: 933, Bias: -23794.040342, T: 447403, Avg. loss: 1157.934209\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 458\n",
            "Norm: 1708000.05, NNZs: 933, Bias: -23794.043306, T: 448382, Avg. loss: 1162.410611\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 459\n",
            "Norm: 1708000.04, NNZs: 933, Bias: -23794.046270, T: 449361, Avg. loss: 1160.826543\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 460\n",
            "Norm: 1708000.03, NNZs: 933, Bias: -23794.049215, T: 450340, Avg. loss: 1159.251801\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 461\n",
            "Norm: 1708000.02, NNZs: 933, Bias: -23794.051241, T: 451319, Avg. loss: 1151.822671\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 462\n",
            "Norm: 1708000.02, NNZs: 933, Bias: -23794.054174, T: 452298, Avg. loss: 1156.232638\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 463\n",
            "Norm: 1708000.01, NNZs: 933, Bias: -23794.056166, T: 453277, Avg. loss: 1148.847692\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 464\n",
            "Norm: 1708000.00, NNZs: 933, Bias: -23794.059085, T: 454256, Avg. loss: 1153.218097\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 465\n",
            "Norm: 1708000.00, NNZs: 933, Bias: -23794.061029, T: 455235, Avg. loss: 1145.891202\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 466\n",
            "Norm: 1707999.99, NNZs: 933, Bias: -23794.063985, T: 456214, Avg. loss: 1150.244965\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 467\n",
            "Norm: 1707999.98, NNZs: 933, Bias: -23794.066831, T: 457193, Avg. loss: 1148.705383\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 468\n",
            "Norm: 1707999.97, NNZs: 933, Bias: -23794.069634, T: 458172, Avg. loss: 1147.182287\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 469\n",
            "Norm: 1707999.97, NNZs: 933, Bias: -23794.072450, T: 459151, Avg. loss: 1145.655128\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 470\n",
            "Norm: 1707999.96, NNZs: 933, Bias: -23794.074436, T: 460130, Avg. loss: 1138.449748\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 471\n",
            "Norm: 1707999.95, NNZs: 933, Bias: -23794.077227, T: 461109, Avg. loss: 1142.732005\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 472\n",
            "Norm: 1707999.95, NNZs: 933, Bias: -23794.080023, T: 462088, Avg. loss: 1141.221579\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 473\n",
            "Norm: 1707999.94, NNZs: 933, Bias: -23794.082762, T: 463067, Avg. loss: 1139.720888\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 474\n",
            "Norm: 1707999.93, NNZs: 933, Bias: -23794.085522, T: 464046, Avg. loss: 1138.229191\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 475\n",
            "Norm: 1707999.93, NNZs: 933, Bias: -23794.088235, T: 465025, Avg. loss: 1136.745958\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 476\n",
            "Norm: 1707999.92, NNZs: 933, Bias: -23794.090116, T: 466004, Avg. loss: 1129.693497\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 477\n",
            "Norm: 1707999.91, NNZs: 933, Bias: -23794.092828, T: 466983, Avg. loss: 1133.888019\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 478\n",
            "Norm: 1707999.91, NNZs: 933, Bias: -23794.094734, T: 467962, Avg. loss: 1126.882416\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 479\n",
            "Norm: 1707999.90, NNZs: 933, Bias: -23794.097405, T: 468941, Avg. loss: 1131.059445\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 480\n",
            "Norm: 1707999.89, NNZs: 933, Bias: -23794.100130, T: 469920, Avg. loss: 1129.589315\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 481\n",
            "Norm: 1707999.89, NNZs: 933, Bias: -23794.102029, T: 470899, Avg. loss: 1122.648628\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 482\n",
            "Norm: 1707999.88, NNZs: 933, Bias: -23794.103854, T: 471878, Avg. loss: 1121.314425\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 483\n",
            "Norm: 1707999.87, NNZs: 933, Bias: -23794.106579, T: 472857, Avg. loss: 1125.428686\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 484\n",
            "Norm: 1707999.87, NNZs: 933, Bias: -23794.108456, T: 473836, Avg. loss: 1118.560068\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 485\n",
            "Norm: 1707999.86, NNZs: 933, Bias: -23794.110253, T: 474815, Avg. loss: 1117.236495\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 486\n",
            "Norm: 1707999.86, NNZs: 933, Bias: -23794.112900, T: 475794, Avg. loss: 1121.300845\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 487\n",
            "Norm: 1707999.85, NNZs: 933, Bias: -23794.114778, T: 476773, Avg. loss: 1114.502155\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 488\n",
            "Norm: 1707999.84, NNZs: 933, Bias: -23794.117386, T: 477752, Avg. loss: 1118.543386\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 489\n",
            "Norm: 1707999.84, NNZs: 933, Bias: -23794.120014, T: 478731, Avg. loss: 1117.117106\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 490\n",
            "Norm: 1707999.83, NNZs: 933, Bias: -23794.122642, T: 479710, Avg. loss: 1115.718842\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 491\n",
            "Norm: 1707999.82, NNZs: 933, Bias: -23794.125222, T: 480689, Avg. loss: 1114.303843\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 492\n",
            "Norm: 1707999.82, NNZs: 933, Bias: -23794.127058, T: 481668, Avg. loss: 1107.605961\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 493\n",
            "Norm: 1707999.81, NNZs: 933, Bias: -23794.129600, T: 482647, Avg. loss: 1111.594307\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 494\n",
            "Norm: 1707999.81, NNZs: 933, Bias: -23794.131417, T: 483626, Avg. loss: 1104.950919\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 495\n",
            "Norm: 1707999.80, NNZs: 933, Bias: -23794.133947, T: 484605, Avg. loss: 1108.902598\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 496\n",
            "Norm: 1707999.79, NNZs: 933, Bias: -23794.135724, T: 485584, Avg. loss: 1102.301274\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 497\n",
            "Norm: 1707999.79, NNZs: 933, Bias: -23794.137540, T: 486563, Avg. loss: 1101.024201\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 498\n",
            "Norm: 1707999.78, NNZs: 933, Bias: -23794.140064, T: 487542, Avg. loss: 1104.939968\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 499\n",
            "Norm: 1707999.78, NNZs: 933, Bias: -23794.142575, T: 488521, Avg. loss: 1103.574362\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 500\n",
            "Norm: 1707999.77, NNZs: 933, Bias: -23794.145101, T: 489500, Avg. loss: 1102.207445\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 501\n",
            "Norm: 1707999.76, NNZs: 933, Bias: -23794.146887, T: 490479, Avg. loss: 1095.692140\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 502\n",
            "Norm: 1707999.76, NNZs: 933, Bias: -23794.148598, T: 491458, Avg. loss: 1094.445405\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 503\n",
            "Norm: 1707999.75, NNZs: 933, Bias: -23794.150359, T: 492437, Avg. loss: 1093.190051\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 504\n",
            "Norm: 1707999.75, NNZs: 933, Bias: -23794.152842, T: 493416, Avg. loss: 1097.024867\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 505\n",
            "Norm: 1707999.74, NNZs: 933, Bias: -23794.155327, T: 494395, Avg. loss: 1095.681701\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 506\n",
            "Norm: 1707999.73, NNZs: 933, Bias: -23794.157069, T: 495374, Avg. loss: 1089.276633\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 507\n",
            "Norm: 1707999.73, NNZs: 933, Bias: -23794.158871, T: 496353, Avg. loss: 1088.048469\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 508\n",
            "Norm: 1707999.72, NNZs: 933, Bias: -23794.160643, T: 497332, Avg. loss: 1086.813917\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 509\n",
            "Norm: 1707999.72, NNZs: 933, Bias: -23794.162446, T: 498311, Avg. loss: 1085.588193\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 510\n",
            "Norm: 1707999.71, NNZs: 933, Bias: -23794.164201, T: 499290, Avg. loss: 1084.359168\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 511\n",
            "Norm: 1707999.71, NNZs: 933, Bias: -23794.166005, T: 500269, Avg. loss: 1083.132796\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 512\n",
            "Norm: 1707999.70, NNZs: 933, Bias: -23794.167820, T: 501248, Avg. loss: 1081.912861\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 513\n",
            "Norm: 1707999.70, NNZs: 933, Bias: -23794.169520, T: 502227, Avg. loss: 1080.697849\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 514\n",
            "Norm: 1707999.69, NNZs: 933, Bias: -23794.171307, T: 503206, Avg. loss: 1079.475143\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 515\n",
            "Norm: 1707999.68, NNZs: 933, Bias: -23794.173767, T: 504185, Avg. loss: 1083.181018\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 516\n",
            "Norm: 1707999.68, NNZs: 933, Bias: -23794.176211, T: 505164, Avg. loss: 1081.879906\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 517\n",
            "Norm: 1707999.67, NNZs: 933, Bias: -23794.178717, T: 506143, Avg. loss: 1080.576961\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 518\n",
            "Norm: 1707999.67, NNZs: 933, Bias: -23794.180500, T: 507122, Avg. loss: 1074.412521\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 519\n",
            "Norm: 1707999.66, NNZs: 933, Bias: -23794.182935, T: 508101, Avg. loss: 1078.067765\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 520\n",
            "Norm: 1707999.65, NNZs: 933, Bias: -23794.185418, T: 509080, Avg. loss: 1076.786430\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 521\n",
            "Norm: 1707999.65, NNZs: 933, Bias: -23794.187148, T: 510059, Avg. loss: 1070.680414\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 522\n",
            "Norm: 1707999.64, NNZs: 933, Bias: -23794.189558, T: 511038, Avg. loss: 1074.313767\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 523\n",
            "Norm: 1707999.64, NNZs: 933, Bias: -23794.191979, T: 512017, Avg. loss: 1073.042996\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 524\n",
            "Norm: 1707999.63, NNZs: 933, Bias: -23794.194342, T: 512996, Avg. loss: 1071.766940\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 525\n",
            "Norm: 1707999.62, NNZs: 933, Bias: -23794.196743, T: 513975, Avg. loss: 1070.505897\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 526\n",
            "Norm: 1707999.62, NNZs: 933, Bias: -23794.198395, T: 514954, Avg. loss: 1064.493862\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 527\n",
            "Norm: 1707999.61, NNZs: 933, Bias: -23794.200727, T: 515933, Avg. loss: 1068.069980\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 528\n",
            "Norm: 1707999.61, NNZs: 933, Bias: -23794.202393, T: 516912, Avg. loss: 1062.087856\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 529\n",
            "Norm: 1707999.60, NNZs: 933, Bias: -23794.204110, T: 517891, Avg. loss: 1060.924653\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 530\n",
            "Norm: 1707999.60, NNZs: 933, Bias: -23794.205781, T: 518870, Avg. loss: 1059.769412\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 531\n",
            "Norm: 1707999.59, NNZs: 933, Bias: -23794.207508, T: 519849, Avg. loss: 1058.615380\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 532\n",
            "Norm: 1707999.59, NNZs: 933, Bias: -23794.209852, T: 520828, Avg. loss: 1062.127966\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 533\n",
            "Norm: 1707999.58, NNZs: 933, Bias: -23794.211575, T: 521807, Avg. loss: 1056.245121\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 534\n",
            "Norm: 1707999.58, NNZs: 933, Bias: -23794.213299, T: 522786, Avg. loss: 1055.105957\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 535\n",
            "Norm: 1707999.57, NNZs: 933, Bias: -23794.215620, T: 523765, Avg. loss: 1058.578401\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 536\n",
            "Norm: 1707999.57, NNZs: 933, Bias: -23794.217913, T: 524744, Avg. loss: 1057.359834\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 537\n",
            "Norm: 1707999.56, NNZs: 933, Bias: -23794.219525, T: 525723, Avg. loss: 1051.541954\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 538\n",
            "Norm: 1707999.55, NNZs: 933, Bias: -23794.221803, T: 526702, Avg. loss: 1054.989150\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 539\n",
            "Norm: 1707999.55, NNZs: 933, Bias: -23794.224111, T: 527681, Avg. loss: 1053.781147\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 540\n",
            "Norm: 1707999.54, NNZs: 933, Bias: -23794.226370, T: 528660, Avg. loss: 1052.573902\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 541\n",
            "Norm: 1707999.54, NNZs: 933, Bias: -23794.228614, T: 529639, Avg. loss: 1051.359350\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 542\n",
            "Norm: 1707999.53, NNZs: 933, Bias: -23794.230278, T: 530618, Avg. loss: 1045.627251\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 543\n",
            "Norm: 1707999.53, NNZs: 933, Bias: -23794.231957, T: 531597, Avg. loss: 1044.518269\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 544\n",
            "Norm: 1707999.52, NNZs: 933, Bias: -23794.234183, T: 532576, Avg. loss: 1047.913858\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 545\n",
            "Norm: 1707999.52, NNZs: 933, Bias: -23794.236400, T: 533555, Avg. loss: 1046.720320\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 546\n",
            "Norm: 1707999.51, NNZs: 933, Bias: -23794.238028, T: 534534, Avg. loss: 1041.057014\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 547\n",
            "Norm: 1707999.51, NNZs: 933, Bias: -23794.240266, T: 535513, Avg. loss: 1044.431680\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 548\n",
            "Norm: 1707999.50, NNZs: 933, Bias: -23794.241892, T: 536492, Avg. loss: 1038.794761\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 549\n",
            "Norm: 1707999.50, NNZs: 933, Bias: -23794.243539, T: 537471, Avg. loss: 1037.708786\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 550\n",
            "Norm: 1707999.49, NNZs: 933, Bias: -23794.245192, T: 538450, Avg. loss: 1036.623578\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 551\n",
            "Norm: 1707999.48, NNZs: 933, Bias: -23794.247459, T: 539429, Avg. loss: 1039.951848\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 552\n",
            "Norm: 1707999.48, NNZs: 933, Bias: -23794.249679, T: 540408, Avg. loss: 1038.788564\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 553\n",
            "Norm: 1707999.47, NNZs: 933, Bias: -23794.251881, T: 541387, Avg. loss: 1037.632773\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 554\n",
            "Norm: 1707999.47, NNZs: 933, Bias: -23794.254040, T: 542366, Avg. loss: 1036.464360\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 555\n",
            "Norm: 1707999.46, NNZs: 933, Bias: -23794.256220, T: 543345, Avg. loss: 1035.316787\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 556\n",
            "Norm: 1707999.46, NNZs: 933, Bias: -23794.257703, T: 544324, Avg. loss: 1029.815793\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 557\n",
            "Norm: 1707999.45, NNZs: 933, Bias: -23794.259303, T: 545303, Avg. loss: 1028.742395\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 558\n",
            "Norm: 1707999.45, NNZs: 933, Bias: -23794.260916, T: 546282, Avg. loss: 1027.678670\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 559\n",
            "Norm: 1707999.44, NNZs: 933, Bias: -23794.263049, T: 547261, Avg. loss: 1030.936958\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 560\n",
            "Norm: 1707999.44, NNZs: 933, Bias: -23794.265174, T: 548240, Avg. loss: 1029.796835\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 561\n",
            "Norm: 1707999.43, NNZs: 933, Bias: -23794.266760, T: 549219, Avg. loss: 1024.367484\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 562\n",
            "Norm: 1707999.43, NNZs: 933, Bias: -23794.268888, T: 550198, Avg. loss: 1027.602704\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 563\n",
            "Norm: 1707999.42, NNZs: 933, Bias: -23794.270998, T: 551177, Avg. loss: 1026.470107\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 564\n",
            "Norm: 1707999.42, NNZs: 933, Bias: -23794.273099, T: 552156, Avg. loss: 1025.345816\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 565\n",
            "Norm: 1707999.41, NNZs: 933, Bias: -23794.275232, T: 553135, Avg. loss: 1024.218480\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 566\n",
            "Norm: 1707999.41, NNZs: 933, Bias: -23794.277339, T: 554114, Avg. loss: 1023.107077\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 567\n",
            "Norm: 1707999.40, NNZs: 933, Bias: -23794.278892, T: 555093, Avg. loss: 1017.768094\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 568\n",
            "Norm: 1707999.40, NNZs: 933, Bias: -23794.280993, T: 556072, Avg. loss: 1020.957074\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 569\n",
            "Norm: 1707999.39, NNZs: 933, Bias: -23794.282504, T: 557051, Avg. loss: 1015.650663\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 570\n",
            "Norm: 1707999.39, NNZs: 933, Bias: -23794.284038, T: 558030, Avg. loss: 1014.615302\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 571\n",
            "Norm: 1707999.38, NNZs: 933, Bias: -23794.285500, T: 559009, Avg. loss: 1013.591481\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 572\n",
            "Norm: 1707999.38, NNZs: 933, Bias: -23794.287026, T: 559988, Avg. loss: 1012.567686\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 573\n",
            "Norm: 1707999.37, NNZs: 933, Bias: -23794.289097, T: 560967, Avg. loss: 1015.690520\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 574\n",
            "Norm: 1707999.37, NNZs: 933, Bias: -23794.291154, T: 561946, Avg. loss: 1014.609053\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 575\n",
            "Norm: 1707999.36, NNZs: 933, Bias: -23794.293190, T: 562925, Avg. loss: 1013.511785\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 576\n",
            "Norm: 1707999.36, NNZs: 933, Bias: -23794.294685, T: 563904, Avg. loss: 1008.317895\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 577\n",
            "Norm: 1707999.35, NNZs: 933, Bias: -23794.296718, T: 564883, Avg. loss: 1011.405939\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 578\n",
            "Norm: 1707999.35, NNZs: 933, Bias: -23794.298807, T: 565862, Avg. loss: 1010.330352\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 579\n",
            "Norm: 1707999.34, NNZs: 933, Bias: -23794.300235, T: 566841, Avg. loss: 1005.174004\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 580\n",
            "Norm: 1707999.34, NNZs: 933, Bias: -23794.302250, T: 567820, Avg. loss: 1008.248729\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 581\n",
            "Norm: 1707999.33, NNZs: 933, Bias: -23794.304303, T: 568799, Avg. loss: 1007.174167\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 582\n",
            "Norm: 1707999.33, NNZs: 933, Bias: -23794.306297, T: 569778, Avg. loss: 1006.112224\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 583\n",
            "Norm: 1707999.32, NNZs: 933, Bias: -23794.308268, T: 570757, Avg. loss: 1005.047806\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 584\n",
            "Norm: 1707999.32, NNZs: 933, Bias: -23794.309732, T: 571736, Avg. loss: 999.958430\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 585\n",
            "Norm: 1707999.31, NNZs: 933, Bias: -23794.311207, T: 572715, Avg. loss: 998.973878\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 586\n",
            "Norm: 1707999.31, NNZs: 933, Bias: -23794.313209, T: 573694, Avg. loss: 1001.992910\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 587\n",
            "Norm: 1707999.30, NNZs: 933, Bias: -23794.314646, T: 574673, Avg. loss: 996.951868\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 588\n",
            "Norm: 1707999.30, NNZs: 933, Bias: -23794.316610, T: 575652, Avg. loss: 999.951052\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 589\n",
            "Norm: 1707999.29, NNZs: 933, Bias: -23794.318068, T: 576631, Avg. loss: 994.938935\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 590\n",
            "Norm: 1707999.29, NNZs: 933, Bias: -23794.319453, T: 577610, Avg. loss: 993.963030\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 591\n",
            "Norm: 1707999.29, NNZs: 933, Bias: -23794.320940, T: 578589, Avg. loss: 992.986980\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 592\n",
            "Norm: 1707999.28, NNZs: 933, Bias: -23794.322367, T: 579568, Avg. loss: 992.017988\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 593\n",
            "Norm: 1707999.28, NNZs: 933, Bias: -23794.323865, T: 580547, Avg. loss: 991.050499\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 594\n",
            "Norm: 1707999.27, NNZs: 933, Bias: -23794.325822, T: 581526, Avg. loss: 993.999939\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 595\n",
            "Norm: 1707999.27, NNZs: 933, Bias: -23794.327232, T: 582505, Avg. loss: 989.064466\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 596\n",
            "Norm: 1707999.26, NNZs: 933, Bias: -23794.329193, T: 583484, Avg. loss: 992.005073\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 597\n",
            "Norm: 1707999.26, NNZs: 933, Bias: -23794.331142, T: 584463, Avg. loss: 990.984840\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 598\n",
            "Norm: 1707999.25, NNZs: 933, Bias: -23794.333088, T: 585442, Avg. loss: 989.962788\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 599\n",
            "Norm: 1707999.25, NNZs: 933, Bias: -23794.334557, T: 586421, Avg. loss: 985.073673\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 600\n",
            "Norm: 1707999.24, NNZs: 933, Bias: -23794.336518, T: 587400, Avg. loss: 987.985465\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 601\n",
            "Norm: 1707999.24, NNZs: 933, Bias: -23794.338484, T: 588379, Avg. loss: 986.977710\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 602\n",
            "Norm: 1707999.23, NNZs: 933, Bias: -23794.339872, T: 589358, Avg. loss: 982.130378\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 603\n",
            "Norm: 1707999.23, NNZs: 933, Bias: -23794.341271, T: 590337, Avg. loss: 981.189039\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 604\n",
            "Norm: 1707999.23, NNZs: 933, Bias: -23794.342701, T: 591316, Avg. loss: 980.250062\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 605\n",
            "Norm: 1707999.22, NNZs: 933, Bias: -23794.344653, T: 592295, Avg. loss: 983.120275\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 606\n",
            "Norm: 1707999.22, NNZs: 933, Bias: -23794.346573, T: 593274, Avg. loss: 982.121027\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 607\n",
            "Norm: 1707999.21, NNZs: 933, Bias: -23794.348009, T: 594253, Avg. loss: 977.342970\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 608\n",
            "Norm: 1707999.21, NNZs: 933, Bias: -23794.349434, T: 595232, Avg. loss: 976.405633\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 609\n",
            "Norm: 1707999.20, NNZs: 933, Bias: -23794.351315, T: 596211, Avg. loss: 979.249738\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 610\n",
            "Norm: 1707999.20, NNZs: 933, Bias: -23794.353201, T: 597190, Avg. loss: 978.257069\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 611\n",
            "Norm: 1707999.19, NNZs: 933, Bias: -23794.355143, T: 598169, Avg. loss: 977.277052\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 612\n",
            "Norm: 1707999.19, NNZs: 933, Bias: -23794.356477, T: 599148, Avg. loss: 972.553503\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 613\n",
            "Norm: 1707999.18, NNZs: 933, Bias: -23794.357912, T: 600127, Avg. loss: 971.631214\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 614\n",
            "Norm: 1707999.18, NNZs: 933, Bias: -23794.359791, T: 601106, Avg. loss: 974.439742\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 615\n",
            "Norm: 1707999.18, NNZs: 933, Bias: -23794.361215, T: 602085, Avg. loss: 969.756180\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 616\n",
            "Norm: 1707999.17, NNZs: 933, Bias: -23794.362610, T: 603064, Avg. loss: 968.843238\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 617\n",
            "Norm: 1707999.17, NNZs: 933, Bias: -23794.363975, T: 604043, Avg. loss: 967.933786\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 618\n",
            "Norm: 1707999.16, NNZs: 933, Bias: -23794.365857, T: 605022, Avg. loss: 970.698254\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 619\n",
            "Norm: 1707999.16, NNZs: 933, Bias: -23794.367266, T: 606001, Avg. loss: 966.068145\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 620\n",
            "Norm: 1707999.15, NNZs: 933, Bias: -23794.368639, T: 606980, Avg. loss: 965.164901\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 621\n",
            "Norm: 1707999.15, NNZs: 933, Bias: -23794.370503, T: 607959, Avg. loss: 967.912305\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 622\n",
            "Norm: 1707999.15, NNZs: 933, Bias: -23794.371869, T: 608938, Avg. loss: 963.312689\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 623\n",
            "Norm: 1707999.14, NNZs: 933, Bias: -23794.373215, T: 609917, Avg. loss: 962.414570\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 624\n",
            "Norm: 1707999.14, NNZs: 933, Bias: -23794.374615, T: 610896, Avg. loss: 961.518928\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 625\n",
            "Norm: 1707999.13, NNZs: 933, Bias: -23794.375984, T: 611875, Avg. loss: 960.625285\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 626\n",
            "Norm: 1707999.13, NNZs: 933, Bias: -23794.377376, T: 612854, Avg. loss: 959.733971\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 627\n",
            "Norm: 1707999.13, NNZs: 933, Bias: -23794.379213, T: 613833, Avg. loss: 962.442774\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 628\n",
            "Norm: 1707999.12, NNZs: 933, Bias: -23794.381068, T: 614812, Avg. loss: 961.495734\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 629\n",
            "Norm: 1707999.12, NNZs: 933, Bias: -23794.382465, T: 615791, Avg. loss: 956.971536\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 630\n",
            "Norm: 1707999.11, NNZs: 933, Bias: -23794.384325, T: 616770, Avg. loss: 959.656609\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 631\n",
            "Norm: 1707999.11, NNZs: 933, Bias: -23794.385737, T: 617749, Avg. loss: 955.161522\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 632\n",
            "Norm: 1707999.10, NNZs: 933, Bias: -23794.387070, T: 618728, Avg. loss: 954.290627\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 633\n",
            "Norm: 1707999.10, NNZs: 933, Bias: -23794.388879, T: 619707, Avg. loss: 956.952716\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 634\n",
            "Norm: 1707999.09, NNZs: 933, Bias: -23794.390689, T: 620686, Avg. loss: 956.021184\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 635\n",
            "Norm: 1707999.09, NNZs: 933, Bias: -23794.392504, T: 621665, Avg. loss: 955.091955\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 636\n",
            "Norm: 1707999.09, NNZs: 933, Bias: -23794.393893, T: 622644, Avg. loss: 950.651035\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 637\n",
            "Norm: 1707999.08, NNZs: 933, Bias: -23794.395683, T: 623623, Avg. loss: 953.299595\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 638\n",
            "Norm: 1707999.08, NNZs: 933, Bias: -23794.397458, T: 624602, Avg. loss: 952.375123\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 639\n",
            "Norm: 1707999.07, NNZs: 933, Bias: -23794.398840, T: 625581, Avg. loss: 947.970473\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 640\n",
            "Norm: 1707999.07, NNZs: 933, Bias: -23794.400643, T: 626560, Avg. loss: 950.588508\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 641\n",
            "Norm: 1707999.06, NNZs: 933, Bias: -23794.401971, T: 627539, Avg. loss: 946.208143\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 642\n",
            "Norm: 1707999.06, NNZs: 933, Bias: -23794.403741, T: 628518, Avg. loss: 948.813694\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 643\n",
            "Norm: 1707999.06, NNZs: 933, Bias: -23794.405484, T: 629497, Avg. loss: 947.907271\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 644\n",
            "Norm: 1707999.05, NNZs: 933, Bias: -23794.407229, T: 630476, Avg. loss: 947.006832\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 645\n",
            "Norm: 1707999.05, NNZs: 933, Bias: -23794.408555, T: 631455, Avg. loss: 942.664509\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 646\n",
            "Norm: 1707999.04, NNZs: 933, Bias: -23794.409823, T: 632434, Avg. loss: 941.817529\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 647\n",
            "Norm: 1707999.04, NNZs: 933, Bias: -23794.411568, T: 633413, Avg. loss: 944.392054\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 648\n",
            "Norm: 1707999.04, NNZs: 933, Bias: -23794.413294, T: 634392, Avg. loss: 943.491025\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 649\n",
            "Norm: 1707999.03, NNZs: 933, Bias: -23794.415083, T: 635371, Avg. loss: 942.601367\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 650\n",
            "Norm: 1707999.03, NNZs: 933, Bias: -23794.416859, T: 636350, Avg. loss: 941.714264\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 651\n",
            "Norm: 1707999.02, NNZs: 933, Bias: -23794.418582, T: 637329, Avg. loss: 940.825670\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 652\n",
            "Norm: 1707999.02, NNZs: 933, Bias: -23794.419910, T: 638308, Avg. loss: 936.565045\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 653\n",
            "Norm: 1707999.01, NNZs: 933, Bias: -23794.421609, T: 639287, Avg. loss: 939.103565\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 654\n",
            "Norm: 1707999.01, NNZs: 933, Bias: -23794.423320, T: 640266, Avg. loss: 938.228713\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 655\n",
            "Norm: 1707999.01, NNZs: 933, Bias: -23794.424603, T: 641245, Avg. loss: 933.991886\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 656\n",
            "Norm: 1707999.00, NNZs: 933, Bias: -23794.426284, T: 642224, Avg. loss: 936.508596\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 657\n",
            "Norm: 1707999.00, NNZs: 933, Bias: -23794.427588, T: 643203, Avg. loss: 932.298518\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 658\n",
            "Norm: 1707998.99, NNZs: 933, Bias: -23794.429264, T: 644182, Avg. loss: 934.812777\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 659\n",
            "Norm: 1707998.99, NNZs: 933, Bias: -23794.430971, T: 645161, Avg. loss: 933.937131\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 660\n",
            "Norm: 1707998.99, NNZs: 933, Bias: -23794.432256, T: 646140, Avg. loss: 929.757242\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 661\n",
            "Norm: 1707998.98, NNZs: 933, Bias: -23794.433543, T: 647119, Avg. loss: 928.944225\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 662\n",
            "Norm: 1707998.98, NNZs: 933, Bias: -23794.434852, T: 648098, Avg. loss: 928.128102\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 663\n",
            "Norm: 1707998.97, NNZs: 933, Bias: -23794.436136, T: 649077, Avg. loss: 927.320534\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 664\n",
            "Norm: 1707998.97, NNZs: 933, Bias: -23794.437414, T: 650056, Avg. loss: 926.508418\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 665\n",
            "Norm: 1707998.97, NNZs: 933, Bias: -23794.439140, T: 651035, Avg. loss: 928.975812\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 666\n",
            "Norm: 1707998.96, NNZs: 933, Bias: -23794.440420, T: 652014, Avg. loss: 924.854817\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 667\n",
            "Norm: 1707998.96, NNZs: 933, Bias: -23794.442076, T: 652993, Avg. loss: 927.308123\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 668\n",
            "Norm: 1707998.95, NNZs: 933, Bias: -23794.443732, T: 653972, Avg. loss: 926.452406\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 669\n",
            "Norm: 1707998.95, NNZs: 933, Bias: -23794.445441, T: 654951, Avg. loss: 925.611006\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 670\n",
            "Norm: 1707998.95, NNZs: 933, Bias: -23794.447076, T: 655930, Avg. loss: 924.767083\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 671\n",
            "Norm: 1707998.94, NNZs: 933, Bias: -23794.448733, T: 656909, Avg. loss: 923.921358\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 672\n",
            "Norm: 1707998.94, NNZs: 933, Bias: -23794.449978, T: 657888, Avg. loss: 919.858240\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 673\n",
            "Norm: 1707998.93, NNZs: 933, Bias: -23794.451595, T: 658867, Avg. loss: 922.276875\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 674\n",
            "Norm: 1707998.93, NNZs: 933, Bias: -23794.453215, T: 659846, Avg. loss: 921.437760\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 675\n",
            "Norm: 1707998.93, NNZs: 933, Bias: -23794.454474, T: 660825, Avg. loss: 917.406152\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 676\n",
            "Norm: 1707998.92, NNZs: 933, Bias: -23794.455706, T: 661804, Avg. loss: 916.620534\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 677\n",
            "Norm: 1707998.92, NNZs: 933, Bias: -23794.457340, T: 662783, Avg. loss: 919.017052\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 678\n",
            "Norm: 1707998.91, NNZs: 933, Bias: -23794.458530, T: 663762, Avg. loss: 915.010698\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 679\n",
            "Norm: 1707998.91, NNZs: 933, Bias: -23794.459796, T: 664741, Avg. loss: 914.229295\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 680\n",
            "Norm: 1707998.91, NNZs: 933, Bias: -23794.461414, T: 665720, Avg. loss: 916.606410\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 681\n",
            "Norm: 1707998.90, NNZs: 933, Bias: -23794.463044, T: 666699, Avg. loss: 915.783267\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 682\n",
            "Norm: 1707998.90, NNZs: 933, Bias: -23794.464685, T: 667678, Avg. loss: 914.960593\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 683\n",
            "Norm: 1707998.89, NNZs: 933, Bias: -23794.466294, T: 668657, Avg. loss: 914.144652\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 684\n",
            "Norm: 1707998.89, NNZs: 933, Bias: -23794.467526, T: 669636, Avg. loss: 910.189754\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 685\n",
            "Norm: 1707998.89, NNZs: 933, Bias: -23794.468753, T: 670615, Avg. loss: 909.421005\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 686\n",
            "Norm: 1707998.88, NNZs: 933, Bias: -23794.470359, T: 671594, Avg. loss: 911.770101\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 687\n",
            "Norm: 1707998.88, NNZs: 933, Bias: -23794.471589, T: 672573, Avg. loss: 907.844673\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 688\n",
            "Norm: 1707998.88, NNZs: 933, Bias: -23794.472826, T: 673552, Avg. loss: 907.079770\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 689\n",
            "Norm: 1707998.87, NNZs: 933, Bias: -23794.474067, T: 674531, Avg. loss: 906.317612\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 690\n",
            "Norm: 1707998.87, NNZs: 933, Bias: -23794.475252, T: 675510, Avg. loss: 905.551786\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 691\n",
            "Norm: 1707998.87, NNZs: 933, Bias: -23794.476493, T: 676489, Avg. loss: 904.792483\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 692\n",
            "Norm: 1707998.86, NNZs: 933, Bias: -23794.477739, T: 677468, Avg. loss: 904.036830\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 693\n",
            "Norm: 1707998.86, NNZs: 933, Bias: -23794.479337, T: 678447, Avg. loss: 906.341837\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 694\n",
            "Norm: 1707998.86, NNZs: 933, Bias: -23794.480559, T: 679426, Avg. loss: 902.485233\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 695\n",
            "Norm: 1707998.85, NNZs: 933, Bias: -23794.482163, T: 680405, Avg. loss: 904.784334\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 696\n",
            "Norm: 1707998.85, NNZs: 933, Bias: -23794.483769, T: 681384, Avg. loss: 903.991034\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 697\n",
            "Norm: 1707998.84, NNZs: 933, Bias: -23794.485334, T: 682363, Avg. loss: 903.198027\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 698\n",
            "Norm: 1707998.84, NNZs: 933, Bias: -23794.486552, T: 683342, Avg. loss: 899.371072\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 699\n",
            "Norm: 1707998.84, NNZs: 933, Bias: -23794.488150, T: 684321, Avg. loss: 901.652070\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 700\n",
            "Norm: 1707998.83, NNZs: 933, Bias: -23794.489729, T: 685300, Avg. loss: 900.861745\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 701\n",
            "Norm: 1707998.83, NNZs: 933, Bias: -23794.490940, T: 686279, Avg. loss: 897.064451\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 702\n",
            "Norm: 1707998.82, NNZs: 933, Bias: -23794.492473, T: 687258, Avg. loss: 899.331183\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 703\n",
            "Norm: 1707998.82, NNZs: 933, Bias: -23794.493675, T: 688237, Avg. loss: 895.546666\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 704\n",
            "Norm: 1707998.82, NNZs: 933, Bias: -23794.494864, T: 689216, Avg. loss: 894.809152\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 705\n",
            "Norm: 1707998.81, NNZs: 933, Bias: -23794.496383, T: 690195, Avg. loss: 897.056540\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 706\n",
            "Norm: 1707998.81, NNZs: 933, Bias: -23794.497924, T: 691174, Avg. loss: 896.275325\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 707\n",
            "Norm: 1707998.81, NNZs: 933, Bias: -23794.499101, T: 692153, Avg. loss: 892.527594\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 708\n",
            "Norm: 1707998.80, NNZs: 933, Bias: -23794.500620, T: 693132, Avg. loss: 894.755218\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 709\n",
            "Norm: 1707998.80, NNZs: 933, Bias: -23794.501791, T: 694111, Avg. loss: 891.028752\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 710\n",
            "Norm: 1707998.80, NNZs: 933, Bias: -23794.502923, T: 695090, Avg. loss: 890.300166\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 711\n",
            "Norm: 1707998.79, NNZs: 933, Bias: -23794.504489, T: 696069, Avg. loss: 892.514964\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 712\n",
            "Norm: 1707998.79, NNZs: 933, Bias: -23794.505675, T: 697048, Avg. loss: 888.810952\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 713\n",
            "Norm: 1707998.79, NNZs: 933, Bias: -23794.506851, T: 698027, Avg. loss: 888.089146\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 714\n",
            "Norm: 1707998.78, NNZs: 933, Bias: -23794.508357, T: 699006, Avg. loss: 890.290208\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 715\n",
            "Norm: 1707998.78, NNZs: 933, Bias: -23794.509847, T: 699985, Avg. loss: 889.525628\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 716\n",
            "Norm: 1707998.77, NNZs: 933, Bias: -23794.510990, T: 700964, Avg. loss: 885.853711\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 717\n",
            "Norm: 1707998.77, NNZs: 933, Bias: -23794.512482, T: 701943, Avg. loss: 888.039302\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 718\n",
            "Norm: 1707998.77, NNZs: 933, Bias: -23794.513963, T: 702922, Avg. loss: 887.281893\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 719\n",
            "Norm: 1707998.76, NNZs: 933, Bias: -23794.515134, T: 703901, Avg. loss: 883.634646\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 720\n",
            "Norm: 1707998.76, NNZs: 933, Bias: -23794.516659, T: 704880, Avg. loss: 885.809971\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 721\n",
            "Norm: 1707998.76, NNZs: 933, Bias: -23794.518155, T: 705859, Avg. loss: 885.058244\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 722\n",
            "Norm: 1707998.75, NNZs: 933, Bias: -23794.519638, T: 706838, Avg. loss: 884.311198\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 723\n",
            "Norm: 1707998.75, NNZs: 933, Bias: -23794.521150, T: 707817, Avg. loss: 883.564238\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 724\n",
            "Norm: 1707998.75, NNZs: 933, Bias: -23794.522652, T: 708796, Avg. loss: 882.819324\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 725\n",
            "Norm: 1707998.74, NNZs: 933, Bias: -23794.523787, T: 709775, Avg. loss: 879.219447\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 726\n",
            "Norm: 1707998.74, NNZs: 933, Bias: -23794.525234, T: 710754, Avg. loss: 881.368843\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 727\n",
            "Norm: 1707998.73, NNZs: 933, Bias: -23794.526716, T: 711733, Avg. loss: 880.622693\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 728\n",
            "Norm: 1707998.73, NNZs: 933, Bias: -23794.528151, T: 712712, Avg. loss: 879.881302\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 729\n",
            "Norm: 1707998.73, NNZs: 933, Bias: -23794.529264, T: 713691, Avg. loss: 876.317036\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 730\n",
            "Norm: 1707998.72, NNZs: 933, Bias: -23794.530725, T: 714670, Avg. loss: 878.445545\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 731\n",
            "Norm: 1707998.72, NNZs: 933, Bias: -23794.532150, T: 715649, Avg. loss: 877.707921\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 732\n",
            "Norm: 1707998.72, NNZs: 933, Bias: -23794.533241, T: 716628, Avg. loss: 874.167958\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 733\n",
            "Norm: 1707998.71, NNZs: 933, Bias: -23794.534332, T: 717607, Avg. loss: 873.472368\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 734\n",
            "Norm: 1707998.71, NNZs: 933, Bias: -23794.535468, T: 718586, Avg. loss: 872.781969\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 735\n",
            "Norm: 1707998.71, NNZs: 933, Bias: -23794.536895, T: 719565, Avg. loss: 874.889653\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 736\n",
            "Norm: 1707998.70, NNZs: 933, Bias: -23794.538023, T: 720544, Avg. loss: 871.373700\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 737\n",
            "Norm: 1707998.70, NNZs: 933, Bias: -23794.539463, T: 721523, Avg. loss: 873.470533\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 738\n",
            "Norm: 1707998.70, NNZs: 933, Bias: -23794.540566, T: 722502, Avg. loss: 869.972871\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 739\n",
            "Norm: 1707998.69, NNZs: 933, Bias: -23794.541674, T: 723481, Avg. loss: 869.288424\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 740\n",
            "Norm: 1707998.69, NNZs: 933, Bias: -23794.542812, T: 724460, Avg. loss: 868.605697\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 741\n",
            "Norm: 1707998.69, NNZs: 933, Bias: -23794.544243, T: 725439, Avg. loss: 870.679841\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 742\n",
            "Norm: 1707998.68, NNZs: 933, Bias: -23794.545374, T: 726418, Avg. loss: 867.213088\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 743\n",
            "Norm: 1707998.68, NNZs: 933, Bias: -23794.546821, T: 727397, Avg. loss: 869.282545\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 744\n",
            "Norm: 1707998.68, NNZs: 933, Bias: -23794.548263, T: 728376, Avg. loss: 868.572835\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 745\n",
            "Norm: 1707998.67, NNZs: 933, Bias: -23794.549385, T: 729355, Avg. loss: 865.125640\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 746\n",
            "Norm: 1707998.67, NNZs: 933, Bias: -23794.550779, T: 730334, Avg. loss: 867.176820\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 747\n",
            "Norm: 1707998.67, NNZs: 933, Bias: -23794.551854, T: 731313, Avg. loss: 863.744814\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 748\n",
            "Norm: 1707998.66, NNZs: 933, Bias: -23794.553247, T: 732292, Avg. loss: 865.792489\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 749\n",
            "Norm: 1707998.66, NNZs: 933, Bias: -23794.554674, T: 733271, Avg. loss: 865.084132\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 750\n",
            "Norm: 1707998.66, NNZs: 933, Bias: -23794.556065, T: 734250, Avg. loss: 864.382803\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 751\n",
            "Norm: 1707998.65, NNZs: 933, Bias: -23794.557463, T: 735229, Avg. loss: 863.677181\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 752\n",
            "Norm: 1707998.65, NNZs: 933, Bias: -23794.558573, T: 736208, Avg. loss: 860.285415\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 753\n",
            "Norm: 1707998.65, NNZs: 933, Bias: -23794.559674, T: 737187, Avg. loss: 859.618013\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 754\n",
            "Norm: 1707998.64, NNZs: 933, Bias: -23794.561083, T: 738166, Avg. loss: 861.640417\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 755\n",
            "Norm: 1707998.64, NNZs: 933, Bias: -23794.562150, T: 739145, Avg. loss: 858.268285\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 756\n",
            "Norm: 1707998.64, NNZs: 933, Bias: -23794.563518, T: 740124, Avg. loss: 860.282578\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 757\n",
            "Norm: 1707998.63, NNZs: 933, Bias: -23794.564884, T: 741103, Avg. loss: 859.584430\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 758\n",
            "Norm: 1707998.63, NNZs: 933, Bias: -23794.565952, T: 742082, Avg. loss: 856.235078\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 759\n",
            "Norm: 1707998.63, NNZs: 933, Bias: -23794.567313, T: 743061, Avg. loss: 858.234357\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 760\n",
            "Norm: 1707998.62, NNZs: 933, Bias: -23794.568399, T: 744040, Avg. loss: 854.897259\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 761\n",
            "Norm: 1707998.62, NNZs: 933, Bias: -23794.569756, T: 745019, Avg. loss: 856.888281\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 762\n",
            "Norm: 1707998.62, NNZs: 933, Bias: -23794.571135, T: 745998, Avg. loss: 856.199072\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 763\n",
            "Norm: 1707998.61, NNZs: 933, Bias: -23794.572182, T: 746977, Avg. loss: 852.883277\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 764\n",
            "Norm: 1707998.61, NNZs: 933, Bias: -23794.573527, T: 747956, Avg. loss: 854.859019\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 765\n",
            "Norm: 1707998.61, NNZs: 933, Bias: -23794.574600, T: 748935, Avg. loss: 851.559544\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 766\n",
            "Norm: 1707998.60, NNZs: 933, Bias: -23794.575639, T: 749914, Avg. loss: 850.913741\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 767\n",
            "Norm: 1707998.60, NNZs: 933, Bias: -23794.576721, T: 750893, Avg. loss: 850.266997\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 768\n",
            "Norm: 1707998.60, NNZs: 933, Bias: -23794.577787, T: 751872, Avg. loss: 849.625031\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 769\n",
            "Norm: 1707998.60, NNZs: 933, Bias: -23794.579130, T: 752851, Avg. loss: 851.582522\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 770\n",
            "Norm: 1707998.59, NNZs: 933, Bias: -23794.580513, T: 753830, Avg. loss: 850.905515\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 771\n",
            "Norm: 1707998.59, NNZs: 933, Bias: -23794.581578, T: 754809, Avg. loss: 847.642959\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 772\n",
            "Norm: 1707998.59, NNZs: 933, Bias: -23794.582908, T: 755788, Avg. loss: 849.586208\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 773\n",
            "Norm: 1707998.58, NNZs: 933, Bias: -23794.584235, T: 756767, Avg. loss: 848.917911\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 774\n",
            "Norm: 1707998.58, NNZs: 933, Bias: -23794.585308, T: 757746, Avg. loss: 845.675347\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 775\n",
            "Norm: 1707998.58, NNZs: 933, Bias: -23794.586380, T: 758725, Avg. loss: 845.040761\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 776\n",
            "Norm: 1707998.57, NNZs: 933, Bias: -23794.587723, T: 759704, Avg. loss: 846.969449\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 777\n",
            "Norm: 1707998.57, NNZs: 933, Bias: -23794.589072, T: 760683, Avg. loss: 846.307378\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 778\n",
            "Norm: 1707998.57, NNZs: 933, Bias: -23794.590384, T: 761662, Avg. loss: 845.643904\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 779\n",
            "Norm: 1707998.56, NNZs: 933, Bias: -23794.591446, T: 762641, Avg. loss: 842.434344\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 780\n",
            "Norm: 1707998.56, NNZs: 933, Bias: -23794.592763, T: 763620, Avg. loss: 844.351618\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 781\n",
            "Norm: 1707998.56, NNZs: 933, Bias: -23794.594111, T: 764599, Avg. loss: 843.689253\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 782\n",
            "Norm: 1707998.55, NNZs: 933, Bias: -23794.595153, T: 765578, Avg. loss: 840.500926\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 783\n",
            "Norm: 1707998.55, NNZs: 933, Bias: -23794.596171, T: 766557, Avg. loss: 839.879744\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 784\n",
            "Norm: 1707998.55, NNZs: 933, Bias: -23794.597473, T: 767536, Avg. loss: 841.775626\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 785\n",
            "Norm: 1707998.54, NNZs: 933, Bias: -23794.598496, T: 768515, Avg. loss: 838.607044\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 786\n",
            "Norm: 1707998.54, NNZs: 933, Bias: -23794.599813, T: 769494, Avg. loss: 840.496255\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 787\n",
            "Norm: 1707998.54, NNZs: 933, Bias: -23794.600853, T: 770473, Avg. loss: 837.340325\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 788\n",
            "Norm: 1707998.54, NNZs: 933, Bias: -23794.601906, T: 771452, Avg. loss: 836.723442\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 789\n",
            "Norm: 1707998.53, NNZs: 933, Bias: -23794.603244, T: 772431, Avg. loss: 838.602429\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 790\n",
            "Norm: 1707998.53, NNZs: 933, Bias: -23794.604574, T: 773410, Avg. loss: 837.958122\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 791\n",
            "Norm: 1707998.53, NNZs: 933, Bias: -23794.605605, T: 774389, Avg. loss: 834.825390\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 792\n",
            "Norm: 1707998.52, NNZs: 933, Bias: -23794.606625, T: 775368, Avg. loss: 834.211781\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 793\n",
            "Norm: 1707998.52, NNZs: 933, Bias: -23794.607657, T: 776347, Avg. loss: 833.601220\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 794\n",
            "Norm: 1707998.52, NNZs: 933, Bias: -23794.608956, T: 777326, Avg. loss: 835.456632\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 795\n",
            "Norm: 1707998.51, NNZs: 933, Bias: -23794.609978, T: 778305, Avg. loss: 832.353404\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 796\n",
            "Norm: 1707998.51, NNZs: 933, Bias: -23794.610984, T: 779284, Avg. loss: 831.743743\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 797\n",
            "Norm: 1707998.51, NNZs: 933, Bias: -23794.612263, T: 780263, Avg. loss: 833.590386\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 798\n",
            "Norm: 1707998.51, NNZs: 933, Bias: -23794.613277, T: 781242, Avg. loss: 830.503682\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 799\n",
            "Norm: 1707998.50, NNZs: 933, Bias: -23794.614548, T: 782221, Avg. loss: 832.344458\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 800\n",
            "Norm: 1707998.50, NNZs: 933, Bias: -23794.615567, T: 783200, Avg. loss: 829.266487\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 801\n",
            "Norm: 1707998.50, NNZs: 933, Bias: -23794.616604, T: 784179, Avg. loss: 828.664125\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 802\n",
            "Norm: 1707998.49, NNZs: 933, Bias: -23794.617628, T: 785158, Avg. loss: 828.063307\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 803\n",
            "Norm: 1707998.49, NNZs: 933, Bias: -23794.618630, T: 786137, Avg. loss: 827.462900\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 804\n",
            "Norm: 1707998.49, NNZs: 933, Bias: -23794.619635, T: 787116, Avg. loss: 826.861539\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 805\n",
            "Norm: 1707998.48, NNZs: 933, Bias: -23794.620668, T: 788095, Avg. loss: 826.263495\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 806\n",
            "Norm: 1707998.48, NNZs: 933, Bias: -23794.621980, T: 789074, Avg. loss: 828.077509\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 807\n",
            "Norm: 1707998.48, NNZs: 933, Bias: -23794.623253, T: 790053, Avg. loss: 827.454738\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 808\n",
            "Norm: 1707998.48, NNZs: 933, Bias: -23794.624274, T: 791032, Avg. loss: 824.428344\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 809\n",
            "Norm: 1707998.47, NNZs: 933, Bias: -23794.625305, T: 792011, Avg. loss: 823.831226\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 810\n",
            "Norm: 1707998.47, NNZs: 933, Bias: -23794.626562, T: 792990, Avg. loss: 825.633277\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 811\n",
            "Norm: 1707998.47, NNZs: 933, Bias: -23794.627568, T: 793969, Avg. loss: 822.623074\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 812\n",
            "Norm: 1707998.46, NNZs: 933, Bias: -23794.628863, T: 794948, Avg. loss: 824.417616\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 813\n",
            "Norm: 1707998.46, NNZs: 933, Bias: -23794.630114, T: 795927, Avg. loss: 823.801929\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 814\n",
            "Norm: 1707998.46, NNZs: 933, Bias: -23794.631126, T: 796906, Avg. loss: 820.808137\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 815\n",
            "Norm: 1707998.46, NNZs: 933, Bias: -23794.632123, T: 797885, Avg. loss: 820.221820\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 816\n",
            "Norm: 1707998.45, NNZs: 933, Bias: -23794.633364, T: 798864, Avg. loss: 821.995585\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 817\n",
            "Norm: 1707998.45, NNZs: 933, Bias: -23794.634618, T: 799843, Avg. loss: 821.385892\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 818\n",
            "Norm: 1707998.45, NNZs: 933, Bias: -23794.635631, T: 800822, Avg. loss: 818.414617\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 819\n",
            "Norm: 1707998.44, NNZs: 933, Bias: -23794.636882, T: 801801, Avg. loss: 820.183869\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 820\n",
            "Norm: 1707998.44, NNZs: 933, Bias: -23794.637881, T: 802780, Avg. loss: 817.229574\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 821\n",
            "Norm: 1707998.44, NNZs: 933, Bias: -23794.639112, T: 803759, Avg. loss: 818.989809\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 822\n",
            "Norm: 1707998.43, NNZs: 933, Bias: -23794.640108, T: 804738, Avg. loss: 816.044355\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 823\n",
            "Norm: 1707998.43, NNZs: 933, Bias: -23794.641351, T: 805717, Avg. loss: 817.797850\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 824\n",
            "Norm: 1707998.43, NNZs: 933, Bias: -23794.642596, T: 806696, Avg. loss: 817.189999\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 825\n",
            "Norm: 1707998.43, NNZs: 933, Bias: -23794.643812, T: 807675, Avg. loss: 816.584726\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 826\n",
            "Norm: 1707998.42, NNZs: 933, Bias: -23794.645064, T: 808654, Avg. loss: 815.984296\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 827\n",
            "Norm: 1707998.42, NNZs: 933, Bias: -23794.646281, T: 809633, Avg. loss: 815.382866\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 828\n",
            "Norm: 1707998.42, NNZs: 933, Bias: -23794.647267, T: 810612, Avg. loss: 812.474018\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 829\n",
            "Norm: 1707998.41, NNZs: 933, Bias: -23794.648504, T: 811591, Avg. loss: 814.209608\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 830\n",
            "Norm: 1707998.41, NNZs: 933, Bias: -23794.649720, T: 812570, Avg. loss: 813.614698\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 831\n",
            "Norm: 1707998.41, NNZs: 933, Bias: -23794.650671, T: 813549, Avg. loss: 810.719750\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 832\n",
            "Norm: 1707998.40, NNZs: 933, Bias: -23794.651870, T: 814528, Avg. loss: 812.441198\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 833\n",
            "Norm: 1707998.40, NNZs: 933, Bias: -23794.652851, T: 815507, Avg. loss: 809.560543\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 834\n",
            "Norm: 1707998.40, NNZs: 933, Bias: -23794.653797, T: 816486, Avg. loss: 808.995588\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 835\n",
            "Norm: 1707998.40, NNZs: 933, Bias: -23794.654991, T: 817465, Avg. loss: 810.706906\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 836\n",
            "Norm: 1707998.39, NNZs: 933, Bias: -23794.655950, T: 818444, Avg. loss: 807.843828\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 837\n",
            "Norm: 1707998.39, NNZs: 933, Bias: -23794.656915, T: 819423, Avg. loss: 807.282013\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 838\n",
            "Norm: 1707998.39, NNZs: 933, Bias: -23794.657885, T: 820402, Avg. loss: 806.719909\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 839\n",
            "Norm: 1707998.39, NNZs: 933, Bias: -23794.658837, T: 821381, Avg. loss: 806.161185\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 840\n",
            "Norm: 1707998.38, NNZs: 933, Bias: -23794.660029, T: 822360, Avg. loss: 807.859461\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 841\n",
            "Norm: 1707998.38, NNZs: 933, Bias: -23794.661259, T: 823339, Avg. loss: 807.273640\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 842\n",
            "Norm: 1707998.38, NNZs: 933, Bias: -23794.662461, T: 824318, Avg. loss: 806.691551\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 843\n",
            "Norm: 1707998.37, NNZs: 933, Bias: -23794.663652, T: 825297, Avg. loss: 806.108727\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 844\n",
            "Norm: 1707998.37, NNZs: 933, Bias: -23794.664838, T: 826276, Avg. loss: 805.523373\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 845\n",
            "Norm: 1707998.37, NNZs: 933, Bias: -23794.666037, T: 827255, Avg. loss: 804.944969\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 846\n",
            "Norm: 1707998.37, NNZs: 933, Bias: -23794.666966, T: 828234, Avg. loss: 802.134010\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 847\n",
            "Norm: 1707998.36, NNZs: 933, Bias: -23794.667929, T: 829213, Avg. loss: 801.580861\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 848\n",
            "Norm: 1707998.36, NNZs: 933, Bias: -23794.668874, T: 830192, Avg. loss: 801.033108\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 849\n",
            "Norm: 1707998.36, NNZs: 933, Bias: -23794.670053, T: 831171, Avg. loss: 802.702906\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 850\n",
            "Norm: 1707998.35, NNZs: 933, Bias: -23794.671228, T: 832150, Avg. loss: 802.127188\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 851\n",
            "Norm: 1707998.35, NNZs: 933, Bias: -23794.672158, T: 833129, Avg. loss: 799.341555\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 852\n",
            "Norm: 1707998.35, NNZs: 933, Bias: -23794.673319, T: 834108, Avg. loss: 801.005125\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 853\n",
            "Norm: 1707998.35, NNZs: 933, Bias: -23794.674494, T: 835087, Avg. loss: 800.431413\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 854\n",
            "Norm: 1707998.34, NNZs: 933, Bias: -23794.675447, T: 836066, Avg. loss: 797.662745\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 855\n",
            "Norm: 1707998.34, NNZs: 933, Bias: -23794.676394, T: 837045, Avg. loss: 797.119346\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 856\n",
            "Norm: 1707998.34, NNZs: 933, Bias: -23794.677565, T: 838024, Avg. loss: 798.767595\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 857\n",
            "Norm: 1707998.34, NNZs: 933, Bias: -23794.678495, T: 839003, Avg. loss: 796.010886\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 858\n",
            "Norm: 1707998.33, NNZs: 933, Bias: -23794.679664, T: 839982, Avg. loss: 797.656794\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 859\n",
            "Norm: 1707998.33, NNZs: 933, Bias: -23794.680600, T: 840961, Avg. loss: 794.912681\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 860\n",
            "Norm: 1707998.33, NNZs: 933, Bias: -23794.681536, T: 841940, Avg. loss: 794.372058\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 861\n",
            "Norm: 1707998.32, NNZs: 933, Bias: -23794.682449, T: 842919, Avg. loss: 793.834023\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 862\n",
            "Norm: 1707998.32, NNZs: 933, Bias: -23794.683397, T: 843898, Avg. loss: 793.297055\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 863\n",
            "Norm: 1707998.32, NNZs: 933, Bias: -23794.684338, T: 844877, Avg. loss: 792.760406\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 864\n",
            "Norm: 1707998.32, NNZs: 933, Bias: -23794.685489, T: 845856, Avg. loss: 794.385397\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 865\n",
            "Norm: 1707998.31, NNZs: 933, Bias: -23794.686419, T: 846835, Avg. loss: 791.670897\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 866\n",
            "Norm: 1707998.31, NNZs: 933, Bias: -23794.687368, T: 847814, Avg. loss: 791.135553\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 867\n",
            "Norm: 1707998.31, NNZs: 933, Bias: -23794.688317, T: 848793, Avg. loss: 790.603811\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 868\n",
            "Norm: 1707998.31, NNZs: 933, Bias: -23794.689264, T: 849772, Avg. loss: 790.073842\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 869\n",
            "Norm: 1707998.30, NNZs: 933, Bias: -23794.690402, T: 850751, Avg. loss: 791.678537\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 870\n",
            "Norm: 1707998.30, NNZs: 933, Bias: -23794.691547, T: 851730, Avg. loss: 791.129097\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 871\n",
            "Norm: 1707998.30, NNZs: 933, Bias: -23794.692714, T: 852709, Avg. loss: 790.573213\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 872\n",
            "Norm: 1707998.29, NNZs: 933, Bias: -23794.693880, T: 853688, Avg. loss: 790.022574\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 873\n",
            "Norm: 1707998.29, NNZs: 933, Bias: -23794.695009, T: 854667, Avg. loss: 789.472839\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 874\n",
            "Norm: 1707998.29, NNZs: 933, Bias: -23794.696135, T: 855646, Avg. loss: 788.921958\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 875\n",
            "Norm: 1707998.29, NNZs: 933, Bias: -23794.697263, T: 856625, Avg. loss: 788.372937\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 876\n",
            "Norm: 1707998.28, NNZs: 933, Bias: -23794.698396, T: 857604, Avg. loss: 787.825977\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 877\n",
            "Norm: 1707998.28, NNZs: 933, Bias: -23794.699529, T: 858583, Avg. loss: 787.278921\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 878\n",
            "Norm: 1707998.28, NNZs: 933, Bias: -23794.700452, T: 859562, Avg. loss: 784.629976\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 879\n",
            "Norm: 1707998.28, NNZs: 933, Bias: -23794.701375, T: 860541, Avg. loss: 784.111003\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 880\n",
            "Norm: 1707998.27, NNZs: 933, Bias: -23794.702258, T: 861520, Avg. loss: 783.592226\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 881\n",
            "Norm: 1707998.27, NNZs: 933, Bias: -23794.703176, T: 862499, Avg. loss: 783.072026\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 882\n",
            "Norm: 1707998.27, NNZs: 933, Bias: -23794.704073, T: 863478, Avg. loss: 782.554013\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 883\n",
            "Norm: 1707998.27, NNZs: 933, Bias: -23794.705183, T: 864457, Avg. loss: 784.123859\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 884\n",
            "Norm: 1707998.26, NNZs: 933, Bias: -23794.706290, T: 865436, Avg. loss: 783.581011\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 885\n",
            "Norm: 1707998.26, NNZs: 933, Bias: -23794.707180, T: 866415, Avg. loss: 780.965737\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 886\n",
            "Norm: 1707998.26, NNZs: 933, Bias: -23794.708096, T: 867394, Avg. loss: 780.452189\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 887\n",
            "Norm: 1707998.26, NNZs: 933, Bias: -23794.709227, T: 868373, Avg. loss: 782.009205\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 888\n",
            "Norm: 1707998.25, NNZs: 933, Bias: -23794.710344, T: 869352, Avg. loss: 781.476266\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 889\n",
            "Norm: 1707998.25, NNZs: 933, Bias: -23794.711231, T: 870331, Avg. loss: 778.877608\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 890\n",
            "Norm: 1707998.25, NNZs: 933, Bias: -23794.712338, T: 871310, Avg. loss: 780.424638\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 891\n",
            "Norm: 1707998.24, NNZs: 933, Bias: -23794.713454, T: 872289, Avg. loss: 779.893522\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 892\n",
            "Norm: 1707998.24, NNZs: 933, Bias: -23794.714344, T: 873268, Avg. loss: 777.308170\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 893\n",
            "Norm: 1707998.24, NNZs: 933, Bias: -23794.715253, T: 874247, Avg. loss: 776.798389\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 894\n",
            "Norm: 1707998.24, NNZs: 933, Bias: -23794.716161, T: 875226, Avg. loss: 776.293458\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 895\n",
            "Norm: 1707998.23, NNZs: 933, Bias: -23794.717261, T: 876205, Avg. loss: 777.830194\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 896\n",
            "Norm: 1707998.23, NNZs: 933, Bias: -23794.718355, T: 877184, Avg. loss: 777.300693\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 897\n",
            "Norm: 1707998.23, NNZs: 933, Bias: -23794.719441, T: 878163, Avg. loss: 776.772597\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 898\n",
            "Norm: 1707998.23, NNZs: 933, Bias: -23794.720333, T: 879142, Avg. loss: 774.215720\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 899\n",
            "Norm: 1707998.22, NNZs: 933, Bias: -23794.721425, T: 880121, Avg. loss: 775.743500\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 900\n",
            "Norm: 1707998.22, NNZs: 933, Bias: -23794.722508, T: 881100, Avg. loss: 775.218812\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 901\n",
            "Norm: 1707998.22, NNZs: 933, Bias: -23794.723603, T: 882079, Avg. loss: 774.698323\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 902\n",
            "Norm: 1707998.22, NNZs: 933, Bias: -23794.724457, T: 883058, Avg. loss: 772.158156\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 903\n",
            "Norm: 1707998.21, NNZs: 933, Bias: -23794.725345, T: 884037, Avg. loss: 771.656919\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 904\n",
            "Norm: 1707998.21, NNZs: 933, Bias: -23794.726445, T: 885016, Avg. loss: 773.170292\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 905\n",
            "Norm: 1707998.21, NNZs: 933, Bias: -23794.727332, T: 885995, Avg. loss: 770.646224\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 906\n",
            "Norm: 1707998.21, NNZs: 933, Bias: -23794.728395, T: 886974, Avg. loss: 772.149243\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 907\n",
            "Norm: 1707998.20, NNZs: 933, Bias: -23794.729282, T: 887953, Avg. loss: 769.634084\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 908\n",
            "Norm: 1707998.20, NNZs: 933, Bias: -23794.730166, T: 888932, Avg. loss: 769.142138\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 909\n",
            "Norm: 1707998.20, NNZs: 933, Bias: -23794.731225, T: 889911, Avg. loss: 770.638818\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 910\n",
            "Norm: 1707998.20, NNZs: 933, Bias: -23794.732112, T: 890890, Avg. loss: 768.135439\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 911\n",
            "Norm: 1707998.19, NNZs: 933, Bias: -23794.733171, T: 891869, Avg. loss: 769.629732\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 912\n",
            "Norm: 1707998.19, NNZs: 933, Bias: -23794.734019, T: 892848, Avg. loss: 767.134179\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 913\n",
            "Norm: 1707998.19, NNZs: 933, Bias: -23794.734877, T: 893827, Avg. loss: 766.643347\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 914\n",
            "Norm: 1707998.19, NNZs: 933, Bias: -23794.735726, T: 894806, Avg. loss: 766.155315\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 915\n",
            "Norm: 1707998.18, NNZs: 933, Bias: -23794.736583, T: 895785, Avg. loss: 765.664803\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 916\n",
            "Norm: 1707998.18, NNZs: 933, Bias: -23794.737450, T: 896764, Avg. loss: 765.176338\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 917\n",
            "Norm: 1707998.18, NNZs: 933, Bias: -23794.738542, T: 897743, Avg. loss: 766.653796\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 918\n",
            "Norm: 1707998.18, NNZs: 933, Bias: -23794.739618, T: 898722, Avg. loss: 766.146685\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 919\n",
            "Norm: 1707998.17, NNZs: 933, Bias: -23794.740694, T: 899701, Avg. loss: 765.637044\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 920\n",
            "Norm: 1707998.17, NNZs: 933, Bias: -23794.741573, T: 900680, Avg. loss: 763.180471\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 921\n",
            "Norm: 1707998.17, NNZs: 933, Bias: -23794.742444, T: 901659, Avg. loss: 762.697086\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 922\n",
            "Norm: 1707998.17, NNZs: 933, Bias: -23794.743304, T: 902638, Avg. loss: 762.212457\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 923\n",
            "Norm: 1707998.16, NNZs: 933, Bias: -23794.744349, T: 903617, Avg. loss: 763.675535\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 924\n",
            "Norm: 1707998.16, NNZs: 933, Bias: -23794.745212, T: 904596, Avg. loss: 761.231414\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 925\n",
            "Norm: 1707998.16, NNZs: 933, Bias: -23794.746267, T: 905575, Avg. loss: 762.686351\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 926\n",
            "Norm: 1707998.16, NNZs: 933, Bias: -23794.747112, T: 906554, Avg. loss: 760.253146\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 927\n",
            "Norm: 1707998.15, NNZs: 933, Bias: -23794.748164, T: 907533, Avg. loss: 761.705866\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 928\n",
            "Norm: 1707998.15, NNZs: 933, Bias: -23794.749022, T: 908512, Avg. loss: 759.281350\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 929\n",
            "Norm: 1707998.15, NNZs: 933, Bias: -23794.750077, T: 909491, Avg. loss: 760.726000\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 930\n",
            "Norm: 1707998.15, NNZs: 933, Bias: -23794.751121, T: 910470, Avg. loss: 760.228523\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 931\n",
            "Norm: 1707998.14, NNZs: 933, Bias: -23794.751981, T: 911449, Avg. loss: 757.816188\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 932\n",
            "Norm: 1707998.14, NNZs: 933, Bias: -23794.752831, T: 912428, Avg. loss: 757.342423\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 933\n",
            "Norm: 1707998.14, NNZs: 933, Bias: -23794.753857, T: 913407, Avg. loss: 758.776793\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 934\n",
            "Norm: 1707998.14, NNZs: 933, Bias: -23794.754720, T: 914386, Avg. loss: 756.377853\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 935\n",
            "Norm: 1707998.14, NNZs: 933, Bias: -23794.755555, T: 915365, Avg. loss: 755.904999\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 936\n",
            "Norm: 1707998.13, NNZs: 933, Bias: -23794.756385, T: 916344, Avg. loss: 755.433312\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 937\n",
            "Norm: 1707998.13, NNZs: 933, Bias: -23794.757246, T: 917323, Avg. loss: 754.961431\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 938\n",
            "Norm: 1707998.13, NNZs: 933, Bias: -23794.758272, T: 918302, Avg. loss: 756.383994\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 939\n",
            "Norm: 1707998.13, NNZs: 933, Bias: -23794.759136, T: 919281, Avg. loss: 754.005611\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 940\n",
            "Norm: 1707998.12, NNZs: 933, Bias: -23794.760187, T: 920260, Avg. loss: 755.424224\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 941\n",
            "Norm: 1707998.12, NNZs: 933, Bias: -23794.761040, T: 921239, Avg. loss: 753.052015\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 942\n",
            "Norm: 1707998.12, NNZs: 933, Bias: -23794.761897, T: 922218, Avg. loss: 752.585062\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 943\n",
            "Norm: 1707998.12, NNZs: 933, Bias: -23794.762746, T: 923197, Avg. loss: 752.118025\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 944\n",
            "Norm: 1707998.11, NNZs: 933, Bias: -23794.763763, T: 924176, Avg. loss: 753.528447\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 945\n",
            "Norm: 1707998.11, NNZs: 933, Bias: -23794.764597, T: 925155, Avg. loss: 751.169872\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 946\n",
            "Norm: 1707998.11, NNZs: 933, Bias: -23794.765610, T: 926134, Avg. loss: 752.572369\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 947\n",
            "Norm: 1707998.11, NNZs: 933, Bias: -23794.766457, T: 927113, Avg. loss: 750.223903\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 948\n",
            "Norm: 1707998.10, NNZs: 933, Bias: -23794.767493, T: 928092, Avg. loss: 751.625555\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 949\n",
            "Norm: 1707998.10, NNZs: 933, Bias: -23794.768503, T: 929071, Avg. loss: 751.143465\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 950\n",
            "Norm: 1707998.10, NNZs: 933, Bias: -23794.769351, T: 930050, Avg. loss: 748.805928\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 951\n",
            "Norm: 1707998.10, NNZs: 933, Bias: -23794.770373, T: 931029, Avg. loss: 750.198981\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 952\n",
            "Norm: 1707998.10, NNZs: 933, Bias: -23794.771219, T: 932008, Avg. loss: 747.869323\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 953\n",
            "Norm: 1707998.09, NNZs: 933, Bias: -23794.772234, T: 932987, Avg. loss: 749.258831\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 954\n",
            "Norm: 1707998.09, NNZs: 933, Bias: -23794.773063, T: 933966, Avg. loss: 746.937740\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 955\n",
            "Norm: 1707998.09, NNZs: 933, Bias: -23794.774085, T: 934945, Avg. loss: 748.323693\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 956\n",
            "Norm: 1707998.09, NNZs: 933, Bias: -23794.774919, T: 935924, Avg. loss: 746.008514\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 957\n",
            "Norm: 1707998.08, NNZs: 933, Bias: -23794.775913, T: 936903, Avg. loss: 747.386756\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 958\n",
            "Norm: 1707998.08, NNZs: 933, Bias: -23794.776750, T: 937882, Avg. loss: 745.080992\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 959\n",
            "Norm: 1707998.08, NNZs: 933, Bias: -23794.777743, T: 938861, Avg. loss: 746.459425\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 960\n",
            "Norm: 1707998.08, NNZs: 933, Bias: -23794.778748, T: 939840, Avg. loss: 745.984975\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 961\n",
            "Norm: 1707998.07, NNZs: 933, Bias: -23794.779743, T: 940819, Avg. loss: 745.513949\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 962\n",
            "Norm: 1707998.07, NNZs: 933, Bias: -23794.780726, T: 941798, Avg. loss: 745.041009\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 963\n",
            "Norm: 1707998.07, NNZs: 933, Bias: -23794.781719, T: 942777, Avg. loss: 744.571879\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 964\n",
            "Norm: 1707998.07, NNZs: 933, Bias: -23794.782696, T: 943756, Avg. loss: 744.102668\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 965\n",
            "Norm: 1707998.06, NNZs: 933, Bias: -23794.783683, T: 944735, Avg. loss: 743.632644\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 966\n",
            "Norm: 1707998.06, NNZs: 933, Bias: -23794.784507, T: 945714, Avg. loss: 741.359282\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 967\n",
            "Norm: 1707998.06, NNZs: 933, Bias: -23794.785481, T: 946693, Avg. loss: 742.717508\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 968\n",
            "Norm: 1707998.06, NNZs: 933, Bias: -23794.786287, T: 947672, Avg. loss: 740.448544\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 969\n",
            "Norm: 1707998.06, NNZs: 933, Bias: -23794.787111, T: 948651, Avg. loss: 740.003754\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 970\n",
            "Norm: 1707998.05, NNZs: 933, Bias: -23794.788095, T: 949630, Avg. loss: 741.351553\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 971\n",
            "Norm: 1707998.05, NNZs: 933, Bias: -23794.788906, T: 950609, Avg. loss: 739.097784\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 972\n",
            "Norm: 1707998.05, NNZs: 933, Bias: -23794.789725, T: 951588, Avg. loss: 738.652585\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 973\n",
            "Norm: 1707998.05, NNZs: 933, Bias: -23794.790717, T: 952567, Avg. loss: 739.997350\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 974\n",
            "Norm: 1707998.04, NNZs: 933, Bias: -23794.791513, T: 953546, Avg. loss: 737.751880\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 975\n",
            "Norm: 1707998.04, NNZs: 933, Bias: -23794.792325, T: 954525, Avg. loss: 737.309602\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 976\n",
            "Norm: 1707998.04, NNZs: 933, Bias: -23794.793303, T: 955504, Avg. loss: 738.647935\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 977\n",
            "Norm: 1707998.04, NNZs: 933, Bias: -23794.794108, T: 956483, Avg. loss: 736.411904\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 978\n",
            "Norm: 1707998.03, NNZs: 933, Bias: -23794.794894, T: 957462, Avg. loss: 735.972687\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 979\n",
            "Norm: 1707998.03, NNZs: 933, Bias: -23794.795711, T: 958441, Avg. loss: 735.533575\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 980\n",
            "Norm: 1707998.03, NNZs: 933, Bias: -23794.796502, T: 959420, Avg. loss: 735.093950\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 981\n",
            "Norm: 1707998.03, NNZs: 933, Bias: -23794.797308, T: 960399, Avg. loss: 734.655058\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 982\n",
            "Norm: 1707998.03, NNZs: 933, Bias: -23794.798293, T: 961378, Avg. loss: 735.979283\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 983\n",
            "Norm: 1707998.02, NNZs: 933, Bias: -23794.799266, T: 962357, Avg. loss: 735.524564\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 984\n",
            "Norm: 1707998.02, NNZs: 933, Bias: -23794.800057, T: 963336, Avg. loss: 733.316739\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 985\n",
            "Norm: 1707998.02, NNZs: 933, Bias: -23794.801020, T: 964315, Avg. loss: 734.633702\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 986\n",
            "Norm: 1707998.02, NNZs: 933, Bias: -23794.801987, T: 965294, Avg. loss: 734.181600\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 987\n",
            "Norm: 1707998.01, NNZs: 933, Bias: -23794.802967, T: 966273, Avg. loss: 733.729565\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 988\n",
            "Norm: 1707998.01, NNZs: 933, Bias: -23794.803915, T: 967252, Avg. loss: 733.276727\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 989\n",
            "Norm: 1707998.01, NNZs: 933, Bias: -23794.804860, T: 968231, Avg. loss: 732.829568\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 990\n",
            "Norm: 1707998.01, NNZs: 933, Bias: -23794.805810, T: 969210, Avg. loss: 732.378293\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 991\n",
            "Norm: 1707998.01, NNZs: 933, Bias: -23794.806588, T: 970189, Avg. loss: 730.195023\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 992\n",
            "Norm: 1707998.00, NNZs: 933, Bias: -23794.807545, T: 971168, Avg. loss: 731.496457\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 993\n",
            "Norm: 1707998.00, NNZs: 933, Bias: -23794.808342, T: 972147, Avg. loss: 729.320095\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 994\n",
            "Norm: 1707998.00, NNZs: 933, Bias: -23794.809283, T: 973126, Avg. loss: 730.616899\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 995\n",
            "Norm: 1707998.00, NNZs: 933, Bias: -23794.810233, T: 974105, Avg. loss: 730.173972\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 996\n",
            "Norm: 1707997.99, NNZs: 933, Bias: -23794.811165, T: 975084, Avg. loss: 729.729460\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 997\n",
            "Norm: 1707997.99, NNZs: 933, Bias: -23794.811947, T: 976063, Avg. loss: 727.565933\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 998\n",
            "Norm: 1707997.99, NNZs: 933, Bias: -23794.812721, T: 977042, Avg. loss: 727.140162\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 999\n",
            "Norm: 1707997.99, NNZs: 933, Bias: -23794.813503, T: 978021, Avg. loss: 726.714888\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 1000\n",
            "Norm: 1707997.99, NNZs: 933, Bias: -23794.814289, T: 979000, Avg. loss: 726.288734\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 1\n",
            "Norm: 205.62, NNZs: 647, Bias: -1.544647, T: 979, Avg. loss: 10.306579\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 203.07, NNZs: 589, Bias: -1.227127, T: 1958, Avg. loss: 2.099705\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 202.83, NNZs: 549, Bias: -1.121938, T: 2937, Avg. loss: 0.519277\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 202.76, NNZs: 518, Bias: -1.015746, T: 3916, Avg. loss: 0.376670\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 202.72, NNZs: 500, Bias: -0.945573, T: 4895, Avg. loss: 0.309359\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 202.70, NNZs: 479, Bias: -0.919133, T: 5874, Avg. loss: 0.271697\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 202.69, NNZs: 477, Bias: -0.890794, T: 6853, Avg. loss: 0.254534\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 202.70, NNZs: 474, Bias: -0.852240, T: 7832, Avg. loss: 0.246691\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 202.70, NNZs: 472, Bias: -0.836137, T: 8811, Avg. loss: 0.231512\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 202.70, NNZs: 469, Bias: -0.814320, T: 9790, Avg. loss: 0.227786\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 202.71, NNZs: 464, Bias: -0.799028, T: 10769, Avg. loss: 0.223855\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 202.71, NNZs: 463, Bias: -0.789592, T: 11748, Avg. loss: 0.217707\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 202.72, NNZs: 458, Bias: -0.774023, T: 12727, Avg. loss: 0.217797\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 202.73, NNZs: 458, Bias: -0.763553, T: 13706, Avg. loss: 0.214505\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 202.73, NNZs: 456, Bias: -0.753148, T: 14685, Avg. loss: 0.213916\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 202.74, NNZs: 452, Bias: -0.741833, T: 15664, Avg. loss: 0.212928\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 202.75, NNZs: 444, Bias: -0.733880, T: 16643, Avg. loss: 0.211256\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 202.76, NNZs: 435, Bias: -0.724733, T: 17622, Avg. loss: 0.211217\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 202.76, NNZs: 436, Bias: -0.716867, T: 18601, Avg. loss: 0.210815\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 202.77, NNZs: 435, Bias: -0.708250, T: 19580, Avg. loss: 0.210631\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 202.78, NNZs: 420, Bias: -0.701013, T: 20559, Avg. loss: 0.210446\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 202.78, NNZs: 419, Bias: -0.694868, T: 21538, Avg. loss: 0.209223\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 202.79, NNZs: 419, Bias: -0.690889, T: 22517, Avg. loss: 0.207399\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 202.80, NNZs: 417, Bias: -0.684579, T: 23496, Avg. loss: 0.209458\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 202.80, NNZs: 415, Bias: -0.680127, T: 24475, Avg. loss: 0.208457\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 202.81, NNZs: 413, Bias: -0.676691, T: 25454, Avg. loss: 0.207004\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 202.82, NNZs: 411, Bias: -0.670713, T: 26433, Avg. loss: 0.209245\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 202.82, NNZs: 411, Bias: -0.667496, T: 27412, Avg. loss: 0.207711\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 28 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 1238.73, NNZs: 799, Bias: -1.793368, T: 979, Avg. loss: 433.808054\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1154.36, NNZs: 870, Bias: 2.143172, T: 1958, Avg. loss: 249.874610\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1145.39, NNZs: 873, Bias: -1.872125, T: 2937, Avg. loss: 24.056650\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1143.86, NNZs: 877, Bias: -1.810079, T: 3916, Avg. loss: 5.079525\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1143.23, NNZs: 868, Bias: -1.782341, T: 4895, Avg. loss: 2.568131\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1142.87, NNZs: 862, Bias: -1.729783, T: 5874, Avg. loss: 1.746207\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1142.62, NNZs: 856, Bias: -1.753499, T: 6853, Avg. loss: 1.327347\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1142.45, NNZs: 854, Bias: -1.749931, T: 7832, Avg. loss: 1.072390\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1142.32, NNZs: 849, Bias: -1.762549, T: 8811, Avg. loss: 0.904132\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1142.23, NNZs: 842, Bias: -1.772928, T: 9790, Avg. loss: 0.786348\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1142.15, NNZs: 842, Bias: -1.775568, T: 10769, Avg. loss: 0.700475\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1142.08, NNZs: 840, Bias: -1.766274, T: 11748, Avg. loss: 0.632738\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 1142.03, NNZs: 835, Bias: -1.765590, T: 12727, Avg. loss: 0.581147\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 1141.98, NNZs: 835, Bias: -1.771618, T: 13706, Avg. loss: 0.540225\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 1141.94, NNZs: 833, Bias: -1.788440, T: 14685, Avg. loss: 0.501990\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 1141.91, NNZs: 832, Bias: -1.788266, T: 15664, Avg. loss: 0.470614\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 1141.88, NNZs: 829, Bias: -1.791706, T: 16643, Avg. loss: 0.445307\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 1141.85, NNZs: 829, Bias: -1.798044, T: 17622, Avg. loss: 0.422668\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 1141.82, NNZs: 822, Bias: -1.803173, T: 18601, Avg. loss: 0.402279\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 1141.80, NNZs: 819, Bias: -1.809854, T: 19580, Avg. loss: 0.384066\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 1141.78, NNZs: 818, Bias: -1.810715, T: 20559, Avg. loss: 0.367077\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 1141.76, NNZs: 815, Bias: -1.813174, T: 21538, Avg. loss: 0.352560\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 1141.75, NNZs: 814, Bias: -1.818035, T: 22517, Avg. loss: 0.340302\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 1141.73, NNZs: 812, Bias: -1.822143, T: 23496, Avg. loss: 0.329285\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 1141.72, NNZs: 807, Bias: -1.823451, T: 24475, Avg. loss: 0.317144\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 1141.70, NNZs: 807, Bias: -1.825682, T: 25454, Avg. loss: 0.308345\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 1141.69, NNZs: 805, Bias: -1.831608, T: 26433, Avg. loss: 0.299695\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 1141.68, NNZs: 801, Bias: -1.837562, T: 27412, Avg. loss: 0.291687\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 1141.67, NNZs: 800, Bias: -1.839832, T: 28391, Avg. loss: 0.282999\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 1141.66, NNZs: 800, Bias: -1.843246, T: 29370, Avg. loss: 0.276422\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 1141.65, NNZs: 800, Bias: -1.845994, T: 30349, Avg. loss: 0.270132\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 1141.64, NNZs: 798, Bias: -1.848014, T: 31328, Avg. loss: 0.263743\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 1141.63, NNZs: 798, Bias: -1.851129, T: 32307, Avg. loss: 0.257600\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 1141.63, NNZs: 797, Bias: -1.854102, T: 33286, Avg. loss: 0.252610\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 1141.62, NNZs: 796, Bias: -1.856655, T: 34265, Avg. loss: 0.247226\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 1141.61, NNZs: 796, Bias: -1.859240, T: 35244, Avg. loss: 0.242746\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 1141.61, NNZs: 796, Bias: -1.861523, T: 36223, Avg. loss: 0.237722\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 1141.60, NNZs: 796, Bias: -1.864184, T: 37202, Avg. loss: 0.234005\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 1141.59, NNZs: 796, Bias: -1.867581, T: 38181, Avg. loss: 0.229781\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 1141.59, NNZs: 796, Bias: -1.870297, T: 39160, Avg. loss: 0.226069\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 1141.58, NNZs: 795, Bias: -1.872847, T: 40139, Avg. loss: 0.222159\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 1141.58, NNZs: 795, Bias: -1.874941, T: 41118, Avg. loss: 0.218612\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 1141.57, NNZs: 794, Bias: -1.876955, T: 42097, Avg. loss: 0.215263\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 1141.57, NNZs: 791, Bias: -1.879067, T: 43076, Avg. loss: 0.212638\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 1141.56, NNZs: 789, Bias: -1.880877, T: 44055, Avg. loss: 0.209104\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 1141.56, NNZs: 789, Bias: -1.883279, T: 45034, Avg. loss: 0.206556\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 1141.55, NNZs: 789, Bias: -1.885070, T: 46013, Avg. loss: 0.203669\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 1141.55, NNZs: 789, Bias: -1.886569, T: 46992, Avg. loss: 0.200744\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 1141.55, NNZs: 786, Bias: -1.888389, T: 47971, Avg. loss: 0.198198\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 1141.54, NNZs: 785, Bias: -1.890477, T: 48950, Avg. loss: 0.196365\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 1141.54, NNZs: 784, Bias: -1.891962, T: 49929, Avg. loss: 0.193627\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 1141.54, NNZs: 784, Bias: -1.893816, T: 50908, Avg. loss: 0.191573\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 1141.53, NNZs: 784, Bias: -1.895644, T: 51887, Avg. loss: 0.189083\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 1141.53, NNZs: 784, Bias: -1.897516, T: 52866, Avg. loss: 0.187388\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 1141.53, NNZs: 784, Bias: -1.899045, T: 53845, Avg. loss: 0.184879\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 1141.52, NNZs: 784, Bias: -1.901130, T: 54824, Avg. loss: 0.183381\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 1141.52, NNZs: 784, Bias: -1.902258, T: 55803, Avg. loss: 0.181255\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 1141.52, NNZs: 784, Bias: -1.903409, T: 56782, Avg. loss: 0.179293\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 1141.52, NNZs: 784, Bias: -1.905036, T: 57761, Avg. loss: 0.177670\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 1141.51, NNZs: 784, Bias: -1.906445, T: 58740, Avg. loss: 0.175711\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 1141.51, NNZs: 783, Bias: -1.907737, T: 59719, Avg. loss: 0.174408\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 1141.51, NNZs: 783, Bias: -1.908983, T: 60698, Avg. loss: 0.172548\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 1141.51, NNZs: 783, Bias: -1.910616, T: 61677, Avg. loss: 0.171309\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 1141.50, NNZs: 783, Bias: -1.912018, T: 62656, Avg. loss: 0.169779\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 1141.50, NNZs: 782, Bias: -1.913430, T: 63635, Avg. loss: 0.168044\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 1141.50, NNZs: 782, Bias: -1.915055, T: 64614, Avg. loss: 0.166959\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 1141.50, NNZs: 782, Bias: -1.916403, T: 65593, Avg. loss: 0.165307\n",
            "Total training time: 0.08 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 68\n",
            "Norm: 1141.49, NNZs: 782, Bias: -1.917740, T: 66572, Avg. loss: 0.163934\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 1141.49, NNZs: 782, Bias: -1.918965, T: 67551, Avg. loss: 0.162550\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 1141.49, NNZs: 782, Bias: -1.920021, T: 68530, Avg. loss: 0.161405\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 1141.49, NNZs: 782, Bias: -1.921152, T: 69509, Avg. loss: 0.160239\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 1141.49, NNZs: 782, Bias: -1.922351, T: 70488, Avg. loss: 0.159197\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 1141.49, NNZs: 782, Bias: -1.923487, T: 71467, Avg. loss: 0.157879\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 1141.48, NNZs: 780, Bias: -1.924559, T: 72446, Avg. loss: 0.156649\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 1141.48, NNZs: 779, Bias: -1.925608, T: 73425, Avg. loss: 0.155576\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 1141.48, NNZs: 779, Bias: -1.926697, T: 74404, Avg. loss: 0.154571\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 1141.48, NNZs: 779, Bias: -1.927597, T: 75383, Avg. loss: 0.153581\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 1141.48, NNZs: 779, Bias: -1.928385, T: 76362, Avg. loss: 0.152343\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 1141.48, NNZs: 779, Bias: -1.929388, T: 77341, Avg. loss: 0.151506\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 1141.47, NNZs: 778, Bias: -1.930296, T: 78320, Avg. loss: 0.150737\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 1141.47, NNZs: 774, Bias: -1.931161, T: 79299, Avg. loss: 0.149683\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 1141.47, NNZs: 774, Bias: -1.932249, T: 80278, Avg. loss: 0.148758\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 1141.47, NNZs: 774, Bias: -1.933248, T: 81257, Avg. loss: 0.148033\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 1141.47, NNZs: 774, Bias: -1.934247, T: 82236, Avg. loss: 0.146914\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 1141.47, NNZs: 773, Bias: -1.935184, T: 83215, Avg. loss: 0.146112\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 1141.47, NNZs: 772, Bias: -1.936025, T: 84194, Avg. loss: 0.145221\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 1141.46, NNZs: 772, Bias: -1.936974, T: 85173, Avg. loss: 0.144578\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 1141.46, NNZs: 772, Bias: -1.937942, T: 86152, Avg. loss: 0.143801\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 1141.46, NNZs: 772, Bias: -1.938768, T: 87131, Avg. loss: 0.142854\n",
            "Total training time: 0.11 seconds.\n",
            "Convergence after 89 epochs took 0.11 seconds\n",
            "-- Epoch 1\n",
            "Norm: 215209.05, NNZs: 762, Bias: -809.549850, T: 979, Avg. loss: 5676815.362861\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 251181.92, NNZs: 939, Bias: -2161.118133, T: 1958, Avg. loss: 15002048.651824\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 246603.37, NNZs: 954, Bias: -2554.022755, T: 2937, Avg. loss: 3295927.404276\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 245948.75, NNZs: 955, Bias: -2751.538484, T: 3916, Avg. loss: 282865.438068\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 245793.67, NNZs: 948, Bias: -2836.771713, T: 4895, Avg. loss: 43122.819895\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 245743.02, NNZs: 946, Bias: -2868.216125, T: 5874, Avg. loss: 12083.593786\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 245723.27, NNZs: 946, Bias: -2880.981105, T: 6853, Avg. loss: 4627.906716\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 245712.31, NNZs: 946, Bias: -2888.597697, T: 7832, Avg. loss: 2239.876353\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 245707.51, NNZs: 943, Bias: -2891.821257, T: 8811, Avg. loss: 1232.924271\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 245703.91, NNZs: 943, Bias: -2894.369659, T: 9790, Avg. loss: 766.666122\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 245700.56, NNZs: 943, Bias: -2896.918938, T: 10769, Avg. loss: 501.797510\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 245698.77, NNZs: 943, Bias: -2898.248595, T: 11748, Avg. loss: 331.285868\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 245697.04, NNZs: 943, Bias: -2899.588631, T: 12727, Avg. loss: 244.360508\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 245695.86, NNZs: 943, Bias: -2900.501587, T: 13706, Avg. loss: 182.453677\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 245695.13, NNZs: 942, Bias: -2901.063652, T: 14685, Avg. loss: 136.144367\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 245694.33, NNZs: 942, Bias: -2901.689095, T: 15664, Avg. loss: 115.125590\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 245693.76, NNZs: 942, Bias: -2902.136283, T: 16643, Avg. loss: 93.733832\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 245693.30, NNZs: 942, Bias: -2902.492023, T: 17622, Avg. loss: 78.796184\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 245693.00, NNZs: 942, Bias: -2902.720976, T: 18601, Avg. loss: 63.751187\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 245692.61, NNZs: 942, Bias: -2903.033868, T: 19580, Avg. loss: 59.716270\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 245692.32, NNZs: 942, Bias: -2903.265835, T: 20559, Avg. loss: 52.622946\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 245692.14, NNZs: 942, Bias: -2903.398319, T: 21538, Avg. loss: 44.263319\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 245691.96, NNZs: 942, Bias: -2903.542074, T: 22517, Avg. loss: 40.883033\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 245691.73, NNZs: 942, Bias: -2903.722385, T: 23496, Avg. loss: 40.004557\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 245691.55, NNZs: 942, Bias: -2903.863116, T: 24475, Avg. loss: 36.725815\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 245691.39, NNZs: 942, Bias: -2903.987311, T: 25454, Avg. loss: 34.032185\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 245691.26, NNZs: 942, Bias: -2904.094505, T: 26433, Avg. loss: 31.798351\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 245691.18, NNZs: 941, Bias: -2904.149832, T: 27412, Avg. loss: 28.142062\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 245691.07, NNZs: 941, Bias: -2904.239943, T: 28391, Avg. loss: 28.756348\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 245690.97, NNZs: 941, Bias: -2904.316603, T: 29370, Avg. loss: 27.240873\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 245690.88, NNZs: 941, Bias: -2904.384671, T: 30349, Avg. loss: 25.981703\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 245690.83, NNZs: 939, Bias: -2904.419560, T: 31328, Avg. loss: 23.380594\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 245690.78, NNZs: 939, Bias: -2904.459073, T: 32307, Avg. loss: 22.821457\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 245690.72, NNZs: 939, Bias: -2904.500811, T: 33286, Avg. loss: 22.283560\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 245690.67, NNZs: 939, Bias: -2904.540437, T: 34265, Avg. loss: 21.724028\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 245690.62, NNZs: 939, Bias: -2904.578606, T: 35244, Avg. loss: 21.167286\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 245690.57, NNZs: 939, Bias: -2904.615019, T: 36223, Avg. loss: 20.622373\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 245690.51, NNZs: 939, Bias: -2904.664051, T: 37202, Avg. loss: 21.090215\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 245690.46, NNZs: 939, Bias: -2904.705703, T: 38181, Avg. loss: 20.362645\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 245690.42, NNZs: 939, Bias: -2904.728789, T: 39160, Avg. loss: 18.814624\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 245690.39, NNZs: 939, Bias: -2904.752958, T: 40139, Avg. loss: 18.487695\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 245690.36, NNZs: 939, Bias: -2904.774732, T: 41118, Avg. loss: 18.157119\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 245690.33, NNZs: 939, Bias: -2904.796831, T: 42097, Avg. loss: 17.832982\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 245690.31, NNZs: 939, Bias: -2904.815182, T: 43076, Avg. loss: 17.504437\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 245690.27, NNZs: 939, Bias: -2904.845380, T: 44055, Avg. loss: 17.921923\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 245690.24, NNZs: 939, Bias: -2904.862277, T: 45034, Avg. loss: 16.745481\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 245690.21, NNZs: 939, Bias: -2904.888775, T: 46013, Avg. loss: 17.205038\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 245690.19, NNZs: 939, Bias: -2904.903000, T: 46992, Avg. loss: 16.119616\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 245690.16, NNZs: 938, Bias: -2904.924771, T: 47971, Avg. loss: 16.547922\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 245690.13, NNZs: 938, Bias: -2904.945673, T: 48950, Avg. loss: 16.204695\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 245690.12, NNZs: 938, Bias: -2904.955053, T: 49929, Avg. loss: 15.245115\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 245690.11, NNZs: 938, Bias: -2904.964912, T: 50908, Avg. loss: 15.074527\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 245690.08, NNZs: 938, Bias: -2904.983618, T: 51887, Avg. loss: 15.489818\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 245690.07, NNZs: 938, Bias: -2904.992318, T: 52866, Avg. loss: 14.626057\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 245690.05, NNZs: 938, Bias: -2905.001293, T: 53845, Avg. loss: 14.470196\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 245690.04, NNZs: 938, Bias: -2905.009591, T: 54824, Avg. loss: 14.309352\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 245690.02, NNZs: 938, Bias: -2905.025339, T: 55803, Avg. loss: 14.673323\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 245690.00, NNZs: 938, Bias: -2905.039665, T: 56782, Avg. loss: 14.417518\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 245689.99, NNZs: 938, Bias: -2905.051941, T: 57761, Avg. loss: 14.191440\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 245689.97, NNZs: 938, Bias: -2905.063696, T: 58740, Avg. loss: 13.973189\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 245689.96, NNZs: 938, Bias: -2905.068373, T: 59719, Avg. loss: 13.286864\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 245689.95, NNZs: 938, Bias: -2905.073693, T: 60698, Avg. loss: 13.177364\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 245689.95, NNZs: 938, Bias: -2905.079297, T: 61677, Avg. loss: 13.060938\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 245689.94, NNZs: 938, Bias: -2905.085218, T: 62656, Avg. loss: 12.946637\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 245689.93, NNZs: 938, Bias: -2905.091221, T: 63635, Avg. loss: 12.831538\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 245689.92, NNZs: 938, Bias: -2905.097336, T: 64614, Avg. loss: 12.710954\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 245689.91, NNZs: 938, Bias: -2905.107246, T: 65593, Avg. loss: 13.000288\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 245689.90, NNZs: 938, Bias: -2905.112531, T: 66572, Avg. loss: 12.426198\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 245689.89, NNZs: 938, Bias: -2905.117996, T: 67551, Avg. loss: 12.324867\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 245689.88, NNZs: 938, Bias: -2905.127190, T: 68530, Avg. loss: 12.596271\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 245689.87, NNZs: 938, Bias: -2905.135242, T: 69509, Avg. loss: 12.432297\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 245689.86, NNZs: 938, Bias: -2905.142325, T: 70488, Avg. loss: 12.281852\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 245689.85, NNZs: 938, Bias: -2905.149154, T: 71467, Avg. loss: 12.139381\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 245689.84, NNZs: 938, Bias: -2905.155679, T: 72446, Avg. loss: 12.009805\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 245689.83, NNZs: 938, Bias: -2905.158463, T: 73425, Avg. loss: 11.530752\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 245689.83, NNZs: 938, Bias: -2905.161459, T: 74404, Avg. loss: 11.454165\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 245689.82, NNZs: 938, Bias: -2905.167518, T: 75383, Avg. loss: 11.700169\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 245689.81, NNZs: 938, Bias: -2905.173604, T: 76362, Avg. loss: 11.583627\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 245689.81, NNZs: 938, Bias: -2905.176136, T: 77341, Avg. loss: 11.147108\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 245689.80, NNZs: 938, Bias: -2905.178667, T: 78320, Avg. loss: 11.074591\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 245689.79, NNZs: 938, Bias: -2905.183955, T: 79299, Avg. loss: 11.304726\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 245689.79, NNZs: 938, Bias: -2905.186293, T: 80278, Avg. loss: 10.894169\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 245689.78, NNZs: 938, Bias: -2905.191504, T: 81257, Avg. loss: 11.116977\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 245689.78, NNZs: 938, Bias: -2905.193841, T: 82236, Avg. loss: 10.722894\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 245689.77, NNZs: 938, Bias: -2905.198819, T: 83215, Avg. loss: 10.937361\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 245689.77, NNZs: 938, Bias: -2905.200894, T: 84194, Avg. loss: 10.559025\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 245689.76, NNZs: 938, Bias: -2905.202897, T: 85173, Avg. loss: 10.494930\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 245689.76, NNZs: 938, Bias: -2905.205108, T: 86152, Avg. loss: 10.429699\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 245689.75, NNZs: 938, Bias: -2905.209468, T: 87131, Avg. loss: 10.625064\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 245689.75, NNZs: 938, Bias: -2905.213718, T: 88110, Avg. loss: 10.530235\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 245689.75, NNZs: 938, Bias: -2905.215638, T: 89089, Avg. loss: 10.186744\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 245689.74, NNZs: 938, Bias: -2905.217504, T: 90068, Avg. loss: 10.126473\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 245689.74, NNZs: 938, Bias: -2905.221408, T: 91047, Avg. loss: 10.306532\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 245689.73, NNZs: 938, Bias: -2905.225035, T: 92026, Avg. loss: 10.220816\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 245689.73, NNZs: 938, Bias: -2905.226530, T: 93005, Avg. loss: 9.901090\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 245689.73, NNZs: 938, Bias: -2905.228240, T: 93984, Avg. loss: 9.845111\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 245689.72, NNZs: 938, Bias: -2905.231822, T: 94963, Avg. loss: 10.017081\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 245689.72, NNZs: 938, Bias: -2905.233264, T: 95942, Avg. loss: 9.711700\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 245689.71, NNZs: 938, Bias: -2905.234985, T: 96921, Avg. loss: 9.658049\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 245689.71, NNZs: 938, Bias: -2905.236656, T: 97900, Avg. loss: 9.603934\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 245689.71, NNZs: 938, Bias: -2905.239818, T: 98879, Avg. loss: 9.761918\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 245689.70, NNZs: 938, Bias: -2905.243058, T: 99858, Avg. loss: 9.687997\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 245689.70, NNZs: 938, Bias: -2905.246133, T: 100837, Avg. loss: 9.617797\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 245689.70, NNZs: 938, Bias: -2905.247337, T: 101816, Avg. loss: 9.342469\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 245689.69, NNZs: 938, Bias: -2905.248664, T: 102795, Avg. loss: 9.294347\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 245689.69, NNZs: 938, Bias: -2905.251447, T: 103774, Avg. loss: 9.444363\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 245689.69, NNZs: 938, Bias: -2905.254000, T: 104753, Avg. loss: 9.374927\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 245689.68, NNZs: 938, Bias: -2905.256453, T: 105732, Avg. loss: 9.310030\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 245689.68, NNZs: 938, Bias: -2905.257532, T: 106711, Avg. loss: 9.058468\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 245689.68, NNZs: 938, Bias: -2905.258525, T: 107690, Avg. loss: 9.013657\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 245689.68, NNZs: 938, Bias: -2905.259551, T: 108669, Avg. loss: 8.968975\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 245689.67, NNZs: 938, Bias: -2905.262063, T: 109648, Avg. loss: 9.106644\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 245689.67, NNZs: 938, Bias: -2905.263001, T: 110627, Avg. loss: 8.867794\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 245689.67, NNZs: 938, Bias: -2905.265158, T: 111606, Avg. loss: 8.999534\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 245689.66, NNZs: 938, Bias: -2905.267411, T: 112585, Avg. loss: 8.944235\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 245689.66, NNZs: 938, Bias: -2905.269293, T: 113564, Avg. loss: 8.886604\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 245689.66, NNZs: 938, Bias: -2905.271149, T: 114543, Avg. loss: 8.830892\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 245689.66, NNZs: 938, Bias: -2905.271697, T: 115522, Avg. loss: 8.608521\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 245689.65, NNZs: 938, Bias: -2905.273517, T: 116501, Avg. loss: 8.735559\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 245689.65, NNZs: 938, Bias: -2905.275243, T: 117480, Avg. loss: 8.682670\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 245689.65, NNZs: 938, Bias: -2905.275850, T: 118459, Avg. loss: 8.470793\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 245689.65, NNZs: 938, Bias: -2905.277526, T: 119438, Avg. loss: 8.594106\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 245689.64, NNZs: 938, Bias: -2905.279056, T: 120417, Avg. loss: 8.543568\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 245689.64, NNZs: 938, Bias: -2905.279466, T: 121396, Avg. loss: 8.339421\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 245689.64, NNZs: 938, Bias: -2905.280993, T: 122375, Avg. loss: 8.458192\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 245689.64, NNZs: 938, Bias: -2905.282376, T: 123354, Avg. loss: 8.409952\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 245689.64, NNZs: 938, Bias: -2905.282780, T: 124333, Avg. loss: 8.214416\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 245689.64, NNZs: 938, Bias: -2905.284126, T: 125312, Avg. loss: 8.326831\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 245689.63, NNZs: 938, Bias: -2905.284517, T: 126291, Avg. loss: 8.136266\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 245689.63, NNZs: 938, Bias: -2905.284852, T: 127270, Avg. loss: 8.102790\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 245689.63, NNZs: 938, Bias: -2905.286294, T: 128249, Avg. loss: 8.211977\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 245689.63, NNZs: 938, Bias: -2905.286795, T: 129228, Avg. loss: 8.027526\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 245689.63, NNZs: 938, Bias: -2905.287228, T: 130207, Avg. loss: 7.994816\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 245689.63, NNZs: 938, Bias: -2905.288658, T: 131186, Avg. loss: 8.099529\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 245689.63, NNZs: 938, Bias: -2905.289110, T: 132165, Avg. loss: 7.921437\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 245689.62, NNZs: 938, Bias: -2905.289538, T: 133144, Avg. loss: 7.889174\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 245689.62, NNZs: 938, Bias: -2905.290901, T: 134123, Avg. loss: 7.989123\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 245689.62, NNZs: 938, Bias: -2905.291336, T: 135102, Avg. loss: 7.817679\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 245689.62, NNZs: 938, Bias: -2905.291788, T: 136081, Avg. loss: 7.786682\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 245689.62, NNZs: 938, Bias: -2905.293149, T: 137060, Avg. loss: 7.883981\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 245689.62, NNZs: 938, Bias: -2905.294401, T: 138039, Avg. loss: 7.843548\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 245689.61, NNZs: 938, Bias: -2905.295479, T: 139018, Avg. loss: 7.804916\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 245689.61, NNZs: 938, Bias: -2905.296633, T: 139997, Avg. loss: 7.766208\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 245689.61, NNZs: 938, Bias: -2905.297663, T: 140976, Avg. loss: 7.728534\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 245689.61, NNZs: 938, Bias: -2905.297974, T: 141955, Avg. loss: 7.570029\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 245689.61, NNZs: 938, Bias: -2905.299042, T: 142934, Avg. loss: 7.662343\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 245689.61, NNZs: 938, Bias: -2905.299314, T: 143913, Avg. loss: 7.507396\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 245689.61, NNZs: 938, Bias: -2905.299635, T: 144892, Avg. loss: 7.480292\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 245689.61, NNZs: 938, Bias: -2905.300005, T: 145871, Avg. loss: 7.452114\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 245689.60, NNZs: 938, Bias: -2905.300376, T: 146850, Avg. loss: 7.424980\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 245689.60, NNZs: 938, Bias: -2905.300718, T: 147829, Avg. loss: 7.397528\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 245689.60, NNZs: 938, Bias: -2905.301160, T: 148808, Avg. loss: 7.369609\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 245689.60, NNZs: 938, Bias: -2905.301587, T: 149787, Avg. loss: 7.342543\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 245689.60, NNZs: 938, Bias: -2905.302069, T: 150766, Avg. loss: 7.315321\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 245689.60, NNZs: 938, Bias: -2905.303103, T: 151745, Avg. loss: 7.396261\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 245689.60, NNZs: 938, Bias: -2905.303515, T: 152724, Avg. loss: 7.255765\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 245689.60, NNZs: 938, Bias: -2905.304506, T: 153703, Avg. loss: 7.334523\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 245689.60, NNZs: 938, Bias: -2905.304900, T: 154682, Avg. loss: 7.196935\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 245689.60, NNZs: 938, Bias: -2905.305289, T: 155661, Avg. loss: 7.171453\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 245689.59, NNZs: 938, Bias: -2905.305715, T: 156640, Avg. loss: 7.145697\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 245689.59, NNZs: 938, Bias: -2905.306111, T: 157619, Avg. loss: 7.120400\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 245689.59, NNZs: 938, Bias: -2905.306506, T: 158598, Avg. loss: 7.094436\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 245689.59, NNZs: 938, Bias: -2905.307565, T: 159577, Avg. loss: 7.169301\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 245689.59, NNZs: 938, Bias: -2905.308473, T: 160556, Avg. loss: 7.137508\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 245689.59, NNZs: 938, Bias: -2905.309402, T: 161535, Avg. loss: 7.106426\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 245689.59, NNZs: 938, Bias: -2905.309733, T: 162514, Avg. loss: 6.979925\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 245689.59, NNZs: 938, Bias: -2905.310528, T: 163493, Avg. loss: 7.050919\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 245689.58, NNZs: 938, Bias: -2905.311423, T: 164472, Avg. loss: 7.022806\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 245689.58, NNZs: 938, Bias: -2905.312174, T: 165451, Avg. loss: 6.993207\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 245689.58, NNZs: 938, Bias: -2905.312935, T: 166430, Avg. loss: 6.964992\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 245689.58, NNZs: 938, Bias: -2905.313689, T: 167409, Avg. loss: 6.936553\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 245689.58, NNZs: 938, Bias: -2905.314411, T: 168388, Avg. loss: 6.909462\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 245689.58, NNZs: 938, Bias: -2905.315111, T: 169367, Avg. loss: 6.881277\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 245689.58, NNZs: 938, Bias: -2905.315314, T: 170346, Avg. loss: 6.765080\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 245689.58, NNZs: 938, Bias: -2905.315539, T: 171325, Avg. loss: 6.743556\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 245689.58, NNZs: 938, Bias: -2905.316197, T: 172304, Avg. loss: 6.810403\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 245689.57, NNZs: 938, Bias: -2905.316426, T: 173283, Avg. loss: 6.697045\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 245689.57, NNZs: 938, Bias: -2905.317127, T: 174262, Avg. loss: 6.762498\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 245689.57, NNZs: 938, Bias: -2905.317759, T: 175241, Avg. loss: 6.735951\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 245689.57, NNZs: 938, Bias: -2905.318396, T: 176220, Avg. loss: 6.710943\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 245689.57, NNZs: 938, Bias: -2905.318990, T: 177199, Avg. loss: 6.686002\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 245689.57, NNZs: 938, Bias: -2905.319154, T: 178178, Avg. loss: 6.577381\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 245689.57, NNZs: 938, Bias: -2905.319783, T: 179157, Avg. loss: 6.640301\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 245689.57, NNZs: 938, Bias: -2905.319981, T: 180136, Avg. loss: 6.534080\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 245689.57, NNZs: 938, Bias: -2905.320173, T: 181115, Avg. loss: 6.514467\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 245689.57, NNZs: 938, Bias: -2905.320383, T: 182094, Avg. loss: 6.495082\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 245689.57, NNZs: 938, Bias: -2905.320615, T: 183073, Avg. loss: 6.475711\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 245689.57, NNZs: 938, Bias: -2905.320844, T: 184052, Avg. loss: 6.456471\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 245689.57, NNZs: 938, Bias: -2905.321508, T: 185031, Avg. loss: 6.516265\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 245689.56, NNZs: 938, Bias: -2905.322142, T: 186010, Avg. loss: 6.492783\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 245689.56, NNZs: 938, Bias: -2905.322355, T: 186989, Avg. loss: 6.391751\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 245689.56, NNZs: 938, Bias: -2905.322904, T: 187968, Avg. loss: 6.449547\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 245689.56, NNZs: 938, Bias: -2905.323469, T: 188947, Avg. loss: 6.427431\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 245689.56, NNZs: 938, Bias: -2905.323993, T: 189926, Avg. loss: 6.404710\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 245689.56, NNZs: 938, Bias: -2905.324188, T: 190905, Avg. loss: 6.307833\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 245689.56, NNZs: 938, Bias: -2905.324749, T: 191884, Avg. loss: 6.364201\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 245689.56, NNZs: 938, Bias: -2905.325300, T: 192863, Avg. loss: 6.342962\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 245689.56, NNZs: 938, Bias: -2905.325471, T: 193842, Avg. loss: 6.248019\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 245689.56, NNZs: 938, Bias: -2905.325966, T: 194821, Avg. loss: 6.302718\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 245689.56, NNZs: 938, Bias: -2905.326461, T: 195800, Avg. loss: 6.281962\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 245689.56, NNZs: 938, Bias: -2905.326928, T: 196779, Avg. loss: 6.261081\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.327072, T: 197758, Avg. loss: 6.169359\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.327546, T: 198737, Avg. loss: 6.223132\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.327674, T: 199716, Avg. loss: 6.132847\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.327827, T: 200695, Avg. loss: 6.116484\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.328303, T: 201674, Avg. loss: 6.168141\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.328444, T: 202653, Avg. loss: 6.080239\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.328893, T: 203632, Avg. loss: 6.130846\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.329372, T: 204611, Avg. loss: 6.112116\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.329790, T: 205590, Avg. loss: 6.092226\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.329914, T: 206569, Avg. loss: 6.006831\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.330055, T: 207548, Avg. loss: 5.990940\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.330512, T: 208527, Avg. loss: 6.039967\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 214\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.330947, T: 209506, Avg. loss: 6.021092\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 215\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.331373, T: 210485, Avg. loss: 6.002934\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 216\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.331490, T: 211464, Avg. loss: 5.920584\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 217\n",
            "Norm: 245689.55, NNZs: 938, Bias: -2905.331644, T: 212443, Avg. loss: 5.905408\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 218\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.331796, T: 213422, Avg. loss: 5.890296\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 219\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.332232, T: 214401, Avg. loss: 5.937442\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 220\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.332347, T: 215380, Avg. loss: 5.857261\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 221\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.332789, T: 216359, Avg. loss: 5.903873\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 222\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.333188, T: 217338, Avg. loss: 5.886204\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 223\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.333591, T: 218317, Avg. loss: 5.868574\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 224\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.333706, T: 219296, Avg. loss: 5.791251\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 225\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.334114, T: 220275, Avg. loss: 5.836466\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 226\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.334513, T: 221254, Avg. loss: 5.819710\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 227\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.334888, T: 222233, Avg. loss: 5.802781\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 228\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.334987, T: 223212, Avg. loss: 5.727334\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 229\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.335358, T: 224191, Avg. loss: 5.771296\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 230\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.335498, T: 225170, Avg. loss: 5.697342\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 231\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.335618, T: 226149, Avg. loss: 5.683684\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 232\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.336001, T: 227128, Avg. loss: 5.726586\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 233\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.336364, T: 228107, Avg. loss: 5.710478\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 234\n",
            "Norm: 245689.54, NNZs: 938, Bias: -2905.336716, T: 229086, Avg. loss: 5.693897\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 235\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.336850, T: 230065, Avg. loss: 5.622623\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 236\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.336978, T: 231044, Avg. loss: 5.609287\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 237\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.337105, T: 232023, Avg. loss: 5.595919\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 238\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.337204, T: 233002, Avg. loss: 5.582425\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 239\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.337364, T: 233981, Avg. loss: 5.569543\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 240\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.337478, T: 234960, Avg. loss: 5.555794\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 241\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.337620, T: 235939, Avg. loss: 5.542939\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 242\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.337781, T: 236918, Avg. loss: 5.529840\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 243\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.337918, T: 237897, Avg. loss: 5.516908\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 244\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.338282, T: 238876, Avg. loss: 5.555911\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 245\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.338426, T: 239855, Avg. loss: 5.489098\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 246\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.338779, T: 240834, Avg. loss: 5.527525\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 247\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.338943, T: 241813, Avg. loss: 5.461989\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 248\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.339302, T: 242792, Avg. loss: 5.499899\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 249\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.339656, T: 243771, Avg. loss: 5.485348\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 250\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.339981, T: 244750, Avg. loss: 5.470745\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 251\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.340110, T: 245729, Avg. loss: 5.406558\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 252\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.340241, T: 246708, Avg. loss: 5.393999\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 253\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.340588, T: 247687, Avg. loss: 5.431328\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 254\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.340908, T: 248666, Avg. loss: 5.416813\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 255\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.341023, T: 249645, Avg. loss: 5.354217\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 256\n",
            "Norm: 245689.53, NNZs: 938, Bias: -2905.341140, T: 250624, Avg. loss: 5.342245\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 257\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.341469, T: 251603, Avg. loss: 5.378380\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 258\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.341601, T: 252582, Avg. loss: 5.317077\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 259\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.341918, T: 253561, Avg. loss: 5.352448\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 260\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.342219, T: 254540, Avg. loss: 5.338885\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 261\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.342532, T: 255519, Avg. loss: 5.325619\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 262\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.342824, T: 256498, Avg. loss: 5.312308\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 263\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.343138, T: 257477, Avg. loss: 5.299259\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 264\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.343254, T: 258456, Avg. loss: 5.239724\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 265\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.343364, T: 259435, Avg. loss: 5.228328\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 266\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.343490, T: 260414, Avg. loss: 5.217601\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 267\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.343587, T: 261393, Avg. loss: 5.205837\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 268\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.343712, T: 262372, Avg. loss: 5.194734\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 269\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.344021, T: 263351, Avg. loss: 5.228612\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 270\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.344139, T: 264330, Avg. loss: 5.171349\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 271\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.344250, T: 265309, Avg. loss: 5.160299\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 272\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.344543, T: 266288, Avg. loss: 5.193372\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 273\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.344831, T: 267267, Avg. loss: 5.180835\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 274\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.344959, T: 268246, Avg. loss: 5.125098\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 275\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.345062, T: 269225, Avg. loss: 5.114183\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 276\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.345338, T: 270204, Avg. loss: 5.146452\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 277\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.345435, T: 271183, Avg. loss: 5.091418\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 278\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.345723, T: 272162, Avg. loss: 5.123616\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 279\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.345847, T: 273141, Avg. loss: 5.069287\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 280\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.345941, T: 274120, Avg. loss: 5.058562\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 281\n",
            "Norm: 245689.52, NNZs: 938, Bias: -2905.346065, T: 275099, Avg. loss: 5.048403\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 282\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.346346, T: 276078, Avg. loss: 5.079509\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 283\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.346471, T: 277057, Avg. loss: 5.026329\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 284\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.346751, T: 278036, Avg. loss: 5.057507\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 285\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.346880, T: 279015, Avg. loss: 5.004915\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 286\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.346999, T: 279994, Avg. loss: 4.994709\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 287\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.347109, T: 280973, Avg. loss: 4.984481\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 288\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.347398, T: 281952, Avg. loss: 5.014754\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 289\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.347499, T: 282931, Avg. loss: 4.963288\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 290\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.347762, T: 283910, Avg. loss: 4.993162\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 291\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.348017, T: 284889, Avg. loss: 4.981859\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 292\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.348256, T: 285868, Avg. loss: 4.970639\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 293\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.348503, T: 286847, Avg. loss: 4.959487\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 294\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.348608, T: 287826, Avg. loss: 4.909622\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 295\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.348708, T: 288805, Avg. loss: 4.900135\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 296\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.348947, T: 289784, Avg. loss: 4.928900\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 297\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.349175, T: 290763, Avg. loss: 4.917998\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 298\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.349413, T: 291742, Avg. loss: 4.907412\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 299\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.349634, T: 292721, Avg. loss: 4.896640\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 300\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.349719, T: 293700, Avg. loss: 4.848426\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 301\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.349956, T: 294679, Avg. loss: 4.876645\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 302\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.350057, T: 295658, Avg. loss: 4.829062\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 303\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.350149, T: 296637, Avg. loss: 4.819721\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 304\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.350381, T: 297616, Avg. loss: 4.847373\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 305\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.350479, T: 298595, Avg. loss: 4.800464\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 306\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.350582, T: 299574, Avg. loss: 4.791428\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 307\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.350819, T: 300553, Avg. loss: 4.818863\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 308\n",
            "Norm: 245689.51, NNZs: 938, Bias: -2905.351043, T: 301532, Avg. loss: 4.808608\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 309\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.351116, T: 302511, Avg. loss: 4.762358\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 310\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.351334, T: 303490, Avg. loss: 4.789361\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 311\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.351538, T: 304469, Avg. loss: 4.779276\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 312\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.351612, T: 305448, Avg. loss: 4.734038\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 313\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.351828, T: 306427, Avg. loss: 4.760323\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 314\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.351916, T: 307406, Avg. loss: 4.715719\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 315\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.352133, T: 308385, Avg. loss: 4.741876\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 316\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.352342, T: 309364, Avg. loss: 4.732228\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 317\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.352548, T: 310343, Avg. loss: 4.722302\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 318\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.352621, T: 311322, Avg. loss: 4.678409\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 319\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.352818, T: 312301, Avg. loss: 4.704271\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 320\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.352903, T: 313280, Avg. loss: 4.660875\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 321\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.353106, T: 314259, Avg. loss: 4.686467\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 322\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.353196, T: 315238, Avg. loss: 4.643498\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 323\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.353264, T: 316217, Avg. loss: 4.634833\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 324\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.353352, T: 317196, Avg. loss: 4.626840\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 325\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.353440, T: 318175, Avg. loss: 4.618541\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 326\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.353518, T: 319154, Avg. loss: 4.610214\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 327\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.353607, T: 320133, Avg. loss: 4.602079\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 328\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.353805, T: 321112, Avg. loss: 4.626315\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 329\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.353992, T: 322091, Avg. loss: 4.617300\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 330\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.354186, T: 323070, Avg. loss: 4.608193\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 331\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.354281, T: 324049, Avg. loss: 4.567314\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 332\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.354362, T: 325028, Avg. loss: 4.559140\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 333\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.354439, T: 326007, Avg. loss: 4.551169\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 334\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.354521, T: 326986, Avg. loss: 4.543284\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 335\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.354602, T: 327965, Avg. loss: 4.535382\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 336\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.354695, T: 328944, Avg. loss: 4.527571\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 337\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.354889, T: 329923, Avg. loss: 4.550967\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 338\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.355074, T: 330902, Avg. loss: 4.542131\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 339\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.355150, T: 331881, Avg. loss: 4.502501\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 340\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.355242, T: 332860, Avg. loss: 4.494950\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 341\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.355436, T: 333839, Avg. loss: 4.517738\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 342\n",
            "Norm: 245689.50, NNZs: 938, Bias: -2905.355517, T: 334818, Avg. loss: 4.478661\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 343\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.355697, T: 335797, Avg. loss: 4.501514\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 344\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.355880, T: 336776, Avg. loss: 4.493164\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 345\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.356058, T: 337755, Avg. loss: 4.484546\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 346\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.356128, T: 338734, Avg. loss: 4.446238\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 347\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.356297, T: 339713, Avg. loss: 4.468714\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 348\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.356471, T: 340692, Avg. loss: 4.460444\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 349\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.356651, T: 341671, Avg. loss: 4.452321\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 350\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.356731, T: 342650, Avg. loss: 4.414816\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 351\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.356806, T: 343629, Avg. loss: 4.407552\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 352\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.356888, T: 344608, Avg. loss: 4.400127\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 353\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.356973, T: 345587, Avg. loss: 4.392984\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 354\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.357143, T: 346566, Avg. loss: 4.414525\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 355\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.357224, T: 347545, Avg. loss: 4.377807\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 356\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.357303, T: 348524, Avg. loss: 4.370622\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 357\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.357388, T: 349503, Avg. loss: 4.363568\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 358\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.357458, T: 350482, Avg. loss: 4.356273\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 359\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.357638, T: 351461, Avg. loss: 4.377603\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 360\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.357808, T: 352440, Avg. loss: 4.369765\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 361\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.357887, T: 353419, Avg. loss: 4.333871\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 362\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.358045, T: 354398, Avg. loss: 4.354531\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 363\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.358126, T: 355377, Avg. loss: 4.319273\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 364\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.358289, T: 356356, Avg. loss: 4.339975\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 365\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.358452, T: 357335, Avg. loss: 4.332256\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 366\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.358528, T: 358314, Avg. loss: 4.297412\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 367\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.358598, T: 359293, Avg. loss: 4.290431\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 368\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.358756, T: 360272, Avg. loss: 4.310990\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 369\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.358921, T: 361251, Avg. loss: 4.303432\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 370\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.359075, T: 362230, Avg. loss: 4.295752\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 371\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.359227, T: 363209, Avg. loss: 4.288507\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 372\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.359301, T: 364188, Avg. loss: 4.254516\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 373\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.359451, T: 365167, Avg. loss: 4.274279\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 374\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.359512, T: 366146, Avg. loss: 4.240415\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 375\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.359662, T: 367125, Avg. loss: 4.260463\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 376\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.359812, T: 368104, Avg. loss: 4.253045\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 377\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.359881, T: 369083, Avg. loss: 4.219867\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 378\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.359937, T: 370062, Avg. loss: 4.213078\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 379\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.360092, T: 371041, Avg. loss: 4.232810\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 380\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.360236, T: 372020, Avg. loss: 4.225639\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 381\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.360377, T: 372999, Avg. loss: 4.218550\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 382\n",
            "Norm: 245689.49, NNZs: 938, Bias: -2905.360439, T: 373978, Avg. loss: 4.185916\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 383\n",
            "Norm: 245689.48, NNZs: 938, Bias: -2905.360499, T: 374957, Avg. loss: 4.179513\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 384\n",
            "Norm: 245689.48, NNZs: 938, Bias: -2905.360635, T: 375936, Avg. loss: 4.198395\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 385\n",
            "Norm: 245689.48, NNZs: 938, Bias: -2905.360771, T: 376915, Avg. loss: 4.191513\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 386\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.360831, T: 377894, Avg. loss: 4.159483\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 387\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.360905, T: 378873, Avg. loss: 4.153420\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 388\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.360976, T: 379852, Avg. loss: 4.147137\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 389\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.361125, T: 380831, Avg. loss: 4.165710\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 390\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.361261, T: 381810, Avg. loss: 4.158671\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 391\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.361409, T: 382789, Avg. loss: 4.152170\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 392\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.361541, T: 383768, Avg. loss: 4.145235\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 393\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.361667, T: 384747, Avg. loss: 4.138588\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 394\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.361721, T: 385726, Avg. loss: 4.107505\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 395\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.361781, T: 386705, Avg. loss: 4.101583\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 396\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.361910, T: 387684, Avg. loss: 4.119642\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 397\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.362044, T: 388663, Avg. loss: 4.113028\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 398\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.362104, T: 389642, Avg. loss: 4.082503\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 399\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.362233, T: 390621, Avg. loss: 4.100448\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 400\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.362287, T: 391600, Avg. loss: 4.070025\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 401\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.362409, T: 392579, Avg. loss: 4.087684\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 402\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.362546, T: 393558, Avg. loss: 4.081600\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 403\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.362602, T: 394537, Avg. loss: 4.051507\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 404\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.362656, T: 395516, Avg. loss: 4.045578\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 405\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.362784, T: 396495, Avg. loss: 4.063246\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 406\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.362907, T: 397474, Avg. loss: 4.056732\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 407\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.363026, T: 398453, Avg. loss: 4.050383\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 408\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.363080, T: 399432, Avg. loss: 4.021055\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 409\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.363201, T: 400411, Avg. loss: 4.038223\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 410\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.363324, T: 401390, Avg. loss: 4.031964\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 411\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.363378, T: 402369, Avg. loss: 4.003110\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 412\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.363494, T: 403348, Avg. loss: 4.020048\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 413\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.363546, T: 404327, Avg. loss: 3.991293\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 414\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.363591, T: 405306, Avg. loss: 3.985674\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 415\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.363709, T: 406285, Avg. loss: 4.002557\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 416\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.363765, T: 407264, Avg. loss: 3.974194\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 417\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.363822, T: 408243, Avg. loss: 3.968592\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 418\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.363932, T: 409222, Avg. loss: 3.985008\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 419\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.363987, T: 410201, Avg. loss: 3.957120\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 420\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.364043, T: 411180, Avg. loss: 3.951656\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 421\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.364158, T: 412159, Avg. loss: 3.968095\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 422\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.364219, T: 413138, Avg. loss: 3.940274\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 423\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.364340, T: 414117, Avg. loss: 3.956554\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 424\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.364457, T: 415096, Avg. loss: 3.950710\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 425\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.364569, T: 416075, Avg. loss: 3.944719\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 426\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.364617, T: 417054, Avg. loss: 3.917380\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 427\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.364732, T: 418033, Avg. loss: 3.933527\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 428\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.364782, T: 419012, Avg. loss: 3.906217\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 429\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.364889, T: 419991, Avg. loss: 3.922153\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 430\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.365006, T: 420970, Avg. loss: 3.916526\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 431\n",
            "Norm: 245689.48, NNZs: 936, Bias: -2905.365058, T: 421949, Avg. loss: 3.889695\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 432\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.365169, T: 422928, Avg. loss: 3.905428\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 433\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.365277, T: 423907, Avg. loss: 3.899773\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 434\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.365384, T: 424886, Avg. loss: 3.894168\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 435\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.365435, T: 425865, Avg. loss: 3.867587\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 436\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.365538, T: 426844, Avg. loss: 3.883176\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 437\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.365646, T: 427823, Avg. loss: 3.877472\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 438\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.365695, T: 428802, Avg. loss: 3.851460\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 439\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.365745, T: 429781, Avg. loss: 3.846360\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 440\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.365796, T: 430760, Avg. loss: 3.841339\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 441\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.365840, T: 431739, Avg. loss: 3.836174\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 442\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.365894, T: 432718, Avg. loss: 3.831141\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 443\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.365940, T: 433697, Avg. loss: 3.825955\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 444\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.365989, T: 434676, Avg. loss: 3.820956\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 445\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.366094, T: 435655, Avg. loss: 3.835969\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 446\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.366140, T: 436634, Avg. loss: 3.810602\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 447\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.366239, T: 437613, Avg. loss: 3.825445\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 448\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.366292, T: 438592, Avg. loss: 3.800434\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 449\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.366395, T: 439571, Avg. loss: 3.815034\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 450\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.366445, T: 440550, Avg. loss: 3.790115\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 451\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.366497, T: 441529, Avg. loss: 3.785178\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 452\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.366544, T: 442508, Avg. loss: 3.780153\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 453\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.366649, T: 443487, Avg. loss: 3.794877\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 454\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.366756, T: 444466, Avg. loss: 3.789583\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 455\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.366862, T: 445445, Avg. loss: 3.784344\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 456\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.366909, T: 446424, Avg. loss: 3.759754\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 457\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.366956, T: 447403, Avg. loss: 3.754878\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 458\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.367059, T: 448382, Avg. loss: 3.769378\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 459\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.367160, T: 449361, Avg. loss: 3.764094\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 460\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.367261, T: 450340, Avg. loss: 3.758984\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 461\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.367306, T: 451319, Avg. loss: 3.734820\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 462\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.367403, T: 452298, Avg. loss: 3.748954\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 463\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.367448, T: 453277, Avg. loss: 3.724926\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 464\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.367545, T: 454256, Avg. loss: 3.739117\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 465\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.367591, T: 455235, Avg. loss: 3.715307\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 466\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.367692, T: 456214, Avg. loss: 3.729434\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 467\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.367788, T: 457193, Avg. loss: 3.724229\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 468\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.367878, T: 458172, Avg. loss: 3.719112\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 469\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.367973, T: 459151, Avg. loss: 3.714136\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 470\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368019, T: 460130, Avg. loss: 3.690913\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 471\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368109, T: 461109, Avg. loss: 3.704516\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 472\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368199, T: 462088, Avg. loss: 3.699631\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 473\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368289, T: 463067, Avg. loss: 3.694705\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 474\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368378, T: 464046, Avg. loss: 3.689767\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 475\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368468, T: 465025, Avg. loss: 3.684918\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 476\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368512, T: 466004, Avg. loss: 3.662094\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 477\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368599, T: 466983, Avg. loss: 3.675458\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 478\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368645, T: 467962, Avg. loss: 3.652956\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 479\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368732, T: 468941, Avg. loss: 3.666159\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 480\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368820, T: 469920, Avg. loss: 3.661396\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 481\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368867, T: 470899, Avg. loss: 3.638977\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 482\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368904, T: 471878, Avg. loss: 3.634438\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 483\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.368995, T: 472857, Avg. loss: 3.647797\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 484\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.369036, T: 473836, Avg. loss: 3.625497\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 485\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.369075, T: 474815, Avg. loss: 3.621068\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 486\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.369165, T: 475794, Avg. loss: 3.634224\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 487\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.369213, T: 476773, Avg. loss: 3.612194\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 488\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.369298, T: 477752, Avg. loss: 3.625027\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 489\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.369384, T: 478731, Avg. loss: 3.620350\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 490\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.369466, T: 479710, Avg. loss: 3.615805\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 491\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.369549, T: 480689, Avg. loss: 3.611117\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 492\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.369591, T: 481668, Avg. loss: 3.589565\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 493\n",
            "Norm: 245689.47, NNZs: 936, Bias: -2905.369672, T: 482647, Avg. loss: 3.602241\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 494\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.369712, T: 483626, Avg. loss: 3.580767\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 495\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.369794, T: 484605, Avg. loss: 3.593308\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 496\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.369832, T: 485584, Avg. loss: 3.572033\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 497\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.369868, T: 486563, Avg. loss: 3.567838\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 498\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.369947, T: 487542, Avg. loss: 3.580305\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 499\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370030, T: 488521, Avg. loss: 3.575876\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 500\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370111, T: 489500, Avg. loss: 3.571334\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 501\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370150, T: 490479, Avg. loss: 3.550328\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 502\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370184, T: 491458, Avg. loss: 3.546088\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 503\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370223, T: 492437, Avg. loss: 3.542042\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 504\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370300, T: 493416, Avg. loss: 3.554276\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 505\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370381, T: 494395, Avg. loss: 3.550002\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 506\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370422, T: 495374, Avg. loss: 3.529215\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 507\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370466, T: 496353, Avg. loss: 3.525269\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 508\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370503, T: 497332, Avg. loss: 3.521048\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 509\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370546, T: 498311, Avg. loss: 3.517134\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 510\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370584, T: 499290, Avg. loss: 3.513020\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 511\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370624, T: 500269, Avg. loss: 3.509011\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 512\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370665, T: 501248, Avg. loss: 3.505032\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 513\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370702, T: 502227, Avg. loss: 3.500998\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 514\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370742, T: 503206, Avg. loss: 3.496963\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 515\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370821, T: 504185, Avg. loss: 3.508776\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 516\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370897, T: 505164, Avg. loss: 3.504532\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 517\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.370978, T: 506143, Avg. loss: 3.500368\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 518\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371021, T: 507122, Avg. loss: 3.480405\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 519\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371097, T: 508101, Avg. loss: 3.491980\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 520\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371178, T: 509080, Avg. loss: 3.487853\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 521\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371216, T: 510059, Avg. loss: 3.468145\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 522\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371293, T: 511038, Avg. loss: 3.479691\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 523\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371369, T: 512017, Avg. loss: 3.475539\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 524\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371446, T: 512996, Avg. loss: 3.471372\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 525\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371526, T: 513975, Avg. loss: 3.467274\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 526\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371564, T: 514954, Avg. loss: 3.447761\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 527\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371635, T: 515933, Avg. loss: 3.459110\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 528\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371671, T: 516912, Avg. loss: 3.439833\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 529\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371712, T: 517891, Avg. loss: 3.436079\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 530\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371746, T: 518870, Avg. loss: 3.432246\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 531\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371783, T: 519849, Avg. loss: 3.428398\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 532\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371856, T: 520828, Avg. loss: 3.439632\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 533\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371898, T: 521807, Avg. loss: 3.420697\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 534\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.371935, T: 522786, Avg. loss: 3.416926\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 535\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372010, T: 523765, Avg. loss: 3.427970\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 536\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372084, T: 524744, Avg. loss: 3.423969\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 537\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372122, T: 525723, Avg. loss: 3.405225\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 538\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372190, T: 526702, Avg. loss: 3.416111\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 539\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372262, T: 527681, Avg. loss: 3.412255\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 540\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372331, T: 528660, Avg. loss: 3.408254\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 541\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372400, T: 529639, Avg. loss: 3.404236\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 542\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372437, T: 530618, Avg. loss: 3.385823\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 543\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372475, T: 531597, Avg. loss: 3.382123\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 544\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372542, T: 532576, Avg. loss: 3.392903\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 545\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372609, T: 533555, Avg. loss: 3.389004\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 546\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372645, T: 534534, Avg. loss: 3.370804\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 547\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372713, T: 535513, Avg. loss: 3.381554\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 548\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372746, T: 536492, Avg. loss: 3.363358\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 549\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372784, T: 537471, Avg. loss: 3.359816\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 550\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372822, T: 538450, Avg. loss: 3.356233\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 551\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372891, T: 539429, Avg. loss: 3.366902\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 552\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.372961, T: 540408, Avg. loss: 3.363098\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 553\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373030, T: 541387, Avg. loss: 3.359231\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 554\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373097, T: 542366, Avg. loss: 3.355312\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 555\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373165, T: 543345, Avg. loss: 3.351705\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 556\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373195, T: 544324, Avg. loss: 3.333741\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 557\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373227, T: 545303, Avg. loss: 3.330261\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 558\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373263, T: 546282, Avg. loss: 3.326865\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 559\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373328, T: 547261, Avg. loss: 3.337215\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 560\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373390, T: 548240, Avg. loss: 3.333384\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 561\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373426, T: 549219, Avg. loss: 3.315990\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 562\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373492, T: 550198, Avg. loss: 3.326276\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 563\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373556, T: 551177, Avg. loss: 3.322510\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 564\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373621, T: 552156, Avg. loss: 3.318855\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 565\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373688, T: 553135, Avg. loss: 3.315219\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 566\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373753, T: 554114, Avg. loss: 3.311543\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 567\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373786, T: 555093, Avg. loss: 3.294256\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 568\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373852, T: 556072, Avg. loss: 3.304550\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 569\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373883, T: 557051, Avg. loss: 3.287318\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 570\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373918, T: 558030, Avg. loss: 3.283910\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 571\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373949, T: 559009, Avg. loss: 3.280554\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 572\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.373982, T: 559988, Avg. loss: 3.277227\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 573\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.374044, T: 560967, Avg. loss: 3.287087\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 574\n",
            "Norm: 245689.46, NNZs: 936, Bias: -2905.374107, T: 561946, Avg. loss: 3.283654\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 575\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374168, T: 562925, Avg. loss: 3.279929\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 576\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374198, T: 563904, Avg. loss: 3.263208\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 577\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374256, T: 564883, Avg. loss: 3.272998\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 578\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374320, T: 565862, Avg. loss: 3.269637\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 579\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374349, T: 566841, Avg. loss: 3.252855\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 580\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374408, T: 567820, Avg. loss: 3.262712\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 581\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374471, T: 568799, Avg. loss: 3.259270\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 582\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374532, T: 569778, Avg. loss: 3.255694\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 583\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374589, T: 570757, Avg. loss: 3.252194\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 584\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374620, T: 571736, Avg. loss: 3.235756\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 585\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374650, T: 572715, Avg. loss: 3.232612\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 586\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374712, T: 573694, Avg. loss: 3.242230\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 587\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374742, T: 574673, Avg. loss: 3.225941\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 588\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374800, T: 575652, Avg. loss: 3.235475\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 589\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374830, T: 576631, Avg. loss: 3.219269\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 590\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374859, T: 577610, Avg. loss: 3.216055\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 591\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374890, T: 578589, Avg. loss: 3.212887\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 592\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374920, T: 579568, Avg. loss: 3.209663\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 593\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.374955, T: 580547, Avg. loss: 3.206625\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 594\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375012, T: 581526, Avg. loss: 3.215928\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 595\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375042, T: 582505, Avg. loss: 3.200020\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 596\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375101, T: 583484, Avg. loss: 3.209432\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 597\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375157, T: 584463, Avg. loss: 3.206048\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 598\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375215, T: 585442, Avg. loss: 3.202707\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 599\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375246, T: 586421, Avg. loss: 3.186948\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 600\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375303, T: 587400, Avg. loss: 3.196243\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 601\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375361, T: 588379, Avg. loss: 3.192904\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 602\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375392, T: 589358, Avg. loss: 3.177256\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 603\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375423, T: 590337, Avg. loss: 3.174154\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 604\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375453, T: 591316, Avg. loss: 3.171101\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 605\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375513, T: 592295, Avg. loss: 3.180316\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 606\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375571, T: 593274, Avg. loss: 3.176981\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 607\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375602, T: 594253, Avg. loss: 3.161529\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 608\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375630, T: 595232, Avg. loss: 3.158411\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 609\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375683, T: 596211, Avg. loss: 3.167501\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 610\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375738, T: 597190, Avg. loss: 3.164240\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 611\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375798, T: 598169, Avg. loss: 3.161170\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 612\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375824, T: 599148, Avg. loss: 3.145724\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 613\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375854, T: 600127, Avg. loss: 3.142844\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 614\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375910, T: 601106, Avg. loss: 3.151746\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 615\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375941, T: 602085, Avg. loss: 3.136677\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 616\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.375971, T: 603064, Avg. loss: 3.133625\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 617\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376001, T: 604043, Avg. loss: 3.130642\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 618\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376058, T: 605022, Avg. loss: 3.139476\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 619\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376085, T: 606001, Avg. loss: 3.124443\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 620\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376114, T: 606980, Avg. loss: 3.121609\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 621\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376168, T: 607959, Avg. loss: 3.130298\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 622\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376197, T: 608938, Avg. loss: 3.115425\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 623\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376225, T: 609917, Avg. loss: 3.112488\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 624\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376252, T: 610896, Avg. loss: 3.109540\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 625\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376281, T: 611875, Avg. loss: 3.106638\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 626\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376312, T: 612854, Avg. loss: 3.103644\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 627\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376366, T: 613833, Avg. loss: 3.112349\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 628\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376421, T: 614812, Avg. loss: 3.109287\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 629\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376449, T: 615791, Avg. loss: 3.094644\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 630\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376503, T: 616770, Avg. loss: 3.103247\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 631\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376533, T: 617749, Avg. loss: 3.088750\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 632\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376560, T: 618728, Avg. loss: 3.085793\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 633\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376613, T: 619707, Avg. loss: 3.094349\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 634\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376667, T: 620686, Avg. loss: 3.091334\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 635\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376718, T: 621665, Avg. loss: 3.088143\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 636\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376744, T: 622644, Avg. loss: 3.073876\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 637\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376797, T: 623623, Avg. loss: 3.082348\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 638\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376849, T: 624602, Avg. loss: 3.079314\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 639\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376879, T: 625581, Avg. loss: 3.065150\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 640\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376934, T: 626560, Avg. loss: 3.073503\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 641\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.376959, T: 627539, Avg. loss: 3.059298\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 642\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377011, T: 628518, Avg. loss: 3.067627\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 643\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377061, T: 629497, Avg. loss: 3.064642\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 644\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377109, T: 630476, Avg. loss: 3.061632\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 645\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377139, T: 631455, Avg. loss: 3.047799\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 646\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377166, T: 632434, Avg. loss: 3.044925\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 647\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377216, T: 633413, Avg. loss: 3.053116\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 648\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377265, T: 634392, Avg. loss: 3.050139\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 649\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377317, T: 635371, Avg. loss: 3.047268\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 650\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377368, T: 636350, Avg. loss: 3.044349\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 651\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377418, T: 637329, Avg. loss: 3.041446\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 652\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377444, T: 638308, Avg. loss: 3.027671\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 653\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377492, T: 639287, Avg. loss: 3.035779\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 654\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377541, T: 640266, Avg. loss: 3.032947\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 655\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377568, T: 641245, Avg. loss: 3.019309\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 656\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377615, T: 642224, Avg. loss: 3.027225\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 657\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377642, T: 643203, Avg. loss: 3.013746\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 658\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377689, T: 644182, Avg. loss: 3.021698\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 659\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377739, T: 645161, Avg. loss: 3.018929\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 660\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377765, T: 646140, Avg. loss: 3.005387\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 661\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377790, T: 647119, Avg. loss: 3.002714\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 662\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377819, T: 648098, Avg. loss: 3.000132\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 663\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377847, T: 649077, Avg. loss: 2.997376\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 664\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377873, T: 650056, Avg. loss: 2.994786\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 665\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377922, T: 651035, Avg. loss: 3.002592\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 666\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377947, T: 652014, Avg. loss: 2.989257\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 667\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.377992, T: 652993, Avg. loss: 2.997048\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 668\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.378038, T: 653972, Avg. loss: 2.994236\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 669\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.378087, T: 654951, Avg. loss: 2.991608\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 670\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.378130, T: 655930, Avg. loss: 2.988727\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 671\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.378179, T: 656909, Avg. loss: 2.986076\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 672\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.378205, T: 657888, Avg. loss: 2.972932\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 673\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.378248, T: 658867, Avg. loss: 2.980517\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 674\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.378292, T: 659846, Avg. loss: 2.977833\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 675\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.378318, T: 660825, Avg. loss: 2.964918\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 676\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.378342, T: 661804, Avg. loss: 2.962291\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 677\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.378389, T: 662783, Avg. loss: 2.969948\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 678\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.378413, T: 663762, Avg. loss: 2.956926\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 679\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.378438, T: 664741, Avg. loss: 2.954428\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 680\n",
            "Norm: 245689.45, NNZs: 936, Bias: -2905.378483, T: 665720, Avg. loss: 2.961959\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 681\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378529, T: 666699, Avg. loss: 2.959301\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 682\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378576, T: 667678, Avg. loss: 2.956636\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 683\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378621, T: 668657, Avg. loss: 2.953952\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 684\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378644, T: 669636, Avg. loss: 2.941164\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 685\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378669, T: 670615, Avg. loss: 2.938663\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 686\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378714, T: 671594, Avg. loss: 2.946129\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 687\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378738, T: 672573, Avg. loss: 2.933511\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 688\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378763, T: 673552, Avg. loss: 2.930987\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 689\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378790, T: 674531, Avg. loss: 2.928529\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 690\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378812, T: 675510, Avg. loss: 2.925889\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 691\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378835, T: 676489, Avg. loss: 2.923506\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 692\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378862, T: 677468, Avg. loss: 2.921057\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 693\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378904, T: 678447, Avg. loss: 2.928284\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 694\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378928, T: 679426, Avg. loss: 2.915891\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 695\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.378970, T: 680405, Avg. loss: 2.923208\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 696\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379014, T: 681384, Avg. loss: 2.920674\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 697\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379059, T: 682363, Avg. loss: 2.918038\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 698\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379084, T: 683342, Avg. loss: 2.905700\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 699\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379128, T: 684321, Avg. loss: 2.912959\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 700\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379170, T: 685300, Avg. loss: 2.910355\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 701\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379194, T: 686279, Avg. loss: 2.898183\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 702\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379235, T: 687258, Avg. loss: 2.905323\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 703\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379259, T: 688237, Avg. loss: 2.893131\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 704\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379282, T: 689216, Avg. loss: 2.890683\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 705\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379323, T: 690195, Avg. loss: 2.897869\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 706\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379365, T: 691174, Avg. loss: 2.895295\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 707\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379388, T: 692153, Avg. loss: 2.883244\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 708\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379429, T: 693132, Avg. loss: 2.890281\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 709\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379453, T: 694111, Avg. loss: 2.878309\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 710\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379472, T: 695090, Avg. loss: 2.875857\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 711\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379516, T: 696069, Avg. loss: 2.883051\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 712\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379540, T: 697048, Avg. loss: 2.871107\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 713\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379564, T: 698027, Avg. loss: 2.868671\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 714\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379605, T: 699006, Avg. loss: 2.875667\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 715\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379644, T: 699985, Avg. loss: 2.873106\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 716\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379664, T: 700964, Avg. loss: 2.861318\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 717\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379705, T: 701943, Avg. loss: 2.868355\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 718\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379744, T: 702922, Avg. loss: 2.865771\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 719\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379767, T: 703901, Avg. loss: 2.854086\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 720\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379808, T: 704880, Avg. loss: 2.860993\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 721\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379849, T: 705859, Avg. loss: 2.858545\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 722\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379887, T: 706838, Avg. loss: 2.856075\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 723\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379928, T: 707817, Avg. loss: 2.853679\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 724\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379969, T: 708796, Avg. loss: 2.851287\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 725\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.379991, T: 709775, Avg. loss: 2.839588\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 726\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380029, T: 710754, Avg. loss: 2.846419\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 727\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380067, T: 711733, Avg. loss: 2.843950\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 728\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380107, T: 712712, Avg. loss: 2.841575\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 729\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380129, T: 713691, Avg. loss: 2.830084\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 730\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380167, T: 714670, Avg. loss: 2.836898\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 731\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380206, T: 715649, Avg. loss: 2.834454\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 732\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380227, T: 716628, Avg. loss: 2.823024\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 733\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380247, T: 717607, Avg. loss: 2.820746\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 734\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380269, T: 718586, Avg. loss: 2.818516\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 735\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380308, T: 719565, Avg. loss: 2.825179\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 736\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380328, T: 720544, Avg. loss: 2.813857\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 737\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380366, T: 721523, Avg. loss: 2.820528\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 738\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380388, T: 722502, Avg. loss: 2.809290\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 739\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380407, T: 723481, Avg. loss: 2.807023\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 740\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380429, T: 724460, Avg. loss: 2.804805\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 741\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380467, T: 725439, Avg. loss: 2.811392\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 742\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380488, T: 726418, Avg. loss: 2.800271\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 743\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380528, T: 727397, Avg. loss: 2.806867\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 744\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380569, T: 728376, Avg. loss: 2.804553\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 745\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380590, T: 729355, Avg. loss: 2.793420\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 746\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380626, T: 730334, Avg. loss: 2.799886\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 747\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380646, T: 731313, Avg. loss: 2.788805\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 748\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380683, T: 732292, Avg. loss: 2.795366\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 749\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380720, T: 733271, Avg. loss: 2.793086\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 750\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380758, T: 734250, Avg. loss: 2.790803\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 751\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380795, T: 735229, Avg. loss: 2.788441\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 752\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380816, T: 736208, Avg. loss: 2.777523\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 753\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380835, T: 737187, Avg. loss: 2.775289\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 754\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380873, T: 738166, Avg. loss: 2.781801\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 755\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380894, T: 739145, Avg. loss: 2.770908\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 756\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380929, T: 740124, Avg. loss: 2.777284\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 757\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380966, T: 741103, Avg. loss: 2.775069\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 758\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.380986, T: 742082, Avg. loss: 2.764270\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 759\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381021, T: 743061, Avg. loss: 2.770560\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 760\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381041, T: 744040, Avg. loss: 2.759921\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 761\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381076, T: 745019, Avg. loss: 2.766149\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 762\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381113, T: 745998, Avg. loss: 2.763978\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 763\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381132, T: 746977, Avg. loss: 2.753260\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 764\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381166, T: 747956, Avg. loss: 2.759506\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 765\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381187, T: 748935, Avg. loss: 2.748955\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 766\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381207, T: 749914, Avg. loss: 2.746792\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 767\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381228, T: 750893, Avg. loss: 2.744668\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 768\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381248, T: 751872, Avg. loss: 2.742595\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 769\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381283, T: 752851, Avg. loss: 2.748784\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 770\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381319, T: 753830, Avg. loss: 2.746628\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 771\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381339, T: 754809, Avg. loss: 2.736105\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 772\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381374, T: 755788, Avg. loss: 2.742225\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 773\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381407, T: 756767, Avg. loss: 2.740023\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 774\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381428, T: 757746, Avg. loss: 2.729714\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 775\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381448, T: 758725, Avg. loss: 2.727586\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 776\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381483, T: 759704, Avg. loss: 2.733685\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 777\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381518, T: 760683, Avg. loss: 2.731542\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 778\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381552, T: 761662, Avg. loss: 2.729334\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 779\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381572, T: 762641, Avg. loss: 2.719045\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 780\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381604, T: 763620, Avg. loss: 2.725089\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 781\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381640, T: 764599, Avg. loss: 2.722976\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 782\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381660, T: 765578, Avg. loss: 2.712715\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 783\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381678, T: 766557, Avg. loss: 2.710616\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 784\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381711, T: 767536, Avg. loss: 2.716618\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 785\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381729, T: 768515, Avg. loss: 2.706475\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 786\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381763, T: 769494, Avg. loss: 2.712449\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 787\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381782, T: 770473, Avg. loss: 2.702340\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 788\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381801, T: 771452, Avg. loss: 2.700270\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 789\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381835, T: 772431, Avg. loss: 2.706341\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 790\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381868, T: 773410, Avg. loss: 2.704180\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 791\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381886, T: 774389, Avg. loss: 2.694106\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 792\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381903, T: 775368, Avg. loss: 2.692003\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 793\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381922, T: 776347, Avg. loss: 2.690043\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 794\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381956, T: 777326, Avg. loss: 2.695964\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 795\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381974, T: 778305, Avg. loss: 2.685957\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 796\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.381992, T: 779284, Avg. loss: 2.683935\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 797\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382023, T: 780263, Avg. loss: 2.689818\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 798\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382042, T: 781242, Avg. loss: 2.679898\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 799\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382073, T: 782221, Avg. loss: 2.685705\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 800\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382092, T: 783200, Avg. loss: 2.675843\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 801\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382112, T: 784179, Avg. loss: 2.673922\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 802\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382131, T: 785158, Avg. loss: 2.671885\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 803\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382149, T: 786137, Avg. loss: 2.669925\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 804\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382167, T: 787116, Avg. loss: 2.667961\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 805\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382186, T: 788095, Avg. loss: 2.666029\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 806\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382219, T: 789074, Avg. loss: 2.671807\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 807\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382252, T: 790053, Avg. loss: 2.669798\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 808\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382269, T: 791032, Avg. loss: 2.659946\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 809\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382289, T: 792011, Avg. loss: 2.658049\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 810\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382322, T: 792990, Avg. loss: 2.663792\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 811\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382339, T: 793969, Avg. loss: 2.654104\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 812\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382374, T: 794948, Avg. loss: 2.659833\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 813\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382406, T: 795927, Avg. loss: 2.657770\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 814\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382424, T: 796906, Avg. loss: 2.648211\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 815\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382442, T: 797885, Avg. loss: 2.646206\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 816\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382474, T: 798864, Avg. loss: 2.651810\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 817\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382504, T: 799843, Avg. loss: 2.649813\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 818\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382522, T: 800822, Avg. loss: 2.640328\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 819\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382555, T: 801801, Avg. loss: 2.645942\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 820\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382572, T: 802780, Avg. loss: 2.636401\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 821\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382603, T: 803759, Avg. loss: 2.641985\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 822\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382620, T: 804738, Avg. loss: 2.632521\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 823\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382650, T: 805717, Avg. loss: 2.638087\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 824\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382680, T: 806696, Avg. loss: 2.636069\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 825\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382710, T: 807675, Avg. loss: 2.634091\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 826\n",
            "Norm: 245689.44, NNZs: 936, Bias: -2905.382741, T: 808654, Avg. loss: 2.632174\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 827\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.382772, T: 809633, Avg. loss: 2.630206\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 828\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.382788, T: 810612, Avg. loss: 2.620818\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 829\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.382818, T: 811591, Avg. loss: 2.626319\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 830\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.382850, T: 812570, Avg. loss: 2.624437\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 831\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.382867, T: 813549, Avg. loss: 2.615072\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 832\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.382897, T: 814528, Avg. loss: 2.620547\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 833\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.382915, T: 815507, Avg. loss: 2.611331\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 834\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.382931, T: 816486, Avg. loss: 2.609397\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 835\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.382960, T: 817465, Avg. loss: 2.614841\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 836\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.382978, T: 818444, Avg. loss: 2.605650\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 837\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.382995, T: 819423, Avg. loss: 2.603825\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 838\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383014, T: 820402, Avg. loss: 2.602078\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 839\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383031, T: 821381, Avg. loss: 2.600135\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 840\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383060, T: 822360, Avg. loss: 2.605528\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 841\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383091, T: 823339, Avg. loss: 2.603659\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 842\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383120, T: 824318, Avg. loss: 2.601734\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 843\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383149, T: 825297, Avg. loss: 2.599830\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 844\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383178, T: 826276, Avg. loss: 2.597899\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 845\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383207, T: 827255, Avg. loss: 2.595978\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 846\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383223, T: 828234, Avg. loss: 2.586958\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 847\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383240, T: 829213, Avg. loss: 2.585161\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 848\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383255, T: 830192, Avg. loss: 2.583319\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 849\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383284, T: 831171, Avg. loss: 2.588673\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 850\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383312, T: 832150, Avg. loss: 2.586766\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 851\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383328, T: 833129, Avg. loss: 2.577795\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 852\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383357, T: 834108, Avg. loss: 2.583089\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 853\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383387, T: 835087, Avg. loss: 2.581246\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 854\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383404, T: 836066, Avg. loss: 2.572321\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 855\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383420, T: 837045, Avg. loss: 2.570519\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 856\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383448, T: 838024, Avg. loss: 2.575794\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 857\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383464, T: 839003, Avg. loss: 2.566897\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 858\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383494, T: 839982, Avg. loss: 2.572197\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 859\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383510, T: 840961, Avg. loss: 2.563323\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 860\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383526, T: 841940, Avg. loss: 2.561545\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 861\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383542, T: 842919, Avg. loss: 2.559755\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 862\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383559, T: 843898, Avg. loss: 2.558059\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 863\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383576, T: 844877, Avg. loss: 2.556252\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 864\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383605, T: 845856, Avg. loss: 2.561429\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 865\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383622, T: 846835, Avg. loss: 2.552714\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 866\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383639, T: 847814, Avg. loss: 2.550961\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 867\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383656, T: 848793, Avg. loss: 2.549190\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 868\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383672, T: 849772, Avg. loss: 2.547455\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 869\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383700, T: 850751, Avg. loss: 2.552529\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 870\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383728, T: 851730, Avg. loss: 2.550762\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 871\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383756, T: 852709, Avg. loss: 2.548917\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 872\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383784, T: 853688, Avg. loss: 2.547192\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 873\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383812, T: 854667, Avg. loss: 2.545317\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 874\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383838, T: 855646, Avg. loss: 2.543507\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 875\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383864, T: 856625, Avg. loss: 2.541700\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 876\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383891, T: 857604, Avg. loss: 2.539960\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 877\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383918, T: 858583, Avg. loss: 2.538154\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 878\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383934, T: 859562, Avg. loss: 2.529631\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 879\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383950, T: 860541, Avg. loss: 2.527934\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 880\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383965, T: 861520, Avg. loss: 2.526251\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 881\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383980, T: 862499, Avg. loss: 2.524517\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 882\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.383996, T: 863478, Avg. loss: 2.522840\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 883\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384023, T: 864457, Avg. loss: 2.527784\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 884\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384049, T: 865436, Avg. loss: 2.526065\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 885\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384065, T: 866415, Avg. loss: 2.517626\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 886\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384080, T: 867394, Avg. loss: 2.515942\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 887\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384107, T: 868373, Avg. loss: 2.520905\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 888\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384134, T: 869352, Avg. loss: 2.519181\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 889\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384149, T: 870331, Avg. loss: 2.510770\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 890\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384176, T: 871310, Avg. loss: 2.515680\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 891\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384201, T: 872289, Avg. loss: 2.513947\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 892\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384217, T: 873268, Avg. loss: 2.505686\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 893\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384233, T: 874247, Avg. loss: 2.504025\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 894\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384249, T: 875226, Avg. loss: 2.502375\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 895\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384275, T: 876205, Avg. loss: 2.507198\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 896\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384301, T: 877184, Avg. loss: 2.505476\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 897\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384326, T: 878163, Avg. loss: 2.503712\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 898\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384342, T: 879142, Avg. loss: 2.495522\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 899\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384367, T: 880121, Avg. loss: 2.500368\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 900\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384393, T: 881100, Avg. loss: 2.498701\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 901\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384419, T: 882079, Avg. loss: 2.496986\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 902\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384432, T: 883058, Avg. loss: 2.488751\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 903\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384447, T: 884037, Avg. loss: 2.487179\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 904\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384473, T: 885016, Avg. loss: 2.491961\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 905\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384487, T: 885995, Avg. loss: 2.483837\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 906\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384511, T: 886974, Avg. loss: 2.488562\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 907\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384527, T: 887953, Avg. loss: 2.480577\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 908\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384542, T: 888932, Avg. loss: 2.478923\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 909\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384567, T: 889911, Avg. loss: 2.483623\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 910\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384582, T: 890890, Avg. loss: 2.475664\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 911\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384606, T: 891869, Avg. loss: 2.480314\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 912\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384619, T: 892848, Avg. loss: 2.472335\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 913\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384633, T: 893827, Avg. loss: 2.470725\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 914\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384646, T: 894806, Avg. loss: 2.469083\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 915\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384660, T: 895785, Avg. loss: 2.467494\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 916\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384675, T: 896764, Avg. loss: 2.465918\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 917\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384702, T: 897743, Avg. loss: 2.470672\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 918\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384727, T: 898722, Avg. loss: 2.468961\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 919\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384752, T: 899701, Avg. loss: 2.467279\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 920\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384767, T: 900680, Avg. loss: 2.459428\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 921\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384782, T: 901659, Avg. loss: 2.457827\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 922\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384796, T: 902638, Avg. loss: 2.456197\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 923\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384821, T: 903617, Avg. loss: 2.460888\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 924\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384837, T: 904596, Avg. loss: 2.453070\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 925\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384862, T: 905575, Avg. loss: 2.457630\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 926\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384875, T: 906554, Avg. loss: 2.449789\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 927\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384899, T: 907533, Avg. loss: 2.454432\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 928\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384914, T: 908512, Avg. loss: 2.446669\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 929\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384939, T: 909491, Avg. loss: 2.451233\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 930\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384965, T: 910470, Avg. loss: 2.449630\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 931\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384979, T: 911449, Avg. loss: 2.441846\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 932\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.384992, T: 912428, Avg. loss: 2.440278\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 933\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385016, T: 913407, Avg. loss: 2.444848\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 934\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385031, T: 914386, Avg. loss: 2.437121\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 935\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385045, T: 915365, Avg. loss: 2.435588\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 936\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385059, T: 916344, Avg. loss: 2.434034\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 937\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385073, T: 917323, Avg. loss: 2.432484\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 938\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385098, T: 918302, Avg. loss: 2.437020\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 939\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385113, T: 919281, Avg. loss: 2.429394\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 940\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385137, T: 920260, Avg. loss: 2.433875\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 941\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385152, T: 921239, Avg. loss: 2.426232\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 942\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385166, T: 922218, Avg. loss: 2.424684\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 943\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385180, T: 923197, Avg. loss: 2.423230\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 944\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385203, T: 924176, Avg. loss: 2.427624\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 945\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385216, T: 925155, Avg. loss: 2.420097\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 946\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385240, T: 926134, Avg. loss: 2.424505\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 947\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385254, T: 927113, Avg. loss: 2.416963\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 948\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385279, T: 928092, Avg. loss: 2.421462\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 949\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385303, T: 929071, Avg. loss: 2.419849\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 950\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385318, T: 930050, Avg. loss: 2.412396\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 951\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385343, T: 931029, Avg. loss: 2.416775\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 952\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385356, T: 932008, Avg. loss: 2.409303\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 953\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385381, T: 932987, Avg. loss: 2.413696\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 954\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385394, T: 933966, Avg. loss: 2.406249\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 955\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385418, T: 934945, Avg. loss: 2.410661\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 956\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385432, T: 935924, Avg. loss: 2.403206\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 957\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385455, T: 936903, Avg. loss: 2.407586\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 958\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385469, T: 937882, Avg. loss: 2.400166\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 959\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385492, T: 938861, Avg. loss: 2.404515\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 960\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385516, T: 939840, Avg. loss: 2.402969\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 961\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385538, T: 940819, Avg. loss: 2.401454\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 962\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385561, T: 941798, Avg. loss: 2.399886\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 963\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385583, T: 942777, Avg. loss: 2.398334\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 964\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385605, T: 943756, Avg. loss: 2.396777\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 965\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385627, T: 944735, Avg. loss: 2.395263\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 966\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385640, T: 945714, Avg. loss: 2.387982\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 967\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385662, T: 946693, Avg. loss: 2.392268\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 968\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385675, T: 947672, Avg. loss: 2.384971\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 969\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385689, T: 948651, Avg. loss: 2.383571\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 970\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385711, T: 949630, Avg. loss: 2.387794\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 971\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385724, T: 950609, Avg. loss: 2.380575\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 972\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385737, T: 951588, Avg. loss: 2.379122\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 973\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385760, T: 952567, Avg. loss: 2.383397\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 974\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385772, T: 953546, Avg. loss: 2.376174\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 975\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385787, T: 954525, Avg. loss: 2.374752\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 976\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385809, T: 955504, Avg. loss: 2.378984\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 977\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385822, T: 956483, Avg. loss: 2.371818\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 978\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385835, T: 957462, Avg. loss: 2.370367\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 979\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385848, T: 958441, Avg. loss: 2.368944\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 980\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385861, T: 959420, Avg. loss: 2.367465\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 981\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385873, T: 960399, Avg. loss: 2.365994\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 982\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385894, T: 961378, Avg. loss: 2.370265\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 983\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385917, T: 962357, Avg. loss: 2.368780\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 984\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385929, T: 963336, Avg. loss: 2.361655\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 985\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385951, T: 964315, Avg. loss: 2.365856\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 986\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385973, T: 965294, Avg. loss: 2.364381\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 987\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.385995, T: 966273, Avg. loss: 2.362880\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 988\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.386016, T: 967252, Avg. loss: 2.361339\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 989\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.386037, T: 968231, Avg. loss: 2.359909\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 990\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.386058, T: 969210, Avg. loss: 2.358414\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 991\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.386070, T: 970189, Avg. loss: 2.351447\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 992\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.386092, T: 971168, Avg. loss: 2.355603\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 993\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.386105, T: 972147, Avg. loss: 2.348601\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 994\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.386126, T: 973126, Avg. loss: 2.352670\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 995\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.386147, T: 974105, Avg. loss: 2.351276\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 996\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.386167, T: 975084, Avg. loss: 2.349751\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 997\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.386179, T: 976063, Avg. loss: 2.342880\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 998\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.386190, T: 977042, Avg. loss: 2.341434\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 999\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.386202, T: 978021, Avg. loss: 2.340072\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 1000\n",
            "Norm: 245689.43, NNZs: 936, Bias: -2905.386214, T: 979000, Avg. loss: 2.338655\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 1\n",
            "Norm: 215.05, NNZs: 653, Bias: -1.815613, T: 979, Avg. loss: 10.028686\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 212.96, NNZs: 590, Bias: -1.323258, T: 1958, Avg. loss: 2.049919\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 212.63, NNZs: 556, Bias: -1.111045, T: 2937, Avg. loss: 0.613676\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 212.52, NNZs: 536, Bias: -1.021871, T: 3916, Avg. loss: 0.372635\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 212.50, NNZs: 516, Bias: -0.930917, T: 4895, Avg. loss: 0.310982\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 212.46, NNZs: 501, Bias: -0.928904, T: 5874, Avg. loss: 0.261442\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 212.44, NNZs: 494, Bias: -0.902095, T: 6853, Avg. loss: 0.250787\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 212.44, NNZs: 490, Bias: -0.866478, T: 7832, Avg. loss: 0.244796\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 212.43, NNZs: 478, Bias: -0.852760, T: 8811, Avg. loss: 0.226032\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 212.43, NNZs: 468, Bias: -0.835069, T: 9790, Avg. loss: 0.222897\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 212.43, NNZs: 463, Bias: -0.819552, T: 10769, Avg. loss: 0.218770\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 212.43, NNZs: 459, Bias: -0.814091, T: 11748, Avg. loss: 0.208210\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 212.43, NNZs: 458, Bias: -0.800320, T: 12727, Avg. loss: 0.208814\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 212.44, NNZs: 455, Bias: -0.790315, T: 13706, Avg. loss: 0.205571\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 212.44, NNZs: 453, Bias: -0.781718, T: 14685, Avg. loss: 0.203819\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 212.45, NNZs: 449, Bias: -0.772373, T: 15664, Avg. loss: 0.203092\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 212.45, NNZs: 447, Bias: -0.763863, T: 16643, Avg. loss: 0.201040\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 212.45, NNZs: 443, Bias: -0.757235, T: 17622, Avg. loss: 0.199498\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 212.46, NNZs: 437, Bias: -0.752263, T: 18601, Avg. loss: 0.199093\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 212.46, NNZs: 437, Bias: -0.744658, T: 19580, Avg. loss: 0.199304\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 212.47, NNZs: 435, Bias: -0.739592, T: 20559, Avg. loss: 0.197735\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 212.47, NNZs: 435, Bias: -0.734359, T: 21538, Avg. loss: 0.197232\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 212.48, NNZs: 432, Bias: -0.730993, T: 22517, Avg. loss: 0.194811\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 212.48, NNZs: 430, Bias: -0.724836, T: 23496, Avg. loss: 0.197492\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 212.49, NNZs: 428, Bias: -0.721685, T: 24475, Avg. loss: 0.195262\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 212.49, NNZs: 425, Bias: -0.718540, T: 25454, Avg. loss: 0.194142\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 212.50, NNZs: 421, Bias: -0.714191, T: 26433, Avg. loss: 0.195595\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 212.50, NNZs: 419, Bias: -0.710755, T: 27412, Avg. loss: 0.194585\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 28 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2081.30, NNZs: 781, Bias: -1.689625, T: 979, Avg. loss: 1315.139254\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1891.74, NNZs: 886, Bias: 2.684673, T: 1958, Avg. loss: 901.875563\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1877.79, NNZs: 902, Bias: -1.930907, T: 2937, Avg. loss: 60.280540\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1875.02, NNZs: 904, Bias: -1.939377, T: 3916, Avg. loss: 14.160070\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1873.92, NNZs: 900, Bias: -1.826759, T: 4895, Avg. loss: 6.837641\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1873.33, NNZs: 892, Bias: -1.669532, T: 5874, Avg. loss: 4.312772\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1872.93, NNZs: 883, Bias: -1.695017, T: 6853, Avg. loss: 3.179382\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1872.66, NNZs: 882, Bias: -1.725940, T: 7832, Avg. loss: 2.494038\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1872.46, NNZs: 878, Bias: -1.714325, T: 8811, Avg. loss: 2.043502\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1872.31, NNZs: 877, Bias: -1.717829, T: 9790, Avg. loss: 1.732449\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1872.20, NNZs: 873, Bias: -1.703392, T: 10769, Avg. loss: 1.514053\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1872.10, NNZs: 860, Bias: -1.688664, T: 11748, Avg. loss: 1.343925\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 1872.02, NNZs: 859, Bias: -1.695296, T: 12727, Avg. loss: 1.215637\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 1871.96, NNZs: 856, Bias: -1.688397, T: 13706, Avg. loss: 1.110359\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 1871.90, NNZs: 856, Bias: -1.696903, T: 14685, Avg. loss: 1.027766\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 1871.85, NNZs: 854, Bias: -1.693164, T: 15664, Avg. loss: 0.948564\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 1871.80, NNZs: 854, Bias: -1.695297, T: 16643, Avg. loss: 0.888095\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 1871.76, NNZs: 853, Bias: -1.696013, T: 17622, Avg. loss: 0.836008\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 1871.73, NNZs: 849, Bias: -1.694527, T: 18601, Avg. loss: 0.785924\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 1871.70, NNZs: 849, Bias: -1.694641, T: 19580, Avg. loss: 0.745927\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 1871.67, NNZs: 849, Bias: -1.692053, T: 20559, Avg. loss: 0.706251\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 1871.64, NNZs: 848, Bias: -1.689592, T: 21538, Avg. loss: 0.674590\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 1871.62, NNZs: 848, Bias: -1.693204, T: 22517, Avg. loss: 0.646858\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 1871.60, NNZs: 848, Bias: -1.693091, T: 23496, Avg. loss: 0.620417\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 1871.58, NNZs: 848, Bias: -1.690403, T: 24475, Avg. loss: 0.594690\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 1871.56, NNZs: 844, Bias: -1.689844, T: 25454, Avg. loss: 0.573507\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 1871.55, NNZs: 843, Bias: -1.692698, T: 26433, Avg. loss: 0.554503\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 1871.53, NNZs: 843, Bias: -1.693946, T: 27412, Avg. loss: 0.536355\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 1871.52, NNZs: 842, Bias: -1.693973, T: 28391, Avg. loss: 0.518860\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 1871.50, NNZs: 842, Bias: -1.692824, T: 29370, Avg. loss: 0.502630\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 1871.49, NNZs: 839, Bias: -1.692032, T: 30349, Avg. loss: 0.489079\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 1871.48, NNZs: 839, Bias: -1.691332, T: 31328, Avg. loss: 0.476311\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 1871.47, NNZs: 839, Bias: -1.690528, T: 32307, Avg. loss: 0.463323\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 1871.46, NNZs: 836, Bias: -1.690114, T: 33286, Avg. loss: 0.451785\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 1871.45, NNZs: 836, Bias: -1.689706, T: 34265, Avg. loss: 0.441351\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 1871.44, NNZs: 836, Bias: -1.690135, T: 35244, Avg. loss: 0.432046\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 1871.43, NNZs: 836, Bias: -1.690587, T: 36223, Avg. loss: 0.421476\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 1871.42, NNZs: 836, Bias: -1.691022, T: 37202, Avg. loss: 0.412914\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 1871.41, NNZs: 836, Bias: -1.691770, T: 38181, Avg. loss: 0.404467\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 1871.40, NNZs: 836, Bias: -1.691783, T: 39160, Avg. loss: 0.395964\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 1871.40, NNZs: 836, Bias: -1.692177, T: 40139, Avg. loss: 0.388679\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 1871.39, NNZs: 835, Bias: -1.692146, T: 41118, Avg. loss: 0.381078\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 1871.38, NNZs: 835, Bias: -1.691760, T: 42097, Avg. loss: 0.373818\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 1871.38, NNZs: 832, Bias: -1.691457, T: 43076, Avg. loss: 0.367865\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 1871.37, NNZs: 832, Bias: -1.691398, T: 44055, Avg. loss: 0.361555\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 1871.37, NNZs: 831, Bias: -1.692552, T: 45034, Avg. loss: 0.355954\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 1871.36, NNZs: 830, Bias: -1.692265, T: 46013, Avg. loss: 0.349504\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 1871.36, NNZs: 827, Bias: -1.692251, T: 46992, Avg. loss: 0.344221\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 1871.35, NNZs: 824, Bias: -1.692554, T: 47971, Avg. loss: 0.339330\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 1871.35, NNZs: 824, Bias: -1.693249, T: 48950, Avg. loss: 0.334739\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 1871.34, NNZs: 824, Bias: -1.693433, T: 49929, Avg. loss: 0.329735\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 1871.34, NNZs: 824, Bias: -1.693272, T: 50908, Avg. loss: 0.324811\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 1871.33, NNZs: 824, Bias: -1.693780, T: 51887, Avg. loss: 0.320565\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 1871.33, NNZs: 824, Bias: -1.693943, T: 52866, Avg. loss: 0.316289\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 1871.32, NNZs: 824, Bias: -1.693822, T: 53845, Avg. loss: 0.311766\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 1871.32, NNZs: 824, Bias: -1.694590, T: 54824, Avg. loss: 0.308595\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 1871.32, NNZs: 823, Bias: -1.694800, T: 55803, Avg. loss: 0.304657\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 1871.31, NNZs: 823, Bias: -1.694740, T: 56782, Avg. loss: 0.300700\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 1871.31, NNZs: 823, Bias: -1.695122, T: 57761, Avg. loss: 0.297597\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 1871.30, NNZs: 823, Bias: -1.695201, T: 58740, Avg. loss: 0.293794\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 1871.30, NNZs: 823, Bias: -1.695234, T: 59719, Avg. loss: 0.290867\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 1871.30, NNZs: 823, Bias: -1.695229, T: 60698, Avg. loss: 0.287108\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 1871.29, NNZs: 823, Bias: -1.695602, T: 61677, Avg. loss: 0.284823\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 1871.29, NNZs: 823, Bias: -1.695659, T: 62656, Avg. loss: 0.281390\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 1871.29, NNZs: 823, Bias: -1.695611, T: 63635, Avg. loss: 0.278332\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 1871.29, NNZs: 823, Bias: -1.695926, T: 64614, Avg. loss: 0.275779\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 1871.28, NNZs: 822, Bias: -1.696099, T: 65593, Avg. loss: 0.273303\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 1871.28, NNZs: 822, Bias: -1.696199, T: 66572, Avg. loss: 0.270401\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 1871.28, NNZs: 821, Bias: -1.696552, T: 67551, Avg. loss: 0.267926\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 1871.27, NNZs: 821, Bias: -1.696512, T: 68530, Avg. loss: 0.265516\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 1871.27, NNZs: 820, Bias: -1.696585, T: 69509, Avg. loss: 0.262974\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 1871.27, NNZs: 820, Bias: -1.696631, T: 70488, Avg. loss: 0.260841\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 1871.27, NNZs: 820, Bias: -1.696792, T: 71467, Avg. loss: 0.258536\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 1871.26, NNZs: 820, Bias: -1.696888, T: 72446, Avg. loss: 0.256216\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 1871.26, NNZs: 820, Bias: -1.697241, T: 73425, Avg. loss: 0.254432\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 1871.26, NNZs: 820, Bias: -1.697437, T: 74404, Avg. loss: 0.252291\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 1871.26, NNZs: 820, Bias: -1.697510, T: 75383, Avg. loss: 0.250092\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 1871.26, NNZs: 820, Bias: -1.697410, T: 76362, Avg. loss: 0.248027\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 1871.25, NNZs: 818, Bias: -1.697412, T: 77341, Avg. loss: 0.245949\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 1871.25, NNZs: 818, Bias: -1.697454, T: 78320, Avg. loss: 0.244374\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 1871.25, NNZs: 818, Bias: -1.697593, T: 79299, Avg. loss: 0.242818\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 1871.25, NNZs: 817, Bias: -1.697898, T: 80278, Avg. loss: 0.240985\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 1871.25, NNZs: 817, Bias: -1.698106, T: 81257, Avg. loss: 0.239243\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 1871.24, NNZs: 817, Bias: -1.698229, T: 82236, Avg. loss: 0.237278\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 1871.24, NNZs: 817, Bias: -1.698452, T: 83215, Avg. loss: 0.235900\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 1871.24, NNZs: 817, Bias: -1.698497, T: 84194, Avg. loss: 0.233867\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 1871.24, NNZs: 817, Bias: -1.698751, T: 85173, Avg. loss: 0.232641\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 1871.24, NNZs: 815, Bias: -1.698923, T: 86152, Avg. loss: 0.231058\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 1871.24, NNZs: 815, Bias: -1.698955, T: 87131, Avg. loss: 0.229459\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 1871.23, NNZs: 813, Bias: -1.698991, T: 88110, Avg. loss: 0.227901\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 1871.23, NNZs: 812, Bias: -1.699122, T: 89089, Avg. loss: 0.226574\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 1871.23, NNZs: 812, Bias: -1.699340, T: 90068, Avg. loss: 0.225265\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 1871.23, NNZs: 812, Bias: -1.699382, T: 91047, Avg. loss: 0.223817\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 1871.23, NNZs: 812, Bias: -1.699531, T: 92026, Avg. loss: 0.222362\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 1871.23, NNZs: 812, Bias: -1.699693, T: 93005, Avg. loss: 0.221198\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 1871.22, NNZs: 812, Bias: -1.699860, T: 93984, Avg. loss: 0.219737\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 1871.22, NNZs: 812, Bias: -1.700003, T: 94963, Avg. loss: 0.218584\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 1871.22, NNZs: 812, Bias: -1.700135, T: 95942, Avg. loss: 0.217390\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 1871.22, NNZs: 812, Bias: -1.700361, T: 96921, Avg. loss: 0.216337\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 1871.22, NNZs: 812, Bias: -1.700491, T: 97900, Avg. loss: 0.214960\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 1871.22, NNZs: 811, Bias: -1.700527, T: 98879, Avg. loss: 0.213575\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 1871.22, NNZs: 810, Bias: -1.700630, T: 99858, Avg. loss: 0.212473\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 1871.21, NNZs: 810, Bias: -1.700650, T: 100837, Avg. loss: 0.211368\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 1871.21, NNZs: 810, Bias: -1.700699, T: 101816, Avg. loss: 0.210340\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 1871.21, NNZs: 810, Bias: -1.700825, T: 102795, Avg. loss: 0.209353\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 1871.21, NNZs: 810, Bias: -1.700940, T: 103774, Avg. loss: 0.208093\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 1871.21, NNZs: 810, Bias: -1.701078, T: 104753, Avg. loss: 0.207154\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 1871.21, NNZs: 810, Bias: -1.701254, T: 105732, Avg. loss: 0.206299\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 1871.21, NNZs: 810, Bias: -1.701458, T: 106711, Avg. loss: 0.205318\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 1871.21, NNZs: 809, Bias: -1.701566, T: 107690, Avg. loss: 0.204155\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 1871.20, NNZs: 809, Bias: -1.701566, T: 108669, Avg. loss: 0.203132\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 1871.20, NNZs: 808, Bias: -1.701722, T: 109648, Avg. loss: 0.202355\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 1871.20, NNZs: 808, Bias: -1.701881, T: 110627, Avg. loss: 0.201452\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 1871.20, NNZs: 808, Bias: -1.701955, T: 111606, Avg. loss: 0.200450\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 1871.20, NNZs: 807, Bias: -1.701991, T: 112585, Avg. loss: 0.199458\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 1871.20, NNZs: 807, Bias: -1.702112, T: 113564, Avg. loss: 0.198775\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 1871.20, NNZs: 807, Bias: -1.702238, T: 114543, Avg. loss: 0.197909\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 1871.20, NNZs: 807, Bias: -1.702304, T: 115522, Avg. loss: 0.196835\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 1871.20, NNZs: 807, Bias: -1.702368, T: 116501, Avg. loss: 0.196092\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 1871.20, NNZs: 807, Bias: -1.702356, T: 117480, Avg. loss: 0.195292\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 1871.19, NNZs: 807, Bias: -1.702445, T: 118459, Avg. loss: 0.194605\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 1871.19, NNZs: 807, Bias: -1.702542, T: 119438, Avg. loss: 0.193753\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 1871.19, NNZs: 807, Bias: -1.702557, T: 120417, Avg. loss: 0.192700\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 1871.19, NNZs: 807, Bias: -1.702572, T: 121396, Avg. loss: 0.192039\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 1871.19, NNZs: 807, Bias: -1.702629, T: 122375, Avg. loss: 0.191304\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 1871.19, NNZs: 807, Bias: -1.702656, T: 123354, Avg. loss: 0.190502\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 1871.19, NNZs: 807, Bias: -1.702747, T: 124333, Avg. loss: 0.189903\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 1871.19, NNZs: 807, Bias: -1.702833, T: 125312, Avg. loss: 0.189225\n",
            "Total training time: 0.06 seconds.\n",
            "Convergence after 128 epochs took 0.06 seconds\n",
            "-- Epoch 1\n",
            "Norm: 128923.99, NNZs: 733, Bias: -413.980557, T: 979, Avg. loss: 2298880.629774\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 152116.82, NNZs: 899, Bias: -1271.284960, T: 1958, Avg. loss: 6100045.146952\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 151152.92, NNZs: 927, Bias: -1155.300477, T: 2937, Avg. loss: 890657.629243\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 150388.73, NNZs: 932, Bias: -1492.940857, T: 3916, Avg. loss: 176283.667306\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 150310.19, NNZs: 928, Bias: -1531.841355, T: 4895, Avg. loss: 18063.140700\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 150271.18, NNZs: 928, Bias: -1559.376947, T: 5874, Avg. loss: 5593.007203\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 150259.32, NNZs: 928, Bias: -1566.893662, T: 6853, Avg. loss: 2249.062045\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 150248.11, NNZs: 927, Bias: -1575.504120, T: 7832, Avg. loss: 1496.385160\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 150243.00, NNZs: 923, Bias: -1578.964272, T: 8811, Avg. loss: 1056.400021\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 150239.32, NNZs: 923, Bias: -1581.409260, T: 9790, Avg. loss: 832.098221\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 150237.68, NNZs: 921, Bias: -1582.197127, T: 10769, Avg. loss: 570.389746\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 150235.56, NNZs: 922, Bias: -1583.572290, T: 11748, Avg. loss: 567.293852\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 150233.75, NNZs: 922, Bias: -1584.758373, T: 12727, Avg. loss: 525.498930\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 150231.73, NNZs: 922, Bias: -1586.223501, T: 13706, Avg. loss: 518.025644\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 150230.84, NNZs: 922, Bias: -1586.674536, T: 14685, Avg. loss: 414.846315\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 150229.54, NNZs: 922, Bias: -1587.552915, T: 15664, Avg. loss: 445.428443\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 150228.61, NNZs: 922, Bias: -1588.124268, T: 16643, Avg. loss: 410.736459\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 150228.15, NNZs: 922, Bias: -1588.271491, T: 17622, Avg. loss: 344.297333\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 150227.61, NNZs: 922, Bias: -1588.521869, T: 18601, Avg. loss: 345.885489\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 150227.06, NNZs: 922, Bias: -1588.795293, T: 19580, Avg. loss: 341.531221\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 150226.34, NNZs: 922, Bias: -1589.259889, T: 20559, Avg. loss: 358.930192\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 150225.77, NNZs: 922, Bias: -1589.597841, T: 21538, Avg. loss: 337.777980\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 150225.46, NNZs: 922, Bias: -1589.698648, T: 22517, Avg. loss: 296.824221\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 150224.97, NNZs: 921, Bias: -1589.981647, T: 23496, Avg. loss: 319.719603\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 150224.55, NNZs: 921, Bias: -1590.220220, T: 24475, Avg. loss: 306.644208\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 150224.20, NNZs: 921, Bias: -1590.394409, T: 25454, Avg. loss: 296.085766\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 150224.02, NNZs: 921, Bias: -1590.417777, T: 26433, Avg. loss: 264.774951\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 150223.68, NNZs: 920, Bias: -1590.597856, T: 27412, Avg. loss: 285.957548\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 150223.39, NNZs: 920, Bias: -1590.743930, T: 28391, Avg. loss: 276.530598\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 150223.23, NNZs: 920, Bias: -1590.770836, T: 29370, Avg. loss: 250.003455\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 150223.04, NNZs: 920, Bias: -1590.834727, T: 30349, Avg. loss: 249.676488\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 150222.83, NNZs: 920, Bias: -1590.914372, T: 31328, Avg. loss: 248.302872\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 150222.55, NNZs: 920, Bias: -1591.074114, T: 32307, Avg. loss: 261.551478\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm: 150222.30, NNZs: 920, Bias: -1591.204931, T: 33286, Avg. loss: 253.763385\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 150222.15, NNZs: 920, Bias: -1591.251412, T: 34265, Avg. loss: 232.850402\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 150221.93, NNZs: 920, Bias: -1591.369795, T: 35244, Avg. loss: 245.474446\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 150221.72, NNZs: 920, Bias: -1591.477665, T: 36223, Avg. loss: 240.250800\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 150221.58, NNZs: 920, Bias: -1591.515311, T: 37202, Avg. loss: 222.292318\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 150221.45, NNZs: 920, Bias: -1591.563277, T: 38181, Avg. loss: 221.031121\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 150221.25, NNZs: 920, Bias: -1591.664745, T: 39160, Avg. loss: 231.471256\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 150221.12, NNZs: 920, Bias: -1591.710088, T: 40139, Avg. loss: 214.925079\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 150220.95, NNZs: 920, Bias: -1591.803667, T: 41118, Avg. loss: 224.442970\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 150220.79, NNZs: 920, Bias: -1591.884180, T: 42097, Avg. loss: 220.758855\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 150220.64, NNZs: 920, Bias: -1591.953976, T: 43076, Avg. loss: 216.609543\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 150220.53, NNZs: 918, Bias: -1591.986422, T: 44055, Avg. loss: 203.052317\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 150220.39, NNZs: 918, Bias: -1592.056101, T: 45034, Avg. loss: 211.871139\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 150220.29, NNZs: 918, Bias: -1592.086966, T: 46013, Avg. loss: 198.730248\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 150220.16, NNZs: 918, Bias: -1592.154235, T: 46992, Avg. loss: 207.353836\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 150220.03, NNZs: 917, Bias: -1592.214580, T: 47971, Avg. loss: 204.085730\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 150219.91, NNZs: 917, Bias: -1592.273349, T: 48950, Avg. loss: 201.490399\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 150219.79, NNZs: 917, Bias: -1592.329075, T: 49929, Avg. loss: 198.708023\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 150219.71, NNZs: 917, Bias: -1592.348211, T: 50908, Avg. loss: 187.727674\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 150219.60, NNZs: 917, Bias: -1592.401271, T: 51887, Avg. loss: 195.077165\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 150219.49, NNZs: 917, Bias: -1592.453452, T: 52866, Avg. loss: 192.720687\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 150219.42, NNZs: 917, Bias: -1592.475468, T: 53845, Avg. loss: 182.867351\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 150219.34, NNZs: 917, Bias: -1592.497977, T: 54824, Avg. loss: 181.950255\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 150219.24, NNZs: 917, Bias: -1592.551475, T: 55803, Avg. loss: 188.486631\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 150219.16, NNZs: 917, Bias: -1592.576661, T: 56782, Avg. loss: 178.965235\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 150219.09, NNZs: 917, Bias: -1592.601990, T: 57761, Avg. loss: 178.113658\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 150218.99, NNZs: 917, Bias: -1592.654848, T: 58740, Avg. loss: 183.958831\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 150218.89, NNZs: 917, Bias: -1592.700451, T: 59719, Avg. loss: 181.864303\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 150218.80, NNZs: 917, Bias: -1592.743874, T: 60698, Avg. loss: 179.697842\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 150218.74, NNZs: 917, Bias: -1592.767391, T: 61677, Avg. loss: 171.639257\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 150218.65, NNZs: 917, Bias: -1592.809230, T: 62656, Avg. loss: 176.982062\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 150218.59, NNZs: 917, Bias: -1592.829740, T: 63635, Avg. loss: 169.196486\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 150218.52, NNZs: 917, Bias: -1592.854343, T: 64614, Avg. loss: 168.445227\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 150218.46, NNZs: 917, Bias: -1592.879555, T: 65593, Avg. loss: 167.454823\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 150218.37, NNZs: 917, Bias: -1592.922275, T: 66572, Avg. loss: 172.254367\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 150218.30, NNZs: 917, Bias: -1592.960365, T: 67551, Avg. loss: 170.445176\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 150218.24, NNZs: 917, Bias: -1592.983038, T: 68530, Avg. loss: 163.587454\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 150218.18, NNZs: 917, Bias: -1593.006715, T: 69509, Avg. loss: 162.810790\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 150218.12, NNZs: 917, Bias: -1593.029240, T: 70488, Avg. loss: 161.917253\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 150218.06, NNZs: 917, Bias: -1593.053341, T: 71467, Avg. loss: 161.000260\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 150217.99, NNZs: 917, Bias: -1593.089067, T: 72446, Avg. loss: 164.997991\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 150217.92, NNZs: 917, Bias: -1593.123794, T: 73425, Avg. loss: 163.592498\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 150217.85, NNZs: 917, Bias: -1593.157535, T: 74404, Avg. loss: 162.154907\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 150217.80, NNZs: 917, Bias: -1593.177627, T: 75383, Avg. loss: 156.084187\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 150217.73, NNZs: 917, Bias: -1593.208199, T: 76362, Avg. loss: 159.826760\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 150217.68, NNZs: 917, Bias: -1593.225127, T: 77341, Avg. loss: 154.046127\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 150217.62, NNZs: 917, Bias: -1593.255931, T: 78320, Avg. loss: 157.739987\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 150217.57, NNZs: 917, Bias: -1593.274751, T: 79299, Avg. loss: 152.207647\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 150217.52, NNZs: 917, Bias: -1593.294059, T: 80278, Avg. loss: 151.515126\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 150217.47, NNZs: 917, Bias: -1593.323284, T: 81257, Avg. loss: 154.973161\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 150217.41, NNZs: 917, Bias: -1593.351066, T: 82236, Avg. loss: 153.911293\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 150217.36, NNZs: 917, Bias: -1593.368739, T: 83215, Avg. loss: 148.610939\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 150217.31, NNZs: 917, Bias: -1593.396346, T: 84194, Avg. loss: 152.062805\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 150217.26, NNZs: 917, Bias: -1593.412199, T: 85173, Avg. loss: 146.951574\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 150217.22, NNZs: 917, Bias: -1593.428801, T: 86152, Avg. loss: 146.331568\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 150217.17, NNZs: 917, Bias: -1593.447316, T: 87131, Avg. loss: 145.813414\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 150217.13, NNZs: 917, Bias: -1593.465564, T: 88110, Avg. loss: 145.095110\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 150217.08, NNZs: 917, Bias: -1593.491678, T: 89089, Avg. loss: 148.098050\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 150217.03, NNZs: 917, Bias: -1593.516697, T: 90068, Avg. loss: 147.098753\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 150216.98, NNZs: 917, Bias: -1593.540920, T: 91047, Avg. loss: 146.056738\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 150216.94, NNZs: 917, Bias: -1593.556901, T: 92026, Avg. loss: 141.593859\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 150216.90, NNZs: 917, Bias: -1593.573529, T: 93005, Avg. loss: 141.078389\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 150216.85, NNZs: 917, Bias: -1593.597011, T: 93984, Avg. loss: 143.861900\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 150216.80, NNZs: 917, Bias: -1593.620024, T: 94963, Avg. loss: 142.902137\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 150216.77, NNZs: 917, Bias: -1593.634670, T: 95942, Avg. loss: 138.694095\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 150216.73, NNZs: 917, Bias: -1593.650362, T: 96921, Avg. loss: 138.276819\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 150216.69, NNZs: 917, Bias: -1593.666131, T: 97900, Avg. loss: 137.716402\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 150216.65, NNZs: 917, Bias: -1593.681536, T: 98879, Avg. loss: 137.096279\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 150216.61, NNZs: 917, Bias: -1593.704293, T: 99858, Avg. loss: 139.672477\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 150216.57, NNZs: 917, Bias: -1593.719602, T: 100837, Avg. loss: 135.788862\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 150216.53, NNZs: 917, Bias: -1593.741539, T: 101816, Avg. loss: 138.272435\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 150216.48, NNZs: 917, Bias: -1593.762243, T: 102795, Avg. loss: 137.458183\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 150216.45, NNZs: 917, Bias: -1593.776592, T: 103774, Avg. loss: 133.698243\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 150216.41, NNZs: 917, Bias: -1593.796300, T: 104753, Avg. loss: 136.093434\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 150216.37, NNZs: 917, Bias: -1593.816205, T: 105732, Avg. loss: 135.365485\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 150216.34, NNZs: 917, Bias: -1593.828936, T: 106711, Avg. loss: 131.757372\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 150216.30, NNZs: 917, Bias: -1593.847549, T: 107690, Avg. loss: 134.067604\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 150216.26, NNZs: 917, Bias: -1593.865789, T: 108669, Avg. loss: 133.407799\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 150216.22, NNZs: 917, Bias: -1593.883404, T: 109648, Avg. loss: 132.627391\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 150216.19, NNZs: 917, Bias: -1593.895848, T: 110627, Avg. loss: 129.303766\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 150216.16, NNZs: 917, Bias: -1593.913869, T: 111606, Avg. loss: 131.571515\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 150216.13, NNZs: 917, Bias: -1593.926572, T: 112585, Avg. loss: 128.263174\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 150216.09, NNZs: 917, Bias: -1593.943518, T: 113564, Avg. loss: 130.399620\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 150216.06, NNZs: 917, Bias: -1593.955466, T: 114543, Avg. loss: 127.203269\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 150216.03, NNZs: 917, Bias: -1593.972876, T: 115522, Avg. loss: 129.331700\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 150215.99, NNZs: 917, Bias: -1593.989470, T: 116501, Avg. loss: 128.725273\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 150215.96, NNZs: 917, Bias: -1594.005316, T: 117480, Avg. loss: 128.066792\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 150215.93, NNZs: 917, Bias: -1594.016533, T: 118459, Avg. loss: 125.069546\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 150215.90, NNZs: 917, Bias: -1594.032707, T: 119438, Avg. loss: 127.079364\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 150215.87, NNZs: 917, Bias: -1594.044204, T: 120417, Avg. loss: 124.116907\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 150215.84, NNZs: 917, Bias: -1594.055439, T: 121396, Avg. loss: 123.737955\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 150215.81, NNZs: 917, Bias: -1594.067356, T: 122375, Avg. loss: 123.386173\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 150215.79, NNZs: 917, Bias: -1594.079136, T: 123354, Avg. loss: 123.006483\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 150215.76, NNZs: 917, Bias: -1594.091115, T: 124333, Avg. loss: 122.637483\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 150215.73, NNZs: 917, Bias: -1594.103142, T: 125312, Avg. loss: 122.247931\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 150215.70, NNZs: 917, Bias: -1594.118972, T: 126291, Avg. loss: 124.021352\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 150215.67, NNZs: 917, Bias: -1594.130929, T: 127270, Avg. loss: 121.349779\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 150215.64, NNZs: 917, Bias: -1594.146678, T: 128249, Avg. loss: 123.106495\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 150215.62, NNZs: 917, Bias: -1594.158113, T: 129228, Avg. loss: 120.428923\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 150215.59, NNZs: 917, Bias: -1594.172708, T: 130207, Avg. loss: 122.101285\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 150215.56, NNZs: 917, Bias: -1594.183841, T: 131186, Avg. loss: 119.504937\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 150215.53, NNZs: 917, Bias: -1594.198243, T: 132165, Avg. loss: 121.210722\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 150215.50, NNZs: 917, Bias: -1594.212056, T: 133144, Avg. loss: 120.686923\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 150215.48, NNZs: 917, Bias: -1594.226009, T: 134123, Avg. loss: 120.207623\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 150215.45, NNZs: 917, Bias: -1594.235878, T: 135102, Avg. loss: 117.698539\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 150215.43, NNZs: 917, Bias: -1594.246305, T: 136081, Avg. loss: 117.361719\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 150215.40, NNZs: 917, Bias: -1594.256787, T: 137060, Avg. loss: 117.033439\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 150215.38, NNZs: 917, Bias: -1594.270170, T: 138039, Avg. loss: 118.603333\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 150215.35, NNZs: 917, Bias: -1594.280597, T: 139018, Avg. loss: 116.222743\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 150215.33, NNZs: 917, Bias: -1594.294430, T: 139997, Avg. loss: 117.837829\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 150215.30, NNZs: 917, Bias: -1594.307643, T: 140976, Avg. loss: 117.325969\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 150215.27, NNZs: 917, Bias: -1594.320713, T: 141955, Avg. loss: 116.874693\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 150215.25, NNZs: 917, Bias: -1594.330343, T: 142934, Avg. loss: 114.630485\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 150215.23, NNZs: 917, Bias: -1594.340291, T: 143913, Avg. loss: 114.273980\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 150215.20, NNZs: 917, Bias: -1594.352727, T: 144892, Avg. loss: 115.749296\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 150215.18, NNZs: 917, Bias: -1594.362513, T: 145871, Avg. loss: 113.531503\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 150215.16, NNZs: 917, Bias: -1594.372006, T: 146850, Avg. loss: 113.222696\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 150215.13, NNZs: 917, Bias: -1594.384299, T: 147829, Avg. loss: 114.701435\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 150215.11, NNZs: 917, Bias: -1594.396263, T: 148808, Avg. loss: 114.245310\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 150215.09, NNZs: 917, Bias: -1594.405466, T: 149787, Avg. loss: 112.104315\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 150215.06, NNZs: 917, Bias: -1594.417463, T: 150766, Avg. loss: 113.547597\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 150215.04, NNZs: 917, Bias: -1594.429346, T: 151745, Avg. loss: 113.138783\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 150215.02, NNZs: 917, Bias: -1594.438260, T: 152724, Avg. loss: 111.041119\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 150215.00, NNZs: 917, Bias: -1594.447132, T: 153703, Avg. loss: 110.744052\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 150214.98, NNZs: 917, Bias: -1594.458606, T: 154682, Avg. loss: 112.138409\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 150214.96, NNZs: 917, Bias: -1594.467428, T: 155661, Avg. loss: 110.082200\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 150214.94, NNZs: 917, Bias: -1594.476419, T: 156640, Avg. loss: 109.798825\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 150214.92, NNZs: 917, Bias: -1594.485648, T: 157619, Avg. loss: 109.552735\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 150214.89, NNZs: 917, Bias: -1594.497151, T: 158598, Avg. loss: 110.846351\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 150214.87, NNZs: 917, Bias: -1594.508455, T: 159577, Avg. loss: 110.475735\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 150214.85, NNZs: 917, Bias: -1594.519536, T: 160556, Avg. loss: 110.093322\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 150214.83, NNZs: 917, Bias: -1594.528357, T: 161535, Avg. loss: 108.200487\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 150214.81, NNZs: 917, Bias: -1594.539216, T: 162514, Avg. loss: 109.453518\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 150214.79, NNZs: 917, Bias: -1594.547758, T: 163493, Avg. loss: 107.585678\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 150214.77, NNZs: 917, Bias: -1594.558255, T: 164472, Avg. loss: 108.775814\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 150214.75, NNZs: 917, Bias: -1594.568611, T: 165451, Avg. loss: 108.441872\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 150214.73, NNZs: 917, Bias: -1594.579106, T: 166430, Avg. loss: 108.112454\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 150214.71, NNZs: 917, Bias: -1594.589165, T: 167409, Avg. loss: 107.768438\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 150214.69, NNZs: 917, Bias: -1594.599123, T: 168388, Avg. loss: 107.442740\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 150214.67, NNZs: 917, Bias: -1594.606834, T: 169367, Avg. loss: 105.636100\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 150214.65, NNZs: 917, Bias: -1594.614833, T: 170346, Avg. loss: 105.389604\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 150214.63, NNZs: 917, Bias: -1594.622596, T: 171325, Avg. loss: 105.148052\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 150214.62, NNZs: 917, Bias: -1594.630469, T: 172304, Avg. loss: 104.895220\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 150214.60, NNZs: 917, Bias: -1594.638765, T: 173283, Avg. loss: 104.686786\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 150214.58, NNZs: 917, Bias: -1594.648737, T: 174262, Avg. loss: 105.795101\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 150214.56, NNZs: 917, Bias: -1594.656874, T: 175241, Avg. loss: 104.121011\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 150214.54, NNZs: 917, Bias: -1594.666941, T: 176220, Avg. loss: 105.256884\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 150214.52, NNZs: 917, Bias: -1594.676842, T: 177199, Avg. loss: 104.940366\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 150214.50, NNZs: 917, Bias: -1594.686583, T: 178178, Avg. loss: 104.614911\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 150214.48, NNZs: 917, Bias: -1594.694321, T: 179157, Avg. loss: 102.984668\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 150214.47, NNZs: 917, Bias: -1594.702149, T: 180136, Avg. loss: 102.751824\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 150214.45, NNZs: 917, Bias: -1594.709931, T: 181115, Avg. loss: 102.525464\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 150214.43, NNZs: 917, Bias: -1594.717697, T: 182094, Avg. loss: 102.281223\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 150214.42, NNZs: 917, Bias: -1594.725580, T: 183073, Avg. loss: 102.061172\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 150214.40, NNZs: 917, Bias: -1594.735250, T: 184052, Avg. loss: 103.110507\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 150214.38, NNZs: 917, Bias: -1594.744862, T: 185031, Avg. loss: 102.839522\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 150214.36, NNZs: 917, Bias: -1594.752754, T: 186010, Avg. loss: 101.288628\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 150214.35, NNZs: 917, Bias: -1594.762084, T: 186989, Avg. loss: 102.288781\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 150214.33, NNZs: 917, Bias: -1594.769839, T: 187968, Avg. loss: 100.786167\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 150214.31, NNZs: 917, Bias: -1594.777600, T: 188947, Avg. loss: 100.545899\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 150214.29, NNZs: 917, Bias: -1594.786750, T: 189926, Avg. loss: 101.563759\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 150214.28, NNZs: 917, Bias: -1594.795968, T: 190905, Avg. loss: 101.283890\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 150214.26, NNZs: 917, Bias: -1594.803394, T: 191884, Avg. loss: 99.812914\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 150214.24, NNZs: 917, Bias: -1594.812432, T: 192863, Avg. loss: 100.780556\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 150214.23, NNZs: 917, Bias: -1594.819981, T: 193842, Avg. loss: 99.331349\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 150214.21, NNZs: 917, Bias: -1594.829018, T: 194821, Avg. loss: 100.302595\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 150214.20, NNZs: 917, Bias: -1594.836252, T: 195800, Avg. loss: 98.849956\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 150214.18, NNZs: 917, Bias: -1594.845011, T: 196779, Avg. loss: 99.824543\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 150214.16, NNZs: 917, Bias: -1594.852203, T: 197758, Avg. loss: 98.393283\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 150214.15, NNZs: 917, Bias: -1594.861061, T: 198737, Avg. loss: 99.327830\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 150214.13, NNZs: 917, Bias: -1594.868107, T: 199716, Avg. loss: 97.935807\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 150214.11, NNZs: 917, Bias: -1594.876706, T: 200695, Avg. loss: 98.889942\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 150214.10, NNZs: 917, Bias: -1594.883822, T: 201674, Avg. loss: 97.504131\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 150214.08, NNZs: 917, Bias: -1594.892545, T: 202653, Avg. loss: 98.428342\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 150214.07, NNZs: 917, Bias: -1594.900929, T: 203632, Avg. loss: 98.165195\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 150214.05, NNZs: 917, Bias: -1594.909351, T: 204611, Avg. loss: 97.931736\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 150214.04, NNZs: 917, Bias: -1594.917802, T: 205590, Avg. loss: 97.691922\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 150214.02, NNZs: 917, Bias: -1594.926249, T: 206569, Avg. loss: 97.457451\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 150214.00, NNZs: 917, Bias: -1594.933113, T: 207548, Avg. loss: 96.138626\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 150213.99, NNZs: 917, Bias: -1594.939930, T: 208527, Avg. loss: 95.936930\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 214\n",
            "Norm: 150213.97, NNZs: 917, Bias: -1594.948153, T: 209506, Avg. loss: 96.822932\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 215\n",
            "Norm: 150213.96, NNZs: 917, Bias: -1594.955071, T: 210485, Avg. loss: 95.526631\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 216\n",
            "Norm: 150213.95, NNZs: 917, Bias: -1594.961916, T: 211464, Avg. loss: 95.348310\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 217\n",
            "Norm: 150213.93, NNZs: 917, Bias: -1594.968724, T: 212443, Avg. loss: 95.192597\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 218\n",
            "Norm: 150213.92, NNZs: 917, Bias: -1594.975592, T: 213422, Avg. loss: 94.980213\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 219\n",
            "Norm: 150213.90, NNZs: 917, Bias: -1594.982486, T: 214401, Avg. loss: 94.813015\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 220\n",
            "Norm: 150213.89, NNZs: 917, Bias: -1594.989317, T: 215380, Avg. loss: 94.631395\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 221\n",
            "Norm: 150213.88, NNZs: 917, Bias: -1594.996220, T: 216359, Avg. loss: 94.440600\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 222\n",
            "Norm: 150213.86, NNZs: 917, Bias: -1595.004207, T: 217338, Avg. loss: 95.266443\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 223\n",
            "Norm: 150213.85, NNZs: 917, Bias: -1595.012121, T: 218317, Avg. loss: 95.058411\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 224\n",
            "Norm: 150213.83, NNZs: 917, Bias: -1595.020160, T: 219296, Avg. loss: 94.830184\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 225\n",
            "Norm: 150213.82, NNZs: 917, Bias: -1595.028158, T: 220275, Avg. loss: 94.620083\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 226\n",
            "Norm: 150213.80, NNZs: 917, Bias: -1595.035948, T: 221254, Avg. loss: 94.398701\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 227\n",
            "Norm: 150213.79, NNZs: 917, Bias: -1595.042617, T: 222233, Avg. loss: 93.217262\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 228\n",
            "Norm: 150213.77, NNZs: 917, Bias: -1595.049308, T: 223212, Avg. loss: 93.051014\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 229\n",
            "Norm: 150213.76, NNZs: 917, Bias: -1595.055723, T: 224191, Avg. loss: 92.850035\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 230\n",
            "Norm: 150213.75, NNZs: 917, Bias: -1595.063304, T: 225170, Avg. loss: 93.634228\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 231\n",
            "Norm: 150213.73, NNZs: 917, Bias: -1595.069749, T: 226149, Avg. loss: 92.492943\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 232\n",
            "Norm: 150213.72, NNZs: 917, Bias: -1595.077333, T: 227128, Avg. loss: 93.264081\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 233\n",
            "Norm: 150213.71, NNZs: 917, Bias: -1595.085020, T: 228107, Avg. loss: 93.069604\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 234\n",
            "Norm: 150213.69, NNZs: 917, Bias: -1595.092400, T: 229086, Avg. loss: 92.858923\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 235\n",
            "Norm: 150213.68, NNZs: 917, Bias: -1595.098765, T: 230065, Avg. loss: 91.716349\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 236\n",
            "Norm: 150213.67, NNZs: 917, Bias: -1595.106126, T: 231044, Avg. loss: 92.493013\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 237\n",
            "Norm: 150213.65, NNZs: 917, Bias: -1595.112378, T: 232023, Avg. loss: 91.353007\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 238\n",
            "Norm: 150213.64, NNZs: 917, Bias: -1595.119699, T: 233002, Avg. loss: 92.102960\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 239\n",
            "Norm: 150213.63, NNZs: 917, Bias: -1595.126010, T: 233981, Avg. loss: 91.030515\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 240\n",
            "Norm: 150213.61, NNZs: 917, Bias: -1595.133181, T: 234960, Avg. loss: 91.744952\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 241\n",
            "Norm: 150213.60, NNZs: 917, Bias: -1595.140338, T: 235939, Avg. loss: 91.567882\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 242\n",
            "Norm: 150213.59, NNZs: 917, Bias: -1595.146483, T: 236918, Avg. loss: 90.471768\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 243\n",
            "Norm: 150213.57, NNZs: 917, Bias: -1595.152587, T: 237897, Avg. loss: 90.317918\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 244\n",
            "Norm: 150213.56, NNZs: 917, Bias: -1595.159674, T: 238876, Avg. loss: 91.044864\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 245\n",
            "Norm: 150213.55, NNZs: 917, Bias: -1595.165821, T: 239855, Avg. loss: 89.977222\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 246\n",
            "Norm: 150213.54, NNZs: 917, Bias: -1595.171936, T: 240834, Avg. loss: 89.828268\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 247\n",
            "Norm: 150213.52, NNZs: 917, Bias: -1595.179053, T: 241813, Avg. loss: 90.555103\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 248\n",
            "Norm: 150213.51, NNZs: 917, Bias: -1595.186095, T: 242792, Avg. loss: 90.351339\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 249\n",
            "Norm: 150213.50, NNZs: 917, Bias: -1595.192928, T: 243771, Avg. loss: 90.164148\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 250\n",
            "Norm: 150213.49, NNZs: 917, Bias: -1595.198914, T: 244750, Avg. loss: 89.140355\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 251\n",
            "Norm: 150213.47, NNZs: 917, Bias: -1595.205797, T: 245729, Avg. loss: 89.850630\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 252\n",
            "Norm: 150213.46, NNZs: 917, Bias: -1595.212549, T: 246708, Avg. loss: 89.663070\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 253\n",
            "Norm: 150213.45, NNZs: 917, Bias: -1595.218392, T: 247687, Avg. loss: 88.634894\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 254\n",
            "Norm: 150213.44, NNZs: 917, Bias: -1595.224177, T: 248666, Avg. loss: 88.490512\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 255\n",
            "Norm: 150213.42, NNZs: 917, Bias: -1595.230856, T: 249645, Avg. loss: 89.174252\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 256\n",
            "Norm: 150213.41, NNZs: 917, Bias: -1595.237592, T: 250624, Avg. loss: 89.001656\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 257\n",
            "Norm: 150213.40, NNZs: 917, Bias: -1595.243429, T: 251603, Avg. loss: 88.006337\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 258\n",
            "Norm: 150213.39, NNZs: 917, Bias: -1595.250060, T: 252582, Avg. loss: 88.694099\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 259\n",
            "Norm: 150213.38, NNZs: 917, Bias: -1595.256756, T: 253561, Avg. loss: 88.509662\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 260\n",
            "Norm: 150213.36, NNZs: 917, Bias: -1595.263282, T: 254540, Avg. loss: 88.346930\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 261\n",
            "Norm: 150213.35, NNZs: 917, Bias: -1595.268969, T: 255519, Avg. loss: 87.365194\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 262\n",
            "Norm: 150213.34, NNZs: 917, Bias: -1595.274627, T: 256498, Avg. loss: 87.231041\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 263\n",
            "Norm: 150213.33, NNZs: 917, Bias: -1595.281066, T: 257477, Avg. loss: 87.890411\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 264\n",
            "Norm: 150213.32, NNZs: 917, Bias: -1595.286704, T: 258456, Avg. loss: 86.926953\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 265\n",
            "Norm: 150213.31, NNZs: 917, Bias: -1595.292279, T: 259435, Avg. loss: 86.795041\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 266\n",
            "Norm: 150213.30, NNZs: 917, Bias: -1595.297890, T: 260414, Avg. loss: 86.658045\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 267\n",
            "Norm: 150213.28, NNZs: 917, Bias: -1595.304321, T: 261393, Avg. loss: 87.303879\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 268\n",
            "Norm: 150213.27, NNZs: 917, Bias: -1595.309916, T: 262372, Avg. loss: 86.360879\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 269\n",
            "Norm: 150213.26, NNZs: 917, Bias: -1595.315482, T: 263351, Avg. loss: 86.234379\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 270\n",
            "Norm: 150213.25, NNZs: 917, Bias: -1595.321877, T: 264330, Avg. loss: 86.868084\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 271\n",
            "Norm: 150213.24, NNZs: 917, Bias: -1595.328241, T: 265309, Avg. loss: 86.706967\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 272\n",
            "Norm: 150213.23, NNZs: 917, Bias: -1595.334413, T: 266288, Avg. loss: 86.523854\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 273\n",
            "Norm: 150213.22, NNZs: 917, Bias: -1595.339935, T: 267267, Avg. loss: 85.629583\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 274\n",
            "Norm: 150213.21, NNZs: 917, Bias: -1595.346155, T: 268246, Avg. loss: 86.255732\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 275\n",
            "Norm: 150213.19, NNZs: 917, Bias: -1595.352311, T: 269225, Avg. loss: 86.091267\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 276\n",
            "Norm: 150213.18, NNZs: 917, Bias: -1595.357652, T: 270204, Avg. loss: 85.186900\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 277\n",
            "Norm: 150213.17, NNZs: 917, Bias: -1595.363061, T: 271183, Avg. loss: 85.058984\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 278\n",
            "Norm: 150213.16, NNZs: 917, Bias: -1595.369123, T: 272162, Avg. loss: 85.660775\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 279\n",
            "Norm: 150213.15, NNZs: 917, Bias: -1595.374471, T: 273141, Avg. loss: 84.793269\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 280\n",
            "Norm: 150213.14, NNZs: 917, Bias: -1595.380462, T: 274120, Avg. loss: 85.371585\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 281\n",
            "Norm: 150213.13, NNZs: 917, Bias: -1595.386456, T: 275099, Avg. loss: 85.238691\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 282\n",
            "Norm: 150213.12, NNZs: 917, Bias: -1595.392483, T: 276078, Avg. loss: 85.079935\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 283\n",
            "Norm: 150213.11, NNZs: 917, Bias: -1595.397707, T: 277057, Avg. loss: 84.216320\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 284\n",
            "Norm: 150213.10, NNZs: 917, Bias: -1595.403736, T: 278036, Avg. loss: 84.814292\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 285\n",
            "Norm: 150213.09, NNZs: 917, Bias: -1595.409572, T: 279015, Avg. loss: 84.646197\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 286\n",
            "Norm: 150213.08, NNZs: 917, Bias: -1595.415482, T: 279994, Avg. loss: 84.520185\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 287\n",
            "Norm: 150213.07, NNZs: 917, Bias: -1595.420549, T: 280973, Avg. loss: 83.659724\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 288\n",
            "Norm: 150213.06, NNZs: 917, Bias: -1595.426390, T: 281952, Avg. loss: 84.250057\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 289\n",
            "Norm: 150213.05, NNZs: 917, Bias: -1595.431516, T: 282931, Avg. loss: 83.402297\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 290\n",
            "Norm: 150213.04, NNZs: 917, Bias: -1595.436670, T: 283910, Avg. loss: 83.280131\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 291\n",
            "Norm: 150213.02, NNZs: 917, Bias: -1595.442360, T: 284889, Avg. loss: 83.836938\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 292\n",
            "Norm: 150213.01, NNZs: 917, Bias: -1595.448116, T: 285868, Avg. loss: 83.716328\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 293\n",
            "Norm: 150213.00, NNZs: 917, Bias: -1595.453189, T: 286847, Avg. loss: 82.902672\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 294\n",
            "Norm: 150212.99, NNZs: 917, Bias: -1595.458979, T: 287826, Avg. loss: 83.456459\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 295\n",
            "Norm: 150212.98, NNZs: 917, Bias: -1595.463903, T: 288805, Avg. loss: 82.630064\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 296\n",
            "Norm: 150212.97, NNZs: 917, Bias: -1595.468983, T: 289784, Avg. loss: 82.516406\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 297\n",
            "Norm: 150212.97, NNZs: 917, Bias: -1595.474036, T: 290763, Avg. loss: 82.407953\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 298\n",
            "Norm: 150212.95, NNZs: 917, Bias: -1595.479704, T: 291742, Avg. loss: 82.951898\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 299\n",
            "Norm: 150212.95, NNZs: 917, Bias: -1595.484671, T: 292721, Avg. loss: 82.149436\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 300\n",
            "Norm: 150212.94, NNZs: 917, Bias: -1595.490222, T: 293700, Avg. loss: 82.700439\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 301\n",
            "Norm: 150212.93, NNZs: 917, Bias: -1595.495708, T: 294679, Avg. loss: 82.547834\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 302\n",
            "Norm: 150212.92, NNZs: 917, Bias: -1595.500686, T: 295658, Avg. loss: 81.778409\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 303\n",
            "Norm: 150212.91, NNZs: 917, Bias: -1595.505535, T: 296637, Avg. loss: 81.665763\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 304\n",
            "Norm: 150212.90, NNZs: 917, Bias: -1595.511026, T: 297616, Avg. loss: 82.182750\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 305\n",
            "Norm: 150212.89, NNZs: 917, Bias: -1595.515967, T: 298595, Avg. loss: 81.426112\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 306\n",
            "Norm: 150212.88, NNZs: 917, Bias: -1595.520869, T: 299574, Avg. loss: 81.300764\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 307\n",
            "Norm: 150212.87, NNZs: 917, Bias: -1595.526348, T: 300553, Avg. loss: 81.836639\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 308\n",
            "Norm: 150212.86, NNZs: 917, Bias: -1595.531770, T: 301532, Avg. loss: 81.703290\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 309\n",
            "Norm: 150212.85, NNZs: 917, Bias: -1595.536635, T: 302511, Avg. loss: 80.946861\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 310\n",
            "Norm: 150212.84, NNZs: 917, Bias: -1595.542055, T: 303490, Avg. loss: 81.460946\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 311\n",
            "Norm: 150212.83, NNZs: 917, Bias: -1595.546799, T: 304469, Avg. loss: 80.698172\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 312\n",
            "Norm: 150212.82, NNZs: 917, Bias: -1595.552129, T: 305448, Avg. loss: 81.200785\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 313\n",
            "Norm: 150212.81, NNZs: 917, Bias: -1595.557487, T: 306427, Avg. loss: 81.091900\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 314\n",
            "Norm: 150212.80, NNZs: 917, Bias: -1595.562746, T: 307406, Avg. loss: 80.957920\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 315\n",
            "Norm: 150212.79, NNZs: 917, Bias: -1595.567934, T: 308385, Avg. loss: 80.821878\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 316\n",
            "Norm: 150212.78, NNZs: 917, Bias: -1595.573229, T: 309364, Avg. loss: 80.713197\n",
            "Total training time: 0.73 seconds.\n",
            "Convergence after 316 epochs took 0.73 seconds\n",
            "-- Epoch 1\n",
            "Norm: 188.76, NNZs: 625, Bias: -1.155967, T: 979, Avg. loss: 9.054325\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 187.86, NNZs: 545, Bias: -0.929127, T: 1958, Avg. loss: 1.138767\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 187.79, NNZs: 513, Bias: -0.830488, T: 2937, Avg. loss: 0.400290\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 187.78, NNZs: 483, Bias: -0.775033, T: 3916, Avg. loss: 0.310029\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 187.79, NNZs: 469, Bias: -0.721198, T: 4895, Avg. loss: 0.268699\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 187.79, NNZs: 459, Bias: -0.735377, T: 5874, Avg. loss: 0.235531\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 187.79, NNZs: 450, Bias: -0.724237, T: 6853, Avg. loss: 0.231948\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 187.80, NNZs: 444, Bias: -0.708607, T: 7832, Avg. loss: 0.228300\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 187.81, NNZs: 440, Bias: -0.699791, T: 8811, Avg. loss: 0.218050\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 187.82, NNZs: 436, Bias: -0.698376, T: 9790, Avg. loss: 0.213022\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 187.83, NNZs: 429, Bias: -0.689504, T: 10769, Avg. loss: 0.212652\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 187.84, NNZs: 425, Bias: -0.691005, T: 11748, Avg. loss: 0.207396\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 187.85, NNZs: 421, Bias: -0.682728, T: 12727, Avg. loss: 0.209181\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 187.86, NNZs: 410, Bias: -0.679019, T: 13706, Avg. loss: 0.206107\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 187.87, NNZs: 407, Bias: -0.675691, T: 14685, Avg. loss: 0.205191\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 187.88, NNZs: 406, Bias: -0.670943, T: 15664, Avg. loss: 0.205063\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 187.89, NNZs: 398, Bias: -0.665451, T: 16643, Avg. loss: 0.204717\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 187.90, NNZs: 395, Bias: -0.664245, T: 17622, Avg. loss: 0.202484\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 187.91, NNZs: 389, Bias: -0.661018, T: 18601, Avg. loss: 0.203164\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 187.92, NNZs: 387, Bias: -0.658775, T: 19580, Avg. loss: 0.201988\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 187.93, NNZs: 380, Bias: -0.656910, T: 20559, Avg. loss: 0.202234\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 187.94, NNZs: 379, Bias: -0.653718, T: 21538, Avg. loss: 0.202073\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 187.95, NNZs: 376, Bias: -0.652656, T: 22517, Avg. loss: 0.201204\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 23 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2454.07, NNZs: 832, Bias: 7.499274, T: 979, Avg. loss: 1590.027340\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2258.93, NNZs: 906, Bias: 3.207509, T: 1958, Avg. loss: 1073.283115\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 2237.60, NNZs: 933, Bias: -2.431599, T: 2937, Avg. loss: 108.125478\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2234.14, NNZs: 930, Bias: -2.172410, T: 3916, Avg. loss: 20.640619\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 2232.71, NNZs: 933, Bias: -2.153301, T: 4895, Avg. loss: 10.271250\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 2231.90, NNZs: 932, Bias: -2.058018, T: 5874, Avg. loss: 6.774069\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 2231.38, NNZs: 932, Bias: -2.106631, T: 6853, Avg. loss: 5.097534\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 2230.99, NNZs: 933, Bias: -2.034876, T: 7832, Avg. loss: 4.235154\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 2230.69, NNZs: 934, Bias: -2.074032, T: 8811, Avg. loss: 3.617870\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 2230.45, NNZs: 929, Bias: -2.071730, T: 9790, Avg. loss: 3.204837\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 2230.25, NNZs: 929, Bias: -2.063722, T: 10769, Avg. loss: 2.864223\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 2230.08, NNZs: 929, Bias: -2.052076, T: 11748, Avg. loss: 2.629473\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 2229.94, NNZs: 923, Bias: -2.052338, T: 12727, Avg. loss: 2.418776\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 2229.82, NNZs: 921, Bias: -2.046290, T: 13706, Avg. loss: 2.256834\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 2229.71, NNZs: 921, Bias: -2.038303, T: 14685, Avg. loss: 2.111197\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 2229.62, NNZs: 919, Bias: -2.038576, T: 15664, Avg. loss: 1.999216\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 2229.53, NNZs: 915, Bias: -2.029062, T: 16643, Avg. loss: 1.887306\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 2229.46, NNZs: 915, Bias: -2.030652, T: 17622, Avg. loss: 1.800761\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 2229.39, NNZs: 915, Bias: -2.022620, T: 18601, Avg. loss: 1.720000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 2229.32, NNZs: 915, Bias: -2.015610, T: 19580, Avg. loss: 1.648019\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 2229.27, NNZs: 915, Bias: -2.013317, T: 20559, Avg. loss: 1.589159\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 2229.21, NNZs: 914, Bias: -2.002850, T: 21538, Avg. loss: 1.525941\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 2229.17, NNZs: 914, Bias: -1.997514, T: 22517, Avg. loss: 1.475650\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 2229.12, NNZs: 915, Bias: -1.997409, T: 23496, Avg. loss: 1.432104\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 2229.08, NNZs: 914, Bias: -1.994320, T: 24475, Avg. loss: 1.385624\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 2229.04, NNZs: 911, Bias: -1.993553, T: 25454, Avg. loss: 1.346346\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 2229.00, NNZs: 909, Bias: -1.991611, T: 26433, Avg. loss: 1.309738\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 2228.97, NNZs: 907, Bias: -1.987737, T: 27412, Avg. loss: 1.272062\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 2228.94, NNZs: 905, Bias: -1.983416, T: 28391, Avg. loss: 1.240745\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 2228.90, NNZs: 905, Bias: -1.983061, T: 29370, Avg. loss: 1.211697\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 2228.88, NNZs: 905, Bias: -1.979968, T: 30349, Avg. loss: 1.183017\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 2228.85, NNZs: 902, Bias: -1.976711, T: 31328, Avg. loss: 1.156071\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 2228.82, NNZs: 902, Bias: -1.975649, T: 32307, Avg. loss: 1.133314\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 2228.80, NNZs: 902, Bias: -1.973419, T: 33286, Avg. loss: 1.109470\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 2228.77, NNZs: 902, Bias: -1.972077, T: 34265, Avg. loss: 1.086638\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 2228.75, NNZs: 902, Bias: -1.970351, T: 35244, Avg. loss: 1.066617\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 2228.73, NNZs: 902, Bias: -1.967955, T: 36223, Avg. loss: 1.045635\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 2228.71, NNZs: 901, Bias: -1.966998, T: 37202, Avg. loss: 1.028695\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 2228.69, NNZs: 901, Bias: -1.964867, T: 38181, Avg. loss: 1.009429\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 2228.67, NNZs: 901, Bias: -1.963604, T: 39160, Avg. loss: 0.992982\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 2228.66, NNZs: 901, Bias: -1.962796, T: 40139, Avg. loss: 0.976623\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 2228.64, NNZs: 901, Bias: -1.960046, T: 41118, Avg. loss: 0.960952\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 2228.62, NNZs: 901, Bias: -1.957667, T: 42097, Avg. loss: 0.946535\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 2228.61, NNZs: 901, Bias: -1.956863, T: 43076, Avg. loss: 0.933555\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 2228.59, NNZs: 901, Bias: -1.955758, T: 44055, Avg. loss: 0.919572\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 2228.58, NNZs: 901, Bias: -1.955033, T: 45034, Avg. loss: 0.906306\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 2228.56, NNZs: 900, Bias: -1.952755, T: 46013, Avg. loss: 0.893175\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 2228.55, NNZs: 900, Bias: -1.951619, T: 46992, Avg. loss: 0.881292\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 2228.54, NNZs: 900, Bias: -1.950540, T: 47971, Avg. loss: 0.868979\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 2228.52, NNZs: 900, Bias: -1.949305, T: 48950, Avg. loss: 0.859144\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 2228.51, NNZs: 900, Bias: -1.947753, T: 49929, Avg. loss: 0.848320\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 2228.50, NNZs: 900, Bias: -1.945810, T: 50908, Avg. loss: 0.837628\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 2228.49, NNZs: 900, Bias: -1.945047, T: 51887, Avg. loss: 0.827595\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 2228.48, NNZs: 900, Bias: -1.944210, T: 52866, Avg. loss: 0.818123\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 2228.47, NNZs: 900, Bias: -1.942561, T: 53845, Avg. loss: 0.808300\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 2228.46, NNZs: 899, Bias: -1.940867, T: 54824, Avg. loss: 0.799200\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 2228.45, NNZs: 898, Bias: -1.939092, T: 55803, Avg. loss: 0.789531\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 2228.44, NNZs: 898, Bias: -1.937675, T: 56782, Avg. loss: 0.781969\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 2228.43, NNZs: 897, Bias: -1.937242, T: 57761, Avg. loss: 0.774139\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 2228.42, NNZs: 897, Bias: -1.936430, T: 58740, Avg. loss: 0.766232\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 2228.41, NNZs: 897, Bias: -1.934626, T: 59719, Avg. loss: 0.758071\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 2228.40, NNZs: 897, Bias: -1.933538, T: 60698, Avg. loss: 0.750723\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 2228.39, NNZs: 897, Bias: -1.932506, T: 61677, Avg. loss: 0.744163\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 2228.38, NNZs: 897, Bias: -1.931032, T: 62656, Avg. loss: 0.736576\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 2228.37, NNZs: 897, Bias: -1.930454, T: 63635, Avg. loss: 0.729440\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 2228.37, NNZs: 897, Bias: -1.929555, T: 64614, Avg. loss: 0.722517\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 2228.36, NNZs: 897, Bias: -1.928128, T: 65593, Avg. loss: 0.715818\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 2228.35, NNZs: 897, Bias: -1.927187, T: 66572, Avg. loss: 0.710289\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 2228.34, NNZs: 896, Bias: -1.926065, T: 67551, Avg. loss: 0.703703\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 2228.34, NNZs: 896, Bias: -1.924904, T: 68530, Avg. loss: 0.697733\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 2228.33, NNZs: 896, Bias: -1.923776, T: 69509, Avg. loss: 0.691671\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 2228.32, NNZs: 896, Bias: -1.923086, T: 70488, Avg. loss: 0.686374\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 2228.32, NNZs: 896, Bias: -1.922530, T: 71467, Avg. loss: 0.680747\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 2228.31, NNZs: 896, Bias: -1.921634, T: 72446, Avg. loss: 0.675399\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 2228.30, NNZs: 896, Bias: -1.921195, T: 73425, Avg. loss: 0.669877\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 2228.30, NNZs: 896, Bias: -1.920170, T: 74404, Avg. loss: 0.664273\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 2228.29, NNZs: 896, Bias: -1.919316, T: 75383, Avg. loss: 0.659750\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 2228.28, NNZs: 896, Bias: -1.918346, T: 76362, Avg. loss: 0.654817\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 2228.28, NNZs: 896, Bias: -1.917298, T: 77341, Avg. loss: 0.649348\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 2228.27, NNZs: 896, Bias: -1.916202, T: 78320, Avg. loss: 0.644908\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 2228.27, NNZs: 896, Bias: -1.915160, T: 79299, Avg. loss: 0.640157\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 2228.26, NNZs: 894, Bias: -1.914834, T: 80278, Avg. loss: 0.635777\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 2228.26, NNZs: 894, Bias: -1.913938, T: 81257, Avg. loss: 0.631166\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 2228.25, NNZs: 894, Bias: -1.913077, T: 82236, Avg. loss: 0.626945\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 2228.25, NNZs: 894, Bias: -1.912552, T: 83215, Avg. loss: 0.622673\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 2228.24, NNZs: 894, Bias: -1.911948, T: 84194, Avg. loss: 0.618688\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 2228.24, NNZs: 894, Bias: -1.911186, T: 85173, Avg. loss: 0.614362\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 2228.23, NNZs: 894, Bias: -1.910413, T: 86152, Avg. loss: 0.610248\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 2228.23, NNZs: 894, Bias: -1.909891, T: 87131, Avg. loss: 0.606542\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 2228.22, NNZs: 894, Bias: -1.909327, T: 88110, Avg. loss: 0.602685\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 2228.22, NNZs: 893, Bias: -1.908512, T: 89089, Avg. loss: 0.598580\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 2228.21, NNZs: 893, Bias: -1.907919, T: 90068, Avg. loss: 0.594884\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 2228.21, NNZs: 893, Bias: -1.907179, T: 91047, Avg. loss: 0.591608\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 2228.20, NNZs: 893, Bias: -1.906457, T: 92026, Avg. loss: 0.587626\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 2228.20, NNZs: 893, Bias: -1.905872, T: 93005, Avg. loss: 0.584246\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 2228.20, NNZs: 893, Bias: -1.905091, T: 93984, Avg. loss: 0.580664\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 2228.19, NNZs: 893, Bias: -1.904339, T: 94963, Avg. loss: 0.577483\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 2228.19, NNZs: 893, Bias: -1.903607, T: 95942, Avg. loss: 0.574122\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 2228.18, NNZs: 893, Bias: -1.902947, T: 96921, Avg. loss: 0.570736\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 2228.18, NNZs: 893, Bias: -1.902336, T: 97900, Avg. loss: 0.567643\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 2228.18, NNZs: 891, Bias: -1.901599, T: 98879, Avg. loss: 0.564668\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 2228.17, NNZs: 891, Bias: -1.901101, T: 99858, Avg. loss: 0.561299\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm: 2228.17, NNZs: 891, Bias: -1.900465, T: 100837, Avg. loss: 0.558069\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 2228.16, NNZs: 891, Bias: -1.899832, T: 101816, Avg. loss: 0.555332\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 2228.16, NNZs: 891, Bias: -1.899252, T: 102795, Avg. loss: 0.552353\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 2228.16, NNZs: 890, Bias: -1.898591, T: 103774, Avg. loss: 0.549354\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 2228.15, NNZs: 890, Bias: -1.898004, T: 104753, Avg. loss: 0.546552\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 2228.15, NNZs: 890, Bias: -1.897327, T: 105732, Avg. loss: 0.543736\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 2228.15, NNZs: 890, Bias: -1.896725, T: 106711, Avg. loss: 0.540657\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 2228.14, NNZs: 889, Bias: -1.896046, T: 107690, Avg. loss: 0.538093\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 2228.14, NNZs: 889, Bias: -1.895388, T: 108669, Avg. loss: 0.535460\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 2228.14, NNZs: 889, Bias: -1.894942, T: 109648, Avg. loss: 0.532901\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 2228.13, NNZs: 889, Bias: -1.894333, T: 110627, Avg. loss: 0.530146\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 2228.13, NNZs: 889, Bias: -1.893622, T: 111606, Avg. loss: 0.527850\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 2228.13, NNZs: 889, Bias: -1.892903, T: 112585, Avg. loss: 0.525332\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 2228.12, NNZs: 889, Bias: -1.892308, T: 113564, Avg. loss: 0.522824\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 2228.12, NNZs: 889, Bias: -1.891693, T: 114543, Avg. loss: 0.520151\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 2228.12, NNZs: 889, Bias: -1.891228, T: 115522, Avg. loss: 0.517952\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 2228.12, NNZs: 889, Bias: -1.890621, T: 116501, Avg. loss: 0.515450\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 2228.11, NNZs: 888, Bias: -1.889990, T: 117480, Avg. loss: 0.513137\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 2228.11, NNZs: 888, Bias: -1.889288, T: 118459, Avg. loss: 0.510697\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 2228.11, NNZs: 888, Bias: -1.888757, T: 119438, Avg. loss: 0.508574\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 2228.10, NNZs: 888, Bias: -1.888151, T: 120417, Avg. loss: 0.506095\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 2228.10, NNZs: 888, Bias: -1.887509, T: 121396, Avg. loss: 0.503915\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 2228.10, NNZs: 888, Bias: -1.886879, T: 122375, Avg. loss: 0.501904\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 2228.10, NNZs: 888, Bias: -1.886228, T: 123354, Avg. loss: 0.499633\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 2228.09, NNZs: 888, Bias: -1.885711, T: 124333, Avg. loss: 0.497710\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 2228.09, NNZs: 888, Bias: -1.885141, T: 125312, Avg. loss: 0.495351\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 2228.09, NNZs: 888, Bias: -1.884583, T: 126291, Avg. loss: 0.493413\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 2228.09, NNZs: 888, Bias: -1.884024, T: 127270, Avg. loss: 0.491213\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 2228.08, NNZs: 888, Bias: -1.883502, T: 128249, Avg. loss: 0.489243\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 2228.08, NNZs: 888, Bias: -1.883008, T: 129228, Avg. loss: 0.487457\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 2228.08, NNZs: 888, Bias: -1.882556, T: 130207, Avg. loss: 0.485397\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 2228.08, NNZs: 887, Bias: -1.882103, T: 131186, Avg. loss: 0.483466\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 2228.07, NNZs: 887, Bias: -1.881572, T: 132165, Avg. loss: 0.481606\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 2228.07, NNZs: 887, Bias: -1.881014, T: 133144, Avg. loss: 0.479608\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 2228.07, NNZs: 883, Bias: -1.880603, T: 134123, Avg. loss: 0.477690\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 2228.07, NNZs: 883, Bias: -1.880066, T: 135102, Avg. loss: 0.475872\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 2228.06, NNZs: 883, Bias: -1.879519, T: 136081, Avg. loss: 0.474195\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 2228.06, NNZs: 883, Bias: -1.879052, T: 137060, Avg. loss: 0.472375\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 2228.06, NNZs: 883, Bias: -1.878471, T: 138039, Avg. loss: 0.470536\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 2228.06, NNZs: 883, Bias: -1.878025, T: 139018, Avg. loss: 0.468867\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 2228.06, NNZs: 883, Bias: -1.877643, T: 139997, Avg. loss: 0.467118\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 2228.05, NNZs: 883, Bias: -1.877189, T: 140976, Avg. loss: 0.465375\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 2228.05, NNZs: 883, Bias: -1.876680, T: 141955, Avg. loss: 0.463623\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 2228.05, NNZs: 883, Bias: -1.876184, T: 142934, Avg. loss: 0.461901\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 2228.05, NNZs: 883, Bias: -1.875711, T: 143913, Avg. loss: 0.460362\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 2228.04, NNZs: 883, Bias: -1.875268, T: 144892, Avg. loss: 0.458464\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 2228.04, NNZs: 883, Bias: -1.874720, T: 145871, Avg. loss: 0.456837\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 2228.04, NNZs: 881, Bias: -1.874260, T: 146850, Avg. loss: 0.455430\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 2228.04, NNZs: 881, Bias: -1.873720, T: 147829, Avg. loss: 0.453776\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 2228.04, NNZs: 881, Bias: -1.873278, T: 148808, Avg. loss: 0.452180\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 2228.04, NNZs: 881, Bias: -1.872817, T: 149787, Avg. loss: 0.450715\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 2228.03, NNZs: 881, Bias: -1.872381, T: 150766, Avg. loss: 0.449172\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 2228.03, NNZs: 880, Bias: -1.871905, T: 151745, Avg. loss: 0.447793\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 2228.03, NNZs: 880, Bias: -1.871450, T: 152724, Avg. loss: 0.446274\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 2228.03, NNZs: 880, Bias: -1.871005, T: 153703, Avg. loss: 0.444515\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 2228.03, NNZs: 880, Bias: -1.870523, T: 154682, Avg. loss: 0.443037\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 2228.02, NNZs: 880, Bias: -1.869977, T: 155661, Avg. loss: 0.441679\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 2228.02, NNZs: 880, Bias: -1.869544, T: 156640, Avg. loss: 0.440219\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 2228.02, NNZs: 880, Bias: -1.869112, T: 157619, Avg. loss: 0.438939\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 2228.02, NNZs: 880, Bias: -1.868594, T: 158598, Avg. loss: 0.437567\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 2228.02, NNZs: 880, Bias: -1.868177, T: 159577, Avg. loss: 0.436125\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 2228.02, NNZs: 880, Bias: -1.867749, T: 160556, Avg. loss: 0.434627\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 2228.01, NNZs: 880, Bias: -1.867251, T: 161535, Avg. loss: 0.433275\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 2228.01, NNZs: 880, Bias: -1.866744, T: 162514, Avg. loss: 0.432038\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 2228.01, NNZs: 880, Bias: -1.866299, T: 163493, Avg. loss: 0.430604\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 2228.01, NNZs: 880, Bias: -1.865879, T: 164472, Avg. loss: 0.429280\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 2228.01, NNZs: 880, Bias: -1.865479, T: 165451, Avg. loss: 0.428078\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 2228.01, NNZs: 880, Bias: -1.865038, T: 166430, Avg. loss: 0.426781\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 2228.00, NNZs: 879, Bias: -1.864581, T: 167409, Avg. loss: 0.425476\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 2228.00, NNZs: 879, Bias: -1.864139, T: 168388, Avg. loss: 0.424042\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 2228.00, NNZs: 879, Bias: -1.863731, T: 169367, Avg. loss: 0.422898\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 2228.00, NNZs: 879, Bias: -1.863240, T: 170346, Avg. loss: 0.421473\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 2228.00, NNZs: 879, Bias: -1.862839, T: 171325, Avg. loss: 0.420322\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 2228.00, NNZs: 879, Bias: -1.862426, T: 172304, Avg. loss: 0.419156\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 2227.99, NNZs: 879, Bias: -1.862016, T: 173283, Avg. loss: 0.417916\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 2227.99, NNZs: 879, Bias: -1.861590, T: 174262, Avg. loss: 0.416749\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 2227.99, NNZs: 879, Bias: -1.861174, T: 175241, Avg. loss: 0.415495\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 2227.99, NNZs: 879, Bias: -1.860763, T: 176220, Avg. loss: 0.414334\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 2227.99, NNZs: 879, Bias: -1.860339, T: 177199, Avg. loss: 0.413143\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 2227.99, NNZs: 879, Bias: -1.859913, T: 178178, Avg. loss: 0.411975\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 2227.99, NNZs: 879, Bias: -1.859453, T: 179157, Avg. loss: 0.410862\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 2227.98, NNZs: 879, Bias: -1.859064, T: 180136, Avg. loss: 0.409742\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 2227.98, NNZs: 879, Bias: -1.858630, T: 181115, Avg. loss: 0.408596\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 2227.98, NNZs: 879, Bias: -1.858184, T: 182094, Avg. loss: 0.407553\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 2227.98, NNZs: 879, Bias: -1.857797, T: 183073, Avg. loss: 0.406417\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 2227.98, NNZs: 879, Bias: -1.857371, T: 184052, Avg. loss: 0.405268\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 2227.98, NNZs: 879, Bias: -1.856940, T: 185031, Avg. loss: 0.404136\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 2227.98, NNZs: 879, Bias: -1.856573, T: 186010, Avg. loss: 0.403041\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 2227.97, NNZs: 879, Bias: -1.856177, T: 186989, Avg. loss: 0.402042\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 2227.97, NNZs: 879, Bias: -1.855796, T: 187968, Avg. loss: 0.401002\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 2227.97, NNZs: 879, Bias: -1.855392, T: 188947, Avg. loss: 0.399969\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 2227.97, NNZs: 879, Bias: -1.854971, T: 189926, Avg. loss: 0.398745\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 2227.97, NNZs: 879, Bias: -1.854613, T: 190905, Avg. loss: 0.397860\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 2227.97, NNZs: 879, Bias: -1.854198, T: 191884, Avg. loss: 0.396673\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 2227.97, NNZs: 879, Bias: -1.853794, T: 192863, Avg. loss: 0.395797\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 2227.97, NNZs: 879, Bias: -1.853455, T: 193842, Avg. loss: 0.394775\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 2227.96, NNZs: 879, Bias: -1.853070, T: 194821, Avg. loss: 0.393815\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 2227.96, NNZs: 879, Bias: -1.852698, T: 195800, Avg. loss: 0.392682\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 2227.96, NNZs: 879, Bias: -1.852290, T: 196779, Avg. loss: 0.391733\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 2227.96, NNZs: 879, Bias: -1.851947, T: 197758, Avg. loss: 0.390906\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 2227.96, NNZs: 878, Bias: -1.851606, T: 198737, Avg. loss: 0.389764\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 2227.96, NNZs: 878, Bias: -1.851261, T: 199716, Avg. loss: 0.389003\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 2227.96, NNZs: 878, Bias: -1.850935, T: 200695, Avg. loss: 0.387917\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 2227.96, NNZs: 878, Bias: -1.850607, T: 201674, Avg. loss: 0.386925\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 2227.95, NNZs: 878, Bias: -1.850270, T: 202653, Avg. loss: 0.386094\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 2227.95, NNZs: 878, Bias: -1.849932, T: 203632, Avg. loss: 0.385055\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 2227.95, NNZs: 878, Bias: -1.849605, T: 204611, Avg. loss: 0.384186\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 2227.95, NNZs: 878, Bias: -1.849257, T: 205590, Avg. loss: 0.383231\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 2227.95, NNZs: 878, Bias: -1.848906, T: 206569, Avg. loss: 0.382287\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 2227.95, NNZs: 878, Bias: -1.848520, T: 207548, Avg. loss: 0.381425\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 2227.95, NNZs: 878, Bias: -1.848216, T: 208527, Avg. loss: 0.380547\n",
            "Total training time: 0.17 seconds.\n",
            "Convergence after 213 epochs took 0.17 seconds\n",
            "-- Epoch 1\n",
            "Norm: 103098.17, NNZs: 786, Bias: -228.730425, T: 979, Avg. loss: 1500258.792680\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 123073.60, NNZs: 945, Bias: -1399.879449, T: 1958, Avg. loss: 4496213.853872\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 121844.72, NNZs: 954, Bias: -1350.757148, T: 2937, Avg. loss: 601756.061387\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 121258.02, NNZs: 954, Bias: -1537.592697, T: 3916, Avg. loss: 114505.279895\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 121142.05, NNZs: 951, Bias: -1595.272381, T: 4895, Avg. loss: 15140.279208\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 121106.41, NNZs: 950, Bias: -1614.469423, T: 5874, Avg. loss: 4191.136062\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 121091.91, NNZs: 950, Bias: -1622.475304, T: 6853, Avg. loss: 1694.253261\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 121079.27, NNZs: 950, Bias: -1630.409997, T: 7832, Avg. loss: 1079.020966\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 121074.38, NNZs: 947, Bias: -1633.058034, T: 8811, Avg. loss: 745.634668\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 121070.35, NNZs: 947, Bias: -1635.335559, T: 9790, Avg. loss: 582.352629\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 121068.79, NNZs: 947, Bias: -1635.977838, T: 10769, Avg. loss: 398.622045\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 121066.78, NNZs: 947, Bias: -1637.029787, T: 11748, Avg. loss: 391.509938\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 121064.97, NNZs: 947, Bias: -1638.020424, T: 12727, Avg. loss: 358.794329\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 121063.06, NNZs: 947, Bias: -1639.139967, T: 13706, Avg. loss: 351.063014\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 121062.32, NNZs: 947, Bias: -1639.423764, T: 14685, Avg. loss: 281.333632\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 121061.09, NNZs: 947, Bias: -1640.109382, T: 15664, Avg. loss: 300.307634\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 121060.16, NNZs: 947, Bias: -1640.589918, T: 16643, Avg. loss: 276.197228\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 121059.81, NNZs: 947, Bias: -1640.663514, T: 17622, Avg. loss: 231.724312\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 121059.37, NNZs: 947, Bias: -1640.813798, T: 18601, Avg. loss: 231.183005\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 121058.89, NNZs: 947, Bias: -1641.018645, T: 19580, Avg. loss: 228.178104\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 121058.22, NNZs: 947, Bias: -1641.369714, T: 20559, Avg. loss: 238.688831\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 121057.73, NNZs: 947, Bias: -1641.599589, T: 21538, Avg. loss: 224.034613\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 121057.49, NNZs: 947, Bias: -1641.654981, T: 22517, Avg. loss: 196.938922\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 121057.05, NNZs: 947, Bias: -1641.867312, T: 23496, Avg. loss: 211.573517\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 121056.65, NNZs: 947, Bias: -1642.057032, T: 24475, Avg. loss: 202.863277\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 121056.34, NNZs: 947, Bias: -1642.189290, T: 25454, Avg. loss: 195.360812\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 121056.20, NNZs: 947, Bias: -1642.200243, T: 26433, Avg. loss: 174.831533\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 121055.89, NNZs: 946, Bias: -1642.336852, T: 27412, Avg. loss: 188.194857\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 121055.63, NNZs: 946, Bias: -1642.447647, T: 28391, Avg. loss: 182.008623\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 121055.49, NNZs: 946, Bias: -1642.465971, T: 29370, Avg. loss: 164.395187\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 121055.34, NNZs: 946, Bias: -1642.502240, T: 30349, Avg. loss: 164.040266\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 121055.17, NNZs: 946, Bias: -1642.557635, T: 31328, Avg. loss: 162.902949\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 121054.91, NNZs: 946, Bias: -1642.679868, T: 32307, Avg. loss: 171.617365\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 121054.68, NNZs: 946, Bias: -1642.786067, T: 33286, Avg. loss: 166.501853\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 121054.54, NNZs: 946, Bias: -1642.823940, T: 34265, Avg. loss: 152.887300\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 121054.34, NNZs: 946, Bias: -1642.918503, T: 35244, Avg. loss: 160.994188\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 121054.14, NNZs: 946, Bias: -1643.006566, T: 36223, Avg. loss: 157.590772\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 121054.01, NNZs: 946, Bias: -1643.045866, T: 37202, Avg. loss: 145.943486\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 121053.89, NNZs: 946, Bias: -1643.081732, T: 38181, Avg. loss: 144.978191\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 121053.71, NNZs: 946, Bias: -1643.167068, T: 39160, Avg. loss: 151.891197\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 121053.59, NNZs: 946, Bias: -1643.201111, T: 40139, Avg. loss: 141.006713\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 121053.42, NNZs: 946, Bias: -1643.278769, T: 41118, Avg. loss: 147.382958\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 121053.27, NNZs: 946, Bias: -1643.348922, T: 42097, Avg. loss: 145.017433\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 121053.13, NNZs: 946, Bias: -1643.408098, T: 43076, Avg. loss: 142.209146\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 121053.03, NNZs: 946, Bias: -1643.438794, T: 44055, Avg. loss: 133.508013\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 121052.89, NNZs: 946, Bias: -1643.500531, T: 45034, Avg. loss: 139.219003\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 121052.79, NNZs: 946, Bias: -1643.530307, T: 46013, Avg. loss: 130.681793\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 121052.65, NNZs: 946, Bias: -1643.593648, T: 46992, Avg. loss: 136.490446\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 121052.53, NNZs: 945, Bias: -1643.647426, T: 47971, Avg. loss: 134.392385\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 121052.41, NNZs: 945, Bias: -1643.701686, T: 48950, Avg. loss: 132.680930\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 121052.30, NNZs: 945, Bias: -1643.749603, T: 49929, Avg. loss: 130.787957\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 121052.23, NNZs: 945, Bias: -1643.766343, T: 50908, Avg. loss: 123.571845\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 121052.12, NNZs: 945, Bias: -1643.814784, T: 51887, Avg. loss: 128.601438\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 121052.01, NNZs: 945, Bias: -1643.858142, T: 52866, Avg. loss: 127.076585\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 121051.94, NNZs: 945, Bias: -1643.879685, T: 53845, Avg. loss: 120.618336\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 121051.87, NNZs: 945, Bias: -1643.902691, T: 54824, Avg. loss: 120.015386\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 121051.76, NNZs: 945, Bias: -1643.951423, T: 55803, Avg. loss: 124.544100\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 121051.69, NNZs: 945, Bias: -1643.977114, T: 56782, Avg. loss: 118.209760\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 121051.61, NNZs: 945, Bias: -1644.004541, T: 57761, Avg. loss: 117.694322\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 121051.51, NNZs: 945, Bias: -1644.052947, T: 58740, Avg. loss: 121.682730\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 121051.41, NNZs: 945, Bias: -1644.094894, T: 59719, Avg. loss: 120.262420\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 121051.33, NNZs: 945, Bias: -1644.133554, T: 60698, Avg. loss: 118.872823\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 121051.26, NNZs: 945, Bias: -1644.156377, T: 61677, Avg. loss: 113.576737\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 121051.17, NNZs: 945, Bias: -1644.194127, T: 62656, Avg. loss: 117.227135\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 121051.11, NNZs: 945, Bias: -1644.214470, T: 63635, Avg. loss: 111.988884\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 121051.05, NNZs: 945, Bias: -1644.237977, T: 64614, Avg. loss: 111.650973\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 121050.98, NNZs: 945, Bias: -1644.263051, T: 65593, Avg. loss: 110.977077\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 121050.90, NNZs: 945, Bias: -1644.303113, T: 66572, Avg. loss: 114.270248\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 121050.82, NNZs: 945, Bias: -1644.338596, T: 67551, Avg. loss: 113.146202\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 121050.76, NNZs: 945, Bias: -1644.361058, T: 68530, Avg. loss: 108.604799\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 121050.70, NNZs: 945, Bias: -1644.384117, T: 69509, Avg. loss: 108.099669\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 121050.64, NNZs: 945, Bias: -1644.406187, T: 70488, Avg. loss: 107.472465\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 121050.58, NNZs: 945, Bias: -1644.428113, T: 71467, Avg. loss: 106.920305\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 121050.51, NNZs: 945, Bias: -1644.461570, T: 72446, Avg. loss: 109.715289\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 121050.44, NNZs: 945, Bias: -1644.494406, T: 73425, Avg. loss: 108.772767\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 121050.37, NNZs: 945, Bias: -1644.525431, T: 74404, Avg. loss: 107.853785\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 121050.31, NNZs: 945, Bias: -1644.545209, T: 75383, Avg. loss: 103.822212\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 121050.25, NNZs: 945, Bias: -1644.573288, T: 76362, Avg. loss: 106.372223\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 121050.20, NNZs: 945, Bias: -1644.590132, T: 77341, Avg. loss: 102.570720\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 121050.14, NNZs: 945, Bias: -1644.618103, T: 78320, Avg. loss: 105.062203\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 121050.09, NNZs: 945, Bias: -1644.635478, T: 79299, Avg. loss: 101.437396\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 121050.04, NNZs: 945, Bias: -1644.653632, T: 80278, Avg. loss: 100.932792\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 121049.98, NNZs: 945, Bias: -1644.680820, T: 81257, Avg. loss: 103.313929\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 121049.92, NNZs: 945, Bias: -1644.706952, T: 82236, Avg. loss: 102.674307\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 121049.88, NNZs: 945, Bias: -1644.724711, T: 83215, Avg. loss: 99.164999\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 121049.82, NNZs: 945, Bias: -1644.749908, T: 84194, Avg. loss: 101.500518\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 121049.78, NNZs: 945, Bias: -1644.765366, T: 85173, Avg. loss: 98.085308\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 121049.74, NNZs: 945, Bias: -1644.781234, T: 86152, Avg. loss: 97.683451\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 121049.69, NNZs: 945, Bias: -1644.800104, T: 87131, Avg. loss: 97.471105\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 121049.64, NNZs: 945, Bias: -1644.818235, T: 88110, Avg. loss: 96.994512\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 121049.59, NNZs: 945, Bias: -1644.841987, T: 89089, Avg. loss: 98.982053\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 121049.54, NNZs: 945, Bias: -1644.865678, T: 90068, Avg. loss: 98.400433\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 121049.49, NNZs: 945, Bias: -1644.888645, T: 91047, Avg. loss: 97.695660\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 121049.45, NNZs: 945, Bias: -1644.905189, T: 92026, Avg. loss: 94.791324\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 121049.40, NNZs: 945, Bias: -1644.921191, T: 93005, Avg. loss: 94.414738\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 121049.36, NNZs: 945, Bias: -1644.942566, T: 93984, Avg. loss: 96.295684\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 121049.31, NNZs: 945, Bias: -1644.964399, T: 94963, Avg. loss: 95.757375\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 121049.27, NNZs: 945, Bias: -1644.978800, T: 95942, Avg. loss: 92.866018\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 121049.23, NNZs: 945, Bias: -1644.993611, T: 96921, Avg. loss: 92.665215\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 121049.19, NNZs: 945, Bias: -1645.008867, T: 97900, Avg. loss: 92.296358\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 121049.16, NNZs: 945, Bias: -1645.024027, T: 98879, Avg. loss: 91.907500\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 121049.11, NNZs: 945, Bias: -1645.045212, T: 99858, Avg. loss: 93.694583\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 121049.07, NNZs: 945, Bias: -1645.060335, T: 100837, Avg. loss: 91.143157\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 121049.03, NNZs: 945, Bias: -1645.080645, T: 101816, Avg. loss: 92.817552\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 121048.99, NNZs: 945, Bias: -1645.100114, T: 102795, Avg. loss: 92.286781\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 121048.95, NNZs: 945, Bias: -1645.114523, T: 103774, Avg. loss: 89.829124\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 121048.91, NNZs: 945, Bias: -1645.133344, T: 104753, Avg. loss: 91.388432\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 121048.87, NNZs: 945, Bias: -1645.151803, T: 105732, Avg. loss: 90.961986\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 121048.83, NNZs: 945, Bias: -1645.164169, T: 106711, Avg. loss: 88.542917\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 121048.80, NNZs: 945, Bias: -1645.181130, T: 107690, Avg. loss: 90.078566\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 121048.76, NNZs: 945, Bias: -1645.198696, T: 108669, Avg. loss: 89.700922\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 121048.72, NNZs: 945, Bias: -1645.215787, T: 109648, Avg. loss: 89.214447\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 121048.69, NNZs: 945, Bias: -1645.227512, T: 110627, Avg. loss: 86.988029\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 121048.65, NNZs: 945, Bias: -1645.244670, T: 111606, Avg. loss: 88.556288\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 121048.62, NNZs: 945, Bias: -1645.257228, T: 112585, Avg. loss: 86.305947\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 121048.58, NNZs: 945, Bias: -1645.273362, T: 113564, Avg. loss: 87.805389\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 121048.55, NNZs: 945, Bias: -1645.284878, T: 114543, Avg. loss: 85.636820\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 121048.52, NNZs: 945, Bias: -1645.300933, T: 115522, Avg. loss: 87.126806\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 121048.48, NNZs: 945, Bias: -1645.316903, T: 116501, Avg. loss: 86.738339\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 121048.45, NNZs: 945, Bias: -1645.332348, T: 117480, Avg. loss: 86.351665\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 121048.42, NNZs: 945, Bias: -1645.343636, T: 118459, Avg. loss: 84.292900\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 121048.38, NNZs: 945, Bias: -1645.358866, T: 119438, Avg. loss: 85.669695\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 121048.35, NNZs: 945, Bias: -1645.370404, T: 120417, Avg. loss: 83.743462\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 121048.33, NNZs: 945, Bias: -1645.381450, T: 121396, Avg. loss: 83.431500\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 121048.30, NNZs: 945, Bias: -1645.393303, T: 122375, Avg. loss: 83.262492\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 121048.27, NNZs: 945, Bias: -1645.404951, T: 123354, Avg. loss: 83.028283\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 121048.24, NNZs: 945, Bias: -1645.416282, T: 124333, Avg. loss: 82.740887\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 121048.21, NNZs: 945, Bias: -1645.428197, T: 125312, Avg. loss: 82.544805\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 121048.18, NNZs: 945, Bias: -1645.442663, T: 126291, Avg. loss: 83.730708\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 121048.15, NNZs: 945, Bias: -1645.454129, T: 127270, Avg. loss: 81.954046\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 121048.12, NNZs: 945, Bias: -1645.469305, T: 128249, Avg. loss: 83.199920\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 121048.09, NNZs: 945, Bias: -1645.480270, T: 129228, Avg. loss: 81.333134\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 121048.06, NNZs: 944, Bias: -1645.494092, T: 130207, Avg. loss: 82.558998\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 121048.04, NNZs: 944, Bias: -1645.504836, T: 131186, Avg. loss: 80.773840\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 121048.01, NNZs: 944, Bias: -1645.518563, T: 132165, Avg. loss: 81.977864\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 121047.98, NNZs: 944, Bias: -1645.531651, T: 133144, Avg. loss: 81.628702\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 121047.95, NNZs: 944, Bias: -1645.544892, T: 134123, Avg. loss: 81.334094\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 121047.92, NNZs: 944, Bias: -1645.554519, T: 135102, Avg. loss: 79.640470\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 121047.90, NNZs: 944, Bias: -1645.564429, T: 136081, Avg. loss: 79.392637\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 121047.87, NNZs: 944, Bias: -1645.574907, T: 137060, Avg. loss: 79.209403\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 121047.85, NNZs: 944, Bias: -1645.587741, T: 138039, Avg. loss: 80.304961\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 121047.82, NNZs: 944, Bias: -1645.597820, T: 139018, Avg. loss: 78.693294\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 121047.79, NNZs: 944, Bias: -1645.611188, T: 139997, Avg. loss: 79.840362\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 121047.76, NNZs: 944, Bias: -1645.623836, T: 140976, Avg. loss: 79.514502\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 121047.74, NNZs: 944, Bias: -1645.636201, T: 141955, Avg. loss: 79.217787\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 121047.71, NNZs: 944, Bias: -1645.645673, T: 142934, Avg. loss: 77.690217\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 121047.69, NNZs: 944, Bias: -1645.655065, T: 143913, Avg. loss: 77.431751\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 121047.67, NNZs: 944, Bias: -1645.666720, T: 144892, Avg. loss: 78.452618\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 121047.64, NNZs: 944, Bias: -1645.676103, T: 145871, Avg. loss: 76.995985\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 121047.62, NNZs: 944, Bias: -1645.685102, T: 146850, Avg. loss: 76.781220\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 121047.59, NNZs: 944, Bias: -1645.696935, T: 147829, Avg. loss: 77.827376\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 121047.57, NNZs: 944, Bias: -1645.708477, T: 148808, Avg. loss: 77.527994\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 121047.55, NNZs: 944, Bias: -1645.717333, T: 149787, Avg. loss: 76.087420\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 121047.52, NNZs: 944, Bias: -1645.728581, T: 150766, Avg. loss: 77.062012\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 121047.50, NNZs: 944, Bias: -1645.740291, T: 151745, Avg. loss: 76.843246\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 121047.48, NNZs: 944, Bias: -1645.748861, T: 152724, Avg. loss: 75.403603\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 121047.45, NNZs: 944, Bias: -1645.757800, T: 153703, Avg. loss: 75.210353\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 121047.43, NNZs: 944, Bias: -1645.768686, T: 154682, Avg. loss: 76.186840\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 121047.41, NNZs: 944, Bias: -1645.777288, T: 155661, Avg. loss: 74.771355\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 121047.39, NNZs: 944, Bias: -1645.785790, T: 156640, Avg. loss: 74.596567\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 121047.37, NNZs: 944, Bias: -1645.794762, T: 157619, Avg. loss: 74.440398\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 121047.34, NNZs: 944, Bias: -1645.805667, T: 158598, Avg. loss: 75.341376\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 121047.32, NNZs: 944, Bias: -1645.816640, T: 159577, Avg. loss: 75.142639\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 121047.30, NNZs: 944, Bias: -1645.827276, T: 160556, Avg. loss: 74.886835\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 121047.28, NNZs: 944, Bias: -1645.836012, T: 161535, Avg. loss: 73.602443\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 121047.25, NNZs: 944, Bias: -1645.846440, T: 162514, Avg. loss: 74.457485\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 121047.24, NNZs: 944, Bias: -1645.854651, T: 163493, Avg. loss: 73.180121\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 121047.21, NNZs: 944, Bias: -1645.864697, T: 164472, Avg. loss: 74.030647\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 121047.19, NNZs: 944, Bias: -1645.874668, T: 165451, Avg. loss: 73.828936\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 121047.17, NNZs: 944, Bias: -1645.884848, T: 166430, Avg. loss: 73.626714\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 121047.15, NNZs: 944, Bias: -1645.894642, T: 167409, Avg. loss: 73.392664\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 121047.13, NNZs: 944, Bias: -1645.904185, T: 168388, Avg. loss: 73.177537\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 121047.11, NNZs: 944, Bias: -1645.911752, T: 169367, Avg. loss: 71.934048\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 121047.09, NNZs: 944, Bias: -1645.919354, T: 170346, Avg. loss: 71.778869\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 121047.07, NNZs: 944, Bias: -1645.926770, T: 171325, Avg. loss: 71.631994\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 121047.05, NNZs: 944, Bias: -1645.934382, T: 172304, Avg. loss: 71.477563\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 121047.04, NNZs: 944, Bias: -1645.942211, T: 173283, Avg. loss: 71.330638\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 121047.01, NNZs: 944, Bias: -1645.951772, T: 174262, Avg. loss: 72.131222\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 121047.00, NNZs: 944, Bias: -1645.959807, T: 175241, Avg. loss: 71.013966\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 121046.98, NNZs: 944, Bias: -1645.969399, T: 176220, Avg. loss: 71.795992\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 121046.95, NNZs: 944, Bias: -1645.978857, T: 177199, Avg. loss: 71.588048\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 121046.94, NNZs: 944, Bias: -1645.987876, T: 178178, Avg. loss: 71.370299\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 121046.92, NNZs: 944, Bias: -1645.995211, T: 179157, Avg. loss: 70.263025\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 121046.90, NNZs: 944, Bias: -1646.002385, T: 180136, Avg. loss: 70.089310\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 121046.88, NNZs: 944, Bias: -1646.009738, T: 181115, Avg. loss: 69.951995\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 121046.87, NNZs: 944, Bias: -1646.017009, T: 182094, Avg. loss: 69.794408\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 121046.85, NNZs: 944, Bias: -1646.024252, T: 183073, Avg. loss: 69.657739\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 121046.83, NNZs: 944, Bias: -1646.033325, T: 184052, Avg. loss: 70.416371\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 121046.81, NNZs: 944, Bias: -1646.042382, T: 185031, Avg. loss: 70.242797\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 121046.79, NNZs: 944, Bias: -1646.049811, T: 186010, Avg. loss: 69.189335\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 121046.77, NNZs: 944, Bias: -1646.058262, T: 186989, Avg. loss: 69.854320\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 121046.76, NNZs: 944, Bias: -1646.065548, T: 187968, Avg. loss: 68.864085\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 121046.74, NNZs: 944, Bias: -1646.072596, T: 188947, Avg. loss: 68.684514\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 121046.72, NNZs: 944, Bias: -1646.081015, T: 189926, Avg. loss: 69.413791\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 121046.70, NNZs: 944, Bias: -1646.089480, T: 190905, Avg. loss: 69.234072\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 121046.69, NNZs: 944, Bias: -1646.096524, T: 191884, Avg. loss: 68.234444\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 121046.67, NNZs: 944, Bias: -1646.104981, T: 192863, Avg. loss: 68.909289\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 121046.65, NNZs: 944, Bias: -1646.111747, T: 193842, Avg. loss: 67.909911\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 121046.64, NNZs: 944, Bias: -1646.119855, T: 194821, Avg. loss: 68.577670\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 121046.62, NNZs: 944, Bias: -1646.126570, T: 195800, Avg. loss: 67.613206\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 121046.60, NNZs: 944, Bias: -1646.134480, T: 196779, Avg. loss: 68.267207\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 121046.59, NNZs: 944, Bias: -1646.140926, T: 197758, Avg. loss: 67.284507\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 121046.57, NNZs: 944, Bias: -1646.148920, T: 198737, Avg. loss: 67.948674\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 121046.56, NNZs: 944, Bias: -1646.155396, T: 199716, Avg. loss: 66.997350\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 121046.54, NNZs: 944, Bias: -1646.163113, T: 200695, Avg. loss: 67.658986\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 121046.52, NNZs: 944, Bias: -1646.169717, T: 201674, Avg. loss: 66.715921\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 121046.51, NNZs: 944, Bias: -1646.177404, T: 202653, Avg. loss: 67.360713\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 121046.49, NNZs: 944, Bias: -1646.184945, T: 203632, Avg. loss: 67.191981\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 121046.47, NNZs: 944, Bias: -1646.192541, T: 204611, Avg. loss: 67.062462\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 121046.46, NNZs: 944, Bias: -1646.200192, T: 205590, Avg. loss: 66.897212\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 121046.44, NNZs: 944, Bias: -1646.207711, T: 206569, Avg. loss: 66.742700\n",
            "Total training time: 0.38 seconds.\n",
            "Convergence after 211 epochs took 0.38 seconds\n",
            "-- Epoch 1\n",
            "Norm: 161.69, NNZs: 627, Bias: -1.329078, T: 979, Avg. loss: 5.873684\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 160.76, NNZs: 520, Bias: -0.919939, T: 1958, Avg. loss: 1.152289\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 160.72, NNZs: 471, Bias: -0.762865, T: 2937, Avg. loss: 0.408116\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 160.71, NNZs: 461, Bias: -0.699236, T: 3916, Avg. loss: 0.328630\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 160.74, NNZs: 436, Bias: -0.632373, T: 4895, Avg. loss: 0.292786\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 160.75, NNZs: 412, Bias: -0.648810, T: 5874, Avg. loss: 0.263523\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 160.77, NNZs: 407, Bias: -0.631555, T: 6853, Avg. loss: 0.257700\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 160.80, NNZs: 409, Bias: -0.612008, T: 7832, Avg. loss: 0.251536\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 160.82, NNZs: 397, Bias: -0.608257, T: 8811, Avg. loss: 0.242359\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 160.85, NNZs: 391, Bias: -0.600711, T: 9790, Avg. loss: 0.239867\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 160.87, NNZs: 390, Bias: -0.594450, T: 10769, Avg. loss: 0.237049\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 160.89, NNZs: 386, Bias: -0.596649, T: 11748, Avg. loss: 0.231791\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 160.92, NNZs: 378, Bias: -0.586905, T: 12727, Avg. loss: 0.235137\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 160.94, NNZs: 376, Bias: -0.583725, T: 13706, Avg. loss: 0.231678\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 160.96, NNZs: 365, Bias: -0.577874, T: 14685, Avg. loss: 0.231938\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 160.99, NNZs: 363, Bias: -0.571589, T: 15664, Avg. loss: 0.232024\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 161.01, NNZs: 357, Bias: -0.567035, T: 16643, Avg. loss: 0.231322\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 17 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 1007.87, NNZs: 788, Bias: -2.318710, T: 979, Avg. loss: 200.650752\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 962.73, NNZs: 811, Bias: -2.232840, T: 1958, Avg. loss: 122.482776\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 959.05, NNZs: 818, Bias: -2.828158, T: 2937, Avg. loss: 8.439570\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 958.35, NNZs: 813, Bias: -2.901884, T: 3916, Avg. loss: 2.047020\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 958.12, NNZs: 806, Bias: -2.776892, T: 4895, Avg. loss: 1.020003\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 957.97, NNZs: 807, Bias: -2.829024, T: 5874, Avg. loss: 0.672699\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 957.88, NNZs: 804, Bias: -2.815430, T: 6853, Avg. loss: 0.524709\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 957.82, NNZs: 800, Bias: -2.802030, T: 7832, Avg. loss: 0.426275\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 957.77, NNZs: 792, Bias: -2.809854, T: 8811, Avg. loss: 0.370259\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 957.73, NNZs: 784, Bias: -2.804897, T: 9790, Avg. loss: 0.324724\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 957.70, NNZs: 781, Bias: -2.801141, T: 10769, Avg. loss: 0.296925\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 957.68, NNZs: 777, Bias: -2.788833, T: 11748, Avg. loss: 0.273891\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 957.66, NNZs: 776, Bias: -2.781538, T: 12727, Avg. loss: 0.251814\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 957.65, NNZs: 776, Bias: -2.778578, T: 13706, Avg. loss: 0.238855\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 957.63, NNZs: 776, Bias: -2.781249, T: 14685, Avg. loss: 0.224261\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 957.62, NNZs: 774, Bias: -2.772791, T: 15664, Avg. loss: 0.215060\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 957.61, NNZs: 776, Bias: -2.768252, T: 16643, Avg. loss: 0.205319\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 957.60, NNZs: 776, Bias: -2.768277, T: 17622, Avg. loss: 0.198248\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 957.59, NNZs: 776, Bias: -2.766236, T: 18601, Avg. loss: 0.190285\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 957.59, NNZs: 773, Bias: -2.763202, T: 19580, Avg. loss: 0.184739\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 957.58, NNZs: 770, Bias: -2.757845, T: 20559, Avg. loss: 0.178687\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 957.57, NNZs: 767, Bias: -2.753525, T: 21538, Avg. loss: 0.174031\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 957.57, NNZs: 766, Bias: -2.752425, T: 22517, Avg. loss: 0.169400\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 957.56, NNZs: 766, Bias: -2.749695, T: 23496, Avg. loss: 0.165800\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 957.56, NNZs: 766, Bias: -2.745353, T: 24475, Avg. loss: 0.162238\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 957.56, NNZs: 766, Bias: -2.743247, T: 25454, Avg. loss: 0.159231\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 957.55, NNZs: 765, Bias: -2.741658, T: 26433, Avg. loss: 0.155738\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 957.55, NNZs: 761, Bias: -2.739699, T: 27412, Avg. loss: 0.152700\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 957.55, NNZs: 760, Bias: -2.736593, T: 28391, Avg. loss: 0.149728\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 957.54, NNZs: 757, Bias: -2.734594, T: 29370, Avg. loss: 0.147605\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 957.54, NNZs: 757, Bias: -2.731901, T: 30349, Avg. loss: 0.145819\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 957.54, NNZs: 757, Bias: -2.729954, T: 31328, Avg. loss: 0.143774\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 957.54, NNZs: 757, Bias: -2.727664, T: 32307, Avg. loss: 0.141201\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 957.53, NNZs: 757, Bias: -2.726419, T: 33286, Avg. loss: 0.139376\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 957.53, NNZs: 757, Bias: -2.724471, T: 34265, Avg. loss: 0.138031\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 957.53, NNZs: 757, Bias: -2.722737, T: 35244, Avg. loss: 0.136221\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 957.53, NNZs: 757, Bias: -2.720936, T: 36223, Avg. loss: 0.134204\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 957.53, NNZs: 757, Bias: -2.719094, T: 37202, Avg. loss: 0.132586\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 957.52, NNZs: 756, Bias: -2.717437, T: 38181, Avg. loss: 0.131510\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 957.52, NNZs: 756, Bias: -2.716134, T: 39160, Avg. loss: 0.130011\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 957.52, NNZs: 756, Bias: -2.714306, T: 40139, Avg. loss: 0.128979\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 957.52, NNZs: 756, Bias: -2.712568, T: 41118, Avg. loss: 0.127713\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 957.52, NNZs: 756, Bias: -2.710847, T: 42097, Avg. loss: 0.126459\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 957.52, NNZs: 755, Bias: -2.709811, T: 43076, Avg. loss: 0.125279\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 957.52, NNZs: 755, Bias: -2.708189, T: 44055, Avg. loss: 0.124120\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 957.51, NNZs: 755, Bias: -2.706960, T: 45034, Avg. loss: 0.123334\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 957.51, NNZs: 755, Bias: -2.705601, T: 46013, Avg. loss: 0.122404\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 957.51, NNZs: 755, Bias: -2.704149, T: 46992, Avg. loss: 0.121025\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 957.51, NNZs: 755, Bias: -2.702571, T: 47971, Avg. loss: 0.119769\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 957.51, NNZs: 754, Bias: -2.701646, T: 48950, Avg. loss: 0.119357\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 957.51, NNZs: 753, Bias: -2.700661, T: 49929, Avg. loss: 0.118646\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 957.51, NNZs: 753, Bias: -2.699616, T: 50908, Avg. loss: 0.117671\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 957.51, NNZs: 753, Bias: -2.698649, T: 51887, Avg. loss: 0.117078\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 957.51, NNZs: 753, Bias: -2.697825, T: 52866, Avg. loss: 0.116394\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 54 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 28893.20, NNZs: 751, Bias: -103.166769, T: 979, Avg. loss: 117841.946473\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 33183.29, NNZs: 930, Bias: -390.763096, T: 1958, Avg. loss: 246281.490092\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 32547.25, NNZs: 948, Bias: -396.668587, T: 2937, Avg. loss: 76994.044829\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 32309.16, NNZs: 950, Bias: -448.838562, T: 3916, Avg. loss: 15148.767699\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 32252.19, NNZs: 946, Bias: -461.867339, T: 4895, Avg. loss: 3659.935057\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 32228.52, NNZs: 946, Bias: -471.078495, T: 5874, Avg. loss: 1176.560959\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 32220.92, NNZs: 945, Bias: -473.393943, T: 6853, Avg. loss: 502.731048\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 32214.04, NNZs: 943, Bias: -476.477185, T: 7832, Avg. loss: 339.863230\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 32211.01, NNZs: 943, Bias: -477.421349, T: 8811, Avg. loss: 249.383774\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 32208.60, NNZs: 943, Bias: -478.208419, T: 9790, Avg. loss: 201.118411\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 32207.46, NNZs: 943, Bias: -478.366984, T: 10769, Avg. loss: 140.445420\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 32206.09, NNZs: 943, Bias: -478.767277, T: 11748, Avg. loss: 138.452961\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 32204.88, NNZs: 943, Bias: -479.154059, T: 12727, Avg. loss: 128.840473\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 32203.51, NNZs: 943, Bias: -479.708504, T: 13706, Avg. loss: 128.858304\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 32202.84, NNZs: 942, Bias: -479.835546, T: 14685, Avg. loss: 102.763278\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 32201.92, NNZs: 941, Bias: -480.167394, T: 15664, Avg. loss: 111.254444\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 32201.19, NNZs: 940, Bias: -480.401001, T: 16643, Avg. loss: 102.032792\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 32200.80, NNZs: 940, Bias: -480.422639, T: 17622, Avg. loss: 87.804001\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 32200.36, NNZs: 940, Bias: -480.503965, T: 18601, Avg. loss: 87.067716\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 32199.91, NNZs: 940, Bias: -480.603848, T: 19580, Avg. loss: 85.637178\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 32199.37, NNZs: 941, Bias: -480.792851, T: 20559, Avg. loss: 88.115029\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 32198.93, NNZs: 941, Bias: -480.926352, T: 21538, Avg. loss: 83.795252\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 32198.66, NNZs: 941, Bias: -480.943493, T: 22517, Avg. loss: 74.724681\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 32198.27, NNZs: 941, Bias: -481.063926, T: 23496, Avg. loss: 79.890798\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 32197.91, NNZs: 941, Bias: -481.165334, T: 24475, Avg. loss: 76.864248\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 32197.62, NNZs: 941, Bias: -481.236463, T: 25454, Avg. loss: 74.146972\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 32197.44, NNZs: 941, Bias: -481.235646, T: 26433, Avg. loss: 66.330049\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 32197.15, NNZs: 940, Bias: -481.315194, T: 27412, Avg. loss: 71.467183\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 32196.90, NNZs: 940, Bias: -481.373886, T: 28391, Avg. loss: 69.073890\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 32196.76, NNZs: 940, Bias: -481.369422, T: 29370, Avg. loss: 62.408854\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 32196.58, NNZs: 940, Bias: -481.388989, T: 30349, Avg. loss: 62.290935\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 32196.39, NNZs: 940, Bias: -481.423444, T: 31328, Avg. loss: 61.899386\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 32196.15, NNZs: 940, Bias: -481.496791, T: 32307, Avg. loss: 65.161111\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 32195.94, NNZs: 940, Bias: -481.556426, T: 33286, Avg. loss: 63.167556\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 32195.79, NNZs: 940, Bias: -481.575045, T: 34265, Avg. loss: 57.979310\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 32195.59, NNZs: 939, Bias: -481.629450, T: 35244, Avg. loss: 61.115831\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 32195.41, NNZs: 939, Bias: -481.677360, T: 36223, Avg. loss: 59.742090\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 32195.28, NNZs: 939, Bias: -481.692107, T: 37202, Avg. loss: 55.293295\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 32195.16, NNZs: 939, Bias: -481.709957, T: 38181, Avg. loss: 54.966524\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 32194.99, NNZs: 939, Bias: -481.758067, T: 39160, Avg. loss: 57.495745\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 32194.87, NNZs: 939, Bias: -481.773726, T: 40139, Avg. loss: 53.409872\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 32194.71, NNZs: 939, Bias: -481.817790, T: 41118, Avg. loss: 55.768727\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 32194.57, NNZs: 939, Bias: -481.854484, T: 42097, Avg. loss: 54.829379\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 32194.44, NNZs: 939, Bias: -481.885464, T: 43076, Avg. loss: 53.778604\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 32194.34, NNZs: 939, Bias: -481.897483, T: 44055, Avg. loss: 50.420153\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 32194.21, NNZs: 939, Bias: -481.930105, T: 45034, Avg. loss: 52.556266\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 32194.12, NNZs: 939, Bias: -481.943558, T: 46013, Avg. loss: 49.330436\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 32193.99, NNZs: 939, Bias: -481.978877, T: 46992, Avg. loss: 51.453068\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 32193.88, NNZs: 939, Bias: -482.006080, T: 47971, Avg. loss: 50.640236\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 32193.77, NNZs: 939, Bias: -482.032609, T: 48950, Avg. loss: 49.955596\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 32193.66, NNZs: 939, Bias: -482.059459, T: 49929, Avg. loss: 49.246243\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 32193.59, NNZs: 939, Bias: -482.064614, T: 50908, Avg. loss: 46.531522\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 32193.49, NNZs: 939, Bias: -482.090071, T: 51887, Avg. loss: 48.344159\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 32193.39, NNZs: 939, Bias: -482.114581, T: 52866, Avg. loss: 47.764256\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 32193.32, NNZs: 939, Bias: -482.123266, T: 53845, Avg. loss: 45.280991\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 32193.24, NNZs: 939, Bias: -482.132582, T: 54824, Avg. loss: 45.079169\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 32193.15, NNZs: 939, Bias: -482.157205, T: 55803, Avg. loss: 46.684408\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 32193.08, NNZs: 939, Bias: -482.168798, T: 56782, Avg. loss: 44.331985\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 32193.01, NNZs: 939, Bias: -482.180496, T: 57761, Avg. loss: 44.109751\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 32192.91, NNZs: 939, Bias: -482.206139, T: 58740, Avg. loss: 45.544812\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 32192.83, NNZs: 938, Bias: -482.227947, T: 59719, Avg. loss: 45.002427\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 32192.75, NNZs: 938, Bias: -482.248318, T: 60698, Avg. loss: 44.496131\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 32192.69, NNZs: 938, Bias: -482.257717, T: 61677, Avg. loss: 42.529071\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 32192.61, NNZs: 938, Bias: -482.275351, T: 62656, Avg. loss: 43.945106\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 32192.56, NNZs: 938, Bias: -482.281413, T: 63635, Avg. loss: 42.032331\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 32192.50, NNZs: 937, Bias: -482.288773, T: 64614, Avg. loss: 41.944306\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 32192.44, NNZs: 937, Bias: -482.297376, T: 65593, Avg. loss: 41.818543\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 32192.37, NNZs: 938, Bias: -482.315527, T: 66572, Avg. loss: 43.186715\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 32192.30, NNZs: 938, Bias: -482.330996, T: 67551, Avg. loss: 42.831483\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 32192.25, NNZs: 938, Bias: -482.337764, T: 68530, Avg. loss: 41.067015\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 32192.19, NNZs: 938, Bias: -482.346405, T: 69509, Avg. loss: 40.938127\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 32192.14, NNZs: 938, Bias: -482.354728, T: 70488, Avg. loss: 40.743025\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 32192.09, NNZs: 938, Bias: -482.363155, T: 71467, Avg. loss: 40.563388\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 32192.03, NNZs: 938, Bias: -482.379268, T: 72446, Avg. loss: 41.646034\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 32191.96, NNZs: 938, Bias: -482.395363, T: 73425, Avg. loss: 41.299160\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 32191.90, NNZs: 938, Bias: -482.411031, T: 74404, Avg. loss: 40.955811\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 32191.85, NNZs: 938, Bias: -482.419071, T: 75383, Avg. loss: 39.402239\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 32191.79, NNZs: 938, Bias: -482.433174, T: 76362, Avg. loss: 40.413047\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 32191.75, NNZs: 938, Bias: -482.440564, T: 77341, Avg. loss: 38.939673\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 32191.69, NNZs: 937, Bias: -482.455005, T: 78320, Avg. loss: 39.912786\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 32191.65, NNZs: 937, Bias: -482.462432, T: 79299, Avg. loss: 38.493925\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 32191.60, NNZs: 937, Bias: -482.471044, T: 80278, Avg. loss: 38.336346\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 32191.55, NNZs: 937, Bias: -482.485016, T: 81257, Avg. loss: 39.224884\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 32191.49, NNZs: 937, Bias: -482.498592, T: 82236, Avg. loss: 38.957117\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 32191.45, NNZs: 937, Bias: -482.506531, T: 83215, Avg. loss: 37.620922\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 32191.40, NNZs: 937, Bias: -482.519465, T: 84194, Avg. loss: 38.495128\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 32191.36, NNZs: 937, Bias: -482.526650, T: 85173, Avg. loss: 37.212140\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 32191.31, NNZs: 937, Bias: -482.534091, T: 86152, Avg. loss: 37.062733\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 32191.27, NNZs: 937, Bias: -482.542421, T: 87131, Avg. loss: 36.919933\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 32191.23, NNZs: 937, Bias: -482.550797, T: 88110, Avg. loss: 36.754260\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 32191.18, NNZs: 937, Bias: -482.563379, T: 89089, Avg. loss: 37.516439\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 32191.14, NNZs: 937, Bias: -482.575517, T: 90068, Avg. loss: 37.251283\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 32191.09, NNZs: 937, Bias: -482.587496, T: 91047, Avg. loss: 36.999259\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 32191.05, NNZs: 937, Bias: -482.595202, T: 92026, Avg. loss: 35.867993\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 32191.01, NNZs: 937, Bias: -482.603122, T: 93005, Avg. loss: 35.738032\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 32190.97, NNZs: 937, Bias: -482.614464, T: 93984, Avg. loss: 36.437831\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 32190.93, NNZs: 937, Bias: -482.625918, T: 94963, Avg. loss: 36.215997\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 32190.89, NNZs: 937, Bias: -482.632756, T: 95942, Avg. loss: 35.138362\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 32190.85, NNZs: 937, Bias: -482.640020, T: 96921, Avg. loss: 35.026237\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 32190.82, NNZs: 937, Bias: -482.647394, T: 97900, Avg. loss: 34.891373\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 32190.78, NNZs: 937, Bias: -482.654749, T: 98879, Avg. loss: 34.746007\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 32190.74, NNZs: 937, Bias: -482.665964, T: 99858, Avg. loss: 35.389244\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 32190.70, NNZs: 937, Bias: -482.673137, T: 100837, Avg. loss: 34.401389\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 32190.66, NNZs: 937, Bias: -482.684144, T: 101816, Avg. loss: 35.043913\n",
            "Total training time: 0.07 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 105\n",
            "Norm: 32190.63, NNZs: 937, Bias: -482.694652, T: 102795, Avg. loss: 34.836801\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 32190.59, NNZs: 937, Bias: -482.701390, T: 103774, Avg. loss: 33.892363\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 32190.55, NNZs: 937, Bias: -482.711305, T: 104753, Avg. loss: 34.489200\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 32190.52, NNZs: 937, Bias: -482.720921, T: 105732, Avg. loss: 34.291877\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 32190.49, NNZs: 937, Bias: -482.727019, T: 106711, Avg. loss: 33.392747\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 32190.45, NNZs: 937, Bias: -482.736126, T: 107690, Avg. loss: 33.975161\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 32190.42, NNZs: 937, Bias: -482.745705, T: 108669, Avg. loss: 33.811942\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 32190.38, NNZs: 937, Bias: -482.754562, T: 109648, Avg. loss: 33.616747\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 32190.35, NNZs: 937, Bias: -482.760168, T: 110627, Avg. loss: 32.767306\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 32190.32, NNZs: 937, Bias: -482.769023, T: 111606, Avg. loss: 33.344282\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 32190.29, NNZs: 937, Bias: -482.775299, T: 112585, Avg. loss: 32.506777\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 32190.26, NNZs: 937, Bias: -482.783888, T: 113564, Avg. loss: 33.058754\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 32190.23, NNZs: 937, Bias: -482.789372, T: 114543, Avg. loss: 32.237328\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 32190.19, NNZs: 937, Bias: -482.798207, T: 115522, Avg. loss: 32.789258\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 32190.16, NNZs: 937, Bias: -482.806727, T: 116501, Avg. loss: 32.626810\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 32190.13, NNZs: 937, Bias: -482.814899, T: 117480, Avg. loss: 32.461739\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 32190.10, NNZs: 937, Bias: -482.820954, T: 118459, Avg. loss: 31.701554\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 32190.07, NNZs: 937, Bias: -482.829376, T: 119438, Avg. loss: 32.218252\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 32190.04, NNZs: 937, Bias: -482.835519, T: 120417, Avg. loss: 31.473006\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 32190.02, NNZs: 937, Bias: -482.841480, T: 121396, Avg. loss: 31.364741\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 32189.99, NNZs: 937, Bias: -482.847843, T: 122375, Avg. loss: 31.282828\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 32189.96, NNZs: 937, Bias: -482.854130, T: 123354, Avg. loss: 31.189962\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 32189.93, NNZs: 937, Bias: -482.860446, T: 124333, Avg. loss: 31.079567\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 32189.91, NNZs: 937, Bias: -482.867016, T: 125312, Avg. loss: 30.999245\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 32189.88, NNZs: 937, Bias: -482.875328, T: 126291, Avg. loss: 31.446925\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 32189.85, NNZs: 937, Bias: -482.881635, T: 127270, Avg. loss: 30.771553\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 32189.82, NNZs: 937, Bias: -482.890281, T: 128249, Avg. loss: 31.222146\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 32189.80, NNZs: 937, Bias: -482.896281, T: 129228, Avg. loss: 30.528304\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 32189.77, NNZs: 937, Bias: -482.904229, T: 130207, Avg. loss: 30.974692\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 32189.74, NNZs: 937, Bias: -482.910283, T: 131186, Avg. loss: 30.311005\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 32189.71, NNZs: 937, Bias: -482.918441, T: 132165, Avg. loss: 30.760876\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 32189.69, NNZs: 937, Bias: -482.925927, T: 133144, Avg. loss: 30.619802\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 32189.66, NNZs: 937, Bias: -482.933744, T: 134123, Avg. loss: 30.508301\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 32189.64, NNZs: 937, Bias: -482.939102, T: 135102, Avg. loss: 29.866085\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 32189.61, NNZs: 937, Bias: -482.944777, T: 136081, Avg. loss: 29.779942\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 32189.59, NNZs: 937, Bias: -482.950597, T: 137060, Avg. loss: 29.704243\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 32189.56, NNZs: 937, Bias: -482.957942, T: 138039, Avg. loss: 30.106925\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 32189.54, NNZs: 937, Bias: -482.963594, T: 139018, Avg. loss: 29.504203\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 32189.51, NNZs: 937, Bias: -482.971322, T: 139997, Avg. loss: 29.915562\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 32189.49, NNZs: 937, Bias: -482.978539, T: 140976, Avg. loss: 29.796872\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 32189.46, NNZs: 937, Bias: -482.985796, T: 141955, Avg. loss: 29.683477\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 32189.44, NNZs: 937, Bias: -482.991133, T: 142934, Avg. loss: 29.106238\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 32189.42, NNZs: 937, Bias: -482.996566, T: 143913, Avg. loss: 29.024319\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 32189.39, NNZs: 937, Bias: -483.003447, T: 144892, Avg. loss: 29.404142\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 32189.37, NNZs: 937, Bias: -483.008940, T: 145871, Avg. loss: 28.849168\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 32189.35, NNZs: 937, Bias: -483.014116, T: 146850, Avg. loss: 28.773579\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 32189.32, NNZs: 937, Bias: -483.021221, T: 147829, Avg. loss: 29.153391\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 32189.30, NNZs: 937, Bias: -483.028154, T: 148808, Avg. loss: 29.045997\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 32189.28, NNZs: 937, Bias: -483.033248, T: 149787, Avg. loss: 28.499829\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 32189.26, NNZs: 937, Bias: -483.039866, T: 150766, Avg. loss: 28.863788\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 32189.23, NNZs: 937, Bias: -483.046718, T: 151745, Avg. loss: 28.771961\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 32189.21, NNZs: 937, Bias: -483.051825, T: 152724, Avg. loss: 28.242590\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 32189.19, NNZs: 937, Bias: -483.056860, T: 153703, Avg. loss: 28.166950\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 32189.17, NNZs: 937, Bias: -483.063286, T: 154682, Avg. loss: 28.526188\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 32189.15, NNZs: 937, Bias: -483.068415, T: 155661, Avg. loss: 28.006621\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 32189.13, NNZs: 937, Bias: -483.073327, T: 156640, Avg. loss: 27.937609\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 32189.11, NNZs: 937, Bias: -483.078688, T: 157619, Avg. loss: 27.881626\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 32189.09, NNZs: 937, Bias: -483.085176, T: 158598, Avg. loss: 28.215791\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 32189.06, NNZs: 937, Bias: -483.091702, T: 159577, Avg. loss: 28.131593\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 32189.04, NNZs: 937, Bias: -483.097996, T: 160556, Avg. loss: 28.033166\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 32189.02, NNZs: 937, Bias: -483.103139, T: 161535, Avg. loss: 27.553319\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 32189.00, NNZs: 937, Bias: -483.109348, T: 162514, Avg. loss: 27.877371\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 32188.98, NNZs: 937, Bias: -483.114152, T: 163493, Avg. loss: 27.397717\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 32188.96, NNZs: 937, Bias: -483.120157, T: 164472, Avg. loss: 27.717566\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 32188.94, NNZs: 937, Bias: -483.126109, T: 165451, Avg. loss: 27.634232\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 32188.92, NNZs: 937, Bias: -483.132193, T: 166430, Avg. loss: 27.555335\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 32188.90, NNZs: 937, Bias: -483.138062, T: 167409, Avg. loss: 27.469463\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 32188.88, NNZs: 937, Bias: -483.143797, T: 168388, Avg. loss: 27.396822\n",
            "Total training time: 0.15 seconds.\n",
            "Convergence after 172 epochs took 0.15 seconds\n",
            "-- Epoch 1\n",
            "Norm: 187.56, NNZs: 632, Bias: -1.406850, T: 979, Avg. loss: 8.589441\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 186.13, NNZs: 559, Bias: -1.086282, T: 1958, Avg. loss: 1.433689\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 186.04, NNZs: 529, Bias: -0.901370, T: 2937, Avg. loss: 0.436383\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 186.02, NNZs: 508, Bias: -0.838949, T: 3916, Avg. loss: 0.311504\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 186.03, NNZs: 485, Bias: -0.786720, T: 4895, Avg. loss: 0.270546\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 186.03, NNZs: 474, Bias: -0.792195, T: 5874, Avg. loss: 0.237426\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 186.04, NNZs: 462, Bias: -0.760841, T: 6853, Avg. loss: 0.236092\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 186.06, NNZs: 452, Bias: -0.744935, T: 7832, Avg. loss: 0.227619\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 186.07, NNZs: 440, Bias: -0.738405, T: 8811, Avg. loss: 0.217301\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 186.08, NNZs: 422, Bias: -0.729679, T: 9790, Avg. loss: 0.214211\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 186.10, NNZs: 416, Bias: -0.716430, T: 10769, Avg. loss: 0.213153\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 186.11, NNZs: 413, Bias: -0.714435, T: 11748, Avg. loss: 0.206636\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 186.12, NNZs: 409, Bias: -0.703650, T: 12727, Avg. loss: 0.206979\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 186.14, NNZs: 403, Bias: -0.697883, T: 13706, Avg. loss: 0.204265\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 186.15, NNZs: 400, Bias: -0.692855, T: 14685, Avg. loss: 0.203775\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 186.16, NNZs: 392, Bias: -0.684681, T: 15664, Avg. loss: 0.203411\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 186.17, NNZs: 386, Bias: -0.680346, T: 16643, Avg. loss: 0.200994\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 186.19, NNZs: 384, Bias: -0.675327, T: 17622, Avg. loss: 0.200732\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 186.20, NNZs: 380, Bias: -0.670465, T: 18601, Avg. loss: 0.201030\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 186.21, NNZs: 375, Bias: -0.665431, T: 19580, Avg. loss: 0.201160\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 186.22, NNZs: 373, Bias: -0.661072, T: 20559, Avg. loss: 0.200515\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 186.23, NNZs: 373, Bias: -0.656672, T: 21538, Avg. loss: 0.200308\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 22 epochs took 0.05 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 1093.13, NNZs: 796, Bias: -6.006795, T: 980, Avg. loss: 355.287983\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1026.61, NNZs: 853, Bias: -2.287448, T: 1960, Avg. loss: 194.520260\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1022.99, NNZs: 852, Bias: -1.243928, T: 2940, Avg. loss: 9.545134\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1022.29, NNZs: 856, Bias: -1.616264, T: 3920, Avg. loss: 2.149595\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1022.04, NNZs: 850, Bias: -1.535643, T: 4900, Avg. loss: 1.117586\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1021.90, NNZs: 847, Bias: -1.501598, T: 5880, Avg. loss: 0.742371\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1021.81, NNZs: 843, Bias: -1.466998, T: 6860, Avg. loss: 0.573206\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1021.75, NNZs: 842, Bias: -1.479072, T: 7840, Avg. loss: 0.461089\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1021.70, NNZs: 836, Bias: -1.475612, T: 8820, Avg. loss: 0.393225\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1021.67, NNZs: 835, Bias: -1.468749, T: 9800, Avg. loss: 0.346939\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1021.64, NNZs: 833, Bias: -1.465627, T: 10780, Avg. loss: 0.312545\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1021.62, NNZs: 832, Bias: -1.447328, T: 11760, Avg. loss: 0.286783\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 1021.60, NNZs: 827, Bias: -1.447125, T: 12740, Avg. loss: 0.262178\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 1021.58, NNZs: 825, Bias: -1.441180, T: 13720, Avg. loss: 0.246850\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 1021.57, NNZs: 826, Bias: -1.433434, T: 14700, Avg. loss: 0.230893\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 1021.56, NNZs: 819, Bias: -1.433926, T: 15680, Avg. loss: 0.218114\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 1021.55, NNZs: 817, Bias: -1.436267, T: 16660, Avg. loss: 0.208069\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 1021.54, NNZs: 815, Bias: -1.433156, T: 17640, Avg. loss: 0.199820\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 1021.53, NNZs: 815, Bias: -1.430119, T: 18620, Avg. loss: 0.191355\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 1021.52, NNZs: 814, Bias: -1.430056, T: 19600, Avg. loss: 0.183864\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 1021.52, NNZs: 813, Bias: -1.428643, T: 20580, Avg. loss: 0.178728\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 1021.51, NNZs: 811, Bias: -1.428255, T: 21560, Avg. loss: 0.172460\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 1021.51, NNZs: 808, Bias: -1.427784, T: 22540, Avg. loss: 0.168121\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 1021.50, NNZs: 805, Bias: -1.424870, T: 23520, Avg. loss: 0.164106\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 1021.50, NNZs: 804, Bias: -1.424526, T: 24500, Avg. loss: 0.159825\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 1021.49, NNZs: 800, Bias: -1.423372, T: 25480, Avg. loss: 0.155858\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 1021.49, NNZs: 800, Bias: -1.420454, T: 26460, Avg. loss: 0.152691\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 1021.49, NNZs: 797, Bias: -1.420184, T: 27440, Avg. loss: 0.149458\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 1021.49, NNZs: 797, Bias: -1.418962, T: 28420, Avg. loss: 0.147048\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 1021.48, NNZs: 796, Bias: -1.418389, T: 29400, Avg. loss: 0.144184\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 1021.48, NNZs: 794, Bias: -1.417386, T: 30380, Avg. loss: 0.141090\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 1021.48, NNZs: 794, Bias: -1.417225, T: 31360, Avg. loss: 0.139241\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 1021.48, NNZs: 793, Bias: -1.416149, T: 32340, Avg. loss: 0.136584\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 1021.47, NNZs: 788, Bias: -1.414998, T: 33320, Avg. loss: 0.135000\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 1021.47, NNZs: 787, Bias: -1.414286, T: 34300, Avg. loss: 0.133081\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 1021.47, NNZs: 784, Bias: -1.412964, T: 35280, Avg. loss: 0.131180\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 1021.47, NNZs: 780, Bias: -1.411737, T: 36260, Avg. loss: 0.129749\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 1021.47, NNZs: 777, Bias: -1.410243, T: 37240, Avg. loss: 0.128270\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 1021.46, NNZs: 777, Bias: -1.409251, T: 38220, Avg. loss: 0.126530\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 1021.46, NNZs: 777, Bias: -1.408276, T: 39200, Avg. loss: 0.125279\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 1021.46, NNZs: 776, Bias: -1.407561, T: 40180, Avg. loss: 0.123696\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 1021.46, NNZs: 774, Bias: -1.406179, T: 41160, Avg. loss: 0.122748\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 1021.46, NNZs: 774, Bias: -1.405143, T: 42140, Avg. loss: 0.121085\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 1021.46, NNZs: 774, Bias: -1.404209, T: 43120, Avg. loss: 0.120029\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 1021.46, NNZs: 774, Bias: -1.403249, T: 44100, Avg. loss: 0.118637\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 1021.46, NNZs: 774, Bias: -1.401829, T: 45080, Avg. loss: 0.117772\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 1021.46, NNZs: 774, Bias: -1.400363, T: 46060, Avg. loss: 0.116843\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 1021.46, NNZs: 774, Bias: -1.399340, T: 47040, Avg. loss: 0.115956\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 1021.45, NNZs: 774, Bias: -1.398388, T: 48020, Avg. loss: 0.115004\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 1021.45, NNZs: 774, Bias: -1.397532, T: 49000, Avg. loss: 0.114081\n",
            "Total training time: 0.24 seconds.\n",
            "Convergence after 50 epochs took 0.24 seconds\n",
            "-- Epoch 1\n",
            "Norm: 118119.53, NNZs: 760, Bias: -161.327883, T: 980, Avg. loss: 1550586.523328\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 135471.62, NNZs: 881, Bias: -1184.752255, T: 1960, Avg. loss: 9257933.887857\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 132341.67, NNZs: 913, Bias: -1266.159841, T: 2940, Avg. loss: 1686791.182941\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 131342.62, NNZs: 919, Bias: -1553.947734, T: 3920, Avg. loss: 240021.221239\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 131155.65, NNZs: 919, Bias: -1635.640445, T: 4900, Avg. loss: 35865.032984\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 131105.78, NNZs: 918, Bias: -1657.596064, T: 5880, Avg. loss: 9862.151077\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 131080.56, NNZs: 917, Bias: -1671.627298, T: 6860, Avg. loss: 3841.028660\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 131068.51, NNZs: 918, Bias: -1678.524690, T: 7840, Avg. loss: 1870.611186\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 131062.49, NNZs: 918, Bias: -1682.001685, T: 8820, Avg. loss: 1013.847763\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 131058.04, NNZs: 918, Bias: -1684.766082, T: 9800, Avg. loss: 618.236585\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 131054.67, NNZs: 917, Bias: -1686.998890, T: 10780, Avg. loss: 390.743429\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 131052.54, NNZs: 917, Bias: -1688.409253, T: 11760, Avg. loss: 260.612291\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 131050.84, NNZs: 917, Bias: -1689.569434, T: 12740, Avg. loss: 182.687388\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 131049.47, NNZs: 917, Bias: -1690.530278, T: 13720, Avg. loss: 131.853918\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 131048.39, NNZs: 917, Bias: -1691.297090, T: 14700, Avg. loss: 97.931824\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 131047.67, NNZs: 917, Bias: -1691.801493, T: 15680, Avg. loss: 74.195365\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 131047.12, NNZs: 917, Bias: -1692.186271, T: 16660, Avg. loss: 58.202122\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 131046.67, NNZs: 917, Bias: -1692.498421, T: 17640, Avg. loss: 47.212813\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 131046.27, NNZs: 917, Bias: -1692.788907, T: 18620, Avg. loss: 39.153889\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 131045.88, NNZs: 917, Bias: -1693.068498, T: 19600, Avg. loss: 33.229076\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 131045.66, NNZs: 917, Bias: -1693.226474, T: 20580, Avg. loss: 27.169829\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 131045.41, NNZs: 917, Bias: -1693.403065, T: 21560, Avg. loss: 23.896434\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 131045.16, NNZs: 917, Bias: -1693.587641, T: 22540, Avg. loss: 21.345774\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 131044.99, NNZs: 917, Bias: -1693.709783, T: 23520, Avg. loss: 18.270218\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 131044.81, NNZs: 917, Bias: -1693.839255, T: 24500, Avg. loss: 16.733320\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 131044.69, NNZs: 917, Bias: -1693.931293, T: 25480, Avg. loss: 14.495163\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 131044.55, NNZs: 917, Bias: -1694.034549, T: 26460, Avg. loss: 13.654399\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 131044.45, NNZs: 917, Bias: -1694.100518, T: 27440, Avg. loss: 11.933962\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 131044.35, NNZs: 917, Bias: -1694.173324, T: 28420, Avg. loss: 11.209150\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 131044.24, NNZs: 917, Bias: -1694.255082, T: 29400, Avg. loss: 10.703765\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 131044.17, NNZs: 917, Bias: -1694.308357, T: 30380, Avg. loss: 9.517278\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 131044.10, NNZs: 917, Bias: -1694.357114, T: 31360, Avg. loss: 8.942852\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 131044.02, NNZs: 917, Bias: -1694.418412, T: 32340, Avg. loss: 8.767780\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 131043.95, NNZs: 917, Bias: -1694.467124, T: 33320, Avg. loss: 8.137808\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 131043.90, NNZs: 917, Bias: -1694.501276, T: 34300, Avg. loss: 7.447674\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 131043.85, NNZs: 917, Bias: -1694.536532, T: 35280, Avg. loss: 7.134884\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 131043.79, NNZs: 917, Bias: -1694.581384, T: 36260, Avg. loss: 7.106336\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 131043.75, NNZs: 917, Bias: -1694.610352, T: 37240, Avg. loss: 6.524796\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 131043.71, NNZs: 917, Bias: -1694.640143, T: 38220, Avg. loss: 6.289778\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 131043.68, NNZs: 917, Bias: -1694.665148, T: 39200, Avg. loss: 6.009894\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 131043.64, NNZs: 917, Bias: -1694.688837, T: 40180, Avg. loss: 5.811656\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 131043.60, NNZs: 917, Bias: -1694.718907, T: 41160, Avg. loss: 5.835207\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 131043.57, NNZs: 917, Bias: -1694.743038, T: 42140, Avg. loss: 5.565676\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 131043.54, NNZs: 917, Bias: -1694.762211, T: 43120, Avg. loss: 5.250467\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 131043.52, NNZs: 917, Bias: -1694.777504, T: 44100, Avg. loss: 5.058721\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 131043.50, NNZs: 917, Bias: -1694.793609, T: 45080, Avg. loss: 4.945477\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 131043.48, NNZs: 917, Bias: -1694.809711, T: 46060, Avg. loss: 4.826413\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 131043.45, NNZs: 917, Bias: -1694.825952, T: 47040, Avg. loss: 4.736866\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 131043.44, NNZs: 917, Bias: -1694.837986, T: 48020, Avg. loss: 4.582368\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 131043.41, NNZs: 917, Bias: -1694.856285, T: 49000, Avg. loss: 4.646549\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 131043.40, NNZs: 917, Bias: -1694.867472, T: 49980, Avg. loss: 4.361644\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 131043.38, NNZs: 917, Bias: -1694.879383, T: 50960, Avg. loss: 4.299958\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 131043.36, NNZs: 917, Bias: -1694.893472, T: 51940, Avg. loss: 4.306444\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 131043.35, NNZs: 917, Bias: -1694.902530, T: 52920, Avg. loss: 4.083386\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 131043.33, NNZs: 917, Bias: -1694.912333, T: 53900, Avg. loss: 4.033660\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 131043.32, NNZs: 917, Bias: -1694.923897, T: 54880, Avg. loss: 4.051149\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 131043.30, NNZs: 917, Bias: -1694.936207, T: 55860, Avg. loss: 3.987698\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 131043.29, NNZs: 917, Bias: -1694.943942, T: 56840, Avg. loss: 3.793676\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 131043.28, NNZs: 917, Bias: -1694.952203, T: 57820, Avg. loss: 3.739665\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 131043.26, NNZs: 917, Bias: -1694.961238, T: 58800, Avg. loss: 3.758773\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 131043.26, NNZs: 917, Bias: -1694.966962, T: 59780, Avg. loss: 3.589981\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 131043.24, NNZs: 917, Bias: -1694.974385, T: 60760, Avg. loss: 3.565205\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 131043.24, NNZs: 917, Bias: -1694.980010, T: 61740, Avg. loss: 3.495684\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 131043.23, NNZs: 917, Bias: -1694.985845, T: 62720, Avg. loss: 3.448680\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 131043.22, NNZs: 917, Bias: -1694.994711, T: 63700, Avg. loss: 3.508495\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 131043.21, NNZs: 917, Bias: -1695.000340, T: 64680, Avg. loss: 3.358657\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 131043.20, NNZs: 917, Bias: -1695.007147, T: 65660, Avg. loss: 3.390314\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 131043.19, NNZs: 917, Bias: -1695.012432, T: 66640, Avg. loss: 3.268003\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 131043.18, NNZs: 917, Bias: -1695.018304, T: 67620, Avg. loss: 3.297468\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 131043.17, NNZs: 917, Bias: -1695.024669, T: 68600, Avg. loss: 3.264388\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 131043.17, NNZs: 917, Bias: -1695.028252, T: 69580, Avg. loss: 3.135055\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 131043.16, NNZs: 917, Bias: -1695.031128, T: 70560, Avg. loss: 3.088783\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 131043.16, NNZs: 917, Bias: -1695.034934, T: 71540, Avg. loss: 3.071545\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 131043.15, NNZs: 917, Bias: -1695.038038, T: 72520, Avg. loss: 3.027188\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 131043.15, NNZs: 917, Bias: -1695.043229, T: 73500, Avg. loss: 3.080102\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 131043.14, NNZs: 917, Bias: -1695.047444, T: 74480, Avg. loss: 3.029604\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 131043.13, NNZs: 917, Bias: -1695.051728, T: 75460, Avg. loss: 2.993037\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 131043.13, NNZs: 917, Bias: -1695.053975, T: 76440, Avg. loss: 2.887865\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 131043.13, NNZs: 917, Bias: -1695.056931, T: 77420, Avg. loss: 2.874672\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 131043.12, NNZs: 917, Bias: -1695.060997, T: 78400, Avg. loss: 2.911069\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 131043.11, NNZs: 917, Bias: -1695.064697, T: 79380, Avg. loss: 2.876727\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 131043.11, NNZs: 917, Bias: -1695.068135, T: 80360, Avg. loss: 2.838222\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 131043.10, NNZs: 917, Bias: -1695.071361, T: 81340, Avg. loss: 2.809284\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 131043.10, NNZs: 917, Bias: -1695.074816, T: 82320, Avg. loss: 2.787754\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 131043.10, NNZs: 917, Bias: -1695.076700, T: 83300, Avg. loss: 2.696583\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 131043.09, NNZs: 917, Bias: -1695.079833, T: 84280, Avg. loss: 2.736832\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 131043.09, NNZs: 917, Bias: -1695.081503, T: 85260, Avg. loss: 2.649464\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 131043.09, NNZs: 917, Bias: -1695.082902, T: 86240, Avg. loss: 2.624480\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 131043.08, NNZs: 917, Bias: -1695.085433, T: 87220, Avg. loss: 2.660649\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 131043.08, NNZs: 917, Bias: -1695.087880, T: 88200, Avg. loss: 2.636516\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 131043.08, NNZs: 917, Bias: -1695.089232, T: 89180, Avg. loss: 2.557778\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 131043.08, NNZs: 917, Bias: -1695.090431, T: 90160, Avg. loss: 2.538293\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 131043.07, NNZs: 917, Bias: -1695.091768, T: 91140, Avg. loss: 2.520787\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 131043.07, NNZs: 917, Bias: -1695.094188, T: 92120, Avg. loss: 2.557338\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 131043.07, NNZs: 917, Bias: -1695.096737, T: 93100, Avg. loss: 2.539647\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 131043.06, NNZs: 917, Bias: -1695.098792, T: 94080, Avg. loss: 2.515843\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 131043.06, NNZs: 917, Bias: -1695.100633, T: 95060, Avg. loss: 2.490526\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 131043.06, NNZs: 917, Bias: -1695.101717, T: 96040, Avg. loss: 2.421089\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 131043.05, NNZs: 917, Bias: -1695.103854, T: 97020, Avg. loss: 2.459159\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 131043.05, NNZs: 917, Bias: -1695.105017, T: 98000, Avg. loss: 2.391333\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 131043.05, NNZs: 917, Bias: -1695.106943, T: 98980, Avg. loss: 2.422751\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 131043.05, NNZs: 917, Bias: -1695.108656, T: 99960, Avg. loss: 2.400099\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 131043.05, NNZs: 917, Bias: -1695.109554, T: 100940, Avg. loss: 2.336777\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 131043.04, NNZs: 917, Bias: -1695.110566, T: 101920, Avg. loss: 2.327211\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 131043.04, NNZs: 917, Bias: -1695.111168, T: 102900, Avg. loss: 2.307167\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 131043.04, NNZs: 917, Bias: -1695.112296, T: 103880, Avg. loss: 2.300855\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 131043.04, NNZs: 917, Bias: -1695.114080, T: 104860, Avg. loss: 2.328458\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 131043.04, NNZs: 917, Bias: -1695.115430, T: 105840, Avg. loss: 2.305101\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 131043.04, NNZs: 917, Bias: -1695.116127, T: 106820, Avg. loss: 2.248121\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 131043.03, NNZs: 917, Bias: -1695.117735, T: 107800, Avg. loss: 2.280703\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 131043.03, NNZs: 917, Bias: -1695.118553, T: 108780, Avg. loss: 2.223666\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 131043.03, NNZs: 917, Bias: -1695.120004, T: 109760, Avg. loss: 2.251146\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 131043.03, NNZs: 917, Bias: -1695.120816, T: 110740, Avg. loss: 2.196234\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 131043.03, NNZs: 917, Bias: -1695.122232, T: 111720, Avg. loss: 2.223819\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 131043.02, NNZs: 917, Bias: -1695.123033, T: 112700, Avg. loss: 2.171121\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 131043.02, NNZs: 917, Bias: -1695.124216, T: 113680, Avg. loss: 2.192927\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 131043.02, NNZs: 917, Bias: -1695.124858, T: 114660, Avg. loss: 2.144098\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 131043.02, NNZs: 917, Bias: -1695.125509, T: 115640, Avg. loss: 2.134371\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 131043.02, NNZs: 917, Bias: -1695.126674, T: 116620, Avg. loss: 2.157564\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 131043.02, NNZs: 917, Bias: -1695.127179, T: 117600, Avg. loss: 2.105812\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 131043.02, NNZs: 917, Bias: -1695.128406, T: 118580, Avg. loss: 2.134005\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 131043.01, NNZs: 917, Bias: -1695.129571, T: 119560, Avg. loss: 2.120805\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 131043.01, NNZs: 917, Bias: -1695.130144, T: 120540, Avg. loss: 2.073040\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 131043.01, NNZs: 917, Bias: -1695.131206, T: 121520, Avg. loss: 2.094298\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 131043.01, NNZs: 917, Bias: -1695.131714, T: 122500, Avg. loss: 2.049853\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 131043.01, NNZs: 917, Bias: -1695.132657, T: 123480, Avg. loss: 2.071126\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 131043.01, NNZs: 917, Bias: -1695.133502, T: 124460, Avg. loss: 2.058938\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 131043.01, NNZs: 917, Bias: -1695.134239, T: 125440, Avg. loss: 2.045612\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 131043.01, NNZs: 917, Bias: -1695.134781, T: 126420, Avg. loss: 2.006871\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 131043.00, NNZs: 917, Bias: -1695.135192, T: 127400, Avg. loss: 1.994804\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 131043.00, NNZs: 917, Bias: -1695.135934, T: 128380, Avg. loss: 2.015756\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 131043.00, NNZs: 917, Bias: -1695.136401, T: 129360, Avg. loss: 1.977692\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 131043.00, NNZs: 917, Bias: -1695.136730, T: 130340, Avg. loss: 1.965338\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 131043.00, NNZs: 917, Bias: -1695.137129, T: 131320, Avg. loss: 1.957613\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 131043.00, NNZs: 917, Bias: -1695.137960, T: 132300, Avg. loss: 1.978309\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 131043.00, NNZs: 917, Bias: -1695.138721, T: 133280, Avg. loss: 1.967746\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 131043.00, NNZs: 917, Bias: -1695.139472, T: 134260, Avg. loss: 1.958652\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 131043.00, NNZs: 917, Bias: -1695.140249, T: 135240, Avg. loss: 1.949220\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 131043.00, NNZs: 917, Bias: -1695.140459, T: 136220, Avg. loss: 1.908078\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 131042.99, NNZs: 917, Bias: -1695.140827, T: 137200, Avg. loss: 1.902312\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 131042.99, NNZs: 917, Bias: -1695.141049, T: 138180, Avg. loss: 1.891603\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 131042.99, NNZs: 917, Bias: -1695.141774, T: 139160, Avg. loss: 1.912223\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 131042.99, NNZs: 917, Bias: -1695.142225, T: 140140, Avg. loss: 1.877792\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 131042.99, NNZs: 917, Bias: -1695.142446, T: 141120, Avg. loss: 1.866649\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 131042.99, NNZs: 917, Bias: -1695.142855, T: 142100, Avg. loss: 1.862642\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 131042.99, NNZs: 917, Bias: -1695.143092, T: 143080, Avg. loss: 1.851827\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 131042.99, NNZs: 917, Bias: -1695.143762, T: 144060, Avg. loss: 1.872254\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 131042.99, NNZs: 917, Bias: -1695.144142, T: 145040, Avg. loss: 1.838197\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 131042.99, NNZs: 917, Bias: -1695.144459, T: 146020, Avg. loss: 1.830312\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 131042.99, NNZs: 917, Bias: -1695.145084, T: 147000, Avg. loss: 1.846286\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 131042.99, NNZs: 917, Bias: -1695.145289, T: 147980, Avg. loss: 1.812618\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 131042.99, NNZs: 917, Bias: -1695.145813, T: 148960, Avg. loss: 1.829805\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.146417, T: 149940, Avg. loss: 1.823555\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.146929, T: 150920, Avg. loss: 1.813409\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.147076, T: 151900, Avg. loss: 1.780239\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.147271, T: 152880, Avg. loss: 1.774122\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.147543, T: 153860, Avg. loss: 1.769342\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.147790, T: 154840, Avg. loss: 1.762090\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.148321, T: 155820, Avg. loss: 1.779151\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.148520, T: 156800, Avg. loss: 1.746774\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.149029, T: 157780, Avg. loss: 1.764331\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.149238, T: 158760, Avg. loss: 1.732862\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.149706, T: 159740, Avg. loss: 1.749978\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.150189, T: 160720, Avg. loss: 1.742331\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.150569, T: 161700, Avg. loss: 1.733672\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.150698, T: 162680, Avg. loss: 1.704398\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.150797, T: 163660, Avg. loss: 1.698373\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.151173, T: 164640, Avg. loss: 1.714001\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 131042.98, NNZs: 917, Bias: -1695.151562, T: 165620, Avg. loss: 1.706966\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.151718, T: 166600, Avg. loss: 1.680468\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.152081, T: 167580, Avg. loss: 1.694109\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.152471, T: 168560, Avg. loss: 1.687868\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.152768, T: 169540, Avg. loss: 1.680327\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.153170, T: 170520, Avg. loss: 1.675910\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.153481, T: 171500, Avg. loss: 1.667622\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.153595, T: 172480, Avg. loss: 1.642846\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.153712, T: 173460, Avg. loss: 1.636738\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.154025, T: 174440, Avg. loss: 1.650739\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.154313, T: 175420, Avg. loss: 1.644197\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.154347, T: 176400, Avg. loss: 1.618777\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.154428, T: 177380, Avg. loss: 1.615064\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.154529, T: 178360, Avg. loss: 1.610425\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.154808, T: 179340, Avg. loss: 1.623552\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.154824, T: 180320, Avg. loss: 1.597586\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.154884, T: 181300, Avg. loss: 1.594050\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.154968, T: 182280, Avg. loss: 1.589355\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.155018, T: 183260, Avg. loss: 1.583362\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.155067, T: 184240, Avg. loss: 1.579362\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.155353, T: 185220, Avg. loss: 1.592941\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.155439, T: 186200, Avg. loss: 1.569315\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.155711, T: 187180, Avg. loss: 1.582284\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.155763, T: 188160, Avg. loss: 1.559124\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.155946, T: 189140, Avg. loss: 1.570055\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.156165, T: 190120, Avg. loss: 1.566162\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.156370, T: 191100, Avg. loss: 1.559767\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 131042.97, NNZs: 917, Bias: -1695.156390, T: 192080, Avg. loss: 1.538587\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.156407, T: 193060, Avg. loss: 1.534070\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.156421, T: 194040, Avg. loss: 1.529330\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.156451, T: 195020, Avg. loss: 1.525518\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.156667, T: 196000, Avg. loss: 1.537662\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.156849, T: 196980, Avg. loss: 1.531484\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157010, T: 197960, Avg. loss: 1.526051\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157032, T: 198940, Avg. loss: 1.506266\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157196, T: 199920, Avg. loss: 1.516978\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157371, T: 200900, Avg. loss: 1.512809\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157517, T: 201880, Avg. loss: 1.506954\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157483, T: 202860, Avg. loss: 1.487150\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157636, T: 203840, Avg. loss: 1.499373\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157716, T: 204820, Avg. loss: 1.493035\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157829, T: 205800, Avg. loss: 1.488909\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157811, T: 206780, Avg. loss: 1.470165\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157929, T: 207760, Avg. loss: 1.480535\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157943, T: 208740, Avg. loss: 1.462423\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 214\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157914, T: 209720, Avg. loss: 1.457537\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 215\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157942, T: 210700, Avg. loss: 1.454719\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 216\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.157937, T: 211680, Avg. loss: 1.450530\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 217\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158037, T: 212660, Avg. loss: 1.460743\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 218\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158182, T: 213640, Avg. loss: 1.456937\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 219\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158340, T: 214620, Avg. loss: 1.452863\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 220\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158405, T: 215600, Avg. loss: 1.446922\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 221\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158401, T: 216580, Avg. loss: 1.430060\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 222\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158529, T: 217560, Avg. loss: 1.440287\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 223\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158518, T: 218540, Avg. loss: 1.422300\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 224\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158630, T: 219520, Avg. loss: 1.432348\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 225\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158729, T: 220500, Avg. loss: 1.428141\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 226\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158813, T: 221480, Avg. loss: 1.423230\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 227\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158874, T: 222460, Avg. loss: 1.419210\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 228\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158804, T: 223440, Avg. loss: 1.401814\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 229\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158786, T: 224420, Avg. loss: 1.399675\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 230\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158897, T: 225400, Avg. loss: 1.409336\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 231\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158860, T: 226380, Avg. loss: 1.391745\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 232\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158960, T: 227360, Avg. loss: 1.401954\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 233\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159058, T: 228340, Avg. loss: 1.398005\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 234\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158991, T: 229320, Avg. loss: 1.380422\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 235\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159047, T: 230300, Avg. loss: 1.389988\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 236\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158997, T: 231280, Avg. loss: 1.374139\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 237\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158962, T: 232260, Avg. loss: 1.371313\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 238\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159059, T: 233240, Avg. loss: 1.380025\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 239\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159032, T: 234220, Avg. loss: 1.364372\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 240\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158968, T: 235200, Avg. loss: 1.360549\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 241\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158910, T: 236180, Avg. loss: 1.357647\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 242\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158998, T: 237160, Avg. loss: 1.367227\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 243\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158950, T: 238140, Avg. loss: 1.351051\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 244\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159008, T: 239120, Avg. loss: 1.360034\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 245\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158962, T: 240100, Avg. loss: 1.344424\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 246\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158915, T: 241080, Avg. loss: 1.341138\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 247\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158879, T: 242060, Avg. loss: 1.338481\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 248\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158959, T: 243040, Avg. loss: 1.347339\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 249\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159011, T: 244020, Avg. loss: 1.343146\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 250\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159087, T: 245000, Avg. loss: 1.340577\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 251\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159049, T: 245980, Avg. loss: 1.325417\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 252\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159004, T: 246960, Avg. loss: 1.322634\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 253\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159069, T: 247940, Avg. loss: 1.330854\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 254\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159054, T: 248920, Avg. loss: 1.316896\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 255\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159020, T: 249900, Avg. loss: 1.313804\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 256\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158960, T: 250880, Avg. loss: 1.310045\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 257\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159031, T: 251860, Avg. loss: 1.318980\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 258\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.158982, T: 252840, Avg. loss: 1.304167\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 259\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159026, T: 253820, Avg. loss: 1.312155\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 260\n",
            "Norm: 131042.96, NNZs: 917, Bias: -1695.159006, T: 254800, Avg. loss: 1.299097\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 261\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159053, T: 255780, Avg. loss: 1.306255\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 262\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159127, T: 256760, Avg. loss: 1.303762\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 263\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159095, T: 257740, Avg. loss: 1.289844\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 264\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159047, T: 258720, Avg. loss: 1.286429\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 265\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159026, T: 259700, Avg. loss: 1.284308\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 266\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158981, T: 260680, Avg. loss: 1.281123\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 267\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159041, T: 261660, Avg. loss: 1.288955\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 268\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159069, T: 262640, Avg. loss: 1.285709\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 269\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159136, T: 263620, Avg. loss: 1.282953\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 270\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159122, T: 264600, Avg. loss: 1.270341\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 271\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159166, T: 265580, Avg. loss: 1.277139\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 272\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159127, T: 266560, Avg. loss: 1.264244\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 273\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159102, T: 267540, Avg. loss: 1.262198\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 274\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159149, T: 268520, Avg. loss: 1.269449\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 275\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159215, T: 269500, Avg. loss: 1.266630\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 276\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159237, T: 270480, Avg. loss: 1.263279\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 277\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159181, T: 271460, Avg. loss: 1.250240\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 278\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159144, T: 272440, Avg. loss: 1.248295\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 279\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159113, T: 273420, Avg. loss: 1.245817\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 280\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159072, T: 274400, Avg. loss: 1.243061\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 281\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159112, T: 275380, Avg. loss: 1.250622\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 282\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159060, T: 276360, Avg. loss: 1.237665\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 283\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159025, T: 277340, Avg. loss: 1.235895\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 284\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158997, T: 278320, Avg. loss: 1.233531\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 285\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158958, T: 279300, Avg. loss: 1.230552\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 286\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158998, T: 280280, Avg. loss: 1.237557\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 287\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.159039, T: 281260, Avg. loss: 1.235244\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 288\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158987, T: 282240, Avg. loss: 1.222984\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 289\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158937, T: 283220, Avg. loss: 1.220190\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 290\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158904, T: 284200, Avg. loss: 1.218326\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 291\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158861, T: 285180, Avg. loss: 1.215757\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 292\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158833, T: 286160, Avg. loss: 1.213614\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 293\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158800, T: 287140, Avg. loss: 1.211267\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 294\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158834, T: 288120, Avg. loss: 1.217948\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 295\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158879, T: 289100, Avg. loss: 1.215475\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 296\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158913, T: 290080, Avg. loss: 1.212914\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 297\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158853, T: 291060, Avg. loss: 1.201005\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 298\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158814, T: 292040, Avg. loss: 1.198871\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 299\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158796, T: 293020, Avg. loss: 1.197328\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 300\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158823, T: 294000, Avg. loss: 1.203355\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 301\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158853, T: 294980, Avg. loss: 1.200988\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 302\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158822, T: 295960, Avg. loss: 1.189788\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 303\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158774, T: 296940, Avg. loss: 1.187261\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 304\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158804, T: 297920, Avg. loss: 1.193866\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 305\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158763, T: 298900, Avg. loss: 1.183233\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 306\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158738, T: 299880, Avg. loss: 1.181068\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 307\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158707, T: 300860, Avg. loss: 1.178890\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 308\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158744, T: 301840, Avg. loss: 1.184987\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 309\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158768, T: 302820, Avg. loss: 1.182507\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 310\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158728, T: 303800, Avg. loss: 1.171767\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 311\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158750, T: 304780, Avg. loss: 1.177898\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 312\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158705, T: 305760, Avg. loss: 1.167511\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 313\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158721, T: 306740, Avg. loss: 1.173232\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 314\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158727, T: 307720, Avg. loss: 1.170834\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 315\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158731, T: 308700, Avg. loss: 1.168362\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 316\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158749, T: 309680, Avg. loss: 1.166587\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 317\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158697, T: 310660, Avg. loss: 1.156124\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 318\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158651, T: 311640, Avg. loss: 1.154063\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 319\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158669, T: 312620, Avg. loss: 1.160275\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 320\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158609, T: 313600, Avg. loss: 1.149723\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 321\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158638, T: 314580, Avg. loss: 1.156063\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 322\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158598, T: 315560, Avg. loss: 1.145953\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 323\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158618, T: 316540, Avg. loss: 1.151779\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 324\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158635, T: 317520, Avg. loss: 1.149506\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 325\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158650, T: 318500, Avg. loss: 1.147289\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 326\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158641, T: 319480, Avg. loss: 1.144649\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 327\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158650, T: 320460, Avg. loss: 1.142874\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 328\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158655, T: 321440, Avg. loss: 1.140622\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 329\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158606, T: 322420, Avg. loss: 1.131285\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 330\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158609, T: 323400, Avg. loss: 1.136471\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 331\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158559, T: 324380, Avg. loss: 1.127199\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 332\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158525, T: 325360, Avg. loss: 1.125679\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 333\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158492, T: 326340, Avg. loss: 1.123798\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 334\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158434, T: 327320, Avg. loss: 1.121296\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 335\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158449, T: 328300, Avg. loss: 1.127295\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 336\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158393, T: 329280, Avg. loss: 1.117429\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 337\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158398, T: 330260, Avg. loss: 1.123032\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 338\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158412, T: 331240, Avg. loss: 1.121382\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 339\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158403, T: 332220, Avg. loss: 1.118948\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 340\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158353, T: 333200, Avg. loss: 1.110028\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 341\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158348, T: 334180, Avg. loss: 1.114969\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 342\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158343, T: 335160, Avg. loss: 1.113011\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 343\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158303, T: 336140, Avg. loss: 1.104526\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 344\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158249, T: 337120, Avg. loss: 1.102337\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 345\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158250, T: 338100, Avg. loss: 1.107863\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 346\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158246, T: 339080, Avg. loss: 1.105687\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 347\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158210, T: 340060, Avg. loss: 1.097168\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 348\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158197, T: 341040, Avg. loss: 1.101756\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 349\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158152, T: 342020, Avg. loss: 1.093357\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 350\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158103, T: 343000, Avg. loss: 1.091431\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 351\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158043, T: 343980, Avg. loss: 1.089532\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 352\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.158039, T: 344960, Avg. loss: 1.094833\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 353\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.157993, T: 345940, Avg. loss: 1.086566\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 354\n",
            "Norm: 131042.95, NNZs: 917, Bias: -1695.157997, T: 346920, Avg. loss: 1.091347\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 355\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157959, T: 347900, Avg. loss: 1.082995\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 356\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157950, T: 348880, Avg. loss: 1.087442\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 357\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157949, T: 349860, Avg. loss: 1.085955\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 358\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157942, T: 350840, Avg. loss: 1.083931\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 359\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157934, T: 351820, Avg. loss: 1.082212\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 360\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157933, T: 352800, Avg. loss: 1.080371\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 361\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157892, T: 353780, Avg. loss: 1.072302\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 362\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157885, T: 354760, Avg. loss: 1.076850\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 363\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157878, T: 355740, Avg. loss: 1.075317\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 364\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157880, T: 356720, Avg. loss: 1.073663\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 365\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157827, T: 357700, Avg. loss: 1.065315\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 366\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157770, T: 358680, Avg. loss: 1.063439\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 367\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157716, T: 359660, Avg. loss: 1.062047\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 368\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157712, T: 360640, Avg. loss: 1.066852\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 369\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157654, T: 361620, Avg. loss: 1.058536\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 370\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157607, T: 362600, Avg. loss: 1.057093\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 371\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157563, T: 363580, Avg. loss: 1.055767\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 372\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157555, T: 364560, Avg. loss: 1.060198\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 373\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157504, T: 365540, Avg. loss: 1.052548\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 374\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157455, T: 366520, Avg. loss: 1.050687\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 375\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157453, T: 367500, Avg. loss: 1.055440\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 376\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157448, T: 368480, Avg. loss: 1.053685\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 377\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157399, T: 369460, Avg. loss: 1.045977\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 378\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157388, T: 370440, Avg. loss: 1.050358\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 379\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157379, T: 371420, Avg. loss: 1.048849\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 380\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157367, T: 372400, Avg. loss: 1.047033\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 381\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157362, T: 373380, Avg. loss: 1.045601\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 382\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157319, T: 374360, Avg. loss: 1.038213\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 383\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157262, T: 375340, Avg. loss: 1.036250\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 384\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157212, T: 376320, Avg. loss: 1.035003\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 385\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157164, T: 377300, Avg. loss: 1.033561\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 386\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157161, T: 378280, Avg. loss: 1.038143\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 387\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157110, T: 379260, Avg. loss: 1.030331\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 388\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157070, T: 380240, Avg. loss: 1.029217\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 389\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157062, T: 381220, Avg. loss: 1.033314\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 390\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157055, T: 382200, Avg. loss: 1.031838\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 391\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157045, T: 383180, Avg. loss: 1.030142\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 392\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.157004, T: 384160, Avg. loss: 1.022987\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 393\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156993, T: 385140, Avg. loss: 1.027065\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 394\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156987, T: 386120, Avg. loss: 1.025727\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 395\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156974, T: 387100, Avg. loss: 1.023937\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 396\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156959, T: 388080, Avg. loss: 1.022514\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 397\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156952, T: 389060, Avg. loss: 1.021157\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 398\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156941, T: 390040, Avg. loss: 1.019370\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 399\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156882, T: 391020, Avg. loss: 1.012084\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 400\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156873, T: 392000, Avg. loss: 1.016644\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 401\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156828, T: 392980, Avg. loss: 1.009491\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 402\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156773, T: 393960, Avg. loss: 1.007856\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 403\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156766, T: 394940, Avg. loss: 1.012414\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 404\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156745, T: 395920, Avg. loss: 1.010558\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 405\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156729, T: 396900, Avg. loss: 1.009121\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 406\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156684, T: 397880, Avg. loss: 1.002277\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 407\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156660, T: 398860, Avg. loss: 1.006126\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 408\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156647, T: 399840, Avg. loss: 1.005048\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 409\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156630, T: 400820, Avg. loss: 1.003396\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 410\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156612, T: 401800, Avg. loss: 1.001922\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 411\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156572, T: 402780, Avg. loss: 0.995422\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 412\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156556, T: 403760, Avg. loss: 0.999193\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 413\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156506, T: 404740, Avg. loss: 0.992356\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 414\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156465, T: 405720, Avg. loss: 0.991391\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 415\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156421, T: 406700, Avg. loss: 0.990079\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 416\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156405, T: 407680, Avg. loss: 0.993835\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 417\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156390, T: 408660, Avg. loss: 0.992528\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 418\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156376, T: 409640, Avg. loss: 0.991064\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 419\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156325, T: 410620, Avg. loss: 0.984359\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 420\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156274, T: 411600, Avg. loss: 0.983164\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 421\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156226, T: 412580, Avg. loss: 0.981762\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 422\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156177, T: 413560, Avg. loss: 0.980517\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 423\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156165, T: 414540, Avg. loss: 0.984497\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 424\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156118, T: 415520, Avg. loss: 0.978074\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 425\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156069, T: 416500, Avg. loss: 0.976802\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 426\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156021, T: 417480, Avg. loss: 0.975412\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 427\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.156003, T: 418460, Avg. loss: 0.979340\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 428\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155948, T: 419440, Avg. loss: 0.972677\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 429\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155902, T: 420420, Avg. loss: 0.971817\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 430\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155896, T: 421400, Avg. loss: 0.975635\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 431\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155851, T: 422380, Avg. loss: 0.969130\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 432\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155814, T: 423360, Avg. loss: 0.968008\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 433\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155795, T: 424340, Avg. loss: 0.971625\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 434\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155740, T: 425320, Avg. loss: 0.965105\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 435\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155721, T: 426300, Avg. loss: 0.969007\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 436\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155710, T: 427280, Avg. loss: 0.967829\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 437\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155672, T: 428260, Avg. loss: 0.961776\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 438\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155658, T: 429240, Avg. loss: 0.965228\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 439\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155616, T: 430220, Avg. loss: 0.959101\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 440\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155602, T: 431200, Avg. loss: 0.962659\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 441\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155551, T: 432180, Avg. loss: 0.956463\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 442\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155509, T: 433160, Avg. loss: 0.955481\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 443\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155458, T: 434140, Avg. loss: 0.954038\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 444\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155414, T: 435120, Avg. loss: 0.953108\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 445\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155372, T: 436100, Avg. loss: 0.952091\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 446\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155358, T: 437080, Avg. loss: 0.955481\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 447\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155349, T: 438060, Avg. loss: 0.954318\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 448\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155309, T: 439040, Avg. loss: 0.948348\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 449\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155265, T: 440020, Avg. loss: 0.947039\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 450\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155252, T: 441000, Avg. loss: 0.950679\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 451\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155214, T: 441980, Avg. loss: 0.944781\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 452\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155200, T: 442960, Avg. loss: 0.948354\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 453\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155186, T: 443940, Avg. loss: 0.947088\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 454\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155144, T: 444920, Avg. loss: 0.941176\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 455\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155099, T: 445900, Avg. loss: 0.939980\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 456\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155080, T: 446880, Avg. loss: 0.943387\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 457\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.155038, T: 447860, Avg. loss: 0.937707\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 458\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154992, T: 448840, Avg. loss: 0.936518\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 459\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154980, T: 449820, Avg. loss: 0.940053\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 460\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154962, T: 450800, Avg. loss: 0.938745\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 461\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154938, T: 451780, Avg. loss: 0.937353\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 462\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154893, T: 452760, Avg. loss: 0.931844\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 463\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154872, T: 453740, Avg. loss: 0.935222\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 464\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154852, T: 454720, Avg. loss: 0.933950\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 465\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154819, T: 455700, Avg. loss: 0.928694\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 466\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154769, T: 456680, Avg. loss: 0.927191\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 467\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154750, T: 457660, Avg. loss: 0.930599\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 468\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154705, T: 458640, Avg. loss: 0.925033\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 469\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154685, T: 459620, Avg. loss: 0.928329\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 470\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154668, T: 460600, Avg. loss: 0.927207\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 471\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154657, T: 461580, Avg. loss: 0.926255\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 472\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154610, T: 462560, Avg. loss: 0.920454\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 473\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154563, T: 463540, Avg. loss: 0.919388\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 474\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154522, T: 464520, Avg. loss: 0.918515\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 475\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154481, T: 465500, Avg. loss: 0.917433\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 476\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154461, T: 466480, Avg. loss: 0.920592\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 477\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154449, T: 467460, Avg. loss: 0.919524\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 478\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154433, T: 468440, Avg. loss: 0.918389\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 479\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154389, T: 469420, Avg. loss: 0.912992\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 480\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154367, T: 470400, Avg. loss: 0.916092\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 481\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154325, T: 471380, Avg. loss: 0.910913\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 482\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154300, T: 472360, Avg. loss: 0.913791\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 483\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154282, T: 473340, Avg. loss: 0.912935\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 484\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154265, T: 474320, Avg. loss: 0.911770\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 485\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154218, T: 475300, Avg. loss: 0.906423\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 486\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154203, T: 476280, Avg. loss: 0.909874\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 487\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154162, T: 477260, Avg. loss: 0.904477\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 488\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154115, T: 478240, Avg. loss: 0.903318\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 489\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154080, T: 479220, Avg. loss: 0.902597\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 490\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154058, T: 480200, Avg. loss: 0.905444\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 491\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154022, T: 481180, Avg. loss: 0.900563\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 492\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.154003, T: 482160, Avg. loss: 0.903395\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 493\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.153990, T: 483140, Avg. loss: 0.902526\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 494\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.153974, T: 484120, Avg. loss: 0.901399\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 495\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.153935, T: 485100, Avg. loss: 0.896252\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 496\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.153896, T: 486080, Avg. loss: 0.895350\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 497\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.153877, T: 487060, Avg. loss: 0.898318\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 498\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.153835, T: 488040, Avg. loss: 0.893140\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 499\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.153794, T: 489020, Avg. loss: 0.892237\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 500\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.153758, T: 490000, Avg. loss: 0.891411\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 501\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.153737, T: 490980, Avg. loss: 0.894268\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 502\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.153718, T: 491960, Avg. loss: 0.893113\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 503\n",
            "Norm: 131042.95, NNZs: 912, Bias: -1695.153698, T: 492940, Avg. loss: 0.892110\n",
            "Total training time: 0.65 seconds.\n",
            "Convergence after 503 epochs took 0.65 seconds\n",
            "-- Epoch 1\n",
            "Norm: 425.32, NNZs: 733, Bias: -1.951255, T: 980, Avg. loss: 55.665796\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 410.75, NNZs: 718, Bias: -2.104986, T: 1960, Avg. loss: 16.617648\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 409.45, NNZs: 694, Bias: -1.457284, T: 2940, Avg. loss: 2.552566\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 408.87, NNZs: 686, Bias: -1.516127, T: 3920, Avg. loss: 1.189863\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 408.63, NNZs: 670, Bias: -1.421493, T: 4900, Avg. loss: 0.834267\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 408.47, NNZs: 666, Bias: -1.405057, T: 5880, Avg. loss: 0.641186\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 408.36, NNZs: 659, Bias: -1.385510, T: 6860, Avg. loss: 0.552662\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 408.28, NNZs: 646, Bias: -1.358491, T: 7840, Avg. loss: 0.495033\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 408.22, NNZs: 638, Bias: -1.343118, T: 8820, Avg. loss: 0.447810\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 408.16, NNZs: 633, Bias: -1.327875, T: 9800, Avg. loss: 0.419296\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 408.13, NNZs: 627, Bias: -1.308838, T: 10780, Avg. loss: 0.396353\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 408.09, NNZs: 623, Bias: -1.298410, T: 11760, Avg. loss: 0.372207\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 408.06, NNZs: 622, Bias: -1.283675, T: 12740, Avg. loss: 0.361259\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 408.04, NNZs: 619, Bias: -1.272323, T: 13720, Avg. loss: 0.347559\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 408.01, NNZs: 613, Bias: -1.263560, T: 14700, Avg. loss: 0.334816\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 407.99, NNZs: 610, Bias: -1.253741, T: 15680, Avg. loss: 0.325141\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 407.98, NNZs: 611, Bias: -1.250041, T: 16660, Avg. loss: 0.313019\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 407.96, NNZs: 610, Bias: -1.241178, T: 17640, Avg. loss: 0.311278\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 407.94, NNZs: 608, Bias: -1.236821, T: 18620, Avg. loss: 0.302566\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 407.93, NNZs: 606, Bias: -1.231334, T: 19600, Avg. loss: 0.298207\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 407.92, NNZs: 603, Bias: -1.224996, T: 20580, Avg. loss: 0.294008\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 407.91, NNZs: 602, Bias: -1.220747, T: 21560, Avg. loss: 0.286631\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 407.90, NNZs: 600, Bias: -1.214544, T: 22540, Avg. loss: 0.284746\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 407.89, NNZs: 599, Bias: -1.210394, T: 23520, Avg. loss: 0.279806\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 407.88, NNZs: 599, Bias: -1.204181, T: 24500, Avg. loss: 0.278821\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 407.87, NNZs: 598, Bias: -1.200332, T: 25480, Avg. loss: 0.273005\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 407.86, NNZs: 596, Bias: -1.195676, T: 26460, Avg. loss: 0.271878\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 407.86, NNZs: 596, Bias: -1.191308, T: 27440, Avg. loss: 0.269721\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 407.85, NNZs: 596, Bias: -1.186578, T: 28420, Avg. loss: 0.267528\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 407.84, NNZs: 596, Bias: -1.181671, T: 29400, Avg. loss: 0.266090\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 407.84, NNZs: 596, Bias: -1.177635, T: 30380, Avg. loss: 0.262969\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 407.83, NNZs: 591, Bias: -1.173811, T: 31360, Avg. loss: 0.260925\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 407.83, NNZs: 590, Bias: -1.171049, T: 32340, Avg. loss: 0.257403\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 407.82, NNZs: 590, Bias: -1.168265, T: 33320, Avg. loss: 0.256069\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 407.82, NNZs: 587, Bias: -1.165355, T: 34300, Avg. loss: 0.254842\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 407.81, NNZs: 587, Bias: -1.161329, T: 35280, Avg. loss: 0.254576\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 407.81, NNZs: 586, Bias: -1.158181, T: 36260, Avg. loss: 0.252024\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 407.80, NNZs: 586, Bias: -1.154788, T: 37240, Avg. loss: 0.251345\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 407.80, NNZs: 581, Bias: -1.152025, T: 38220, Avg. loss: 0.248222\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 407.80, NNZs: 583, Bias: -1.149253, T: 39200, Avg. loss: 0.247779\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 407.79, NNZs: 583, Bias: -1.146154, T: 40180, Avg. loss: 0.246973\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 407.79, NNZs: 583, Bias: -1.143247, T: 41160, Avg. loss: 0.245403\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 407.79, NNZs: 583, Bias: -1.140341, T: 42140, Avg. loss: 0.244884\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 407.78, NNZs: 583, Bias: -1.137571, T: 43120, Avg. loss: 0.243407\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 407.78, NNZs: 583, Bias: -1.135075, T: 44100, Avg. loss: 0.241467\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 407.78, NNZs: 583, Bias: -1.132271, T: 45080, Avg. loss: 0.241615\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 407.77, NNZs: 583, Bias: -1.129577, T: 46060, Avg. loss: 0.240573\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 407.77, NNZs: 581, Bias: -1.126985, T: 47040, Avg. loss: 0.239164\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 407.77, NNZs: 581, Bias: -1.124729, T: 48020, Avg. loss: 0.237700\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 407.77, NNZs: 580, Bias: -1.122437, T: 49000, Avg. loss: 0.236727\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 407.76, NNZs: 580, Bias: -1.119892, T: 49980, Avg. loss: 0.237137\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 407.76, NNZs: 580, Bias: -1.117783, T: 50960, Avg. loss: 0.235788\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 407.76, NNZs: 581, Bias: -1.115697, T: 51940, Avg. loss: 0.234425\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 407.76, NNZs: 579, Bias: -1.113697, T: 52920, Avg. loss: 0.234322\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 407.75, NNZs: 578, Bias: -1.111715, T: 53900, Avg. loss: 0.233605\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 407.75, NNZs: 578, Bias: -1.109973, T: 54880, Avg. loss: 0.232056\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 407.75, NNZs: 578, Bias: -1.108123, T: 55860, Avg. loss: 0.232293\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 407.75, NNZs: 574, Bias: -1.105995, T: 56840, Avg. loss: 0.232256\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 407.75, NNZs: 574, Bias: -1.104296, T: 57820, Avg. loss: 0.230832\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 407.74, NNZs: 572, Bias: -1.102380, T: 58800, Avg. loss: 0.231049\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 407.74, NNZs: 572, Bias: -1.100630, T: 59780, Avg. loss: 0.229901\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 407.74, NNZs: 572, Bias: -1.098669, T: 60760, Avg. loss: 0.230221\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 407.74, NNZs: 572, Bias: -1.096917, T: 61740, Avg. loss: 0.229176\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 407.74, NNZs: 572, Bias: -1.095436, T: 62720, Avg. loss: 0.228181\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 64 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 452.40, NNZs: 749, Bias: -1.081687, T: 980, Avg. loss: 34.520173\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 440.34, NNZs: 782, Bias: 0.169342, T: 1960, Avg. loss: 15.759045\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 439.44, NNZs: 738, Bias: 0.025997, T: 2940, Avg. loss: 1.317068\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 439.33, NNZs: 718, Bias: -0.081604, T: 3920, Avg. loss: 0.330328\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 439.30, NNZs: 712, Bias: -0.086566, T: 4900, Avg. loss: 0.192983\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 439.30, NNZs: 703, Bias: -0.062004, T: 5880, Avg. loss: 0.141941\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 439.30, NNZs: 697, Bias: -0.085452, T: 6860, Avg. loss: 0.124816\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 439.30, NNZs: 689, Bias: -0.092380, T: 7840, Avg. loss: 0.110989\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 439.30, NNZs: 682, Bias: -0.106451, T: 8820, Avg. loss: 0.101079\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 439.31, NNZs: 681, Bias: -0.105721, T: 9800, Avg. loss: 0.094315\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 439.31, NNZs: 676, Bias: -0.108093, T: 10780, Avg. loss: 0.089476\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 439.32, NNZs: 673, Bias: -0.107844, T: 11760, Avg. loss: 0.086871\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 439.32, NNZs: 670, Bias: -0.107728, T: 12740, Avg. loss: 0.083560\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 439.33, NNZs: 665, Bias: -0.110788, T: 13720, Avg. loss: 0.082685\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 439.33, NNZs: 662, Bias: -0.109160, T: 14700, Avg. loss: 0.079887\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 439.33, NNZs: 656, Bias: -0.110890, T: 15680, Avg. loss: 0.078584\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 439.34, NNZs: 655, Bias: -0.113925, T: 16660, Avg. loss: 0.076643\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 439.34, NNZs: 654, Bias: -0.115046, T: 17640, Avg. loss: 0.075675\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 439.35, NNZs: 654, Bias: -0.116378, T: 18620, Avg. loss: 0.074829\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 439.35, NNZs: 651, Bias: -0.118010, T: 19600, Avg. loss: 0.074512\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 439.35, NNZs: 645, Bias: -0.119938, T: 20580, Avg. loss: 0.073237\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 439.36, NNZs: 645, Bias: -0.121193, T: 21560, Avg. loss: 0.072365\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 439.36, NNZs: 643, Bias: -0.120933, T: 22540, Avg. loss: 0.071814\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 439.36, NNZs: 643, Bias: -0.120564, T: 23520, Avg. loss: 0.071203\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 439.37, NNZs: 642, Bias: -0.121825, T: 24500, Avg. loss: 0.070973\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 439.37, NNZs: 640, Bias: -0.122709, T: 25480, Avg. loss: 0.070564\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 26 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 348473.64, NNZs: 814, Bias: 1079.958257, T: 980, Avg. loss: 13526104.601287\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 388534.15, NNZs: 971, Bias: -3176.257434, T: 1960, Avg. loss: 89948672.481451\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 364421.09, NNZs: 993, Bias: -3820.811591, T: 2940, Avg. loss: 27551771.979379\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 359255.44, NNZs: 994, Bias: -4871.701890, T: 3920, Avg. loss: 3740807.100923\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 358551.17, NNZs: 995, Bias: -5010.673452, T: 4900, Avg. loss: 574968.467546\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 358277.85, NNZs: 997, Bias: -5093.203676, T: 5880, Avg. loss: 189707.539536\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 358168.32, NNZs: 997, Bias: -5123.115596, T: 6860, Avg. loss: 86221.626122\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 358080.44, NNZs: 997, Bias: -5157.718259, T: 7840, Avg. loss: 60011.228176\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 358036.21, NNZs: 997, Bias: -5171.287259, T: 8820, Avg. loss: 41575.090653\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 357995.90, NNZs: 997, Bias: -5186.250806, T: 9800, Avg. loss: 35140.791058\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 357983.77, NNZs: 997, Bias: -5184.352994, T: 10780, Avg. loss: 25343.346062\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 357962.77, NNZs: 997, Bias: -5190.067907, T: 11760, Avg. loss: 25705.411859\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 357943.61, NNZs: 997, Bias: -5195.968671, T: 12740, Avg. loss: 24283.326560\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 357926.32, NNZs: 997, Bias: -5201.592941, T: 13720, Avg. loss: 22574.693205\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 357913.44, NNZs: 997, Bias: -5204.986099, T: 14700, Avg. loss: 20769.687252\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 357906.35, NNZs: 997, Bias: -5204.990732, T: 15680, Avg. loss: 18701.520871\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 357901.47, NNZs: 997, Bias: -5203.946459, T: 16660, Avg. loss: 16275.008237\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 357893.61, NNZs: 997, Bias: -5205.329680, T: 17640, Avg. loss: 16934.747249\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 357886.54, NNZs: 997, Bias: -5206.575554, T: 18620, Avg. loss: 16339.022978\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 357878.71, NNZs: 997, Bias: -5208.652199, T: 19600, Avg. loss: 16478.713558\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 357874.69, NNZs: 997, Bias: -5208.366708, T: 20580, Avg. loss: 14597.237563\n",
            "Total training time: 0.00 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 22\n",
            "Norm: 357870.10, NNZs: 997, Bias: -5208.660399, T: 21560, Avg. loss: 14465.010959\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 357863.75, NNZs: 997, Bias: -5210.420451, T: 22540, Avg. loss: 15051.341959\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 357859.52, NNZs: 997, Bias: -5210.870084, T: 23520, Avg. loss: 13718.169494\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 357854.00, NNZs: 997, Bias: -5212.392066, T: 24500, Avg. loss: 14201.585211\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 357850.63, NNZs: 997, Bias: -5212.552843, T: 25480, Avg. loss: 12849.342807\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 357846.34, NNZs: 997, Bias: -5213.497344, T: 26460, Avg. loss: 13335.981890\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 357844.16, NNZs: 997, Bias: -5213.071185, T: 27440, Avg. loss: 11949.070737\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 357841.83, NNZs: 997, Bias: -5212.851122, T: 28420, Avg. loss: 11859.959286\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 357837.86, NNZs: 997, Bias: -5213.857613, T: 29400, Avg. loss: 12556.501348\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 357835.30, NNZs: 997, Bias: -5213.974680, T: 30380, Avg. loss: 11546.282647\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 357832.73, NNZs: 997, Bias: -5214.169962, T: 31360, Avg. loss: 11376.082093\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 357829.58, NNZs: 997, Bias: -5214.871616, T: 32340, Avg. loss: 11719.645071\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 357827.21, NNZs: 997, Bias: -5215.105457, T: 33320, Avg. loss: 11220.906066\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 357825.57, NNZs: 997, Bias: -5214.893491, T: 34300, Avg. loss: 10458.949319\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 357823.48, NNZs: 997, Bias: -5215.048930, T: 35280, Avg. loss: 10588.976213\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 357820.85, NNZs: 997, Bias: -5215.625189, T: 36260, Avg. loss: 10878.649680\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 357819.42, NNZs: 997, Bias: -5215.422736, T: 37240, Avg. loss: 9997.133389\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 357817.54, NNZs: 997, Bias: -5215.581686, T: 38220, Avg. loss: 10104.481308\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 357815.78, NNZs: 997, Bias: -5215.714507, T: 39200, Avg. loss: 9951.847388\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 357814.24, NNZs: 997, Bias: -5215.719029, T: 40180, Avg. loss: 9707.464148\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 357812.05, NNZs: 997, Bias: -5216.222744, T: 41160, Avg. loss: 10101.035194\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 357810.14, NNZs: 997, Bias: -5216.559120, T: 42140, Avg. loss: 9876.407734\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 357808.76, NNZs: 997, Bias: -5216.563436, T: 43120, Avg. loss: 9327.504191\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 357807.57, NNZs: 997, Bias: -5216.475213, T: 44100, Avg. loss: 9110.055441\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 357806.32, NNZs: 997, Bias: -5216.456994, T: 45080, Avg. loss: 9053.682921\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 357804.88, NNZs: 997, Bias: -5216.601438, T: 46060, Avg. loss: 9093.623965\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 357803.64, NNZs: 997, Bias: -5216.639087, T: 47040, Avg. loss: 8893.838637\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 357802.44, NNZs: 997, Bias: -5216.665265, T: 48020, Avg. loss: 8796.499813\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 357800.96, NNZs: 997, Bias: -5216.915362, T: 49000, Avg. loss: 8998.798170\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 357799.74, NNZs: 997, Bias: -5217.006096, T: 49980, Avg. loss: 8665.896806\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 357798.68, NNZs: 997, Bias: -5216.999671, T: 50960, Avg. loss: 8480.020444\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 357797.37, NNZs: 997, Bias: -5217.203374, T: 51940, Avg. loss: 8676.474955\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 357796.27, NNZs: 997, Bias: -5217.275066, T: 52920, Avg. loss: 8372.519665\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 357795.33, NNZs: 997, Bias: -5217.252880, T: 53900, Avg. loss: 8206.862314\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 357794.16, NNZs: 997, Bias: -5217.411545, T: 54880, Avg. loss: 8386.623030\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 357792.90, NNZs: 997, Bias: -5217.647902, T: 55860, Avg. loss: 8349.666316\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 357792.07, NNZs: 997, Bias: -5217.610504, T: 56840, Avg. loss: 7916.884752\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 357791.06, NNZs: 997, Bias: -5217.705750, T: 57820, Avg. loss: 7959.643814\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 357790.02, NNZs: 997, Bias: -5217.846828, T: 58800, Avg. loss: 8032.826787\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 357789.24, NNZs: 997, Bias: -5217.814358, T: 59780, Avg. loss: 7708.300911\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 357788.33, NNZs: 997, Bias: -5217.883958, T: 60760, Avg. loss: 7739.204940\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 357787.51, NNZs: 997, Bias: -5217.912920, T: 61740, Avg. loss: 7607.887651\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 357786.71, NNZs: 997, Bias: -5217.936918, T: 62720, Avg. loss: 7554.555303\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 357785.67, NNZs: 997, Bias: -5218.136022, T: 63700, Avg. loss: 7757.441537\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 357784.87, NNZs: 997, Bias: -5218.184444, T: 64680, Avg. loss: 7465.667966\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 357784.00, NNZs: 997, Bias: -5218.295077, T: 65660, Avg. loss: 7533.988549\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 357783.22, NNZs: 997, Bias: -5218.354318, T: 66640, Avg. loss: 7335.855175\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 357782.33, NNZs: 997, Bias: -5218.496660, T: 67620, Avg. loss: 7458.241986\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 357781.49, NNZs: 997, Bias: -5218.617127, T: 68600, Avg. loss: 7381.332577\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 357780.82, NNZs: 997, Bias: -5218.627117, T: 69580, Avg. loss: 7124.535254\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 357780.21, NNZs: 997, Bias: -5218.612550, T: 70560, Avg. loss: 7025.070251\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 357779.55, NNZs: 993, Bias: -5218.633407, T: 71540, Avg. loss: 7039.195904\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 357778.92, NNZs: 993, Bias: -5218.641946, T: 72520, Avg. loss: 6951.040652\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 357778.15, NNZs: 993, Bias: -5218.762262, T: 73500, Avg. loss: 7115.083518\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 357777.46, NNZs: 993, Bias: -5218.832176, T: 74480, Avg. loss: 6992.568823\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 357776.79, NNZs: 993, Bias: -5218.900791, T: 75460, Avg. loss: 6933.522486\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 357776.23, NNZs: 993, Bias: -5218.896578, T: 76440, Avg. loss: 6728.354640\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 357775.62, NNZs: 993, Bias: -5218.933131, T: 77420, Avg. loss: 6750.043942\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 357774.98, NNZs: 993, Bias: -5219.003200, T: 78400, Avg. loss: 6796.412808\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 357774.37, NNZs: 993, Bias: -5219.053353, T: 79380, Avg. loss: 6738.493571\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 357773.75, NNZs: 993, Bias: -5219.117606, T: 80360, Avg. loss: 6695.195232\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 357773.10, NNZs: 993, Bias: -5219.214745, T: 81340, Avg. loss: 6694.949133\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 357772.52, NNZs: 993, Bias: -5219.264856, T: 82320, Avg. loss: 6589.905400\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 357772.00, NNZs: 993, Bias: -5219.282769, T: 83300, Avg. loss: 6458.160623\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 357771.43, NNZs: 993, Bias: -5219.345449, T: 84280, Avg. loss: 6516.453792\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 357770.97, NNZs: 993, Bias: -5219.334067, T: 85260, Avg. loss: 6336.560425\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 357770.50, NNZs: 993, Bias: -5219.331135, T: 86240, Avg. loss: 6307.269453\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 357769.92, NNZs: 993, Bias: -5219.412932, T: 87220, Avg. loss: 6442.351007\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 357769.36, NNZs: 993, Bias: -5219.491618, T: 88200, Avg. loss: 6399.171385\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 357768.93, NNZs: 993, Bias: -5219.478826, T: 89180, Avg. loss: 6183.794105\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 357768.50, NNZs: 993, Bias: -5219.469861, T: 90160, Avg. loss: 6156.723102\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 357768.08, NNZs: 993, Bias: -5219.467400, T: 91140, Avg. loss: 6130.585607\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 357767.58, NNZs: 993, Bias: -5219.516034, T: 92120, Avg. loss: 6214.613601\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 357767.05, NNZs: 993, Bias: -5219.594948, T: 93100, Avg. loss: 6217.696425\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 357766.54, NNZs: 993, Bias: -5219.659520, T: 94080, Avg. loss: 6169.090379\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 357766.08, NNZs: 993, Bias: -5219.697906, T: 95060, Avg. loss: 6094.362844\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 357765.69, NNZs: 993, Bias: -5219.694053, T: 96040, Avg. loss: 5946.858520\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 357765.20, NNZs: 993, Bias: -5219.760515, T: 97020, Avg. loss: 6067.543497\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 357764.82, NNZs: 993, Bias: -5219.760758, T: 98000, Avg. loss: 5889.434992\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 357764.38, NNZs: 993, Bias: -5219.797611, T: 98980, Avg. loss: 5966.051911\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 357763.92, NNZs: 993, Bias: -5219.859070, T: 99960, Avg. loss: 5970.322320\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 357763.55, NNZs: 993, Bias: -5219.859919, T: 100940, Avg. loss: 5797.327208\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 357763.18, NNZs: 993, Bias: -5219.862706, T: 101920, Avg. loss: 5776.567434\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 357762.82, NNZs: 993, Bias: -5219.867488, T: 102900, Avg. loss: 5748.516955\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 357762.45, NNZs: 993, Bias: -5219.879398, T: 103880, Avg. loss: 5727.710283\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 357762.01, NNZs: 993, Bias: -5219.943068, T: 104860, Avg. loss: 5831.508037\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 357761.62, NNZs: 993, Bias: -5219.975697, T: 105840, Avg. loss: 5757.981102\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 357761.27, NNZs: 993, Bias: -5219.978617, T: 106820, Avg. loss: 5635.242328\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 357760.86, NNZs: 993, Bias: -5220.033513, T: 107800, Avg. loss: 5737.750721\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 357760.50, NNZs: 993, Bias: -5220.052228, T: 108780, Avg. loss: 5611.584416\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 357760.13, NNZs: 993, Bias: -5220.086541, T: 109760, Avg. loss: 5644.597033\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 357759.78, NNZs: 993, Bias: -5220.109187, T: 110740, Avg. loss: 5564.563238\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 357759.40, NNZs: 993, Bias: -5220.148751, T: 111720, Avg. loss: 5594.517770\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 357759.06, NNZs: 993, Bias: -5220.172693, T: 112700, Avg. loss: 5515.207041\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 357758.70, NNZs: 993, Bias: -5220.208161, T: 113680, Avg. loss: 5543.206291\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 357758.37, NNZs: 993, Bias: -5220.225378, T: 114660, Avg. loss: 5458.829358\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 357758.06, NNZs: 993, Bias: -5220.232861, T: 115640, Avg. loss: 5410.912316\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 357757.70, NNZs: 993, Bias: -5220.277153, T: 116620, Avg. loss: 5493.045653\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 357757.37, NNZs: 993, Bias: -5220.298006, T: 117600, Avg. loss: 5393.116303\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 357757.04, NNZs: 993, Bias: -5220.325706, T: 118580, Avg. loss: 5412.456251\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 357756.69, NNZs: 993, Bias: -5220.372599, T: 119560, Avg. loss: 5420.586800\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 357756.40, NNZs: 993, Bias: -5220.377571, T: 120540, Avg. loss: 5290.355688\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 357756.08, NNZs: 993, Bias: -5220.412218, T: 121520, Avg. loss: 5349.133435\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 357755.78, NNZs: 993, Bias: -5220.427461, T: 122500, Avg. loss: 5269.185171\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 357755.44, NNZs: 993, Bias: -5220.471905, T: 123480, Avg. loss: 5326.664834\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 357755.14, NNZs: 993, Bias: -5220.497806, T: 124460, Avg. loss: 5270.975412\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 357754.82, NNZs: 993, Bias: -5220.534010, T: 125440, Avg. loss: 5274.153869\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 357754.53, NNZs: 993, Bias: -5220.550721, T: 126420, Avg. loss: 5183.350582\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 357754.27, NNZs: 993, Bias: -5220.555273, T: 127400, Avg. loss: 5138.269406\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 357753.96, NNZs: 993, Bias: -5220.592332, T: 128380, Avg. loss: 5211.958025\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 357753.68, NNZs: 993, Bias: -5220.609215, T: 129360, Avg. loss: 5123.780668\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 357753.41, NNZs: 993, Bias: -5220.620556, T: 130340, Avg. loss: 5100.057063\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 357753.16, NNZs: 993, Bias: -5220.627336, T: 131320, Avg. loss: 5060.986907\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 357752.86, NNZs: 993, Bias: -5220.662623, T: 132300, Avg. loss: 5130.034532\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 357752.58, NNZs: 993, Bias: -5220.687195, T: 133280, Avg. loss: 5085.088919\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 357752.32, NNZs: 993, Bias: -5220.705222, T: 134260, Avg. loss: 5061.445160\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 357752.04, NNZs: 993, Bias: -5220.736900, T: 135240, Avg. loss: 5064.326840\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 357751.80, NNZs: 993, Bias: -5220.741760, T: 136220, Avg. loss: 4958.987376\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 357751.56, NNZs: 993, Bias: -5220.746181, T: 137200, Avg. loss: 4942.614942\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 357751.31, NNZs: 993, Bias: -5220.759612, T: 138180, Avg. loss: 4949.736983\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 357751.04, NNZs: 993, Bias: -5220.786856, T: 139160, Avg. loss: 4973.304806\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 357750.81, NNZs: 993, Bias: -5220.793178, T: 140140, Avg. loss: 4891.078171\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 357750.58, NNZs: 993, Bias: -5220.797755, T: 141120, Avg. loss: 4876.725373\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 357750.35, NNZs: 993, Bias: -5220.808217, T: 142100, Avg. loss: 4866.025204\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 357750.11, NNZs: 993, Bias: -5220.823943, T: 143080, Avg. loss: 4870.158475\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 357749.86, NNZs: 993, Bias: -5220.847551, T: 144060, Avg. loss: 4888.108488\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 357749.63, NNZs: 993, Bias: -5220.855687, T: 145040, Avg. loss: 4812.162574\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 357749.40, NNZs: 993, Bias: -5220.872200, T: 146020, Avg. loss: 4816.811727\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 357749.15, NNZs: 993, Bias: -5220.898767, T: 147000, Avg. loss: 4838.717941\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 357748.94, NNZs: 993, Bias: -5220.905285, T: 147980, Avg. loss: 4762.714595\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 357748.68, NNZs: 993, Bias: -5220.939669, T: 148960, Avg. loss: 4824.544715\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 357748.45, NNZs: 993, Bias: -5220.962788, T: 149940, Avg. loss: 4784.878758\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 357748.20, NNZs: 993, Bias: -5220.992827, T: 150920, Avg. loss: 4788.008202\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 357747.98, NNZs: 993, Bias: -5221.006575, T: 151900, Avg. loss: 4713.986668\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 357747.77, NNZs: 993, Bias: -5221.021082, T: 152880, Avg. loss: 4701.096440\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 357747.55, NNZs: 993, Bias: -5221.035655, T: 153860, Avg. loss: 4684.857546\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 357747.35, NNZs: 993, Bias: -5221.043440, T: 154840, Avg. loss: 4650.578402\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 357747.12, NNZs: 993, Bias: -5221.067217, T: 155820, Avg. loss: 4689.725447\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 357746.92, NNZs: 993, Bias: -5221.075208, T: 156800, Avg. loss: 4623.243103\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 357746.69, NNZs: 993, Bias: -5221.104766, T: 157780, Avg. loss: 4678.933067\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 357746.49, NNZs: 993, Bias: -5221.117350, T: 158760, Avg. loss: 4610.113771\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 357746.28, NNZs: 993, Bias: -5221.137054, T: 159740, Avg. loss: 4625.898400\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 357746.06, NNZs: 993, Bias: -5221.156372, T: 160720, Avg. loss: 4610.695124\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 357745.85, NNZs: 993, Bias: -5221.177068, T: 161700, Avg. loss: 4595.688467\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 357745.67, NNZs: 993, Bias: -5221.182267, T: 162680, Avg. loss: 4531.999962\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 357745.47, NNZs: 993, Bias: -5221.196406, T: 163660, Avg. loss: 4539.394636\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 357745.27, NNZs: 993, Bias: -5221.214601, T: 164640, Avg. loss: 4553.186801\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 357745.07, NNZs: 993, Bias: -5221.232502, T: 165620, Avg. loss: 4537.841066\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 357744.89, NNZs: 993, Bias: -5221.237172, T: 166600, Avg. loss: 4475.768230\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 357744.68, NNZs: 993, Bias: -5221.264367, T: 167580, Avg. loss: 4529.331176\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 357744.47, NNZs: 993, Bias: -5221.291887, T: 168560, Avg. loss: 4513.778083\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 357744.28, NNZs: 993, Bias: -5221.306624, T: 169540, Avg. loss: 4477.906471\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 357744.09, NNZs: 993, Bias: -5221.324917, T: 170520, Avg. loss: 4467.747883\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 357743.89, NNZs: 993, Bias: -5221.347596, T: 171500, Avg. loss: 4467.173919\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 357743.72, NNZs: 993, Bias: -5221.351116, T: 172480, Avg. loss: 4392.599925\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 357743.55, NNZs: 993, Bias: -5221.357189, T: 173460, Avg. loss: 4383.112000\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 357743.35, NNZs: 993, Bias: -5221.382209, T: 174440, Avg. loss: 4431.934819\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 357743.16, NNZs: 993, Bias: -5221.404632, T: 175420, Avg. loss: 4415.889164\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 357743.00, NNZs: 993, Bias: -5221.408380, T: 176400, Avg. loss: 4341.690223\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 357742.83, NNZs: 993, Bias: -5221.418989, T: 177380, Avg. loss: 4348.152682\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 357742.66, NNZs: 993, Bias: -5221.426044, T: 178360, Avg. loss: 4322.905347\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 357742.48, NNZs: 993, Bias: -5221.441362, T: 179340, Avg. loss: 4349.840128\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 357742.32, NNZs: 993, Bias: -5221.445943, T: 180320, Avg. loss: 4296.388080\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 357742.16, NNZs: 993, Bias: -5221.456638, T: 181300, Avg. loss: 4300.649224\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 357742.00, NNZs: 993, Bias: -5221.462086, T: 182280, Avg. loss: 4275.193556\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 357741.83, NNZs: 993, Bias: -5221.474160, T: 183260, Avg. loss: 4279.204328\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 357741.67, NNZs: 993, Bias: -5221.485699, T: 184240, Avg. loss: 4267.192884\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 357741.49, NNZs: 993, Bias: -5221.504584, T: 185220, Avg. loss: 4283.366163\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 357741.34, NNZs: 993, Bias: -5221.511019, T: 186200, Avg. loss: 4230.891405\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 357741.17, NNZs: 993, Bias: -5221.529311, T: 187180, Avg. loss: 4260.146257\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 357741.00, NNZs: 993, Bias: -5221.541091, T: 188160, Avg. loss: 4222.393808\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 357740.84, NNZs: 993, Bias: -5221.556586, T: 189140, Avg. loss: 4234.349829\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 357740.67, NNZs: 993, Bias: -5221.579235, T: 190120, Avg. loss: 4239.487680\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 357740.50, NNZs: 993, Bias: -5221.594691, T: 191100, Avg. loss: 4211.042577\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 357740.35, NNZs: 993, Bias: -5221.601552, T: 192080, Avg. loss: 4163.259990\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 357740.21, NNZs: 993, Bias: -5221.608583, T: 193060, Avg. loss: 4153.149494\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 357740.06, NNZs: 993, Bias: -5221.615772, T: 194040, Avg. loss: 4142.039372\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 357739.91, NNZs: 993, Bias: -5221.627620, T: 195020, Avg. loss: 4146.549854\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 357739.74, NNZs: 993, Bias: -5221.650000, T: 196000, Avg. loss: 4173.878343\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 357739.58, NNZs: 993, Bias: -5221.664802, T: 196980, Avg. loss: 4145.910893\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 357739.42, NNZs: 993, Bias: -5221.682886, T: 197960, Avg. loss: 4147.330042\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 357739.28, NNZs: 993, Bias: -5221.688858, T: 198940, Avg. loss: 4088.338886\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 357739.13, NNZs: 993, Bias: -5221.703101, T: 199920, Avg. loss: 4112.807524\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 357738.97, NNZs: 993, Bias: -5221.723396, T: 200900, Avg. loss: 4116.955848\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 357738.82, NNZs: 993, Bias: -5221.736458, T: 201880, Avg. loss: 4090.632195\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 357738.68, NNZs: 993, Bias: -5221.745610, T: 202860, Avg. loss: 4058.462438\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 357738.53, NNZs: 993, Bias: -5221.765473, T: 203840, Avg. loss: 4085.665752\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 357738.38, NNZs: 993, Bias: -5221.776890, T: 204820, Avg. loss: 4058.845271\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 357738.24, NNZs: 993, Bias: -5221.789748, T: 205800, Avg. loss: 4049.987393\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 357738.10, NNZs: 993, Bias: -5221.799395, T: 206780, Avg. loss: 4019.933037\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 357737.95, NNZs: 993, Bias: -5221.817337, T: 207760, Avg. loss: 4043.948036\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 357737.82, NNZs: 993, Bias: -5221.822909, T: 208740, Avg. loss: 3985.860519\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 214\n",
            "Norm: 357737.69, NNZs: 993, Bias: -5221.833009, T: 209720, Avg. loss: 3990.611350\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 215\n",
            "Norm: 357737.55, NNZs: 993, Bias: -5221.844151, T: 210700, Avg. loss: 3983.982842\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 216\n",
            "Norm: 357737.42, NNZs: 993, Bias: -5221.850420, T: 211680, Avg. loss: 3961.028549\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 217\n",
            "Norm: 357737.28, NNZs: 993, Bias: -5221.862009, T: 212660, Avg. loss: 3982.650215\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 218\n",
            "Norm: 357737.14, NNZs: 993, Bias: -5221.878345, T: 213640, Avg. loss: 3985.954797\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 219\n",
            "Norm: 357737.00, NNZs: 993, Bias: -5221.891994, T: 214620, Avg. loss: 3964.922708\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 220\n",
            "Norm: 357736.86, NNZs: 993, Bias: -5221.907815, T: 215600, Avg. loss: 3965.065700\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 221\n",
            "Norm: 357736.74, NNZs: 993, Bias: -5221.913592, T: 216580, Avg. loss: 3912.370521\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 222\n",
            "Norm: 357736.61, NNZs: 993, Bias: -5221.925374, T: 217560, Avg. loss: 3935.384231\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 223\n",
            "Norm: 357736.48, NNZs: 993, Bias: -5221.934192, T: 218540, Avg. loss: 3906.497908\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 224\n",
            "Norm: 357736.34, NNZs: 993, Bias: -5221.949815, T: 219520, Avg. loss: 3929.484063\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 225\n",
            "Norm: 357736.21, NNZs: 993, Bias: -5221.960934, T: 220500, Avg. loss: 3907.056723\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 226\n",
            "Norm: 357736.08, NNZs: 993, Bias: -5221.975458, T: 221480, Avg. loss: 3909.465638\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 227\n",
            "Norm: 357735.95, NNZs: 993, Bias: -5221.990127, T: 222460, Avg. loss: 3899.883438\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 228\n",
            "Norm: 357735.83, NNZs: 993, Bias: -5221.993076, T: 223440, Avg. loss: 3847.704511\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 229\n",
            "Norm: 357735.72, NNZs: 993, Bias: -5221.998646, T: 224420, Avg. loss: 3842.965909\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 230\n",
            "Norm: 357735.58, NNZs: 993, Bias: -5222.014431, T: 225400, Avg. loss: 3875.633641\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 231\n",
            "Norm: 357735.46, NNZs: 993, Bias: -5222.023293, T: 226380, Avg. loss: 3835.967407\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 232\n",
            "Norm: 357735.33, NNZs: 993, Bias: -5222.037805, T: 227360, Avg. loss: 3857.158802\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 233\n",
            "Norm: 357735.21, NNZs: 993, Bias: -5222.051806, T: 228340, Avg. loss: 3846.634861\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 234\n",
            "Norm: 357735.09, NNZs: 993, Bias: -5222.056222, T: 229320, Avg. loss: 3797.760709\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 235\n",
            "Norm: 357734.97, NNZs: 993, Bias: -5222.066970, T: 230300, Avg. loss: 3820.692891\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 236\n",
            "Norm: 357734.86, NNZs: 993, Bias: -5222.071929, T: 231280, Avg. loss: 3782.452378\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 237\n",
            "Norm: 357734.74, NNZs: 993, Bias: -5222.079195, T: 232260, Avg. loss: 3785.811635\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 238\n",
            "Norm: 357734.62, NNZs: 993, Bias: -5222.093718, T: 233240, Avg. loss: 3806.747483\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 239\n",
            "Norm: 357734.50, NNZs: 993, Bias: -5222.101706, T: 234220, Avg. loss: 3769.417397\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 240\n",
            "Norm: 357734.40, NNZs: 993, Bias: -5222.106156, T: 235200, Avg. loss: 3751.173971\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 241\n",
            "Norm: 357734.29, NNZs: 993, Bias: -5222.111690, T: 236180, Avg. loss: 3745.114581\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 242\n",
            "Norm: 357734.16, NNZs: 993, Bias: -5222.126209, T: 237160, Avg. loss: 3775.026708\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 243\n",
            "Norm: 357734.05, NNZs: 993, Bias: -5222.134882, T: 238140, Avg. loss: 3739.659708\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 244\n",
            "Norm: 357733.93, NNZs: 993, Bias: -5222.148497, T: 239120, Avg. loss: 3758.431521\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 245\n",
            "Norm: 357733.82, NNZs: 993, Bias: -5222.153865, T: 240100, Avg. loss: 3713.994737\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 246\n",
            "Norm: 357733.72, NNZs: 993, Bias: -5222.159359, T: 241080, Avg. loss: 3707.008530\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 247\n",
            "Norm: 357733.60, NNZs: 993, Bias: -5222.168589, T: 242060, Avg. loss: 3709.857703\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 248\n",
            "Norm: 357733.49, NNZs: 993, Bias: -5222.181582, T: 243040, Avg. loss: 3727.152330\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 249\n",
            "Norm: 357733.37, NNZs: 993, Bias: -5222.194107, T: 244020, Avg. loss: 3718.274752\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 250\n",
            "Norm: 357733.26, NNZs: 993, Bias: -5222.204339, T: 245000, Avg. loss: 3700.390930\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 251\n",
            "Norm: 357733.15, NNZs: 993, Bias: -5222.212817, T: 245980, Avg. loss: 3678.329939\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 252\n",
            "Norm: 357733.05, NNZs: 993, Bias: -5222.216940, T: 246960, Avg. loss: 3659.819989\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 253\n",
            "Norm: 357732.94, NNZs: 993, Bias: -5222.228358, T: 247940, Avg. loss: 3681.157337\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 254\n",
            "Norm: 357732.84, NNZs: 993, Bias: -5222.236599, T: 248920, Avg. loss: 3656.628069\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 255\n",
            "Norm: 357732.74, NNZs: 993, Bias: -5222.240863, T: 249900, Avg. loss: 3639.070605\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 256\n",
            "Norm: 357732.64, NNZs: 993, Bias: -5222.245420, T: 250880, Avg. loss: 3632.372184\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 257\n",
            "Norm: 357732.52, NNZs: 993, Bias: -5222.258838, T: 251860, Avg. loss: 3661.362183\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 258\n",
            "Norm: 357732.43, NNZs: 993, Bias: -5222.263710, T: 252840, Avg. loss: 3617.966287\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 259\n",
            "Norm: 357732.32, NNZs: 993, Bias: -5222.276133, T: 253820, Avg. loss: 3644.904054\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 260\n",
            "Norm: 357732.22, NNZs: 993, Bias: -5222.281955, T: 254800, Avg. loss: 3604.052148\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 261\n",
            "Norm: 357732.11, NNZs: 993, Bias: -5222.291992, T: 255780, Avg. loss: 3621.147819\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 262\n",
            "Norm: 357732.00, NNZs: 993, Bias: -5222.305033, T: 256760, Avg. loss: 3623.764208\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 263\n",
            "Norm: 357731.90, NNZs: 993, Bias: -5222.312800, T: 257740, Avg. loss: 3591.687524\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 264\n",
            "Norm: 357731.81, NNZs: 993, Bias: -5222.318055, T: 258720, Avg. loss: 3577.168361\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 265\n",
            "Norm: 357731.71, NNZs: 993, Bias: -5222.325375, T: 259700, Avg. loss: 3578.595546\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 266\n",
            "Norm: 357731.61, NNZs: 993, Bias: -5222.333482, T: 260680, Avg. loss: 3573.049060\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 267\n",
            "Norm: 357731.51, NNZs: 993, Bias: -5222.343988, T: 261660, Avg. loss: 3581.663886\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 268\n",
            "Norm: 357731.40, NNZs: 993, Bias: -5222.356203, T: 262640, Avg. loss: 3581.754090\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 269\n",
            "Norm: 357731.30, NNZs: 993, Bias: -5222.366208, T: 263620, Avg. loss: 3566.095918\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 270\n",
            "Norm: 357731.20, NNZs: 993, Bias: -5222.374007, T: 264600, Avg. loss: 3545.103877\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 271\n",
            "Norm: 357731.10, NNZs: 993, Bias: -5222.386313, T: 265580, Avg. loss: 3561.914909\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 272\n",
            "Norm: 357731.01, NNZs: 993, Bias: -5222.391896, T: 266560, Avg. loss: 3522.253869\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 273\n",
            "Norm: 357730.91, NNZs: 993, Bias: -5222.399986, T: 267540, Avg. loss: 3525.528138\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 274\n",
            "Norm: 357730.81, NNZs: 993, Bias: -5222.412875, T: 268520, Avg. loss: 3541.947587\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 275\n",
            "Norm: 357730.71, NNZs: 993, Bias: -5222.421814, T: 269500, Avg. loss: 3525.136678\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 276\n",
            "Norm: 357730.62, NNZs: 993, Bias: -5222.430068, T: 270480, Avg. loss: 3517.829107\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 277\n",
            "Norm: 357730.53, NNZs: 993, Bias: -5222.434100, T: 271460, Avg. loss: 3488.672232\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 278\n",
            "Norm: 357730.44, NNZs: 993, Bias: -5222.439717, T: 272440, Avg. loss: 3484.596662\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 279\n",
            "Norm: 357730.35, NNZs: 993, Bias: -5222.446773, T: 273420, Avg. loss: 3486.915898\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 280\n",
            "Norm: 357730.25, NNZs: 993, Bias: -5222.454103, T: 274400, Avg. loss: 3480.490304\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 281\n",
            "Norm: 357730.16, NNZs: 993, Bias: -5222.463330, T: 275380, Avg. loss: 3487.574760\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 282\n",
            "Norm: 357730.07, NNZs: 993, Bias: -5222.468454, T: 276360, Avg. loss: 3459.538085\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 283\n",
            "Norm: 357729.98, NNZs: 993, Bias: -5222.474044, T: 277340, Avg. loss: 3453.987076\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 284\n",
            "Norm: 357729.89, NNZs: 993, Bias: -5222.481761, T: 278320, Avg. loss: 3456.259619\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 285\n",
            "Norm: 357729.81, NNZs: 993, Bias: -5222.487354, T: 279300, Avg. loss: 3442.763206\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 286\n",
            "Norm: 357729.71, NNZs: 993, Bias: -5222.499044, T: 280280, Avg. loss: 3465.909898\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 287\n",
            "Norm: 357729.62, NNZs: 993, Bias: -5222.508247, T: 281260, Avg. loss: 3450.999578\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 288\n",
            "Norm: 357729.53, NNZs: 993, Bias: -5222.512870, T: 282240, Avg. loss: 3422.414965\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 289\n",
            "Norm: 357729.45, NNZs: 993, Bias: -5222.518085, T: 283220, Avg. loss: 3417.573545\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 290\n",
            "Norm: 357729.36, NNZs: 993, Bias: -5222.526188, T: 284200, Avg. loss: 3420.817891\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 291\n",
            "Norm: 357729.27, NNZs: 993, Bias: -5222.533230, T: 285180, Avg. loss: 3413.937368\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 292\n",
            "Norm: 357729.19, NNZs: 993, Bias: -5222.541105, T: 286160, Avg. loss: 3409.063522\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 293\n",
            "Norm: 357729.10, NNZs: 993, Bias: -5222.547100, T: 287140, Avg. loss: 3395.199161\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 294\n",
            "Norm: 357729.01, NNZs: 993, Bias: -5222.557494, T: 288120, Avg. loss: 3416.342060\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 295\n",
            "Norm: 357728.92, NNZs: 993, Bias: -5222.568155, T: 289100, Avg. loss: 3410.778624\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 296\n",
            "Norm: 357728.83, NNZs: 993, Bias: -5222.576636, T: 290080, Avg. loss: 3396.744931\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 297\n",
            "Norm: 357728.75, NNZs: 993, Bias: -5222.583466, T: 291060, Avg. loss: 3378.232866\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 298\n",
            "Norm: 357728.67, NNZs: 993, Bias: -5222.588429, T: 292040, Avg. loss: 3365.277559\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 299\n",
            "Norm: 357728.58, NNZs: 993, Bias: -5222.596155, T: 293020, Avg. loss: 3368.040460\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 300\n",
            "Norm: 357728.49, NNZs: 993, Bias: -5222.606418, T: 294000, Avg. loss: 3380.770154\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 301\n",
            "Norm: 357728.41, NNZs: 993, Bias: -5222.614844, T: 294980, Avg. loss: 3367.427265\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 302\n",
            "Norm: 357728.33, NNZs: 993, Bias: -5222.620212, T: 295960, Avg. loss: 3343.052887\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 303\n",
            "Norm: 357728.25, NNZs: 993, Bias: -5222.627176, T: 296940, Avg. loss: 3344.569979\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 304\n",
            "Norm: 357728.16, NNZs: 993, Bias: -5222.637482, T: 297920, Avg. loss: 3358.630960\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 305\n",
            "Norm: 357728.08, NNZs: 993, Bias: -5222.642418, T: 298900, Avg. loss: 3326.244579\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 306\n",
            "Norm: 357728.00, NNZs: 993, Bias: -5222.647854, T: 299880, Avg. loss: 3320.621114\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 307\n",
            "Norm: 357727.92, NNZs: 993, Bias: -5222.655442, T: 300860, Avg. loss: 3323.572478\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 308\n",
            "Norm: 357727.84, NNZs: 993, Bias: -5222.663697, T: 301840, Avg. loss: 3328.569810\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 309\n",
            "Norm: 357727.76, NNZs: 993, Bias: -5222.672095, T: 302820, Avg. loss: 3322.923571\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 310\n",
            "Norm: 357727.68, NNZs: 993, Bias: -5222.678777, T: 303800, Avg. loss: 3305.470650\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 311\n",
            "Norm: 357727.59, NNZs: 993, Bias: -5222.687698, T: 304780, Avg. loss: 3313.322406\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 312\n",
            "Norm: 357727.52, NNZs: 993, Bias: -5222.692556, T: 305760, Avg. loss: 3288.455054\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 313\n",
            "Norm: 357727.44, NNZs: 993, Bias: -5222.702816, T: 306740, Avg. loss: 3309.256549\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 314\n",
            "Norm: 357727.36, NNZs: 993, Bias: -5222.710258, T: 307720, Avg. loss: 3294.454918\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 315\n",
            "Norm: 357727.27, NNZs: 993, Bias: -5222.719764, T: 308700, Avg. loss: 3297.096508\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 316\n",
            "Norm: 357727.20, NNZs: 993, Bias: -5222.727392, T: 309680, Avg. loss: 3284.335682\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 317\n",
            "Norm: 357727.12, NNZs: 993, Bias: -5222.732237, T: 310660, Avg. loss: 3260.599273\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 318\n",
            "Norm: 357727.04, NNZs: 993, Bias: -5222.738441, T: 311640, Avg. loss: 3263.281932\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 319\n",
            "Norm: 357726.97, NNZs: 993, Bias: -5222.746163, T: 312620, Avg. loss: 3268.843250\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 320\n",
            "Norm: 357726.89, NNZs: 993, Bias: -5222.752444, T: 313600, Avg. loss: 3252.308790\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 321\n",
            "Norm: 357726.81, NNZs: 993, Bias: -5222.760041, T: 314580, Avg. loss: 3258.289722\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 322\n",
            "Norm: 357726.74, NNZs: 993, Bias: -5222.765120, T: 315560, Avg. loss: 3236.111604\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 323\n",
            "Norm: 357726.66, NNZs: 993, Bias: -5222.772767, T: 316540, Avg. loss: 3248.051530\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 324\n",
            "Norm: 357726.59, NNZs: 993, Bias: -5222.779656, T: 317520, Avg. loss: 3242.482175\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 325\n",
            "Norm: 357726.51, NNZs: 993, Bias: -5222.787216, T: 318500, Avg. loss: 3237.899136\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 326\n",
            "Norm: 357726.44, NNZs: 993, Bias: -5222.794276, T: 319480, Avg. loss: 3231.353552\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 327\n",
            "Norm: 357726.36, NNZs: 993, Bias: -5222.802146, T: 320460, Avg. loss: 3228.180860\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 328\n",
            "Norm: 357726.29, NNZs: 993, Bias: -5222.809367, T: 321440, Avg. loss: 3221.812977\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 329\n",
            "Norm: 357726.22, NNZs: 993, Bias: -5222.813474, T: 322420, Avg. loss: 3199.392772\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 330\n",
            "Norm: 357726.14, NNZs: 993, Bias: -5222.822112, T: 323400, Avg. loss: 3218.368463\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 331\n",
            "Norm: 357726.07, NNZs: 993, Bias: -5222.828069, T: 324380, Avg. loss: 3196.480776\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 332\n",
            "Norm: 357726.00, NNZs: 993, Bias: -5222.832913, T: 325360, Avg. loss: 3186.224566\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 333\n",
            "Norm: 357725.93, NNZs: 993, Bias: -5222.839387, T: 326340, Avg. loss: 3187.918730\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 334\n",
            "Norm: 357725.86, NNZs: 993, Bias: -5222.843741, T: 327320, Avg. loss: 3175.877106\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 335\n",
            "Norm: 357725.79, NNZs: 993, Bias: -5222.850894, T: 328300, Avg. loss: 3188.087100\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 336\n",
            "Norm: 357725.72, NNZs: 993, Bias: -5222.855404, T: 329280, Avg. loss: 3166.463824\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 337\n",
            "Norm: 357725.65, NNZs: 993, Bias: -5222.862747, T: 330260, Avg. loss: 3179.010203\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 338\n",
            "Norm: 357725.57, NNZs: 993, Bias: -5222.871550, T: 331240, Avg. loss: 3180.213455\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 339\n",
            "Norm: 357725.50, NNZs: 993, Bias: -5222.879982, T: 332220, Avg. loss: 3174.550922\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 340\n",
            "Norm: 357725.43, NNZs: 993, Bias: -5222.885655, T: 333200, Avg. loss: 3153.414719\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 341\n",
            "Norm: 357725.36, NNZs: 993, Bias: -5222.893775, T: 334180, Avg. loss: 3164.301447\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 342\n",
            "Norm: 357725.29, NNZs: 993, Bias: -5222.901642, T: 335160, Avg. loss: 3159.337975\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 343\n",
            "Norm: 357725.22, NNZs: 993, Bias: -5222.906384, T: 336140, Avg. loss: 3133.196357\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 344\n",
            "Norm: 357725.15, NNZs: 993, Bias: -5222.912349, T: 337120, Avg. loss: 3135.314019\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 345\n",
            "Norm: 357725.08, NNZs: 993, Bias: -5222.919495, T: 338100, Avg. loss: 3139.318474\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 346\n",
            "Norm: 357725.01, NNZs: 993, Bias: -5222.928122, T: 339080, Avg. loss: 3141.162844\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 347\n",
            "Norm: 357724.94, NNZs: 993, Bias: -5222.933642, T: 340060, Avg. loss: 3121.145104\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 348\n",
            "Norm: 357724.88, NNZs: 993, Bias: -5222.939785, T: 341040, Avg. loss: 3123.550481\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 349\n",
            "Norm: 357724.81, NNZs: 993, Bias: -5222.945671, T: 342020, Avg. loss: 3112.259497\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 350\n",
            "Norm: 357724.75, NNZs: 993, Bias: -5222.950057, T: 343000, Avg. loss: 3101.882093\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 351\n",
            "Norm: 357724.68, NNZs: 993, Bias: -5222.955837, T: 343980, Avg. loss: 3103.551193\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 352\n",
            "Norm: 357724.61, NNZs: 993, Bias: -5222.962614, T: 344960, Avg. loss: 3106.515347\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 353\n",
            "Norm: 357724.55, NNZs: 993, Bias: -5222.967249, T: 345940, Avg. loss: 3089.132172\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 354\n",
            "Norm: 357724.48, NNZs: 993, Bias: -5222.975400, T: 346920, Avg. loss: 3103.097055\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 355\n",
            "Norm: 357724.41, NNZs: 993, Bias: -5222.981261, T: 347900, Avg. loss: 3085.997970\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 356\n",
            "Norm: 357724.35, NNZs: 993, Bias: -5222.989028, T: 348880, Avg. loss: 3094.019490\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 357\n",
            "Norm: 357724.28, NNZs: 993, Bias: -5222.996321, T: 349860, Avg. loss: 3088.731249\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 358\n",
            "Norm: 357724.21, NNZs: 993, Bias: -5223.002466, T: 350840, Avg. loss: 3078.061747\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 359\n",
            "Norm: 357724.15, NNZs: 993, Bias: -5223.008886, T: 351820, Avg. loss: 3073.751623\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 360\n",
            "Norm: 357724.08, NNZs: 993, Bias: -5223.014912, T: 352800, Avg. loss: 3069.001870\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 361\n",
            "Norm: 357724.02, NNZs: 993, Bias: -5223.018736, T: 353780, Avg. loss: 3052.598847\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 362\n",
            "Norm: 357723.96, NNZs: 992, Bias: -5223.026140, T: 354760, Avg. loss: 3065.866214\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 363\n",
            "Norm: 357723.89, NNZs: 992, Bias: -5223.032487, T: 355740, Avg. loss: 3056.204067\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 364\n",
            "Norm: 357723.83, NNZs: 992, Bias: -5223.038642, T: 356720, Avg. loss: 3051.554798\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 365\n",
            "Norm: 357723.77, NNZs: 992, Bias: -5223.042584, T: 357700, Avg. loss: 3035.962649\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 366\n",
            "Norm: 357723.71, NNZs: 992, Bias: -5223.047883, T: 358680, Avg. loss: 3037.598575\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 367\n",
            "Norm: 357723.65, NNZs: 992, Bias: -5223.053039, T: 359660, Avg. loss: 3033.452510\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 368\n",
            "Norm: 357723.58, NNZs: 992, Bias: -5223.058917, T: 360640, Avg. loss: 3034.093405\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 369\n",
            "Norm: 357723.52, NNZs: 992, Bias: -5223.063769, T: 361620, Avg. loss: 3023.817742\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 370\n",
            "Norm: 357723.46, NNZs: 992, Bias: -5223.067770, T: 362600, Avg. loss: 3014.740943\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 371\n",
            "Norm: 357723.40, NNZs: 992, Bias: -5223.072923, T: 363580, Avg. loss: 3016.456474\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 372\n",
            "Norm: 357723.34, NNZs: 992, Bias: -5223.080142, T: 364560, Avg. loss: 3023.890026\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 373\n",
            "Norm: 357723.28, NNZs: 992, Bias: -5223.084326, T: 365540, Avg. loss: 3002.477134\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 374\n",
            "Norm: 357723.22, NNZs: 992, Bias: -5223.088361, T: 366520, Avg. loss: 2998.110695\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 375\n",
            "Norm: 357723.16, NNZs: 992, Bias: -5223.094674, T: 367500, Avg. loss: 3006.087157\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 376\n",
            "Norm: 357723.10, NNZs: 992, Bias: -5223.100288, T: 368480, Avg. loss: 3000.837375\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 377\n",
            "Norm: 357723.04, NNZs: 992, Bias: -5223.104013, T: 369460, Avg. loss: 2985.725438\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 378\n",
            "Norm: 357722.98, NNZs: 992, Bias: -5223.111202, T: 370440, Avg. loss: 2998.892752\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 379\n",
            "Norm: 357722.92, NNZs: 992, Bias: -5223.117411, T: 371420, Avg. loss: 2989.450689\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 380\n",
            "Norm: 357722.86, NNZs: 992, Bias: -5223.124308, T: 372400, Avg. loss: 2990.402033\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 381\n",
            "Norm: 357722.80, NNZs: 992, Bias: -5223.130876, T: 373380, Avg. loss: 2986.130177\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 382\n",
            "Norm: 357722.74, NNZs: 992, Bias: -5223.134846, T: 374360, Avg. loss: 2965.150183\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 383\n",
            "Norm: 357722.69, NNZs: 992, Bias: -5223.138665, T: 375340, Avg. loss: 2961.675912\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 384\n",
            "Norm: 357722.63, NNZs: 992, Bias: -5223.142853, T: 376320, Avg. loss: 2958.206148\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 385\n",
            "Norm: 357722.57, NNZs: 992, Bias: -5223.147133, T: 377300, Avg. loss: 2954.526047\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 386\n",
            "Norm: 357722.51, NNZs: 992, Bias: -5223.153973, T: 378280, Avg. loss: 2966.530996\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 387\n",
            "Norm: 357722.46, NNZs: 992, Bias: -5223.158024, T: 379260, Avg. loss: 2945.890087\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 388\n",
            "Norm: 357722.40, NNZs: 992, Bias: -5223.162065, T: 380240, Avg. loss: 2942.687763\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 389\n",
            "Norm: 357722.34, NNZs: 992, Bias: -5223.167543, T: 381220, Avg. loss: 2949.437075\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 390\n",
            "Norm: 357722.29, NNZs: 992, Bias: -5223.173185, T: 382200, Avg. loss: 2945.731985\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 391\n",
            "Norm: 357722.23, NNZs: 992, Bias: -5223.178855, T: 383180, Avg. loss: 2942.478993\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 392\n",
            "Norm: 357722.17, NNZs: 992, Bias: -5223.183781, T: 384160, Avg. loss: 2932.562779\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 393\n",
            "Norm: 357722.12, NNZs: 992, Bias: -5223.189388, T: 385140, Avg. loss: 2934.262361\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 394\n",
            "Norm: 357722.06, NNZs: 992, Bias: -5223.194969, T: 386120, Avg. loss: 2930.584717\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 395\n",
            "Norm: 357722.00, NNZs: 992, Bias: -5223.201565, T: 387100, Avg. loss: 2931.814342\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 396\n",
            "Norm: 357721.95, NNZs: 992, Bias: -5223.207175, T: 388080, Avg. loss: 2922.902588\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 397\n",
            "Norm: 357721.89, NNZs: 992, Bias: -5223.212549, T: 389060, Avg. loss: 2918.453549\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 398\n",
            "Norm: 357721.83, NNZs: 992, Bias: -5223.219085, T: 390040, Avg. loss: 2920.178843\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 399\n",
            "Norm: 357721.78, NNZs: 992, Bias: -5223.222928, T: 391020, Avg. loss: 2900.531604\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 400\n",
            "Norm: 357721.72, NNZs: 992, Bias: -5223.229110, T: 392000, Avg. loss: 2912.059699\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 401\n",
            "Norm: 357721.67, NNZs: 992, Bias: -5223.233552, T: 392980, Avg. loss: 2897.330839\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 402\n",
            "Norm: 357721.62, NNZs: 992, Bias: -5223.237138, T: 393960, Avg. loss: 2889.024395\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 403\n",
            "Norm: 357721.56, NNZs: 992, Bias: -5223.242599, T: 394940, Avg. loss: 2896.517298\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 404\n",
            "Norm: 357721.51, NNZs: 992, Bias: -5223.248629, T: 395920, Avg. loss: 2897.574592\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 405\n",
            "Norm: 357721.45, NNZs: 992, Bias: -5223.254895, T: 396900, Avg. loss: 2894.223600\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 406\n",
            "Norm: 357721.40, NNZs: 992, Bias: -5223.258715, T: 397880, Avg. loss: 2875.185509\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 407\n",
            "Norm: 357721.34, NNZs: 992, Bias: -5223.264694, T: 398860, Avg. loss: 2886.539085\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 408\n",
            "Norm: 357721.29, NNZs: 992, Bias: -5223.270718, T: 399840, Avg. loss: 2882.868611\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 409\n",
            "Norm: 357721.24, NNZs: 992, Bias: -5223.275562, T: 400820, Avg. loss: 2873.517389\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 410\n",
            "Norm: 357721.18, NNZs: 992, Bias: -5223.281486, T: 401800, Avg. loss: 2875.326481\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 411\n",
            "Norm: 357721.13, NNZs: 992, Bias: -5223.286001, T: 402780, Avg. loss: 2861.794877\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 412\n",
            "Norm: 357721.08, NNZs: 992, Bias: -5223.291106, T: 403760, Avg. loss: 2863.385502\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 413\n",
            "Norm: 357721.03, NNZs: 992, Bias: -5223.294229, T: 404740, Avg. loss: 2848.942206\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 414\n",
            "Norm: 357720.98, NNZs: 992, Bias: -5223.298698, T: 405720, Avg. loss: 2851.043342\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 415\n",
            "Norm: 357720.92, NNZs: 992, Bias: -5223.302437, T: 406700, Avg. loss: 2842.960108\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 416\n",
            "Norm: 357720.87, NNZs: 992, Bias: -5223.306903, T: 407680, Avg. loss: 2849.036062\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 417\n",
            "Norm: 357720.82, NNZs: 992, Bias: -5223.311786, T: 408660, Avg. loss: 2845.810032\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 418\n",
            "Norm: 357720.77, NNZs: 992, Bias: -5223.316679, T: 409640, Avg. loss: 2842.455015\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 419\n",
            "Norm: 357720.72, NNZs: 992, Bias: -5223.320651, T: 410620, Avg. loss: 2833.028857\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 420\n",
            "Norm: 357720.67, NNZs: 992, Bias: -5223.324441, T: 411600, Avg. loss: 2826.340157\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 421\n",
            "Norm: 357720.62, NNZs: 992, Bias: -5223.327671, T: 412580, Avg. loss: 2822.010043\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 422\n",
            "Norm: 357720.57, NNZs: 992, Bias: -5223.332208, T: 413560, Avg. loss: 2824.015581\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 423\n",
            "Norm: 357720.52, NNZs: 992, Bias: -5223.337019, T: 414540, Avg. loss: 2825.495328\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 424\n",
            "Norm: 357720.47, NNZs: 992, Bias: -5223.341256, T: 415520, Avg. loss: 2816.975883\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 425\n",
            "Norm: 357720.42, NNZs: 992, Bias: -5223.344489, T: 416500, Avg. loss: 2808.916600\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 426\n",
            "Norm: 357720.37, NNZs: 992, Bias: -5223.348893, T: 417480, Avg. loss: 2810.663542\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 427\n",
            "Norm: 357720.32, NNZs: 992, Bias: -5223.353636, T: 418460, Avg. loss: 2812.136497\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 428\n",
            "Norm: 357720.28, NNZs: 992, Bias: -5223.357001, T: 419440, Avg. loss: 2799.053518\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 429\n",
            "Norm: 357720.23, NNZs: 992, Bias: -5223.360424, T: 420420, Avg. loss: 2796.288787\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 430\n",
            "Norm: 357720.18, NNZs: 992, Bias: -5223.366182, T: 421400, Avg. loss: 2807.000494\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 431\n",
            "Norm: 357720.13, NNZs: 992, Bias: -5223.370444, T: 422380, Avg. loss: 2793.764558\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 432\n",
            "Norm: 357720.08, NNZs: 992, Bias: -5223.374731, T: 423360, Avg. loss: 2790.879216\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 433\n",
            "Norm: 357720.03, NNZs: 992, Bias: -5223.380357, T: 424340, Avg. loss: 2797.213367\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 434\n",
            "Norm: 357719.98, NNZs: 992, Bias: -5223.383538, T: 425320, Avg. loss: 2779.418617\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 435\n",
            "Norm: 357719.93, NNZs: 992, Bias: -5223.388326, T: 426300, Avg. loss: 2786.304850\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 436\n",
            "Norm: 357719.88, NNZs: 992, Bias: -5223.393720, T: 427280, Avg. loss: 2787.084410\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 437\n",
            "Norm: 357719.84, NNZs: 992, Bias: -5223.397924, T: 428260, Avg. loss: 2774.582607\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 438\n",
            "Norm: 357719.79, NNZs: 992, Bias: -5223.403529, T: 429240, Avg. loss: 2780.918019\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 439\n",
            "Norm: 357719.74, NNZs: 992, Bias: -5223.407701, T: 430220, Avg. loss: 2768.138763\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 440\n",
            "Norm: 357719.69, NNZs: 992, Bias: -5223.412028, T: 431200, Avg. loss: 2769.426277\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 441\n",
            "Norm: 357719.65, NNZs: 992, Bias: -5223.415356, T: 432180, Avg. loss: 2757.514507\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 442\n",
            "Norm: 357719.60, NNZs: 992, Bias: -5223.418565, T: 433160, Avg. loss: 2754.371238\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 443\n",
            "Norm: 357719.55, NNZs: 992, Bias: -5223.422625, T: 434140, Avg. loss: 2755.511890\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 444\n",
            "Norm: 357719.51, NNZs: 992, Bias: -5223.426967, T: 435120, Avg. loss: 2753.247807\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 445\n",
            "Norm: 357719.46, NNZs: 992, Bias: -5223.430488, T: 436100, Avg. loss: 2745.124421\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 446\n",
            "Norm: 357719.42, NNZs: 992, Bias: -5223.435095, T: 437080, Avg. loss: 2751.247314\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 447\n",
            "Norm: 357719.37, NNZs: 992, Bias: -5223.440537, T: 438060, Avg. loss: 2752.382765\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 448\n",
            "Norm: 357719.32, NNZs: 992, Bias: -5223.444499, T: 439040, Avg. loss: 2739.814615\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 449\n",
            "Norm: 357719.28, NNZs: 992, Bias: -5223.448512, T: 440020, Avg. loss: 2736.920500\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 450\n",
            "Norm: 357719.23, NNZs: 992, Bias: -5223.452797, T: 441000, Avg. loss: 2737.918648\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 451\n",
            "Norm: 357719.18, NNZs: 992, Bias: -5223.456998, T: 441980, Avg. loss: 2731.207565\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 452\n",
            "Norm: 357719.14, NNZs: 992, Bias: -5223.461474, T: 442960, Avg. loss: 2732.139540\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 453\n",
            "Norm: 357719.09, NNZs: 992, Bias: -5223.465654, T: 443940, Avg. loss: 2728.705857\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 454\n",
            "Norm: 357719.05, NNZs: 992, Bias: -5223.468980, T: 444920, Avg. loss: 2717.871367\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 455\n",
            "Norm: 357719.00, NNZs: 992, Bias: -5223.472164, T: 445900, Avg. loss: 2714.996020\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 456\n",
            "Norm: 357718.96, NNZs: 992, Bias: -5223.476621, T: 446880, Avg. loss: 2720.130060\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 457\n",
            "Norm: 357718.91, NNZs: 992, Bias: -5223.480757, T: 447860, Avg. loss: 2713.392232\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 458\n",
            "Norm: 357718.87, NNZs: 992, Bias: -5223.484014, T: 448840, Avg. loss: 2705.685497\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 459\n",
            "Norm: 357718.83, NNZs: 992, Bias: -5223.488319, T: 449820, Avg. loss: 2711.526280\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 460\n",
            "Norm: 357718.78, NNZs: 992, Bias: -5223.492641, T: 450800, Avg. loss: 2708.054895\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 461\n",
            "Norm: 357718.74, NNZs: 992, Bias: -5223.496785, T: 451780, Avg. loss: 2705.182596\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 462\n",
            "Norm: 357718.69, NNZs: 992, Bias: -5223.500602, T: 452760, Avg. loss: 2697.804591\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 463\n",
            "Norm: 357718.65, NNZs: 992, Bias: -5223.505320, T: 453740, Avg. loss: 2703.082384\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 464\n",
            "Norm: 357718.60, NNZs: 992, Bias: -5223.510218, T: 454720, Avg. loss: 2700.350083\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 465\n",
            "Norm: 357718.56, NNZs: 992, Bias: -5223.514083, T: 455700, Avg. loss: 2689.097495\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 466\n",
            "Norm: 357718.52, NNZs: 992, Bias: -5223.517259, T: 456680, Avg. loss: 2682.453182\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 467\n",
            "Norm: 357718.47, NNZs: 992, Bias: -5223.521541, T: 457660, Avg. loss: 2687.539012\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 468\n",
            "Norm: 357718.43, NNZs: 992, Bias: -5223.525392, T: 458640, Avg. loss: 2680.480751\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 469\n",
            "Norm: 357718.39, NNZs: 992, Bias: -5223.529498, T: 459620, Avg. loss: 2682.082115\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 470\n",
            "Norm: 357718.34, NNZs: 992, Bias: -5223.534394, T: 460600, Avg. loss: 2683.472461\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 471\n",
            "Norm: 357718.30, NNZs: 992, Bias: -5223.538597, T: 461580, Avg. loss: 2676.534810\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 472\n",
            "Norm: 357718.26, NNZs: 992, Bias: -5223.541622, T: 462560, Avg. loss: 2664.940258\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 473\n",
            "Norm: 357718.22, NNZs: 992, Bias: -5223.544888, T: 463540, Avg. loss: 2662.256856\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 474\n",
            "Norm: 357718.17, NNZs: 992, Bias: -5223.548956, T: 464520, Avg. loss: 2664.294748\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 475\n",
            "Norm: 357718.13, NNZs: 992, Bias: -5223.552580, T: 465500, Avg. loss: 2660.434413\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 476\n",
            "Norm: 357718.09, NNZs: 992, Bias: -5223.557336, T: 466480, Avg. loss: 2667.000549\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 477\n",
            "Norm: 357718.05, NNZs: 992, Bias: -5223.561334, T: 467460, Avg. loss: 2660.110074\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 478\n",
            "Norm: 357718.00, NNZs: 992, Bias: -5223.566101, T: 468440, Avg. loss: 2661.516625\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 479\n",
            "Norm: 357717.96, NNZs: 992, Bias: -5223.569132, T: 469420, Avg. loss: 2645.062553\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 480\n",
            "Norm: 357717.92, NNZs: 992, Bias: -5223.573121, T: 470400, Avg. loss: 2651.680128\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 481\n",
            "Norm: 357717.88, NNZs: 992, Bias: -5223.576110, T: 471380, Avg. loss: 2639.830837\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 482\n",
            "Norm: 357717.84, NNZs: 992, Bias: -5223.580059, T: 472360, Avg. loss: 2646.570894\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 483\n",
            "Norm: 357717.80, NNZs: 992, Bias: -5223.584126, T: 473340, Avg. loss: 2644.351026\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 484\n",
            "Norm: 357717.75, NNZs: 992, Bias: -5223.588786, T: 474320, Avg. loss: 2645.220850\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 485\n",
            "Norm: 357717.71, NNZs: 992, Bias: -5223.592559, T: 475300, Avg. loss: 2632.806614\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 486\n",
            "Norm: 357717.67, NNZs: 992, Bias: -5223.596757, T: 476280, Avg. loss: 2636.279049\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 487\n",
            "Norm: 357717.63, NNZs: 992, Bias: -5223.600392, T: 477260, Avg. loss: 2627.144848\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 488\n",
            "Norm: 357717.59, NNZs: 993, Bias: -5223.603324, T: 478240, Avg. loss: 2620.846726\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 489\n",
            "Norm: 357717.55, NNZs: 993, Bias: -5223.606252, T: 479220, Avg. loss: 2618.068923\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 490\n",
            "Norm: 357717.51, NNZs: 993, Bias: -5223.610961, T: 480200, Avg. loss: 2629.762127\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 491\n",
            "Norm: 357717.47, NNZs: 993, Bias: -5223.614625, T: 481180, Avg. loss: 2616.585607\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 492\n",
            "Norm: 357717.43, NNZs: 993, Bias: -5223.619383, T: 482160, Avg. loss: 2624.481090\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 493\n",
            "Norm: 357717.39, NNZs: 993, Bias: -5223.623558, T: 483140, Avg. loss: 2617.876582\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 494\n",
            "Norm: 357717.35, NNZs: 993, Bias: -5223.627634, T: 484120, Avg. loss: 2615.334265\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 495\n",
            "Norm: 357717.31, NNZs: 993, Bias: -5223.630422, T: 485100, Avg. loss: 2602.418300\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 496\n",
            "Norm: 357717.27, NNZs: 993, Bias: -5223.634203, T: 486080, Avg. loss: 2604.643352\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 497\n",
            "Norm: 357717.23, NNZs: 993, Bias: -5223.639036, T: 487060, Avg. loss: 2611.369138\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 498\n",
            "Norm: 357717.19, NNZs: 993, Bias: -5223.641917, T: 488040, Avg. loss: 2595.132238\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 499\n",
            "Norm: 357717.15, NNZs: 993, Bias: -5223.645527, T: 489020, Avg. loss: 2596.857699\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 500\n",
            "Norm: 357717.11, NNZs: 993, Bias: -5223.649204, T: 490000, Avg. loss: 2594.614361\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 501\n",
            "Norm: 357717.07, NNZs: 993, Bias: -5223.653074, T: 490980, Avg. loss: 2596.847284\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 502\n",
            "Norm: 357717.03, NNZs: 993, Bias: -5223.657592, T: 491960, Avg. loss: 2598.149444\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 503\n",
            "Norm: 357716.99, NNZs: 993, Bias: -5223.661967, T: 492940, Avg. loss: 2595.097623\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 504\n",
            "Norm: 357716.95, NNZs: 993, Bias: -5223.664933, T: 493920, Avg. loss: 2580.443269\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 505\n",
            "Norm: 357716.91, NNZs: 993, Bias: -5223.667848, T: 494900, Avg. loss: 2577.969097\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 506\n",
            "Norm: 357716.87, NNZs: 993, Bias: -5223.671762, T: 495880, Avg. loss: 2584.161195\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 507\n",
            "Norm: 357716.84, NNZs: 993, Bias: -5223.675094, T: 496860, Avg. loss: 2576.109636\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 508\n",
            "Norm: 357716.80, NNZs: 993, Bias: -5223.679039, T: 497840, Avg. loss: 2579.738157\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 509\n",
            "Norm: 357716.76, NNZs: 993, Bias: -5223.681942, T: 498820, Avg. loss: 2567.742349\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 510\n",
            "Norm: 357716.72, NNZs: 993, Bias: -5223.684967, T: 499800, Avg. loss: 2566.001639\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 511\n",
            "Norm: 357716.68, NNZs: 993, Bias: -5223.688396, T: 500780, Avg. loss: 2566.928760\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 512\n",
            "Norm: 357716.65, NNZs: 993, Bias: -5223.692149, T: 501760, Avg. loss: 2569.147793\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 513\n",
            "Norm: 357716.61, NNZs: 993, Bias: -5223.696778, T: 502740, Avg. loss: 2570.958197\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 514\n",
            "Norm: 357716.57, NNZs: 993, Bias: -5223.700062, T: 503720, Avg. loss: 2559.091849\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 515\n",
            "Norm: 357716.53, NNZs: 993, Bias: -5223.702926, T: 504700, Avg. loss: 2553.535887\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 516\n",
            "Norm: 357716.50, NNZs: 993, Bias: -5223.705764, T: 505680, Avg. loss: 2550.864153\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 517\n",
            "Norm: 357716.46, NNZs: 993, Bias: -5223.708801, T: 506660, Avg. loss: 2549.051380\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 518\n",
            "Norm: 357716.42, NNZs: 993, Bias: -5223.713495, T: 507640, Avg. loss: 2558.878980\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 519\n",
            "Norm: 357716.38, NNZs: 993, Bias: -5223.716307, T: 508620, Avg. loss: 2543.919216\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 520\n",
            "Norm: 357716.35, NNZs: 993, Bias: -5223.720812, T: 509600, Avg. loss: 2553.660232\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 521\n",
            "Norm: 357716.31, NNZs: 993, Bias: -5223.723736, T: 510580, Avg. loss: 2538.876637\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 522\n",
            "Norm: 357716.27, NNZs: 993, Bias: -5223.727573, T: 511560, Avg. loss: 2545.234069\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 523\n",
            "Norm: 357716.24, NNZs: 993, Bias: -5223.730491, T: 512540, Avg. loss: 2534.204004\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 524\n",
            "Norm: 357716.20, NNZs: 993, Bias: -5223.733829, T: 513520, Avg. loss: 2535.108412\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 525\n",
            "Norm: 357716.16, NNZs: 993, Bias: -5223.738205, T: 514500, Avg. loss: 2541.317242\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 526\n",
            "Norm: 357716.13, NNZs: 993, Bias: -5223.740923, T: 515480, Avg. loss: 2526.885253\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 527\n",
            "Norm: 357716.09, NNZs: 993, Bias: -5223.743965, T: 516460, Avg. loss: 2525.249870\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 528\n",
            "Norm: 357716.05, NNZs: 993, Bias: -5223.747233, T: 517440, Avg. loss: 2525.939467\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 529\n",
            "Norm: 357716.02, NNZs: 993, Bias: -5223.750865, T: 518420, Avg. loss: 2524.181805\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 530\n",
            "Norm: 357715.98, NNZs: 993, Bias: -5223.755504, T: 519400, Avg. loss: 2530.135495\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 531\n",
            "Norm: 357715.94, NNZs: 993, Bias: -5223.760043, T: 520380, Avg. loss: 2527.578458\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 532\n",
            "Norm: 357715.91, NNZs: 993, Bias: -5223.762800, T: 521360, Avg. loss: 2513.173299\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 533\n",
            "Norm: 357715.87, NNZs: 993, Bias: -5223.767121, T: 522340, Avg. loss: 2522.360837\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 534\n",
            "Norm: 357715.83, NNZs: 993, Bias: -5223.770703, T: 523320, Avg. loss: 2516.189972\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 535\n",
            "Norm: 357715.80, NNZs: 993, Bias: -5223.774170, T: 524300, Avg. loss: 2509.657790\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 536\n",
            "Norm: 357715.76, NNZs: 993, Bias: -5223.777131, T: 525280, Avg. loss: 2504.027721\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 537\n",
            "Norm: 357715.73, NNZs: 993, Bias: -5223.780157, T: 526260, Avg. loss: 2501.796303\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 538\n",
            "Norm: 357715.69, NNZs: 993, Bias: -5223.784082, T: 527240, Avg. loss: 2508.035972\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 539\n",
            "Norm: 357715.66, NNZs: 993, Bias: -5223.788511, T: 528220, Avg. loss: 2508.594725\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 540\n",
            "Norm: 357715.62, NNZs: 993, Bias: -5223.791387, T: 529200, Avg. loss: 2494.875327\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 541\n",
            "Norm: 357715.58, NNZs: 993, Bias: -5223.795523, T: 530180, Avg. loss: 2503.410755\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 542\n",
            "Norm: 357715.55, NNZs: 993, Bias: -5223.799024, T: 531160, Avg. loss: 2493.654087\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 543\n",
            "Norm: 357715.51, NNZs: 993, Bias: -5223.801975, T: 532140, Avg. loss: 2488.057359\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 544\n",
            "Norm: 357715.48, NNZs: 993, Bias: -5223.805910, T: 533120, Avg. loss: 2493.953273\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 545\n",
            "Norm: 357715.44, NNZs: 993, Bias: -5223.809505, T: 534100, Avg. loss: 2491.042744\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 546\n",
            "Norm: 357715.41, NNZs: 993, Bias: -5223.812820, T: 535080, Avg. loss: 2484.481837\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 547\n",
            "Norm: 357715.37, NNZs: 993, Bias: -5223.816971, T: 536060, Avg. loss: 2490.131577\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 548\n",
            "Norm: 357715.34, NNZs: 993, Bias: -5223.820757, T: 537040, Avg. loss: 2484.326962\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 549\n",
            "Norm: 357715.30, NNZs: 993, Bias: -5223.824935, T: 538020, Avg. loss: 2485.594066\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 550\n",
            "Norm: 357715.27, NNZs: 993, Bias: -5223.828087, T: 539000, Avg. loss: 2474.843974\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 551\n",
            "Norm: 357715.24, NNZs: 993, Bias: -5223.830784, T: 539980, Avg. loss: 2469.765580\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 552\n",
            "Norm: 357715.20, NNZs: 993, Bias: -5223.833536, T: 540960, Avg. loss: 2467.639952\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 553\n",
            "Norm: 357715.17, NNZs: 993, Bias: -5223.837692, T: 541940, Avg. loss: 2476.370897\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 554\n",
            "Norm: 357715.13, NNZs: 993, Bias: -5223.840958, T: 542920, Avg. loss: 2466.566839\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 555\n",
            "Norm: 357715.10, NNZs: 993, Bias: -5223.845047, T: 543900, Avg. loss: 2471.754700\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 556\n",
            "Norm: 357715.06, NNZs: 993, Bias: -5223.848150, T: 544880, Avg. loss: 2461.509260\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 557\n",
            "Norm: 357715.03, NNZs: 993, Bias: -5223.852279, T: 545860, Avg. loss: 2467.664578\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 558\n",
            "Norm: 357715.00, NNZs: 993, Bias: -5223.855082, T: 546840, Avg. loss: 2454.251320\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 559\n",
            "Norm: 357714.96, NNZs: 993, Bias: -5223.858573, T: 547820, Avg. loss: 2459.506990\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 560\n",
            "Norm: 357714.93, NNZs: 993, Bias: -5223.862011, T: 548800, Avg. loss: 2457.288730\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 561\n",
            "Norm: 357714.90, NNZs: 993, Bias: -5223.865547, T: 549780, Avg. loss: 2455.343833\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 562\n",
            "Norm: 357714.86, NNZs: 993, Bias: -5223.868795, T: 550760, Avg. loss: 2449.228196\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 563\n",
            "Norm: 357714.83, NNZs: 993, Bias: -5223.872444, T: 551740, Avg. loss: 2451.254956\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 564\n",
            "Norm: 357714.80, NNZs: 993, Bias: -5223.875870, T: 552720, Avg. loss: 2448.705001\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 565\n",
            "Norm: 357714.76, NNZs: 993, Bias: -5223.879273, T: 553700, Avg. loss: 2446.384806\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 566\n",
            "Norm: 357714.73, NNZs: 993, Bias: -5223.882128, T: 554680, Avg. loss: 2437.236448\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 567\n",
            "Norm: 357714.70, NNZs: 993, Bias: -5223.885329, T: 555660, Avg. loss: 2438.074310\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 568\n",
            "Norm: 357714.66, NNZs: 993, Bias: -5223.888725, T: 556640, Avg. loss: 2440.219982\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 569\n",
            "Norm: 357714.63, NNZs: 993, Bias: -5223.892265, T: 557620, Avg. loss: 2438.399997\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 570\n",
            "Norm: 357714.60, NNZs: 993, Bias: -5223.896293, T: 558600, Avg. loss: 2439.128697\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 571\n",
            "Norm: 357714.56, NNZs: 993, Bias: -5223.899946, T: 559580, Avg. loss: 2434.270980\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 572\n",
            "Norm: 357714.53, NNZs: 993, Bias: -5223.903392, T: 560560, Avg. loss: 2431.569196\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 573\n",
            "Norm: 357714.50, NNZs: 993, Bias: -5223.906378, T: 561540, Avg. loss: 2425.007508\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 574\n",
            "Norm: 357714.47, NNZs: 993, Bias: -5223.909750, T: 562520, Avg. loss: 2427.255905\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 575\n",
            "Norm: 357714.43, NNZs: 993, Bias: -5223.913698, T: 563500, Avg. loss: 2428.405979\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 576\n",
            "Norm: 357714.40, NNZs: 993, Bias: -5223.916370, T: 564480, Avg. loss: 2415.682606\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 577\n",
            "Norm: 357714.37, NNZs: 993, Bias: -5223.919431, T: 565460, Avg. loss: 2416.473724\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 578\n",
            "Norm: 357714.34, NNZs: 993, Bias: -5223.923383, T: 566440, Avg. loss: 2421.988470\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 579\n",
            "Norm: 357714.30, NNZs: 993, Bias: -5223.926410, T: 567420, Avg. loss: 2412.467040\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 580\n",
            "Norm: 357714.27, NNZs: 993, Bias: -5223.930305, T: 568400, Avg. loss: 2417.639639\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 581\n",
            "Norm: 357714.24, NNZs: 993, Bias: -5223.933773, T: 569380, Avg. loss: 2412.883734\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 582\n",
            "Norm: 357714.21, NNZs: 993, Bias: -5223.937290, T: 570360, Avg. loss: 2410.956080\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 583\n",
            "Norm: 357714.18, NNZs: 993, Bias: -5223.940835, T: 571340, Avg. loss: 2408.659936\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 584\n",
            "Norm: 357714.14, NNZs: 993, Bias: -5223.943455, T: 572320, Avg. loss: 2399.294909\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 585\n",
            "Norm: 357714.11, NNZs: 993, Bias: -5223.947325, T: 573300, Avg. loss: 2407.518370\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 586\n",
            "Norm: 357714.08, NNZs: 993, Bias: -5223.949833, T: 574280, Avg. loss: 2395.227901\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 587\n",
            "Norm: 357714.05, NNZs: 993, Bias: -5223.952416, T: 575260, Avg. loss: 2393.216928\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 588\n",
            "Norm: 357714.02, NNZs: 993, Bias: -5223.956309, T: 576240, Avg. loss: 2401.611640\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 589\n",
            "Norm: 357713.99, NNZs: 993, Bias: -5223.959786, T: 577220, Avg. loss: 2396.633996\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 590\n",
            "Norm: 357713.96, NNZs: 993, Bias: -5223.963075, T: 578200, Avg. loss: 2394.227406\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 591\n",
            "Norm: 357713.93, NNZs: 993, Bias: -5223.966141, T: 579180, Avg. loss: 2388.266115\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 592\n",
            "Norm: 357713.89, NNZs: 993, Bias: -5223.970030, T: 580160, Avg. loss: 2393.352774\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 593\n",
            "Norm: 357713.86, NNZs: 993, Bias: -5223.972620, T: 581140, Avg. loss: 2381.046860\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 594\n",
            "Norm: 357713.83, NNZs: 993, Bias: -5223.975661, T: 582120, Avg. loss: 2382.078197\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 595\n",
            "Norm: 357713.80, NNZs: 993, Bias: -5223.979467, T: 583100, Avg. loss: 2387.307896\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 596\n",
            "Norm: 357713.77, NNZs: 993, Bias: -5223.982632, T: 584080, Avg. loss: 2381.760833\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 597\n",
            "Norm: 357713.74, NNZs: 993, Bias: -5223.985843, T: 585060, Avg. loss: 2380.001527\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 598\n",
            "Norm: 357713.71, NNZs: 993, Bias: -5223.988920, T: 586040, Avg. loss: 2374.183167\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 599\n",
            "Norm: 357713.68, NNZs: 993, Bias: -5223.992100, T: 587020, Avg. loss: 2375.793622\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 600\n",
            "Norm: 357713.65, NNZs: 993, Bias: -5223.995750, T: 588000, Avg. loss: 2377.055092\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 601\n",
            "Norm: 357713.62, NNZs: 993, Bias: -5223.998248, T: 588980, Avg. loss: 2364.921196\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 602\n",
            "Norm: 357713.59, NNZs: 993, Bias: -5224.001550, T: 589960, Avg. loss: 2370.004382\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 603\n",
            "Norm: 357713.56, NNZs: 993, Bias: -5224.004869, T: 590940, Avg. loss: 2368.436265\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 604\n",
            "Norm: 357713.53, NNZs: 993, Bias: -5224.007388, T: 591920, Avg. loss: 2359.264879\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 605\n",
            "Norm: 357713.50, NNZs: 993, Bias: -5224.011016, T: 592900, Avg. loss: 2367.173431\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 606\n",
            "Norm: 357713.47, NNZs: 993, Bias: -5224.013850, T: 593880, Avg. loss: 2358.225328\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 607\n",
            "Norm: 357713.44, NNZs: 993, Bias: -5224.016314, T: 594860, Avg. loss: 2353.581242\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 608\n",
            "Norm: 357713.41, NNZs: 993, Bias: -5224.019617, T: 595840, Avg. loss: 2358.652878\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 609\n",
            "Norm: 357713.38, NNZs: 993, Bias: -5224.022818, T: 596820, Avg. loss: 2356.619661\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 610\n",
            "Norm: 357713.35, NNZs: 993, Bias: -5224.025751, T: 597800, Avg. loss: 2350.687135\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 611\n",
            "Norm: 357713.32, NNZs: 993, Bias: -5224.028184, T: 598780, Avg. loss: 2345.943348\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 612\n",
            "Norm: 357713.29, NNZs: 993, Bias: -5224.031882, T: 599760, Avg. loss: 2353.902523\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 613\n",
            "Norm: 357713.26, NNZs: 993, Bias: -5224.034273, T: 600740, Avg. loss: 2341.934580\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 614\n",
            "Norm: 357713.23, NNZs: 993, Bias: -5224.037984, T: 601720, Avg. loss: 2350.057662\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 615\n",
            "Norm: 357713.20, NNZs: 993, Bias: -5224.041571, T: 602700, Avg. loss: 2347.878461\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 616\n",
            "Norm: 357713.17, NNZs: 993, Bias: -5224.043985, T: 603680, Avg. loss: 2336.501162\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 617\n",
            "Norm: 357713.14, NNZs: 993, Bias: -5224.046547, T: 604660, Avg. loss: 2334.713536\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 618\n",
            "Norm: 357713.11, NNZs: 993, Bias: -5224.049353, T: 605640, Avg. loss: 2335.217632\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 619\n",
            "Norm: 357713.08, NNZs: 993, Bias: -5224.052965, T: 606620, Avg. loss: 2340.334711\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 620\n",
            "Norm: 357713.05, NNZs: 993, Bias: -5224.056452, T: 607600, Avg. loss: 2338.166798\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 621\n",
            "Norm: 357713.03, NNZs: 993, Bias: -5224.058981, T: 608580, Avg. loss: 2327.181706\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 622\n",
            "Norm: 357713.00, NNZs: 993, Bias: -5224.061907, T: 609560, Avg. loss: 2328.230219\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 623\n",
            "Norm: 357712.97, NNZs: 993, Bias: -5224.065208, T: 610540, Avg. loss: 2330.499781\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 624\n",
            "Norm: 357712.94, NNZs: 993, Bias: -5224.068255, T: 611520, Avg. loss: 2327.987859\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 625\n",
            "Norm: 357712.91, NNZs: 993, Bias: -5224.071751, T: 612500, Avg. loss: 2328.970947\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 626\n",
            "Norm: 357712.88, NNZs: 993, Bias: -5224.074129, T: 613480, Avg. loss: 2317.619870\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 627\n",
            "Norm: 357712.85, NNZs: 993, Bias: -5224.076928, T: 614460, Avg. loss: 2318.609850\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 628\n",
            "Norm: 357712.83, NNZs: 993, Bias: -5224.079279, T: 615440, Avg. loss: 2314.173175\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 629\n",
            "Norm: 357712.80, NNZs: 993, Bias: -5224.081725, T: 616420, Avg. loss: 2312.528883\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 630\n",
            "Norm: 357712.77, NNZs: 993, Bias: -5224.085143, T: 617400, Avg. loss: 2319.644966\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 631\n",
            "Norm: 357712.74, NNZs: 993, Bias: -5224.088120, T: 618380, Avg. loss: 2311.989534\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 632\n",
            "Norm: 357712.71, NNZs: 993, Bias: -5224.091634, T: 619360, Avg. loss: 2315.985013\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 633\n",
            "Norm: 357712.68, NNZs: 993, Bias: -5224.094520, T: 620340, Avg. loss: 2308.223993\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 634\n",
            "Norm: 357712.66, NNZs: 993, Bias: -5224.096836, T: 621320, Avg. loss: 2303.143373\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 635\n",
            "Norm: 357712.63, NNZs: 993, Bias: -5224.099912, T: 622300, Avg. loss: 2308.121781\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 636\n",
            "Norm: 357712.60, NNZs: 993, Bias: -5224.102353, T: 623280, Avg. loss: 2300.024643\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 637\n",
            "Norm: 357712.57, NNZs: 993, Bias: -5224.105402, T: 624260, Avg. loss: 2304.410674\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 638\n",
            "Norm: 357712.55, NNZs: 993, Bias: -5224.108487, T: 625240, Avg. loss: 2302.709279\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 639\n",
            "Norm: 357712.52, NNZs: 993, Bias: -5224.111978, T: 626220, Avg. loss: 2303.457752\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 640\n",
            "Norm: 357712.49, NNZs: 993, Bias: -5224.115068, T: 627200, Avg. loss: 2299.198204\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 641\n",
            "Norm: 357712.46, NNZs: 993, Bias: -5224.118517, T: 628180, Avg. loss: 2299.725588\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 642\n",
            "Norm: 357712.43, NNZs: 993, Bias: -5224.121910, T: 629160, Avg. loss: 2297.937007\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 643\n",
            "Norm: 357712.40, NNZs: 993, Bias: -5224.124855, T: 630140, Avg. loss: 2293.289699\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 644\n",
            "Norm: 357712.38, NNZs: 993, Bias: -5224.128075, T: 631120, Avg. loss: 2293.709148\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 645\n",
            "Norm: 357712.35, NNZs: 993, Bias: -5224.130406, T: 632100, Avg. loss: 2283.290590\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 646\n",
            "Norm: 357712.32, NNZs: 993, Bias: -5224.132745, T: 633080, Avg. loss: 2281.821067\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 647\n",
            "Norm: 357712.30, NNZs: 993, Bias: -5224.135547, T: 634060, Avg. loss: 2282.775540\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 648\n",
            "Norm: 357712.27, NNZs: 993, Bias: -5224.138247, T: 635040, Avg. loss: 2280.771010\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 649\n",
            "Norm: 357712.24, NNZs: 993, Bias: -5224.141596, T: 636020, Avg. loss: 2285.611689\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 650\n",
            "Norm: 357712.22, NNZs: 993, Bias: -5224.143931, T: 637000, Avg. loss: 2274.797416\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 651\n",
            "Norm: 357712.19, NNZs: 993, Bias: -5224.146424, T: 637980, Avg. loss: 2273.609016\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 652\n",
            "Norm: 357712.16, NNZs: 993, Bias: -5224.149467, T: 638960, Avg. loss: 2277.771148\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 653\n",
            "Norm: 357712.13, NNZs: 993, Bias: -5224.152342, T: 639940, Avg. loss: 2275.503244\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 654\n",
            "Norm: 357712.11, NNZs: 993, Bias: -5224.155595, T: 640920, Avg. loss: 2276.430493\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 655\n",
            "Norm: 357712.08, NNZs: 993, Bias: -5224.158033, T: 641900, Avg. loss: 2266.341903\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 656\n",
            "Norm: 357712.05, NNZs: 993, Bias: -5224.160918, T: 642880, Avg. loss: 2270.333731\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 657\n",
            "Norm: 357712.03, NNZs: 993, Bias: -5224.164135, T: 643860, Avg. loss: 2271.207575\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 658\n",
            "Norm: 357712.00, NNZs: 993, Bias: -5224.166446, T: 644840, Avg. loss: 2261.090919\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 659\n",
            "Norm: 357711.97, NNZs: 993, Bias: -5224.169079, T: 645820, Avg. loss: 2262.030540\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 660\n",
            "Norm: 357711.95, NNZs: 993, Bias: -5224.171968, T: 646800, Avg. loss: 2263.711339\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 661\n",
            "Norm: 357711.92, NNZs: 993, Bias: -5224.174885, T: 647780, Avg. loss: 2262.053973\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 662\n",
            "Norm: 357711.89, NNZs: 993, Bias: -5224.178214, T: 648760, Avg. loss: 2262.970580\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 663\n",
            "Norm: 357711.87, NNZs: 993, Bias: -5224.180491, T: 649740, Avg. loss: 2252.355626\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 664\n",
            "Norm: 357711.84, NNZs: 993, Bias: -5224.182779, T: 650720, Avg. loss: 2250.955431\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 665\n",
            "Norm: 357711.82, NNZs: 993, Bias: -5224.185380, T: 651700, Avg. loss: 2251.528201\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 666\n",
            "Norm: 357711.79, NNZs: 993, Bias: -5224.188271, T: 652680, Avg. loss: 2253.765148\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 667\n",
            "Norm: 357711.77, NNZs: 993, Bias: -5224.190605, T: 653660, Avg. loss: 2245.943274\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 668\n",
            "Norm: 357711.74, NNZs: 993, Bias: -5224.193334, T: 654640, Avg. loss: 2247.098025\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 669\n",
            "Norm: 357711.71, NNZs: 993, Bias: -5224.196187, T: 655620, Avg. loss: 2248.483827\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 670\n",
            "Norm: 357711.69, NNZs: 993, Bias: -5224.198598, T: 656600, Avg. loss: 2241.364212\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 671\n",
            "Norm: 357711.66, NNZs: 993, Bias: -5224.201835, T: 657580, Avg. loss: 2247.832253\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 672\n",
            "Norm: 357711.64, NNZs: 993, Bias: -5224.204095, T: 658560, Avg. loss: 2237.606429\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 673\n",
            "Norm: 357711.61, NNZs: 993, Bias: -5224.206373, T: 659540, Avg. loss: 2236.096340\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 674\n",
            "Norm: 357711.58, NNZs: 993, Bias: -5224.209607, T: 660520, Avg. loss: 2242.701493\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 675\n",
            "Norm: 357711.56, NNZs: 993, Bias: -5224.212264, T: 661500, Avg. loss: 2235.281558\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 676\n",
            "Norm: 357711.53, NNZs: 993, Bias: -5224.214760, T: 662480, Avg. loss: 2233.273128\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 677\n",
            "Norm: 357711.51, NNZs: 993, Bias: -5224.217717, T: 663460, Avg. loss: 2235.495647\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 678\n",
            "Norm: 357711.48, NNZs: 993, Bias: -5224.220411, T: 664440, Avg. loss: 2230.387492\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 679\n",
            "Norm: 357711.46, NNZs: 993, Bias: -5224.223239, T: 665420, Avg. loss: 2232.050223\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 680\n",
            "Norm: 357711.43, NNZs: 993, Bias: -5224.225820, T: 666400, Avg. loss: 2226.929006\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 681\n",
            "Norm: 357711.41, NNZs: 993, Bias: -5224.228664, T: 667380, Avg. loss: 2228.716053\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 682\n",
            "Norm: 357711.38, NNZs: 993, Bias: -5224.231578, T: 668360, Avg. loss: 2227.234923\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 683\n",
            "Norm: 357711.36, NNZs: 993, Bias: -5224.234157, T: 669340, Avg. loss: 2222.015569\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 684\n",
            "Norm: 357711.33, NNZs: 993, Bias: -5224.237352, T: 670320, Avg. loss: 2226.427924\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 685\n",
            "Norm: 357711.30, NNZs: 993, Bias: -5224.240158, T: 671300, Avg. loss: 2222.193298\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 686\n",
            "Norm: 357711.28, NNZs: 993, Bias: -5224.242745, T: 672280, Avg. loss: 2217.346364\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 687\n",
            "Norm: 357711.25, NNZs: 993, Bias: -5224.245037, T: 673260, Avg. loss: 2213.482465\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 688\n",
            "Norm: 357711.23, NNZs: 993, Bias: -5224.247230, T: 674240, Avg. loss: 2211.609452\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 689\n",
            "Norm: 357711.21, NNZs: 993, Bias: -5224.249496, T: 675220, Avg. loss: 2210.082908\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 690\n",
            "Norm: 357711.18, NNZs: 993, Bias: -5224.252697, T: 676200, Avg. loss: 2216.782165\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 691\n",
            "Norm: 357711.16, NNZs: 993, Bias: -5224.255537, T: 677180, Avg. loss: 2212.786924\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 692\n",
            "Norm: 357711.13, NNZs: 993, Bias: -5224.258065, T: 678160, Avg. loss: 2207.600217\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 693\n",
            "Norm: 357711.11, NNZs: 993, Bias: -5224.260898, T: 679140, Avg. loss: 2209.665814\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 694\n",
            "Norm: 357711.08, NNZs: 993, Bias: -5224.263550, T: 680120, Avg. loss: 2207.293963\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 695\n",
            "Norm: 357711.06, NNZs: 993, Bias: -5224.266136, T: 681100, Avg. loss: 2203.027806\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 696\n",
            "Norm: 357711.03, NNZs: 993, Bias: -5224.268869, T: 682080, Avg. loss: 2204.634503\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 697\n",
            "Norm: 357711.01, NNZs: 993, Bias: -5224.271658, T: 683060, Avg. loss: 2203.179839\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 698\n",
            "Norm: 357710.98, NNZs: 993, Bias: -5224.274146, T: 684040, Avg. loss: 2198.119835\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 699\n",
            "Norm: 357710.96, NNZs: 993, Bias: -5224.276398, T: 685020, Avg. loss: 2194.629785\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 700\n",
            "Norm: 357710.94, NNZs: 993, Bias: -5224.278702, T: 686000, Avg. loss: 2192.953718\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 701\n",
            "Norm: 357710.91, NNZs: 993, Bias: -5224.280934, T: 686980, Avg. loss: 2191.276448\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 702\n",
            "Norm: 357710.89, NNZs: 993, Bias: -5224.283964, T: 687960, Avg. loss: 2197.444864\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 703\n",
            "Norm: 357710.86, NNZs: 993, Bias: -5224.286684, T: 688940, Avg. loss: 2193.535978\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 704\n",
            "Norm: 357710.84, NNZs: 993, Bias: -5224.289198, T: 689920, Avg. loss: 2188.979451\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 705\n",
            "Norm: 357710.81, NNZs: 993, Bias: -5224.292012, T: 690900, Avg. loss: 2190.889696\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 706\n",
            "Norm: 357710.79, NNZs: 993, Bias: -5224.294221, T: 691880, Avg. loss: 2183.510805\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 707\n",
            "Norm: 357710.77, NNZs: 993, Bias: -5224.297292, T: 692860, Avg. loss: 2189.841860\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 708\n",
            "Norm: 357710.74, NNZs: 993, Bias: -5224.299882, T: 693840, Avg. loss: 2185.625470\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 709\n",
            "Norm: 357710.72, NNZs: 993, Bias: -5224.302319, T: 694820, Avg. loss: 2180.936810\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 710\n",
            "Norm: 357710.69, NNZs: 993, Bias: -5224.304755, T: 695800, Avg. loss: 2179.458589\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 711\n",
            "Norm: 357710.67, NNZs: 993, Bias: -5224.307223, T: 696780, Avg. loss: 2178.191009\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 712\n",
            "Norm: 357710.65, NNZs: 993, Bias: -5224.309825, T: 697760, Avg. loss: 2179.424189\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 713\n",
            "Norm: 357710.62, NNZs: 993, Bias: -5224.312275, T: 698740, Avg. loss: 2175.165150\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 714\n",
            "Norm: 357710.60, NNZs: 993, Bias: -5224.314961, T: 699720, Avg. loss: 2176.810030\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 715\n",
            "Norm: 357710.58, NNZs: 993, Bias: -5224.317873, T: 700700, Avg. loss: 2177.204580\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 716\n",
            "Norm: 357710.55, NNZs: 993, Bias: -5224.319943, T: 701680, Avg. loss: 2168.089805\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 717\n",
            "Norm: 357710.53, NNZs: 993, Bias: -5224.322463, T: 702660, Avg. loss: 2169.102530\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 718\n",
            "Norm: 357710.51, NNZs: 993, Bias: -5224.325105, T: 703640, Avg. loss: 2170.625997\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 719\n",
            "Norm: 357710.48, NNZs: 993, Bias: -5224.327533, T: 704620, Avg. loss: 2166.027109\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 720\n",
            "Norm: 357710.46, NNZs: 993, Bias: -5224.330474, T: 705600, Avg. loss: 2170.011276\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 721\n",
            "Norm: 357710.44, NNZs: 993, Bias: -5224.332687, T: 706580, Avg. loss: 2160.863201\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 722\n",
            "Norm: 357710.41, NNZs: 993, Bias: -5224.334847, T: 707560, Avg. loss: 2159.326542\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 723\n",
            "Norm: 357710.39, NNZs: 993, Bias: -5224.337326, T: 708540, Avg. loss: 2160.149192\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 724\n",
            "Norm: 357710.37, NNZs: 993, Bias: -5224.340046, T: 709520, Avg. loss: 2161.988469\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 725\n",
            "Norm: 357710.34, NNZs: 993, Bias: -5224.342417, T: 710500, Avg. loss: 2157.033556\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 726\n",
            "Norm: 357710.32, NNZs: 993, Bias: -5224.345023, T: 711480, Avg. loss: 2158.634177\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 727\n",
            "Norm: 357710.30, NNZs: 993, Bias: -5224.347633, T: 712460, Avg. loss: 2157.318168\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 728\n",
            "Norm: 357710.27, NNZs: 993, Bias: -5224.349811, T: 713440, Avg. loss: 2150.739713\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 729\n",
            "Norm: 357710.25, NNZs: 993, Bias: -5224.352401, T: 714420, Avg. loss: 2154.345718\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 730\n",
            "Norm: 357710.23, NNZs: 993, Bias: -5224.355042, T: 715400, Avg. loss: 2152.904274\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 731\n",
            "Norm: 357710.21, NNZs: 993, Bias: -5224.357478, T: 716380, Avg. loss: 2148.434731\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 732\n",
            "Norm: 357710.18, NNZs: 993, Bias: -5224.360129, T: 717360, Avg. loss: 2150.060024\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 733\n",
            "Norm: 357710.16, NNZs: 993, Bias: -5224.362635, T: 718340, Avg. loss: 2145.517043\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 734\n",
            "Norm: 357710.14, NNZs: 993, Bias: -5224.365311, T: 719320, Avg. loss: 2147.026552\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 735\n",
            "Norm: 357710.11, NNZs: 993, Bias: -5224.367343, T: 720300, Avg. loss: 2140.126398\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 736\n",
            "Norm: 357710.09, NNZs: 993, Bias: -5224.369446, T: 721280, Avg. loss: 2138.869100\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 737\n",
            "Norm: 357710.07, NNZs: 993, Bias: -5224.371975, T: 722260, Avg. loss: 2142.417130\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 738\n",
            "Norm: 357710.05, NNZs: 993, Bias: -5224.374566, T: 723240, Avg. loss: 2141.055912\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 739\n",
            "Norm: 357710.02, NNZs: 993, Bias: -5224.376859, T: 724220, Avg. loss: 2136.580784\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 740\n",
            "Norm: 357710.00, NNZs: 993, Bias: -5224.379164, T: 725200, Avg. loss: 2135.026794\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 741\n",
            "Norm: 357709.98, NNZs: 993, Bias: -5224.381541, T: 726180, Avg. loss: 2133.937759\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 742\n",
            "Norm: 357709.96, NNZs: 993, Bias: -5224.383870, T: 727160, Avg. loss: 2132.401491\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 743\n",
            "Norm: 357709.94, NNZs: 993, Bias: -5224.385914, T: 728140, Avg. loss: 2128.857669\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 744\n",
            "Norm: 357709.91, NNZs: 993, Bias: -5224.388356, T: 729120, Avg. loss: 2129.834944\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 745\n",
            "Norm: 357709.89, NNZs: 993, Bias: -5224.390840, T: 730100, Avg. loss: 2130.856366\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 746\n",
            "Norm: 357709.87, NNZs: 993, Bias: -5224.393676, T: 731080, Avg. loss: 2131.900153\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 747\n",
            "Norm: 357709.85, NNZs: 993, Bias: -5224.395818, T: 732060, Avg. loss: 2123.415243\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 748\n",
            "Norm: 357709.83, NNZs: 993, Bias: -5224.398305, T: 733040, Avg. loss: 2126.495715\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 749\n",
            "Norm: 357709.80, NNZs: 993, Bias: -5224.400817, T: 734020, Avg. loss: 2125.370040\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 750\n",
            "Norm: 357709.78, NNZs: 993, Bias: -5224.403347, T: 735000, Avg. loss: 2123.993255\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 751\n",
            "Norm: 357709.76, NNZs: 993, Bias: -5224.405646, T: 735980, Avg. loss: 2119.415149\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 752\n",
            "Norm: 357709.74, NNZs: 993, Bias: -5224.407959, T: 736960, Avg. loss: 2118.311745\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 753\n",
            "Norm: 357709.72, NNZs: 993, Bias: -5224.409988, T: 737940, Avg. loss: 2114.852790\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 754\n",
            "Norm: 357709.69, NNZs: 993, Bias: -5224.412347, T: 738920, Avg. loss: 2115.706537\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 755\n",
            "Norm: 357709.67, NNZs: 993, Bias: -5224.415140, T: 739900, Avg. loss: 2118.973765\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 756\n",
            "Norm: 357709.65, NNZs: 993, Bias: -5224.417875, T: 740880, Avg. loss: 2117.407238\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 757\n",
            "Norm: 357709.63, NNZs: 993, Bias: -5224.420575, T: 741860, Avg. loss: 2115.896051\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 758\n",
            "Norm: 357709.61, NNZs: 993, Bias: -5224.422503, T: 742840, Avg. loss: 2107.601320\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 759\n",
            "Norm: 357709.58, NNZs: 993, Bias: -5224.424593, T: 743820, Avg. loss: 2106.541371\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 760\n",
            "Norm: 357709.56, NNZs: 993, Bias: -5224.427262, T: 744800, Avg. loss: 2111.878253\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 761\n",
            "Norm: 357709.54, NNZs: 993, Bias: -5224.429245, T: 745780, Avg. loss: 2103.510754\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 762\n",
            "Norm: 357709.52, NNZs: 993, Bias: -5224.431928, T: 746760, Avg. loss: 2109.061758\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 763\n",
            "Norm: 357709.50, NNZs: 993, Bias: -5224.434658, T: 747740, Avg. loss: 2107.762028\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 764\n",
            "Norm: 357709.48, NNZs: 993, Bias: -5224.437390, T: 748720, Avg. loss: 2106.329096\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 765\n",
            "Norm: 357709.45, NNZs: 993, Bias: -5224.439841, T: 749700, Avg. loss: 2102.834695\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 766\n",
            "Norm: 357709.43, NNZs: 993, Bias: -5224.442536, T: 750680, Avg. loss: 2103.584941\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 767\n",
            "Norm: 357709.41, NNZs: 993, Bias: -5224.445201, T: 751660, Avg. loss: 2101.952897\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 768\n",
            "Norm: 357709.39, NNZs: 993, Bias: -5224.447167, T: 752640, Avg. loss: 2093.823386\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 769\n",
            "Norm: 357709.37, NNZs: 993, Bias: -5224.449752, T: 753620, Avg. loss: 2099.122835\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 770\n",
            "Norm: 357709.35, NNZs: 993, Bias: -5224.452031, T: 754600, Avg. loss: 2093.288881\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 771\n",
            "Norm: 357709.33, NNZs: 993, Bias: -5224.454044, T: 755580, Avg. loss: 2089.993987\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 772\n",
            "Norm: 357709.31, NNZs: 993, Bias: -5224.456206, T: 756560, Avg. loss: 2090.351928\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 773\n",
            "Norm: 357709.28, NNZs: 993, Bias: -5224.458700, T: 757540, Avg. loss: 2092.278123\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 774\n",
            "Norm: 357709.26, NNZs: 993, Bias: -5224.461155, T: 758520, Avg. loss: 2090.642555\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 775\n",
            "Norm: 357709.24, NNZs: 993, Bias: -5224.463757, T: 759500, Avg. loss: 2091.123063\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 776\n",
            "Norm: 357709.22, NNZs: 993, Bias: -5224.465799, T: 760480, Avg. loss: 2083.405628\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 777\n",
            "Norm: 357709.20, NNZs: 993, Bias: -5224.468186, T: 761460, Avg. loss: 2086.645634\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 778\n",
            "Norm: 357709.18, NNZs: 993, Bias: -5224.470457, T: 762440, Avg. loss: 2082.726844\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 779\n",
            "Norm: 357709.16, NNZs: 993, Bias: -5224.472690, T: 763420, Avg. loss: 2081.074354\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 780\n",
            "Norm: 357709.14, NNZs: 993, Bias: -5224.474742, T: 764400, Avg. loss: 2078.185119\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 781\n",
            "Norm: 357709.12, NNZs: 993, Bias: -5224.476956, T: 765380, Avg. loss: 2078.651764\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 782\n",
            "Norm: 357709.10, NNZs: 993, Bias: -5224.479382, T: 766360, Avg. loss: 2080.197117\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 783\n",
            "Norm: 357709.08, NNZs: 993, Bias: -5224.481710, T: 767340, Avg. loss: 2078.514316\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 784\n",
            "Norm: 357709.06, NNZs: 993, Bias: -5224.484096, T: 768320, Avg. loss: 2077.286088\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 785\n",
            "Norm: 357709.03, NNZs: 993, Bias: -5224.485992, T: 769300, Avg. loss: 2071.298507\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 786\n",
            "Norm: 357709.01, NNZs: 993, Bias: -5224.487875, T: 770280, Avg. loss: 2069.850316\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 787\n",
            "Norm: 357708.99, NNZs: 993, Bias: -5224.489926, T: 771260, Avg. loss: 2069.046513\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 788\n",
            "Norm: 357708.97, NNZs: 993, Bias: -5224.492374, T: 772240, Avg. loss: 2072.379808\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 789\n",
            "Norm: 357708.95, NNZs: 993, Bias: -5224.494281, T: 773220, Avg. loss: 2065.992768\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 790\n",
            "Norm: 357708.93, NNZs: 993, Bias: -5224.496826, T: 774200, Avg. loss: 2071.368480\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 791\n",
            "Norm: 357708.91, NNZs: 993, Bias: -5224.498752, T: 775180, Avg. loss: 2063.530338\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 792\n",
            "Norm: 357708.89, NNZs: 993, Bias: -5224.500925, T: 776160, Avg. loss: 2064.075301\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 793\n",
            "Norm: 357708.87, NNZs: 993, Bias: -5224.503263, T: 777140, Avg. loss: 2065.554795\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 794\n",
            "Norm: 357708.85, NNZs: 993, Bias: -5224.505399, T: 778120, Avg. loss: 2061.540622\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 795\n",
            "Norm: 357708.83, NNZs: 993, Bias: -5224.507523, T: 779100, Avg. loss: 2060.203097\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 796\n",
            "Norm: 357708.81, NNZs: 993, Bias: -5224.509451, T: 780080, Avg. loss: 2057.099064\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 797\n",
            "Norm: 357708.79, NNZs: 993, Bias: -5224.511792, T: 781060, Avg. loss: 2060.448757\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 798\n",
            "Norm: 357708.77, NNZs: 993, Bias: -5224.513737, T: 782040, Avg. loss: 2054.647843\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 799\n",
            "Norm: 357708.75, NNZs: 993, Bias: -5224.516085, T: 783020, Avg. loss: 2057.920706\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 800\n",
            "Norm: 357708.73, NNZs: 993, Bias: -5224.518238, T: 784000, Avg. loss: 2053.893682\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 801\n",
            "Norm: 357708.71, NNZs: 993, Bias: -5224.520423, T: 784980, Avg. loss: 2052.745491\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 802\n",
            "Norm: 357708.69, NNZs: 993, Bias: -5224.522689, T: 785960, Avg. loss: 2053.925489\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 803\n",
            "Norm: 357708.67, NNZs: 993, Bias: -5224.524662, T: 786940, Avg. loss: 2048.324367\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 804\n",
            "Norm: 357708.65, NNZs: 993, Bias: -5224.526604, T: 787920, Avg. loss: 2046.972004\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 805\n",
            "Norm: 357708.63, NNZs: 993, Bias: -5224.529114, T: 788900, Avg. loss: 2052.117652\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 806\n",
            "Norm: 357708.61, NNZs: 993, Bias: -5224.531658, T: 789880, Avg. loss: 2050.904832\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 807\n",
            "Norm: 357708.59, NNZs: 993, Bias: -5224.533955, T: 790860, Avg. loss: 2047.629868\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 808\n",
            "Norm: 357708.57, NNZs: 993, Bias: -5224.536216, T: 791840, Avg. loss: 2046.300286\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 809\n",
            "Norm: 357708.55, NNZs: 993, Bias: -5224.538133, T: 792820, Avg. loss: 2040.620750\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 810\n",
            "Norm: 357708.53, NNZs: 993, Bias: -5224.540402, T: 793800, Avg. loss: 2043.827920\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 811\n",
            "Norm: 357708.51, NNZs: 993, Bias: -5224.542514, T: 794780, Avg. loss: 2039.947670\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 812\n",
            "Norm: 357708.49, NNZs: 993, Bias: -5224.545047, T: 795760, Avg. loss: 2043.281970\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 813\n",
            "Norm: 357708.48, NNZs: 993, Bias: -5224.546936, T: 796740, Avg. loss: 2035.694666\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 814\n",
            "Norm: 357708.46, NNZs: 993, Bias: -5224.549343, T: 797720, Avg. loss: 2040.305679\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 815\n",
            "Norm: 357708.44, NNZs: 993, Bias: -5224.551831, T: 798700, Avg. loss: 2039.397328\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 816\n",
            "Norm: 357708.42, NNZs: 993, Bias: -5224.554073, T: 799680, Avg. loss: 2036.095334\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 817\n",
            "Norm: 357708.40, NNZs: 993, Bias: -5224.556315, T: 800660, Avg. loss: 2034.790930\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 818\n",
            "Norm: 357708.38, NNZs: 993, Bias: -5224.558209, T: 801640, Avg. loss: 2029.530775\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 819\n",
            "Norm: 357708.36, NNZs: 993, Bias: -5224.560278, T: 802620, Avg. loss: 2029.985812\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 820\n",
            "Norm: 357708.34, NNZs: 993, Bias: -5224.562220, T: 803600, Avg. loss: 2027.231852\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 821\n",
            "Norm: 357708.32, NNZs: 993, Bias: -5224.564085, T: 804580, Avg. loss: 2025.815473\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 822\n",
            "Norm: 357708.30, NNZs: 993, Bias: -5224.566372, T: 805560, Avg. loss: 2028.979089\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 823\n",
            "Norm: 357708.28, NNZs: 993, Bias: -5224.568467, T: 806540, Avg. loss: 2025.287691\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 824\n",
            "Norm: 357708.26, NNZs: 993, Bias: -5224.570711, T: 807520, Avg. loss: 2026.450970\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 825\n",
            "Norm: 357708.24, NNZs: 993, Bias: -5224.572801, T: 808500, Avg. loss: 2022.797551\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 826\n",
            "Norm: 357708.22, NNZs: 993, Bias: -5224.575278, T: 809480, Avg. loss: 2025.880568\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 827\n",
            "Norm: 357708.20, NNZs: 993, Bias: -5224.577473, T: 810460, Avg. loss: 2022.681487\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 828\n",
            "Norm: 357708.19, NNZs: 993, Bias: -5224.579612, T: 811440, Avg. loss: 2021.148174\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 829\n",
            "Norm: 357708.17, NNZs: 993, Bias: -5224.581862, T: 812420, Avg. loss: 2020.420256\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 830\n",
            "Norm: 357708.15, NNZs: 993, Bias: -5224.584250, T: 813400, Avg. loss: 2020.660625\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 831\n",
            "Norm: 357708.13, NNZs: 993, Bias: -5224.586317, T: 814380, Avg. loss: 2015.327434\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 832\n",
            "Norm: 357708.11, NNZs: 993, Bias: -5224.588353, T: 815360, Avg. loss: 2014.191142\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 833\n",
            "Norm: 357708.09, NNZs: 993, Bias: -5224.590775, T: 816340, Avg. loss: 2017.296407\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 834\n",
            "Norm: 357708.07, NNZs: 993, Bias: -5224.592873, T: 817320, Avg. loss: 2013.798796\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 835\n",
            "Norm: 357708.05, NNZs: 993, Bias: -5224.594712, T: 818300, Avg. loss: 2008.655542\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 836\n",
            "Norm: 357708.03, NNZs: 993, Bias: -5224.597100, T: 819280, Avg. loss: 2013.534160\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 837\n",
            "Norm: 357708.01, NNZs: 993, Bias: -5224.598920, T: 820260, Avg. loss: 2006.364559\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 838\n",
            "Norm: 357708.00, NNZs: 993, Bias: -5224.600910, T: 821240, Avg. loss: 2006.717438\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 839\n",
            "Norm: 357707.98, NNZs: 993, Bias: -5224.603298, T: 822220, Avg. loss: 2009.885953\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 840\n",
            "Norm: 357707.96, NNZs: 993, Bias: -5224.605639, T: 823200, Avg. loss: 2008.659111\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 841\n",
            "Norm: 357707.94, NNZs: 993, Bias: -5224.607803, T: 824180, Avg. loss: 2005.629768\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 842\n",
            "Norm: 357707.92, NNZs: 993, Bias: -5224.610166, T: 825160, Avg. loss: 2006.219349\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 843\n",
            "Norm: 357707.90, NNZs: 993, Bias: -5224.612176, T: 826140, Avg. loss: 2000.841011\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 844\n",
            "Norm: 357707.88, NNZs: 993, Bias: -5224.614193, T: 827120, Avg. loss: 1999.881758\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 845\n",
            "Norm: 357707.87, NNZs: 993, Bias: -5224.616000, T: 828100, Avg. loss: 1996.736400\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 846\n",
            "Norm: 357707.85, NNZs: 993, Bias: -5224.618272, T: 829080, Avg. loss: 2001.275116\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 847\n",
            "Norm: 357707.83, NNZs: 993, Bias: -5224.620646, T: 830060, Avg. loss: 2000.370139\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 848\n",
            "Norm: 357707.81, NNZs: 993, Bias: -5224.622723, T: 831040, Avg. loss: 1997.114746\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 849\n",
            "Norm: 357707.79, NNZs: 993, Bias: -5224.624701, T: 832020, Avg. loss: 1993.804972\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 850\n",
            "Norm: 357707.77, NNZs: 993, Bias: -5224.626448, T: 833000, Avg. loss: 1990.851917\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 851\n",
            "Norm: 357707.75, NNZs: 993, Bias: -5224.628451, T: 833980, Avg. loss: 1991.520169\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 852\n",
            "Norm: 357707.74, NNZs: 993, Bias: -5224.630710, T: 834960, Avg. loss: 1994.266650\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 853\n",
            "Norm: 357707.72, NNZs: 993, Bias: -5224.632779, T: 835940, Avg. loss: 1991.426501\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 854\n",
            "Norm: 357707.70, NNZs: 993, Bias: -5224.634889, T: 836920, Avg. loss: 1990.318082\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 855\n",
            "Norm: 357707.68, NNZs: 993, Bias: -5224.636813, T: 837900, Avg. loss: 1986.645996\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 856\n",
            "Norm: 357707.66, NNZs: 993, Bias: -5224.638956, T: 838880, Avg. loss: 1988.249551\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 857\n",
            "Norm: 357707.65, NNZs: 993, Bias: -5224.640742, T: 839860, Avg. loss: 1982.985840\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 858\n",
            "Norm: 357707.63, NNZs: 993, Bias: -5224.642536, T: 840840, Avg. loss: 1981.870700\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 859\n",
            "Norm: 357707.61, NNZs: 993, Bias: -5224.644871, T: 841820, Avg. loss: 1986.400567\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 860\n",
            "Norm: 357707.59, NNZs: 993, Bias: -5224.646643, T: 842800, Avg. loss: 1979.459596\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 861\n",
            "Norm: 357707.57, NNZs: 993, Bias: -5224.648373, T: 843780, Avg. loss: 1978.147081\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 862\n",
            "Norm: 357707.56, NNZs: 993, Bias: -5224.650351, T: 844760, Avg. loss: 1978.969917\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 863\n",
            "Norm: 357707.54, NNZs: 993, Bias: -5224.652494, T: 845740, Avg. loss: 1980.139851\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 864\n",
            "Norm: 357707.52, NNZs: 993, Bias: -5224.654736, T: 846720, Avg. loss: 1980.431212\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 865\n",
            "Norm: 357707.50, NNZs: 993, Bias: -5224.656639, T: 847700, Avg. loss: 1975.259461\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 866\n",
            "Norm: 357707.48, NNZs: 993, Bias: -5224.658737, T: 848680, Avg. loss: 1976.674410\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 867\n",
            "Norm: 357707.47, NNZs: 993, Bias: -5224.660960, T: 849660, Avg. loss: 1976.999976\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 868\n",
            "Norm: 357707.45, NNZs: 993, Bias: -5224.663229, T: 850640, Avg. loss: 1976.040483\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 869\n",
            "Norm: 357707.43, NNZs: 993, Bias: -5224.665285, T: 851620, Avg. loss: 1973.077312\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 870\n",
            "Norm: 357707.41, NNZs: 993, Bias: -5224.667530, T: 852600, Avg. loss: 1973.664808\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 871\n",
            "Norm: 357707.39, NNZs: 993, Bias: -5224.669476, T: 853580, Avg. loss: 1968.779322\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 872\n",
            "Norm: 357707.38, NNZs: 993, Bias: -5224.671166, T: 854560, Avg. loss: 1965.681354\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 873\n",
            "Norm: 357707.36, NNZs: 993, Bias: -5224.672873, T: 855540, Avg. loss: 1964.611559\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 874\n",
            "Norm: 357707.34, NNZs: 993, Bias: -5224.675103, T: 856520, Avg. loss: 1969.064340\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 875\n",
            "Norm: 357707.32, NNZs: 993, Bias: -5224.677059, T: 857500, Avg. loss: 1964.084045\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 876\n",
            "Norm: 357707.31, NNZs: 993, Bias: -5224.679133, T: 858480, Avg. loss: 1965.339188\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 877\n",
            "Norm: 357707.29, NNZs: 993, Bias: -5224.680823, T: 859460, Avg. loss: 1960.192207\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 878\n",
            "Norm: 357707.27, NNZs: 993, Bias: -5224.682861, T: 860440, Avg. loss: 1963.035527\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 879\n",
            "Norm: 357707.25, NNZs: 993, Bias: -5224.685046, T: 861420, Avg. loss: 1963.446328\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 880\n",
            "Norm: 357707.24, NNZs: 993, Bias: -5224.686757, T: 862400, Avg. loss: 1956.859774\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 881\n",
            "Norm: 357707.22, NNZs: 993, Bias: -5224.688459, T: 863380, Avg. loss: 1955.770497\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 882\n",
            "Norm: 357707.20, NNZs: 993, Bias: -5224.690721, T: 864360, Avg. loss: 1960.394426\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 883\n",
            "Norm: 357707.19, NNZs: 993, Bias: -5224.692387, T: 865340, Avg. loss: 1953.626860\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 884\n",
            "Norm: 357707.17, NNZs: 993, Bias: -5224.694559, T: 866320, Avg. loss: 1957.870475\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 885\n",
            "Norm: 357707.15, NNZs: 993, Bias: -5224.696735, T: 867300, Avg. loss: 1956.905005\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 886\n",
            "Norm: 357707.13, NNZs: 993, Bias: -5224.698730, T: 868280, Avg. loss: 1954.070426\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 887\n",
            "Norm: 357707.12, NNZs: 993, Bias: -5224.700744, T: 869260, Avg. loss: 1952.991156\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 888\n",
            "Norm: 357707.10, NNZs: 993, Bias: -5224.702724, T: 870240, Avg. loss: 1951.817597\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 889\n",
            "Norm: 357707.08, NNZs: 993, Bias: -5224.704357, T: 871220, Avg. loss: 1946.884963\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 890\n",
            "Norm: 357707.06, NNZs: 993, Bias: -5224.706293, T: 872200, Avg. loss: 1947.652793\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 891\n",
            "Norm: 357707.05, NNZs: 993, Bias: -5224.708481, T: 873180, Avg. loss: 1950.227717\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 892\n",
            "Norm: 357707.03, NNZs: 993, Bias: -5224.710325, T: 874160, Avg. loss: 1945.295246\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 893\n",
            "Norm: 357707.01, NNZs: 993, Bias: -5224.712053, T: 875140, Avg. loss: 1942.842907\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 894\n",
            "Norm: 357707.00, NNZs: 993, Bias: -5224.713711, T: 876120, Avg. loss: 1941.515588\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 895\n",
            "Norm: 357706.98, NNZs: 993, Bias: -5224.715524, T: 877100, Avg. loss: 1942.059869\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 896\n",
            "Norm: 357706.96, NNZs: 993, Bias: -5224.717475, T: 878080, Avg. loss: 1943.207443\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 897\n",
            "Norm: 357706.95, NNZs: 993, Bias: -5224.719646, T: 879060, Avg. loss: 1943.746918\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 898\n",
            "Norm: 357706.93, NNZs: 993, Bias: -5224.721510, T: 880040, Avg. loss: 1938.965701\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 899\n",
            "Norm: 357706.91, NNZs: 993, Bias: -5224.723406, T: 881020, Avg. loss: 1937.984741\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 900\n",
            "Norm: 357706.89, NNZs: 993, Bias: -5224.725402, T: 882000, Avg. loss: 1938.921658\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 901\n",
            "Norm: 357706.88, NNZs: 993, Bias: -5224.727123, T: 882980, Avg. loss: 1934.268698\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 902\n",
            "Norm: 357706.86, NNZs: 993, Bias: -5224.728787, T: 883960, Avg. loss: 1933.168304\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 903\n",
            "Norm: 357706.84, NNZs: 993, Bias: -5224.730686, T: 884940, Avg. loss: 1933.754264\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 904\n",
            "Norm: 357706.83, NNZs: 993, Bias: -5224.732694, T: 885920, Avg. loss: 1934.980786\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 905\n",
            "Norm: 357706.81, NNZs: 993, Bias: -5224.734644, T: 886900, Avg. loss: 1933.557085\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 906\n",
            "Norm: 357706.79, NNZs: 993, Bias: -5224.736574, T: 887880, Avg. loss: 1932.545315\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 907\n",
            "Norm: 357706.78, NNZs: 993, Bias: -5224.738460, T: 888860, Avg. loss: 1929.475749\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 908\n",
            "Norm: 357706.76, NNZs: 993, Bias: -5224.740128, T: 889840, Avg. loss: 1926.686625\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 909\n",
            "Norm: 357706.74, NNZs: 993, Bias: -5224.741977, T: 890820, Avg. loss: 1927.283659\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 910\n",
            "Norm: 357706.73, NNZs: 993, Bias: -5224.744069, T: 891800, Avg. loss: 1929.782720\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 911\n",
            "Norm: 357706.71, NNZs: 993, Bias: -5224.746182, T: 892780, Avg. loss: 1928.699578\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 912\n",
            "Norm: 357706.69, NNZs: 993, Bias: -5224.748103, T: 893760, Avg. loss: 1926.036135\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 913\n",
            "Norm: 357706.68, NNZs: 993, Bias: -5224.749956, T: 894740, Avg. loss: 1923.121495\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 914\n",
            "Norm: 357706.66, NNZs: 993, Bias: -5224.751898, T: 895720, Avg. loss: 1924.059652\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 915\n",
            "Norm: 357706.64, NNZs: 993, Bias: -5224.753992, T: 896700, Avg. loss: 1924.629793\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 916\n",
            "Norm: 357706.63, NNZs: 993, Bias: -5224.755594, T: 897680, Avg. loss: 1918.274543\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 917\n",
            "Norm: 357706.61, NNZs: 993, Bias: -5224.757452, T: 898660, Avg. loss: 1919.019547\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 918\n",
            "Norm: 357706.59, NNZs: 993, Bias: -5224.759570, T: 899640, Avg. loss: 1921.426312\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 919\n",
            "Norm: 357706.58, NNZs: 993, Bias: -5224.761506, T: 900620, Avg. loss: 1918.825037\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 920\n",
            "Norm: 357706.56, NNZs: 993, Bias: -5224.763256, T: 901600, Avg. loss: 1915.638869\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 921\n",
            "Norm: 357706.55, NNZs: 993, Bias: -5224.764911, T: 902580, Avg. loss: 1913.284482\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 922\n",
            "Norm: 357706.53, NNZs: 993, Bias: -5224.766554, T: 903560, Avg. loss: 1912.153828\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 923\n",
            "Norm: 357706.51, NNZs: 993, Bias: -5224.768608, T: 904540, Avg. loss: 1916.221346\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 924\n",
            "Norm: 357706.50, NNZs: 993, Bias: -5224.770625, T: 905520, Avg. loss: 1914.953617\n",
            "Total training time: 1.43 seconds.\n",
            "-- Epoch 925\n",
            "Norm: 357706.48, NNZs: 993, Bias: -5224.772411, T: 906500, Avg. loss: 1910.582380\n",
            "Total training time: 1.43 seconds.\n",
            "-- Epoch 926\n",
            "Norm: 357706.47, NNZs: 993, Bias: -5224.774044, T: 907480, Avg. loss: 1907.934591\n",
            "Total training time: 1.43 seconds.\n",
            "-- Epoch 927\n",
            "Norm: 357706.45, NNZs: 993, Bias: -5224.775701, T: 908460, Avg. loss: 1906.992172\n",
            "Total training time: 1.43 seconds.\n",
            "-- Epoch 928\n",
            "Norm: 357706.43, NNZs: 993, Bias: -5224.777481, T: 909440, Avg. loss: 1907.403774\n",
            "Total training time: 1.43 seconds.\n",
            "-- Epoch 929\n",
            "Norm: 357706.42, NNZs: 993, Bias: -5224.779394, T: 910420, Avg. loss: 1908.534301\n",
            "Total training time: 1.43 seconds.\n",
            "-- Epoch 930\n",
            "Norm: 357706.40, NNZs: 993, Bias: -5224.781183, T: 911400, Avg. loss: 1905.492227\n",
            "Total training time: 1.43 seconds.\n",
            "-- Epoch 931\n",
            "Norm: 357706.38, NNZs: 993, Bias: -5224.783227, T: 912380, Avg. loss: 1907.910336\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 932\n",
            "Norm: 357706.37, NNZs: 993, Bias: -5224.785032, T: 913360, Avg. loss: 1903.509155\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 933\n",
            "Norm: 357706.35, NNZs: 993, Bias: -5224.787087, T: 914340, Avg. loss: 1905.916465\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 934\n",
            "Norm: 357706.34, NNZs: 993, Bias: -5224.789010, T: 915320, Avg. loss: 1903.513246\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 935\n",
            "Norm: 357706.32, NNZs: 993, Bias: -5224.791058, T: 916300, Avg. loss: 1903.949708\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 936\n",
            "Norm: 357706.30, NNZs: 993, Bias: -5224.792882, T: 917280, Avg. loss: 1901.219990\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 937\n",
            "Norm: 357706.29, NNZs: 993, Bias: -5224.794722, T: 918260, Avg. loss: 1900.286309\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 938\n",
            "Norm: 357706.27, NNZs: 993, Bias: -5224.796500, T: 919240, Avg. loss: 1897.401489\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 939\n",
            "Norm: 357706.26, NNZs: 993, Bias: -5224.798253, T: 920220, Avg. loss: 1896.293698\n",
            "Total training time: 1.45 seconds.\n",
            "-- Epoch 940\n",
            "Norm: 357706.24, NNZs: 993, Bias: -5224.800187, T: 921200, Avg. loss: 1897.577745\n",
            "Total training time: 1.45 seconds.\n",
            "-- Epoch 941\n",
            "Norm: 357706.22, NNZs: 993, Bias: -5224.802197, T: 922180, Avg. loss: 1897.753495\n",
            "Total training time: 1.45 seconds.\n",
            "-- Epoch 942\n",
            "Norm: 357706.21, NNZs: 993, Bias: -5224.803948, T: 923160, Avg. loss: 1893.314560\n",
            "Total training time: 1.45 seconds.\n",
            "-- Epoch 943\n",
            "Norm: 357706.19, NNZs: 993, Bias: -5224.805536, T: 924140, Avg. loss: 1890.908375\n",
            "Total training time: 1.45 seconds.\n",
            "-- Epoch 944\n",
            "Norm: 357706.18, NNZs: 993, Bias: -5224.807536, T: 925120, Avg. loss: 1894.816753\n",
            "Total training time: 1.45 seconds.\n",
            "-- Epoch 945\n",
            "Norm: 357706.16, NNZs: 993, Bias: -5224.809322, T: 926100, Avg. loss: 1892.047442\n",
            "Total training time: 1.45 seconds.\n",
            "-- Epoch 946\n",
            "Norm: 357706.15, NNZs: 993, Bias: -5224.810968, T: 927080, Avg. loss: 1888.048804\n",
            "Total training time: 1.46 seconds.\n",
            "-- Epoch 947\n",
            "Norm: 357706.13, NNZs: 993, Bias: -5224.812734, T: 928060, Avg. loss: 1888.491708\n",
            "Total training time: 1.46 seconds.\n",
            "-- Epoch 948\n",
            "Norm: 357706.11, NNZs: 993, Bias: -5224.814638, T: 929040, Avg. loss: 1889.552256\n",
            "Total training time: 1.46 seconds.\n",
            "-- Epoch 949\n",
            "Norm: 357706.10, NNZs: 993, Bias: -5224.816503, T: 930020, Avg. loss: 1888.517182\n",
            "Total training time: 1.46 seconds.\n",
            "-- Epoch 950\n",
            "Norm: 357706.08, NNZs: 993, Bias: -5224.818488, T: 931000, Avg. loss: 1888.832677\n",
            "Total training time: 1.46 seconds.\n",
            "-- Epoch 951\n",
            "Norm: 357706.07, NNZs: 993, Bias: -5224.820341, T: 931980, Avg. loss: 1886.318694\n",
            "Total training time: 1.46 seconds.\n",
            "-- Epoch 952\n",
            "Norm: 357706.05, NNZs: 993, Bias: -5224.822186, T: 932960, Avg. loss: 1885.415667\n",
            "Total training time: 1.46 seconds.\n",
            "-- Epoch 953\n",
            "Norm: 357706.04, NNZs: 993, Bias: -5224.823922, T: 933940, Avg. loss: 1882.381344\n",
            "Total training time: 1.46 seconds.\n",
            "-- Epoch 954\n",
            "Norm: 357706.02, NNZs: 993, Bias: -5224.825702, T: 934920, Avg. loss: 1881.668179\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 955\n",
            "Norm: 357706.01, NNZs: 993, Bias: -5224.827566, T: 935900, Avg. loss: 1882.481634\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 956\n",
            "Norm: 357705.99, NNZs: 993, Bias: -5224.829314, T: 936880, Avg. loss: 1879.604851\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 957\n",
            "Norm: 357705.97, NNZs: 993, Bias: -5224.830965, T: 937860, Avg. loss: 1878.259671\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 958\n",
            "Norm: 357705.96, NNZs: 993, Bias: -5224.832806, T: 938840, Avg. loss: 1879.541204\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 959\n",
            "Norm: 357705.94, NNZs: 993, Bias: -5224.834628, T: 939820, Avg. loss: 1878.589021\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 960\n",
            "Norm: 357705.93, NNZs: 993, Bias: -5224.836375, T: 940800, Avg. loss: 1875.654250\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 961\n",
            "Norm: 357705.91, NNZs: 993, Bias: -5224.837989, T: 941780, Avg. loss: 1873.350837\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 962\n",
            "Norm: 357705.90, NNZs: 993, Bias: -5224.840018, T: 942760, Avg. loss: 1877.164853\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 963\n",
            "Norm: 357705.88, NNZs: 993, Bias: -5224.841617, T: 943740, Avg. loss: 1871.398046\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 964\n",
            "Norm: 357705.87, NNZs: 993, Bias: -5224.843182, T: 944720, Avg. loss: 1870.244538\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 965\n",
            "Norm: 357705.85, NNZs: 993, Bias: -5224.844983, T: 945700, Avg. loss: 1872.550240\n",
            "Total training time: 1.48 seconds.\n",
            "-- Epoch 966\n",
            "Norm: 357705.84, NNZs: 993, Bias: -5224.846799, T: 946680, Avg. loss: 1871.831075\n",
            "Total training time: 1.48 seconds.\n",
            "-- Epoch 967\n",
            "Norm: 357705.82, NNZs: 993, Bias: -5224.848758, T: 947660, Avg. loss: 1872.108048\n",
            "Total training time: 1.48 seconds.\n",
            "-- Epoch 968\n",
            "Norm: 357705.81, NNZs: 993, Bias: -5224.850509, T: 948640, Avg. loss: 1869.665811\n",
            "Total training time: 1.48 seconds.\n",
            "-- Epoch 969\n",
            "Norm: 357705.79, NNZs: 993, Bias: -5224.852256, T: 949620, Avg. loss: 1868.661354\n",
            "Total training time: 1.48 seconds.\n",
            "-- Epoch 970\n",
            "Norm: 357705.78, NNZs: 993, Bias: -5224.854012, T: 950600, Avg. loss: 1867.666609\n",
            "Total training time: 1.48 seconds.\n",
            "-- Epoch 971\n",
            "Norm: 357705.76, NNZs: 993, Bias: -5224.855679, T: 951580, Avg. loss: 1864.862056\n",
            "Total training time: 1.48 seconds.\n",
            "-- Epoch 972\n",
            "Norm: 357705.75, NNZs: 993, Bias: -5224.857450, T: 952560, Avg. loss: 1865.911756\n",
            "Total training time: 1.48 seconds.\n",
            "-- Epoch 973\n",
            "Norm: 357705.73, NNZs: 993, Bias: -5224.859269, T: 953540, Avg. loss: 1865.013278\n",
            "Total training time: 1.49 seconds.\n",
            "-- Epoch 974\n",
            "Norm: 357705.72, NNZs: 993, Bias: -5224.860942, T: 954520, Avg. loss: 1862.087164\n",
            "Total training time: 1.49 seconds.\n",
            "-- Epoch 975\n",
            "Norm: 357705.70, NNZs: 993, Bias: -5224.862738, T: 955500, Avg. loss: 1863.038973\n",
            "Total training time: 1.49 seconds.\n",
            "-- Epoch 976\n",
            "Norm: 357705.69, NNZs: 993, Bias: -5224.864262, T: 956480, Avg. loss: 1858.850462\n",
            "Total training time: 1.49 seconds.\n",
            "-- Epoch 977\n",
            "Norm: 357705.67, NNZs: 993, Bias: -5224.865781, T: 957460, Avg. loss: 1857.848386\n",
            "Total training time: 1.49 seconds.\n",
            "-- Epoch 978\n",
            "Norm: 357705.66, NNZs: 993, Bias: -5224.867743, T: 958440, Avg. loss: 1861.755481\n",
            "Total training time: 1.49 seconds.\n",
            "-- Epoch 979\n",
            "Norm: 357705.64, NNZs: 993, Bias: -5224.869315, T: 959420, Avg. loss: 1856.003619\n",
            "Total training time: 1.49 seconds.\n",
            "-- Epoch 980\n",
            "Norm: 357705.63, NNZs: 993, Bias: -5224.871007, T: 960400, Avg. loss: 1856.491654\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 981\n",
            "Norm: 357705.61, NNZs: 993, Bias: -5224.872902, T: 961380, Avg. loss: 1858.798780\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 982\n",
            "Norm: 357705.60, NNZs: 993, Bias: -5224.874676, T: 962360, Avg. loss: 1856.404820\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 983\n",
            "Norm: 357705.58, NNZs: 993, Bias: -5224.876176, T: 963340, Avg. loss: 1852.171704\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 984\n",
            "Norm: 357705.57, NNZs: 993, Bias: -5224.877689, T: 964320, Avg. loss: 1851.242152\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 985\n",
            "Norm: 357705.55, NNZs: 993, Bias: -5224.879633, T: 965300, Avg. loss: 1855.120086\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 986\n",
            "Norm: 357705.54, NNZs: 993, Bias: -5224.881115, T: 966280, Avg. loss: 1849.302792\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 987\n",
            "Norm: 357705.52, NNZs: 993, Bias: -5224.882656, T: 967260, Avg. loss: 1848.603189\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 988\n",
            "Norm: 357705.51, NNZs: 993, Bias: -5224.884151, T: 968240, Avg. loss: 1847.609477\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 989\n",
            "Norm: 357705.49, NNZs: 993, Bias: -5224.885902, T: 969220, Avg. loss: 1850.013879\n",
            "Total training time: 1.51 seconds.\n",
            "-- Epoch 990\n",
            "Norm: 357705.48, NNZs: 993, Bias: -5224.887796, T: 970200, Avg. loss: 1850.418429\n",
            "Total training time: 1.51 seconds.\n",
            "-- Epoch 991\n",
            "Norm: 357705.46, NNZs: 993, Bias: -5224.889552, T: 971180, Avg. loss: 1848.065120\n",
            "Total training time: 1.51 seconds.\n",
            "-- Epoch 992\n",
            "Norm: 357705.45, NNZs: 993, Bias: -5224.891062, T: 972160, Avg. loss: 1843.906941\n",
            "Total training time: 1.51 seconds.\n",
            "-- Epoch 993\n",
            "Norm: 357705.43, NNZs: 993, Bias: -5224.892813, T: 973140, Avg. loss: 1846.187095\n",
            "Total training time: 1.51 seconds.\n",
            "-- Epoch 994\n",
            "Norm: 357705.42, NNZs: 993, Bias: -5224.894322, T: 974120, Avg. loss: 1842.164697\n",
            "Total training time: 1.51 seconds.\n",
            "-- Epoch 995\n",
            "Norm: 357705.41, NNZs: 993, Bias: -5224.896005, T: 975100, Avg. loss: 1842.642804\n",
            "Total training time: 1.51 seconds.\n",
            "-- Epoch 996\n",
            "Norm: 357705.39, NNZs: 993, Bias: -5224.897517, T: 976080, Avg. loss: 1840.404867\n",
            "Total training time: 1.52 seconds.\n",
            "-- Epoch 997\n",
            "Norm: 357705.38, NNZs: 993, Bias: -5224.899016, T: 977060, Avg. loss: 1839.330525\n",
            "Total training time: 1.52 seconds.\n",
            "-- Epoch 998\n",
            "Norm: 357705.36, NNZs: 993, Bias: -5224.900505, T: 978040, Avg. loss: 1838.454919\n",
            "Total training time: 1.52 seconds.\n",
            "-- Epoch 999\n",
            "Norm: 357705.35, NNZs: 993, Bias: -5224.902182, T: 979020, Avg. loss: 1839.071502\n",
            "Total training time: 1.52 seconds.\n",
            "-- Epoch 1000\n",
            "Norm: 357705.33, NNZs: 993, Bias: -5224.903854, T: 980000, Avg. loss: 1838.052109\n",
            "Total training time: 1.52 seconds.\n",
            "-- Epoch 1\n",
            "Norm: 263.88, NNZs: 648, Bias: -1.167237, T: 980, Avg. loss: 18.625669\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 257.66, NNZs: 610, Bias: -1.093339, T: 1960, Avg. loss: 4.875828\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 257.18, NNZs: 564, Bias: -0.891607, T: 2940, Avg. loss: 0.806814\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 256.98, NNZs: 544, Bias: -0.859686, T: 3920, Avg. loss: 0.485028\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 256.89, NNZs: 527, Bias: -0.822415, T: 4900, Avg. loss: 0.379022\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 256.83, NNZs: 519, Bias: -0.828672, T: 5880, Avg. loss: 0.321564\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 256.79, NNZs: 504, Bias: -0.824739, T: 6860, Avg. loss: 0.292875\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 256.76, NNZs: 489, Bias: -0.811265, T: 7840, Avg. loss: 0.278148\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 256.74, NNZs: 482, Bias: -0.804044, T: 8820, Avg. loss: 0.266032\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 256.72, NNZs: 479, Bias: -0.797270, T: 9800, Avg. loss: 0.258303\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 256.71, NNZs: 474, Bias: -0.783866, T: 10780, Avg. loss: 0.250042\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 256.69, NNZs: 470, Bias: -0.783702, T: 11760, Avg. loss: 0.240254\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 256.68, NNZs: 466, Bias: -0.771373, T: 12740, Avg. loss: 0.241545\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 256.68, NNZs: 462, Bias: -0.765147, T: 13720, Avg. loss: 0.235000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 256.67, NNZs: 460, Bias: -0.762578, T: 14700, Avg. loss: 0.229293\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 256.66, NNZs: 457, Bias: -0.755134, T: 15680, Avg. loss: 0.228061\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 256.66, NNZs: 452, Bias: -0.753646, T: 16660, Avg. loss: 0.222097\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 256.65, NNZs: 447, Bias: -0.748432, T: 17640, Avg. loss: 0.222186\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 256.65, NNZs: 442, Bias: -0.745612, T: 18620, Avg. loss: 0.218963\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 256.65, NNZs: 441, Bias: -0.743002, T: 19600, Avg. loss: 0.218127\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 256.64, NNZs: 440, Bias: -0.739172, T: 20580, Avg. loss: 0.218067\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 256.64, NNZs: 440, Bias: -0.737364, T: 21560, Avg. loss: 0.214498\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 256.64, NNZs: 434, Bias: -0.733423, T: 22540, Avg. loss: 0.215002\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 256.64, NNZs: 434, Bias: -0.730329, T: 23520, Avg. loss: 0.214222\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 256.64, NNZs: 433, Bias: -0.726877, T: 24500, Avg. loss: 0.212803\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 256.64, NNZs: 432, Bias: -0.725079, T: 25480, Avg. loss: 0.210583\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 256.63, NNZs: 430, Bias: -0.723549, T: 26460, Avg. loss: 0.209498\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 256.63, NNZs: 430, Bias: -0.720957, T: 27440, Avg. loss: 0.210398\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 256.63, NNZs: 430, Bias: -0.718724, T: 28420, Avg. loss: 0.209273\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 256.63, NNZs: 430, Bias: -0.716515, T: 29400, Avg. loss: 0.209025\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 256.63, NNZs: 428, Bias: -0.714366, T: 30380, Avg. loss: 0.208290\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 256.63, NNZs: 428, Bias: -0.712476, T: 31360, Avg. loss: 0.207577\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 32 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 24108454471230.73, NNZs: 848, Bias: -38331155915.054810, T: 979, Avg. loss: 27413759641060091363328.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 41522752307398.38, NNZs: 970, Bias: -66257468358.747566, T: 1958, Avg. loss: 281024403666023070302208.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 43808934836733.82, NNZs: 1035, Bias: -63328337183.231918, T: 2937, Avg. loss: 110663705665114619248640.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 44116670374321.46, NNZs: 1048, Bias: -186582894177.655273, T: 3916, Avg. loss: 36696210256039073808384.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 44234997524293.88, NNZs: 1052, Bias: -79999793955.826340, T: 4895, Avg. loss: 13269518746324941406208.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 44242783633431.14, NNZs: 1057, Bias: -153818114758.654144, T: 5874, Avg. loss: 9968036284759942365184.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 44264324403958.39, NNZs: 1057, Bias: -96435597698.690262, T: 6853, Avg. loss: 4472848495979973312512.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 44244788902514.89, NNZs: 1057, Bias: -129305998145.714966, T: 7832, Avg. loss: 5441384867928662343680.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 44231658544888.48, NNZs: 1057, Bias: -146899588859.613831, T: 8811, Avg. loss: 3672953905559898685440.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 44226508019230.52, NNZs: 1057, Bias: -144810548231.080200, T: 9790, Avg. loss: 2542526661969379328000.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 44225668909217.74, NNZs: 1057, Bias: -129900027143.197250, T: 10769, Avg. loss: 1646315602048413859840.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 44220581248066.98, NNZs: 1057, Bias: -128552060499.426437, T: 11748, Avg. loss: 953378844279510925312.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 44216823485408.95, NNZs: 1057, Bias: -133908879887.269653, T: 12727, Avg. loss: 275885149823281627136.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 44217366155339.59, NNZs: 1057, Bias: -130539789220.924667, T: 13706, Avg. loss: 55627112807671324672.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 44217019822873.20, NNZs: 1057, Bias: -131481977415.934052, T: 14685, Avg. loss: 6884306816482990080.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 44216982571044.66, NNZs: 1057, Bias: -131596726582.479263, T: 15664, Avg. loss: 307964416798975936.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 44216981442749.74, NNZs: 1057, Bias: -131600075318.299881, T: 16643, Avg. loss: 12487495849270592.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 44216979718686.38, NNZs: 1057, Bias: -131605860429.989975, T: 17622, Avg. loss: 208174800106077.656250\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 44216979730180.69, NNZs: 1057, Bias: -131605821802.293442, T: 18601, Avg. loss: 249247559396.539520\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 44216979727425.46, NNZs: 1057, Bias: -131605831059.261871, T: 19580, Avg. loss: 1374878457.212553\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 44216979727461.12, NNZs: 1057, Bias: -131605830939.450058, T: 20559, Avg. loss: 2748054.603524\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 44216979727441.84, NNZs: 1057, Bias: -131605831004.276978, T: 21538, Avg. loss: 31294.534353\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 44216979727440.98, NNZs: 1057, Bias: -131605831007.160904, T: 22517, Avg. loss: 384.340828\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 44216979727440.90, NNZs: 1057, Bias: -131605831007.412308, T: 23496, Avg. loss: 16.999914\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 44216979727440.87, NNZs: 1057, Bias: -131605831007.536148, T: 24475, Avg. loss: 0.982151\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.516678, T: 25454, Avg. loss: 0.093487\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 44216979727440.87, NNZs: 1057, Bias: -131605831007.536774, T: 26433, Avg. loss: 0.039359\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.516281, T: 27412, Avg. loss: 0.036524\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.527222, T: 28391, Avg. loss: 0.038562\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.497177, T: 29370, Avg. loss: 0.032940\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.534866, T: 30349, Avg. loss: 0.031737\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 44216979727440.86, NNZs: 1057, Bias: -131605831007.560028, T: 31328, Avg. loss: 0.033108\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 44216979727440.87, NNZs: 1057, Bias: -131605831007.544098, T: 32307, Avg. loss: 0.033732\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.505920, T: 33286, Avg. loss: 0.029212\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.516510, T: 34265, Avg. loss: 0.031428\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.513458, T: 35244, Avg. loss: 0.030400\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.524704, T: 36223, Avg. loss: 0.028874\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.532150, T: 37202, Avg. loss: 0.029268\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.517029, T: 38181, Avg. loss: 0.027426\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.524246, T: 39160, Avg. loss: 0.028566\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.515274, T: 40139, Avg. loss: 0.028550\n",
            "Total training time: 0.04 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 42\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.525528, T: 41118, Avg. loss: 0.027662\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 44216979727440.88, NNZs: 1057, Bias: -131605831007.525391, T: 42097, Avg. loss: 0.026781\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 44216979727440.87, NNZs: 1057, Bias: -131605831007.526703, T: 43076, Avg. loss: 0.026828\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 44 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 112612478706033.09, NNZs: 722, Bias: -300593000270.106628, T: 979, Avg. loss: 2346158224808556791595008.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 120903255922791.41, NNZs: 842, Bias: -929697773012.792847, T: 1958, Avg. loss: 2030738790270940167012352.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 121220683937153.97, NNZs: 861, Bias: -1226994422790.001709, T: 2937, Avg. loss: 511300543943911009157120.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 121575508429280.98, NNZs: 866, Bias: -1225946560374.664551, T: 3916, Avg. loss: 236482081722013352722432.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 121673239571153.39, NNZs: 871, Bias: -1267094756901.260010, T: 4895, Avg. loss: 137161253178166103506944.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 121385096229918.86, NNZs: 872, Bias: -1363414357276.308105, T: 5874, Avg. loss: 104206021735134844157952.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 121314220193938.44, NNZs: 872, Bias: -1410407010598.672607, T: 6853, Avg. loss: 27108575170764356976640.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 121263048867216.88, NNZs: 872, Bias: -1442551230097.839600, T: 7832, Avg. loss: 19998248695074580856832.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 121174826159392.30, NNZs: 872, Bias: -1484076973233.268311, T: 8811, Avg. loss: 16367544270518518546432.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 121219473467050.62, NNZs: 872, Bias: -1457882515908.564453, T: 9790, Avg. loss: 6719499328734868013056.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 121177219180829.59, NNZs: 872, Bias: -1474209400151.512695, T: 10769, Avg. loss: 14816557729687812440064.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 121175694074064.98, NNZs: 872, Bias: -1473572728372.747559, T: 11748, Avg. loss: 5274784355694254489600.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 121175013344962.91, NNZs: 872, Bias: -1475506132787.088379, T: 12727, Avg. loss: 3407471772018595594240.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 121156808147670.84, NNZs: 872, Bias: -1483821089162.120850, T: 13706, Avg. loss: 4272372372426561945600.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 121126920457437.27, NNZs: 872, Bias: -1504005265135.729248, T: 14685, Avg. loss: 1341097600471976378368.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 121123363331778.98, NNZs: 872, Bias: -1506674527308.277344, T: 15664, Avg. loss: 401824577813731934208.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 121127491994578.56, NNZs: 872, Bias: -1503372762063.656494, T: 16643, Avg. loss: 161542026297513377792.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 121128464845285.47, NNZs: 872, Bias: -1502561432184.171143, T: 17622, Avg. loss: 38032703429644492800.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 121125075154972.05, NNZs: 872, Bias: -1505199470116.860840, T: 18601, Avg. loss: 30580759014587387904.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 121125331469468.00, NNZs: 872, Bias: -1504992897919.590576, T: 19580, Avg. loss: 703959777253279872.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 121125091231136.84, NNZs: 872, Bias: -1505185311051.563477, T: 20559, Avg. loss: 289273720736179264.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 121125076653436.80, NNZs: 872, Bias: -1505197039466.322754, T: 21538, Avg. loss: 777660231553009.625000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 121125076654651.81, NNZs: 872, Bias: -1505197038482.105469, T: 22517, Avg. loss: 5084165494391.254883\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 121125075975484.05, NNZs: 872, Bias: -1505197585007.282959, T: 23496, Avg. loss: 3019549044353.895020\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 121125075866413.20, NNZs: 872, Bias: -1505197672777.656250, T: 24475, Avg. loss: 83902463657.219742\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 121125075848095.77, NNZs: 872, Bias: -1505197687517.910889, T: 25454, Avg. loss: 2691958316.306599\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 121125075844312.72, NNZs: 872, Bias: -1505197690562.171143, T: 26433, Avg. loss: 122074998.966795\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 121125075843376.03, NNZs: 872, Bias: -1505197691315.936523, T: 27412, Avg. loss: 6884825.823619\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 121125075843149.78, NNZs: 872, Bias: -1505197691498.003174, T: 28391, Avg. loss: 473846.370802\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 121125075843111.53, NNZs: 872, Bias: -1505197691528.783203, T: 29370, Avg. loss: 43774.863152\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 121125075843084.45, NNZs: 872, Bias: -1505197691550.556396, T: 30349, Avg. loss: 5327.966095\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 121125075843077.12, NNZs: 872, Bias: -1505197691556.461914, T: 31328, Avg. loss: 593.188164\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 121125075843074.75, NNZs: 872, Bias: -1505197691558.381836, T: 32307, Avg. loss: 76.164054\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 121125075843073.86, NNZs: 872, Bias: -1505197691559.108643, T: 33286, Avg. loss: 10.370178\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 121125075843073.52, NNZs: 872, Bias: -1505197691559.379883, T: 34265, Avg. loss: 1.625418\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 121125075843073.44, NNZs: 872, Bias: -1505197691559.441162, T: 35244, Avg. loss: 0.304319\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 121125075843073.41, NNZs: 872, Bias: -1505197691559.455566, T: 36223, Avg. loss: 0.091797\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 121125075843073.39, NNZs: 872, Bias: -1505197691559.477051, T: 37202, Avg. loss: 0.055722\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 121125075843073.34, NNZs: 872, Bias: -1505197691559.499512, T: 38181, Avg. loss: 0.039047\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 121125075843073.38, NNZs: 872, Bias: -1505197691559.468262, T: 39160, Avg. loss: 0.039877\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 121125075843073.34, NNZs: 872, Bias: -1505197691559.486572, T: 40139, Avg. loss: 0.044081\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 121125075843073.33, NNZs: 872, Bias: -1505197691559.493652, T: 41118, Avg. loss: 0.036384\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 121125075843073.33, NNZs: 872, Bias: -1505197691559.489746, T: 42097, Avg. loss: 0.042076\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 121125075843073.33, NNZs: 872, Bias: -1505197691559.485352, T: 43076, Avg. loss: 0.032134\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 121125075843073.34, NNZs: 872, Bias: -1505197691559.479492, T: 44055, Avg. loss: 0.035390\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 121125075843073.33, NNZs: 872, Bias: -1505197691559.497559, T: 45034, Avg. loss: 0.037365\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 121125075843073.38, NNZs: 872, Bias: -1505197691559.467773, T: 46013, Avg. loss: 0.029166\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 121125075843073.34, NNZs: 872, Bias: -1505197691559.482910, T: 46992, Avg. loss: 0.036746\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 121125075843073.34, NNZs: 872, Bias: -1505197691559.481934, T: 47971, Avg. loss: 0.031152\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 121125075843073.33, NNZs: 872, Bias: -1505197691559.494141, T: 48950, Avg. loss: 0.032596\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 121125075843073.36, NNZs: 872, Bias: -1505197691559.462891, T: 49929, Avg. loss: 0.027579\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 121125075843073.34, NNZs: 872, Bias: -1505197691559.484375, T: 50908, Avg. loss: 0.035717\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 121125075843073.34, NNZs: 872, Bias: -1505197691559.484863, T: 51887, Avg. loss: 0.030034\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 121125075843073.33, NNZs: 872, Bias: -1505197691559.493652, T: 52866, Avg. loss: 0.032734\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 121125075843073.31, NNZs: 872, Bias: -1505197691559.509277, T: 53845, Avg. loss: 0.030783\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 121125075843073.33, NNZs: 872, Bias: -1505197691559.488037, T: 54824, Avg. loss: 0.027595\n",
            "Total training time: 0.06 seconds.\n",
            "Convergence after 56 epochs took 0.07 seconds\n",
            "-- Epoch 1\n",
            "Norm: 21610904927.17, NNZs: 837, Bias: -150582878.473313, T: 979, Avg. loss: 10643515918429438.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 6862124654963.82, NNZs: 1032, Bias: -19473903948.225521, T: 1958, Avg. loss: 2982429491517031186432.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 14286252911781.37, NNZs: 1081, Bias: -138043034837.646484, T: 2937, Avg. loss: 40470750379146288824320.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 15578263493491.98, NNZs: 1103, Bias: -102086374158.032700, T: 3916, Avg. loss: 33612479691876137435136.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 15743847998569.81, NNZs: 1106, Bias: -125527584404.113205, T: 4895, Avg. loss: 12432519848419024437248.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 15710871038330.19, NNZs: 1112, Bias: -102013925789.921036, T: 5874, Avg. loss: 6334389544406895034368.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 14435260494323.31, NNZs: 813, Bias: -107858572992.390854, T: 979, Avg. loss: 2842361291822929543168.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 39354411558327.86, NNZs: 969, Bias: -2428176766.906334, T: 1958, Avg. loss: 259598892551405834338304.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 41559323480911.88, NNZs: 1009, Bias: -147627483036.975311, T: 2937, Avg. loss: 118241569677605798412288.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 41912565151335.92, NNZs: 1016, Bias: -80937023996.720062, T: 3916, Avg. loss: 39451626843375491088384.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 41982646717437.20, NNZs: 1019, Bias: -9239007325.599506, T: 4895, Avg. loss: 16428224478984173256704.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 42030286401616.38, NNZs: 1021, Bias: -100736656087.479065, T: 5874, Avg. loss: 6555487774466567569408.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 117847724639023.77, NNZs: 752, Bias: 37580752281.739105, T: 979, Avg. loss: 2143914918433664186449920.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 121660282020063.38, NNZs: 867, Bias: -660472689735.363647, T: 1958, Avg. loss: 2714961662446667405721600.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 123270104662948.02, NNZs: 882, Bias: -820752906865.733765, T: 2937, Avg. loss: 655396091022155108581376.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 123172458817121.50, NNZs: 888, Bias: -1073279101271.687256, T: 3916, Avg. loss: 308027062140496364175360.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 123065271445368.47, NNZs: 888, Bias: -1174984964161.499268, T: 4895, Avg. loss: 170087752542076507521024.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 122660755187592.19, NNZs: 888, Bias: -1299455422429.061279, T: 5874, Avg. loss: 121795397228299104026624.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 122432893675117.70, NNZs: 888, Bias: -1396850467418.363281, T: 6853, Avg. loss: 39596877843782032162816.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 122403997289800.03, NNZs: 888, Bias: -1426236777835.869629, T: 7832, Avg. loss: 7768704023512842502144.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 122487895972171.11, NNZs: 888, Bias: -1389514863925.157471, T: 8811, Avg. loss: 22478635972436961525760.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 122457373803639.84, NNZs: 888, Bias: -1390719136465.060059, T: 9790, Avg. loss: 19632997825918489591808.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 122375510809787.81, NNZs: 888, Bias: -1432938143399.554932, T: 10769, Avg. loss: 11874814489266350981120.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 122389620916279.83, NNZs: 888, Bias: -1420269490823.433594, T: 11748, Avg. loss: 6145290355083264917504.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 122355167677898.69, NNZs: 888, Bias: -1442219328181.472900, T: 12727, Avg. loss: 3327108185055560204288.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 122362440657597.06, NNZs: 888, Bias: -1436678479254.801025, T: 13706, Avg. loss: 2181219487393472053248.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 122347876611813.83, NNZs: 888, Bias: -1446637229213.566650, T: 14685, Avg. loss: 1289767634975719424000.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 122333020206093.27, NNZs: 888, Bias: -1457521599346.655518, T: 15664, Avg. loss: 653111953695397117952.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 122343149912393.62, NNZs: 888, Bias: -1449456497328.662598, T: 16643, Avg. loss: 39000145022456512512.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 122336426813016.75, NNZs: 888, Bias: -1454611321162.926270, T: 17622, Avg. loss: 164021516174487977984.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 122336174041126.34, NNZs: 888, Bias: -1454809888865.834229, T: 18601, Avg. loss: 14051206009709225984.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 122335322152749.02, NNZs: 888, Bias: -1455508801224.129639, T: 19580, Avg. loss: 5259499354914414592.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 122335281925184.45, NNZs: 888, Bias: -1455542483220.508301, T: 20559, Avg. loss: 56349561756088912.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 122335275133416.84, NNZs: 888, Bias: -1455548191254.864502, T: 21538, Avg. loss: 88288704591474.656250\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 122335274761088.69, NNZs: 888, Bias: -1455548504186.665283, T: 22517, Avg. loss: 276273682107.938232\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 122335274728161.89, NNZs: 888, Bias: -1455548531860.818604, T: 23496, Avg. loss: 2575126064.549724\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 122335274724115.14, NNZs: 888, Bias: -1455548535262.012939, T: 24475, Avg. loss: 43399388.086027\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 122335274723496.89, NNZs: 888, Bias: -1455548535781.643311, T: 25454, Avg. loss: 1347270.938135\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 122335274723359.03, NNZs: 888, Bias: -1455548535897.509521, T: 26433, Avg. loss: 65595.461270\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 122335274723331.81, NNZs: 888, Bias: -1455548535920.390869, T: 27412, Avg. loss: 3785.346027\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 122335274723322.44, NNZs: 888, Bias: -1455548535928.266357, T: 28391, Avg. loss: 311.950398\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 122335274723319.91, NNZs: 888, Bias: -1455548535930.387939, T: 29370, Avg. loss: 24.657012\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 122335274723319.17, NNZs: 888, Bias: -1455548535930.999023, T: 30349, Avg. loss: 2.283857\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 122335274723318.94, NNZs: 888, Bias: -1455548535931.183594, T: 31328, Avg. loss: 0.239553\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 122335274723318.92, NNZs: 888, Bias: -1455548535931.200195, T: 32307, Avg. loss: 0.044258\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 122335274723318.97, NNZs: 888, Bias: -1455548535931.166260, T: 33286, Avg. loss: 0.041585\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 122335274723318.94, NNZs: 888, Bias: -1455548535931.177490, T: 34265, Avg. loss: 0.049625\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 122335274723318.92, NNZs: 888, Bias: -1455548535931.188965, T: 35244, Avg. loss: 0.038664\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 122335274723318.94, NNZs: 888, Bias: -1455548535931.169922, T: 36223, Avg. loss: 0.036545\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 122335274723318.94, NNZs: 888, Bias: -1455548535931.173096, T: 37202, Avg. loss: 0.041374\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 122335274723318.91, NNZs: 888, Bias: -1455548535931.190430, T: 38181, Avg. loss: 0.034464\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 122335274723318.92, NNZs: 888, Bias: -1455548535931.172119, T: 39160, Avg. loss: 0.035533\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 122335274723318.89, NNZs: 888, Bias: -1455548535931.184570, T: 40139, Avg. loss: 0.034975\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 122335274723318.89, NNZs: 888, Bias: -1455548535931.180176, T: 41118, Avg. loss: 0.030412\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 122335274723318.89, NNZs: 888, Bias: -1455548535931.177002, T: 42097, Avg. loss: 0.034976\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 122335274723318.88, NNZs: 888, Bias: -1455548535931.187256, T: 43076, Avg. loss: 0.028638\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 122335274723318.91, NNZs: 888, Bias: -1455548535931.168213, T: 44055, Avg. loss: 0.027344\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 122335274723318.89, NNZs: 888, Bias: -1455548535931.184082, T: 45034, Avg. loss: 0.032052\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 122335274723318.91, NNZs: 888, Bias: -1455548535931.179443, T: 46013, Avg. loss: 0.025076\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 122335274723318.92, NNZs: 888, Bias: -1455548535931.168701, T: 46992, Avg. loss: 0.027259\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 122335274723318.91, NNZs: 888, Bias: -1455548535931.179199, T: 47971, Avg. loss: 0.028438\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 122335274723318.91, NNZs: 888, Bias: -1455548535931.182617, T: 48950, Avg. loss: 0.026786\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 122335274723318.92, NNZs: 888, Bias: -1455548535931.160889, T: 49929, Avg. loss: 0.024159\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 122335274723318.91, NNZs: 888, Bias: -1455548535931.181885, T: 50908, Avg. loss: 0.028938\n",
            "Total training time: 0.07 seconds.\n",
            "Convergence after 52 epochs took 0.07 seconds\n",
            "-- Epoch 1\n",
            "Norm: 936622767.57, NNZs: 853, Bias: -2585524.099201, T: 979, Avg. loss: 21594720462060.171875\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 127909753108.56, NNZs: 1035, Bias: -737994714.081377, T: 1958, Avg. loss: 949636302432747520.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 649233346326.93, NNZs: 1091, Bias: -1118670814.185717, T: 2937, Avg. loss: 45690883950258814976.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1817265043592.89, NNZs: 1123, Bias: -3742623856.002371, T: 3916, Avg. loss: 719166456303989161984.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 2425662340309.88, NNZs: 1135, Bias: -8664014877.647610, T: 4895, Avg. loss: 1338879357272826052608.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 2559773184170.18, NNZs: 1139, Bias: -18441113565.279964, T: 5874, Avg. loss: 648235770340350754816.000000\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 15607845033213.32, NNZs: 834, Bias: -43635371496.209892, T: 979, Avg. loss: 3119953761473723367424.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 39100457115331.26, NNZs: 987, Bias: -81137400367.282990, T: 1958, Avg. loss: 234226983556832598949888.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 41417251878280.39, NNZs: 1020, Bias: -105873578719.339722, T: 2937, Avg. loss: 105070268932097212153856.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 42085086048380.80, NNZs: 1054, Bias: -110395559496.887146, T: 3916, Avg. loss: 31089407769038905409536.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 42256095262900.77, NNZs: 1057, Bias: -32524405453.189369, T: 4895, Avg. loss: 13784586833837330268160.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 42249247924744.03, NNZs: 1057, Bias: -71117905663.089523, T: 5874, Avg. loss: 8568702240966579322880.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 109632147262744.48, NNZs: 746, Bias: -194291639052.603485, T: 979, Avg. loss: 2648285289090458899709952.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 118422175621179.62, NNZs: 890, Bias: -1314184199436.655518, T: 1958, Avg. loss: 2381438933158792144093184.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 119693968753318.86, NNZs: 901, Bias: -1410651431101.468750, T: 2937, Avg. loss: 503909243924205241630720.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 119575598653099.75, NNZs: 902, Bias: -1626940463464.950439, T: 3916, Avg. loss: 242969323022271666716672.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 119642082567029.33, NNZs: 902, Bias: -1620944330724.746582, T: 4895, Avg. loss: 133519559808556646531072.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 119419748211167.19, NNZs: 904, Bias: -1696238307802.306152, T: 5874, Avg. loss: 126317403504959421415424.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 119355241758415.25, NNZs: 904, Bias: -1727942677727.652832, T: 6853, Avg. loss: 44737022884095136366592.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 119199882602813.64, NNZs: 907, Bias: -1778037372852.644287, T: 7832, Avg. loss: 52243737281943616618496.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 119293787659110.77, NNZs: 907, Bias: -1748219430500.033691, T: 8811, Avg. loss: 31435330500196497883136.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 119180694377656.30, NNZs: 907, Bias: -1787022131058.166992, T: 9790, Avg. loss: 26814195975005614899200.000000\n",
            "Total training time: 0.01 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 11\n",
            "Norm: 119158479360112.16, NNZs: 907, Bias: -1800155012427.834961, T: 10769, Avg. loss: 8058986277598577819648.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 119153643650397.73, NNZs: 907, Bias: -1797907621151.166504, T: 11748, Avg. loss: 6831373464295172997120.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 119150166361029.20, NNZs: 907, Bias: -1799905004176.391602, T: 12727, Avg. loss: 5144635761592408997888.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 119099842111093.19, NNZs: 907, Bias: -1823898500222.803467, T: 13706, Avg. loss: 4990081355364584914944.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 119105022982482.52, NNZs: 907, Bias: -1818639808568.792725, T: 14685, Avg. loss: 2024265567381910716416.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 119091739488692.14, NNZs: 907, Bias: -1826982404696.637939, T: 15664, Avg. loss: 884621305192270528512.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 119096530699275.25, NNZs: 907, Bias: -1823027221298.390625, T: 16643, Avg. loss: 621153823529206939648.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 119085469204950.92, NNZs: 907, Bias: -1829787444835.814209, T: 17622, Avg. loss: 186031955898174406656.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 119088091985247.36, NNZs: 907, Bias: -1828090860901.803955, T: 18601, Avg. loss: 14283914743358894080.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 119086677980955.52, NNZs: 907, Bias: -1828992817342.800293, T: 19580, Avg. loss: 7369641823685206016.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 119086519674128.14, NNZs: 907, Bias: -1829095327792.442139, T: 20559, Avg. loss: 236026921167020992.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 119086487402880.20, NNZs: 907, Bias: -1829116334393.926758, T: 21538, Avg. loss: 1539397237158982.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 119086488972556.77, NNZs: 907, Bias: -1829115312410.001465, T: 22517, Avg. loss: 32228880389894.593750\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 119086488268875.12, NNZs: 907, Bias: -1829115770540.687988, T: 23496, Avg. loss: 3546761390576.333008\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 119086487980392.41, NNZs: 907, Bias: -1829115958359.773682, T: 24475, Avg. loss: 226969195668.657166\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 119086487952032.50, NNZs: 907, Bias: -1829115976823.763184, T: 25454, Avg. loss: 8390636836.483064\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 119086487938876.56, NNZs: 907, Bias: -1829115985389.062256, T: 26433, Avg. loss: 626460247.563233\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 119086487936337.28, NNZs: 907, Bias: -1829115987042.293457, T: 27412, Avg. loss: 46422790.383816\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 119086487935046.34, NNZs: 907, Bias: -1829115987882.779297, T: 28391, Avg. loss: 3333626.315395\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 119086487934749.95, NNZs: 907, Bias: -1829115988075.739502, T: 29370, Avg. loss: 203442.509759\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 119086487934677.09, NNZs: 907, Bias: -1829115988123.183105, T: 30349, Avg. loss: 17012.439695\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 119086487934661.45, NNZs: 907, Bias: -1829115988133.365234, T: 31328, Avg. loss: 1849.690768\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 119086487934655.64, NNZs: 907, Bias: -1829115988137.148438, T: 32307, Avg. loss: 260.491277\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 119086487934652.20, NNZs: 907, Bias: -1829115988139.381348, T: 33286, Avg. loss: 43.158226\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 119086487934651.11, NNZs: 907, Bias: -1829115988140.095703, T: 34265, Avg. loss: 6.216403\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 119086487934650.80, NNZs: 907, Bias: -1829115988140.305420, T: 35244, Avg. loss: 0.983999\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 119086487934650.67, NNZs: 907, Bias: -1829115988140.390625, T: 36223, Avg. loss: 0.213242\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 119086487934650.70, NNZs: 907, Bias: -1829115988140.379395, T: 37202, Avg. loss: 0.068824\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 119086487934650.67, NNZs: 907, Bias: -1829115988140.403320, T: 38181, Avg. loss: 0.050147\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 119086487934650.67, NNZs: 907, Bias: -1829115988140.402588, T: 39160, Avg. loss: 0.038154\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 119086487934650.69, NNZs: 907, Bias: -1829115988140.404053, T: 40139, Avg. loss: 0.039493\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 119086487934650.73, NNZs: 907, Bias: -1829115988140.383789, T: 41118, Avg. loss: 0.037080\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 119086487934650.70, NNZs: 907, Bias: -1829115988140.409912, T: 42097, Avg. loss: 0.042798\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 119086487934650.70, NNZs: 907, Bias: -1829115988140.406738, T: 43076, Avg. loss: 0.033955\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 119086487934650.70, NNZs: 907, Bias: -1829115988140.395508, T: 44055, Avg. loss: 0.035149\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 119086487934650.67, NNZs: 907, Bias: -1829115988140.400879, T: 45034, Avg. loss: 0.032193\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 119086487934650.69, NNZs: 907, Bias: -1829115988140.385498, T: 46013, Avg. loss: 0.031857\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 119086487934650.66, NNZs: 907, Bias: -1829115988140.406250, T: 46992, Avg. loss: 0.036574\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 119086487934650.66, NNZs: 907, Bias: -1829115988140.397949, T: 47971, Avg. loss: 0.030481\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 119086487934650.62, NNZs: 907, Bias: -1829115988140.414551, T: 48950, Avg. loss: 0.032818\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 119086487934650.66, NNZs: 907, Bias: -1829115988140.402100, T: 49929, Avg. loss: 0.029510\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 119086487934650.67, NNZs: 907, Bias: -1829115988140.387695, T: 50908, Avg. loss: 0.030916\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 119086487934650.61, NNZs: 907, Bias: -1829115988140.422363, T: 51887, Avg. loss: 0.033377\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 119086487934650.62, NNZs: 907, Bias: -1829115988140.414551, T: 52866, Avg. loss: 0.026890\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 119086487934650.64, NNZs: 907, Bias: -1829115988140.413330, T: 53845, Avg. loss: 0.030621\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 119086487934650.64, NNZs: 907, Bias: -1829115988140.412598, T: 54824, Avg. loss: 0.029786\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 119086487934650.64, NNZs: 907, Bias: -1829115988140.412109, T: 55803, Avg. loss: 0.029338\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 119086487934650.64, NNZs: 907, Bias: -1829115988140.413574, T: 56782, Avg. loss: 0.028177\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 119086487934650.66, NNZs: 907, Bias: -1829115988140.409424, T: 57761, Avg. loss: 0.026798\n",
            "Total training time: 0.07 seconds.\n",
            "Convergence after 59 epochs took 0.07 seconds\n",
            "-- Epoch 1\n",
            "Norm: 8204265297.12, NNZs: 855, Bias: -40189911.813373, T: 979, Avg. loss: 1490461791063059.500000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 3237593106552.82, NNZs: 1022, Bias: -29822955969.179752, T: 1958, Avg. loss: 585117625337303531520.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 11876244527167.82, NNZs: 1094, Bias: -36570924040.882652, T: 2937, Avg. loss: 23980261759297351843840.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 13900864572006.05, NNZs: 1114, Bias: -61204804164.068298, T: 3916, Avg. loss: 32936465629457086939136.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 14063796253638.36, NNZs: 1129, Bias: -75858779245.921082, T: 4895, Avg. loss: 10460276533780230963200.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 14054276556368.44, NNZs: 1132, Bias: -42792845019.377426, T: 5874, Avg. loss: 6714864568738609364992.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 26981802888794.43, NNZs: 839, Bias: 112747620324.386078, T: 979, Avg. loss: 29569119787734051848192.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42122517282588.57, NNZs: 1000, Bias: -57201976895.435364, T: 1958, Avg. loss: 290696146112517031067648.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 44567004870834.87, NNZs: 1046, Bias: -98949612673.632416, T: 2937, Avg. loss: 139316727519469851639808.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 44986212738091.52, NNZs: 1061, Bias: -167243239702.024048, T: 3916, Avg. loss: 45229252643740008316928.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 45202678880069.64, NNZs: 1066, Bias: -69322982776.827301, T: 4895, Avg. loss: 17537102784874806247424.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 45251791515897.98, NNZs: 1067, Bias: -69003133140.389236, T: 5874, Avg. loss: 9105804090748398731264.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 45246646499011.14, NNZs: 1067, Bias: -53879668769.480499, T: 6853, Avg. loss: 5466580602917632344064.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 45239510421514.55, NNZs: 1067, Bias: -53609687121.332458, T: 7832, Avg. loss: 3288170299585374191616.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 45235223431339.46, NNZs: 1067, Bias: -58756259994.731750, T: 8811, Avg. loss: 2614708521667830218752.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 45231214965648.86, NNZs: 1067, Bias: -54945393333.920959, T: 9790, Avg. loss: 2273259113655291084800.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 45223556682714.40, NNZs: 1067, Bias: -72831103114.211288, T: 10769, Avg. loss: 1669068685186844852224.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 45220881473014.82, NNZs: 1067, Bias: -60941013840.338242, T: 11748, Avg. loss: 776867998127960752128.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 45218348709252.01, NNZs: 1067, Bias: -64387400398.749199, T: 12727, Avg. loss: 278061474761961897984.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 45217953865082.45, NNZs: 1067, Bias: -63328434513.343483, T: 13706, Avg. loss: 65816713877297045504.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 45217854066437.62, NNZs: 1067, Bias: -63607467939.195320, T: 14685, Avg. loss: 6570017939887406080.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 45217892219254.81, NNZs: 1067, Bias: -63320291347.406876, T: 15664, Avg. loss: 221391268809944288.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 45217881674267.66, NNZs: 1067, Bias: -63394116411.295906, T: 16643, Avg. loss: 19424508728051116.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 45217881966400.02, NNZs: 1067, Bias: -63392019712.221214, T: 17622, Avg. loss: 173712504660682.875000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 45217882004404.86, NNZs: 1067, Bias: -63391748547.223694, T: 18601, Avg. loss: 975307720013.786133\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 45217882001733.76, NNZs: 1067, Bias: -63391767600.349518, T: 19580, Avg. loss: 1104880299.582226\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 45217882001867.62, NNZs: 1067, Bias: -63391766645.439964, T: 20559, Avg. loss: 3572838.002843\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 45217882001859.02, NNZs: 1067, Bias: -63391766706.788010, T: 21538, Avg. loss: 13624.406155\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 45217882001859.76, NNZs: 1067, Bias: -63391766701.585411, T: 22517, Avg. loss: 350.104317\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 45217882001859.66, NNZs: 1067, Bias: -63391766702.327431, T: 23496, Avg. loss: 16.122866\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.172371, T: 24475, Avg. loss: 0.915807\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.171173, T: 25454, Avg. loss: 0.093231\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.194756, T: 26433, Avg. loss: 0.034811\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 45217882001859.66, NNZs: 1067, Bias: -63391766702.221817, T: 27412, Avg. loss: 0.035872\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 45217882001859.68, NNZs: 1067, Bias: -63391766702.170341, T: 28391, Avg. loss: 0.032192\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 45217882001859.68, NNZs: 1067, Bias: -63391766702.184258, T: 29370, Avg. loss: 0.036088\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.205719, T: 30349, Avg. loss: 0.032559\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.215874, T: 31328, Avg. loss: 0.033291\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.198418, T: 32307, Avg. loss: 0.031899\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.197685, T: 33286, Avg. loss: 0.030243\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.191536, T: 34265, Avg. loss: 0.031234\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.209892, T: 35244, Avg. loss: 0.029607\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.193024, T: 36223, Avg. loss: 0.028277\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.190506, T: 37202, Avg. loss: 0.028762\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.208900, T: 38181, Avg. loss: 0.028363\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.198921, T: 39160, Avg. loss: 0.028835\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.198120, T: 40139, Avg. loss: 0.028645\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 45217882001859.67, NNZs: 1067, Bias: -63391766702.199570, T: 41118, Avg. loss: 0.027604\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 42 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 125490271534373.09, NNZs: 794, Bias: -233472668331.905121, T: 979, Avg. loss: 3354774662938685270392832.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 127853455677657.38, NNZs: 898, Bias: -1410101694943.931641, T: 1958, Avg. loss: 2487789344006914523004928.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 129114659302038.31, NNZs: 906, Bias: -1651308307533.530518, T: 2937, Avg. loss: 507808605510872944607232.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 129034330981149.48, NNZs: 910, Bias: -1822397575582.020020, T: 3916, Avg. loss: 347126209837929058009088.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 128739519275814.31, NNZs: 910, Bias: -1937203347876.100830, T: 4895, Avg. loss: 176390289827536104849408.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 128672013072199.69, NNZs: 910, Bias: -1926973008571.771484, T: 5874, Avg. loss: 184107304066927275540480.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 128382860285983.91, NNZs: 911, Bias: -2014508838727.969238, T: 6853, Avg. loss: 88393694912964074668032.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 128398581277260.19, NNZs: 911, Bias: -2019218635932.290283, T: 7832, Avg. loss: 48937021385571288743936.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 128244066313922.53, NNZs: 911, Bias: -2063679009988.690430, T: 8811, Avg. loss: 56320005067350827073536.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 128155000419781.16, NNZs: 911, Bias: -2095218551240.711426, T: 9790, Avg. loss: 22726079738686969217024.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 128209939239113.00, NNZs: 911, Bias: -2078097633307.900635, T: 10769, Avg. loss: 11820854715919192031232.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 128170910086882.14, NNZs: 911, Bias: -2090605769089.806885, T: 11748, Avg. loss: 18766825187222765961216.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 128137375009147.22, NNZs: 911, Bias: -2096400378117.822510, T: 12727, Avg. loss: 16046096633017235668992.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 128055347228311.75, NNZs: 911, Bias: -2133080051487.183350, T: 13706, Avg. loss: 7871631788018984026112.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 128024736097256.75, NNZs: 911, Bias: -2148414375704.914795, T: 14685, Avg. loss: 2837201048625960976384.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 128017678941080.61, NNZs: 911, Bias: -2152452709801.685303, T: 15664, Avg. loss: 644717375418383073280.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 128016736836669.27, NNZs: 911, Bias: -2152639509746.252197, T: 16643, Avg. loss: 355521316898967322624.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 128011962039806.92, NNZs: 911, Bias: -2155270959400.793213, T: 17622, Avg. loss: 159724186318609612800.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 128011792552255.98, NNZs: 911, Bias: -2155337716952.003662, T: 18601, Avg. loss: 30915338533891960832.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 128010709743895.12, NNZs: 911, Bias: -2155969247879.700928, T: 19580, Avg. loss: 5398938963795400704.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 128010326505181.55, NNZs: 911, Bias: -2156196155840.968750, T: 20559, Avg. loss: 280021518355062752.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 128010371859870.89, NNZs: 911, Bias: -2156169227988.466064, T: 21538, Avg. loss: 5164647093054186.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 128010345528107.81, NNZs: 911, Bias: -2156184858935.237793, T: 22517, Avg. loss: 902389283068739.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 128010344001518.94, NNZs: 911, Bias: -2156185765237.646729, T: 23496, Avg. loss: 7751486543581.396484\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 128010343688175.50, NNZs: 911, Bias: -2156185951265.704590, T: 24475, Avg. loss: 199156161103.467987\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 128010343638787.97, NNZs: 911, Bias: -2156185980586.526367, T: 25454, Avg. loss: 5705149477.008120\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 128010343630892.48, NNZs: 911, Bias: -2156185985273.985352, T: 26433, Avg. loss: 348297566.607602\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 128010343627788.28, NNZs: 911, Bias: -2156185987116.907715, T: 27412, Avg. loss: 23485227.066308\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 128010343627050.47, NNZs: 911, Bias: -2156185987554.957520, T: 28391, Avg. loss: 1618543.880702\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 128010343626801.66, NNZs: 911, Bias: -2156185987702.671631, T: 29370, Avg. loss: 128415.724878\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 128010343626718.45, NNZs: 911, Bias: -2156185987752.071289, T: 30349, Avg. loss: 12288.841992\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 128010343626697.53, NNZs: 911, Bias: -2156185987764.479736, T: 31328, Avg. loss: 1163.627558\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 128010343626694.17, NNZs: 911, Bias: -2156185987766.480225, T: 32307, Avg. loss: 153.670118\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 128010343626690.80, NNZs: 911, Bias: -2156185987768.481201, T: 33286, Avg. loss: 33.915814\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 128010343626689.70, NNZs: 911, Bias: -2156185987769.137451, T: 34265, Avg. loss: 4.570735\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 128010343626689.42, NNZs: 911, Bias: -2156185987769.305908, T: 35244, Avg. loss: 0.719578\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 128010343626689.20, NNZs: 911, Bias: -2156185987769.441650, T: 36223, Avg. loss: 0.170685\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 128010343626689.27, NNZs: 911, Bias: -2156185987769.415771, T: 37202, Avg. loss: 0.059519\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 128010343626689.25, NNZs: 911, Bias: -2156185987769.420410, T: 38181, Avg. loss: 0.058819\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 128010343626689.27, NNZs: 911, Bias: -2156185987769.423096, T: 39160, Avg. loss: 0.056957\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 128010343626689.25, NNZs: 911, Bias: -2156185987769.433838, T: 40139, Avg. loss: 0.052643\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 128010343626689.28, NNZs: 911, Bias: -2156185987769.414062, T: 41118, Avg. loss: 0.052397\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 128010343626689.25, NNZs: 911, Bias: -2156185987769.436279, T: 42097, Avg. loss: 0.051540\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 128010343626689.27, NNZs: 911, Bias: -2156185987769.423828, T: 43076, Avg. loss: 0.046168\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 128010343626689.28, NNZs: 911, Bias: -2156185987769.422119, T: 44055, Avg. loss: 0.048837\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 128010343626689.27, NNZs: 911, Bias: -2156185987769.442139, T: 45034, Avg. loss: 0.044166\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 128010343626689.33, NNZs: 911, Bias: -2156185987769.411621, T: 46013, Avg. loss: 0.040461\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 128010343626689.28, NNZs: 911, Bias: -2156185987769.436035, T: 46992, Avg. loss: 0.050452\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 128010343626689.30, NNZs: 911, Bias: -2156185987769.419189, T: 47971, Avg. loss: 0.042733\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 128010343626689.25, NNZs: 911, Bias: -2156185987769.453125, T: 48950, Avg. loss: 0.042957\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 128010343626689.28, NNZs: 911, Bias: -2156185987769.417236, T: 49929, Avg. loss: 0.035805\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 128010343626689.30, NNZs: 911, Bias: -2156185987769.407959, T: 50908, Avg. loss: 0.044129\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 128010343626689.20, NNZs: 911, Bias: -2156185987769.450928, T: 51887, Avg. loss: 0.043950\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 128010343626689.22, NNZs: 911, Bias: -2156185987769.441895, T: 52866, Avg. loss: 0.035576\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 128010343626689.22, NNZs: 911, Bias: -2156185987769.432373, T: 53845, Avg. loss: 0.039583\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 128010343626689.19, NNZs: 911, Bias: -2156185987769.434326, T: 54824, Avg. loss: 0.040801\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 56 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 153483844.49, NNZs: 874, Bias: -862257.809634, T: 979, Avg. loss: 508885522852.789917\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 27046507414.14, NNZs: 1018, Bias: -185051150.922213, T: 1958, Avg. loss: 38293859647425664.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 263564745770.21, NNZs: 1112, Bias: -3072836282.524309, T: 2937, Avg. loss: 7478713778830274560.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 506803202801.62, NNZs: 1154, Bias: -3069873608.095572, T: 3916, Avg. loss: 47852422628198146048.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 614674178899.06, NNZs: 1171, Bias: -2199852793.731386, T: 4895, Avg. loss: 60434552400256245760.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 682598884248.71, NNZs: 1184, Bias: -3402788912.492350, T: 5874, Avg. loss: 95066055248284172288.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 25582324747421.78, NNZs: 871, Bias: 17137549403.747555, T: 979, Avg. loss: 24747390384831406800896.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 44275956953181.62, NNZs: 1027, Bias: -85936169261.195312, T: 1958, Avg. loss: 227277878839978735173632.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 45990515117723.04, NNZs: 1060, Bias: -121918185110.547714, T: 2937, Avg. loss: 132472589050537897361408.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 46407487937796.38, NNZs: 1069, Bias: -256600653249.776459, T: 3916, Avg. loss: 37495034687672035573760.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 46472338507522.27, NNZs: 1071, Bias: -170089512835.964142, T: 4895, Avg. loss: 19950076822486423437312.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 46497984000210.20, NNZs: 1075, Bias: -156131196798.524292, T: 5874, Avg. loss: 9044846556538064601088.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 46476306517344.11, NNZs: 1075, Bias: -185678020964.006226, T: 6853, Avg. loss: 7177642639510253600768.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 46485764245893.30, NNZs: 1075, Bias: -173757106191.186432, T: 7832, Avg. loss: 4792384381929855647744.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 46476723198390.06, NNZs: 1075, Bias: -165132513567.545044, T: 8811, Avg. loss: 4480395864793679396864.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 46472755670077.34, NNZs: 1075, Bias: -157822340979.687225, T: 9790, Avg. loss: 2465291910477464993792.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 46459857159080.62, NNZs: 1075, Bias: -175515517589.590393, T: 10769, Avg. loss: 1698094887221220605952.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 46458042928893.66, NNZs: 1075, Bias: -166692359374.812866, T: 11748, Avg. loss: 929740013541059461120.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 46454753951389.09, NNZs: 1075, Bias: -170499598138.142731, T: 12727, Avg. loss: 265746866639163293696.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 46454358215048.29, NNZs: 1075, Bias: -170666763248.561707, T: 13706, Avg. loss: 41337484222651187200.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 46454327309410.23, NNZs: 1075, Bias: -170657483227.684509, T: 14685, Avg. loss: 3833408482393610240.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 46454436617499.38, NNZs: 1075, Bias: -170354243095.681915, T: 15664, Avg. loss: 209055156320504928.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 46454403339919.79, NNZs: 1075, Bias: -170444475640.158417, T: 16643, Avg. loss: 17813622337056348.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 46454404895486.16, NNZs: 1075, Bias: -170440231711.978607, T: 17622, Avg. loss: 152659582813510.937500\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 46454404862326.38, NNZs: 1075, Bias: -170440322068.141571, T: 18601, Avg. loss: 799480030674.207886\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 46454404860902.12, NNZs: 1075, Bias: -170440325950.024567, T: 19580, Avg. loss: 214084866.588688\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 46454404860927.07, NNZs: 1075, Bias: -170440325882.015533, T: 20559, Avg. loss: 294952.948124\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 46454404860917.39, NNZs: 1075, Bias: -170440325908.392883, T: 21538, Avg. loss: 2262.115709\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 46454404860917.73, NNZs: 1075, Bias: -170440325907.490295, T: 22517, Avg. loss: 32.517231\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 46454404860917.70, NNZs: 1075, Bias: -170440325907.581146, T: 23496, Avg. loss: 0.639041\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 46454404860917.69, NNZs: 1075, Bias: -170440325907.595154, T: 24475, Avg. loss: 0.054354\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 46454404860917.70, NNZs: 1075, Bias: -170440325907.560303, T: 25454, Avg. loss: 0.045593\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 46454404860917.68, NNZs: 1075, Bias: -170440325907.604706, T: 26433, Avg. loss: 0.039786\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 46454404860917.67, NNZs: 1075, Bias: -170440325907.627380, T: 27412, Avg. loss: 0.041282\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 46454404860917.70, NNZs: 1075, Bias: -170440325907.578949, T: 28391, Avg. loss: 0.037957\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 46454404860917.70, NNZs: 1075, Bias: -170440325907.578461, T: 29370, Avg. loss: 0.040158\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 46454404860917.68, NNZs: 1075, Bias: -170440325907.614502, T: 30349, Avg. loss: 0.035612\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 46454404860917.69, NNZs: 1075, Bias: -170440325907.607391, T: 31328, Avg. loss: 0.035717\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 46454404860917.69, NNZs: 1075, Bias: -170440325907.608093, T: 32307, Avg. loss: 0.036338\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 46454404860917.69, NNZs: 1075, Bias: -170440325907.606140, T: 33286, Avg. loss: 0.034941\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 46454404860917.69, NNZs: 1075, Bias: -170440325907.602509, T: 34265, Avg. loss: 0.034010\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 46454404860917.69, NNZs: 1075, Bias: -170440325907.605865, T: 35244, Avg. loss: 0.033274\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 36 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 94849909092690.89, NNZs: 746, Bias: 113449975112.426422, T: 979, Avg. loss: 1172698229862969647300608.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 109998307306975.97, NNZs: 924, Bias: -634329337929.943115, T: 1958, Avg. loss: 2885903687576953634684928.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 111161283117545.80, NNZs: 939, Bias: -1007226503727.791626, T: 2937, Avg. loss: 572032694585290223779840.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 111299881047513.94, NNZs: 940, Bias: -1147405182592.203369, T: 3916, Avg. loss: 262745494132169249914880.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 111290033327914.95, NNZs: 940, Bias: -1158286715235.660156, T: 4895, Avg. loss: 168735598730726059540480.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 110996603117773.09, NNZs: 942, Bias: -1270691590879.503418, T: 5874, Avg. loss: 118928366164137996189696.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 110765840563275.28, NNZs: 945, Bias: -1362895162140.954102, T: 6853, Avg. loss: 33305289016039666876416.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 110884395241209.59, NNZs: 945, Bias: -1331434256044.163818, T: 7832, Avg. loss: 6003837999391950503936.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 110933743014684.05, NNZs: 945, Bias: -1304175698009.460693, T: 8811, Avg. loss: 11848477143162930855936.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 110963104563080.47, NNZs: 945, Bias: -1286011408897.286377, T: 9790, Avg. loss: 24056700110218018684928.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 110934078773273.59, NNZs: 945, Bias: -1288831373808.604248, T: 10769, Avg. loss: 15146194758603020173312.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 110875870109108.22, NNZs: 945, Bias: -1322723552631.634766, T: 11748, Avg. loss: 8421069267112443248640.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 110886615740484.03, NNZs: 945, Bias: -1316214864797.421875, T: 12727, Avg. loss: 3336693294959035940864.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 110840945688104.55, NNZs: 945, Bias: -1342773387904.415771, T: 13706, Avg. loss: 4167942466421752070144.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 110843220719773.88, NNZs: 945, Bias: -1341386414951.632080, T: 14685, Avg. loss: 749299890688362676224.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 110835873558140.73, NNZs: 945, Bias: -1346132587791.960938, T: 15664, Avg. loss: 949905785817329303552.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 110832176647254.36, NNZs: 945, Bias: -1348664161560.620117, T: 16643, Avg. loss: 230929820242888851456.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 110831231353699.12, NNZs: 945, Bias: -1349266677531.553223, T: 17622, Avg. loss: 78240160878611841024.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 110830158313144.20, NNZs: 945, Bias: -1350105387766.145264, T: 18601, Avg. loss: 15359132833921673216.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 110829431901603.48, NNZs: 945, Bias: -1350691526819.409180, T: 19580, Avg. loss: 2943512533420750336.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 110829356425040.56, NNZs: 945, Bias: -1350753400614.466064, T: 20559, Avg. loss: 15956745181146690.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 110829357025681.69, NNZs: 945, Bias: -1350752907668.526123, T: 21538, Avg. loss: 119286096395627.125000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 110829356405441.39, NNZs: 945, Bias: -1350753416573.300537, T: 22517, Avg. loss: 754562291885.516602\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 110829356353497.08, NNZs: 945, Bias: -1350753459193.599365, T: 23496, Avg. loss: 5828243743.852105\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 110829356347685.62, NNZs: 945, Bias: -1350753463961.912598, T: 24475, Avg. loss: 87996402.229150\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 110829356346905.66, NNZs: 945, Bias: -1350753464601.869629, T: 25454, Avg. loss: 2647543.027932\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 110829356346752.41, NNZs: 945, Bias: -1350753464727.614258, T: 26433, Avg. loss: 112525.068005\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 110829356346723.98, NNZs: 945, Bias: -1350753464750.932373, T: 27412, Avg. loss: 6576.793878\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 110829356346716.20, NNZs: 945, Bias: -1350753464757.312256, T: 28391, Avg. loss: 519.026192\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 110829356346713.22, NNZs: 945, Bias: -1350753464759.769287, T: 29370, Avg. loss: 50.179658\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 110829356346712.42, NNZs: 945, Bias: -1350753464760.406738, T: 30349, Avg. loss: 4.899124\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 110829356346712.19, NNZs: 945, Bias: -1350753464760.607422, T: 31328, Avg. loss: 0.566658\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 110829356346712.11, NNZs: 945, Bias: -1350753464760.672852, T: 32307, Avg. loss: 0.098248\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 110829356346712.14, NNZs: 945, Bias: -1350753464760.648438, T: 33286, Avg. loss: 0.049835\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 110829356346712.12, NNZs: 945, Bias: -1350753464760.658691, T: 34265, Avg. loss: 0.050689\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 110829356346712.11, NNZs: 945, Bias: -1350753464760.660645, T: 35244, Avg. loss: 0.049887\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 110829356346712.08, NNZs: 945, Bias: -1350753464760.680420, T: 36223, Avg. loss: 0.048717\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 110829356346712.09, NNZs: 945, Bias: -1350753464760.661865, T: 37202, Avg. loss: 0.043199\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 110829356346712.11, NNZs: 945, Bias: -1350753464760.645752, T: 38181, Avg. loss: 0.042064\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 110829356346712.09, NNZs: 945, Bias: -1350753464760.668213, T: 39160, Avg. loss: 0.048411\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 110829356346712.09, NNZs: 945, Bias: -1350753464760.667725, T: 40139, Avg. loss: 0.032962\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 110829356346712.08, NNZs: 945, Bias: -1350753464760.667725, T: 41118, Avg. loss: 0.044310\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 110829356346712.08, NNZs: 945, Bias: -1350753464760.685059, T: 42097, Avg. loss: 0.036751\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 110829356346712.11, NNZs: 945, Bias: -1350753464760.650391, T: 43076, Avg. loss: 0.033846\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 110829356346712.08, NNZs: 945, Bias: -1350753464760.667480, T: 44055, Avg. loss: 0.039505\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 110829356346712.06, NNZs: 945, Bias: -1350753464760.689453, T: 45034, Avg. loss: 0.032547\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 46 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 11006111449.21, NNZs: 907, Bias: -63943262.090103, T: 979, Avg. loss: 2603486235396031.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1898364427316.02, NNZs: 1067, Bias: -22221298553.147739, T: 1958, Avg. loss: 198762619302517211136.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 10019449609035.02, NNZs: 1145, Bias: -44080815130.606514, T: 2937, Avg. loss: 17637335861509062918144.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 12393050193398.27, NNZs: 1171, Bias: -96820603448.664551, T: 3916, Avg. loss: 24695143221052787654656.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 12754777731272.55, NNZs: 1187, Bias: -58100499590.599236, T: 4895, Avg. loss: 13048938774044598075392.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 12780393373859.61, NNZs: 1189, Bias: -75951733911.321960, T: 5874, Avg. loss: 6801734566352421650432.000000\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 24000100234651.23, NNZs: 824, Bias: 31164372606.625584, T: 979, Avg. loss: 11732238527706360184832.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 43757973276895.66, NNZs: 1008, Bias: -83673207895.863998, T: 1958, Avg. loss: 284489686390517964734464.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 45470049059281.40, NNZs: 1051, Bias: -86931654708.831192, T: 2937, Avg. loss: 108328852371310538391552.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 45776560467008.94, NNZs: 1063, Bias: -197906488330.988495, T: 3916, Avg. loss: 39475223259466761764864.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 45868690737139.12, NNZs: 1070, Bias: -144362886642.200592, T: 4895, Avg. loss: 14384325158657570373632.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 45846771161278.37, NNZs: 1072, Bias: -117415407680.340698, T: 5874, Avg. loss: 11223349871618784493568.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 45845999222124.98, NNZs: 1076, Bias: -117517717430.924606, T: 6853, Avg. loss: 6333439589137159028736.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 45841434924133.31, NNZs: 1074, Bias: -126259066501.871155, T: 7832, Avg. loss: 4867902487300112646144.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 45837715797902.61, NNZs: 1074, Bias: -119822147841.916519, T: 8811, Avg. loss: 5174281913808981065728.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 45833193354830.59, NNZs: 1074, Bias: -107610956709.390533, T: 9790, Avg. loss: 3501599690635780030464.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 45813295340556.29, NNZs: 1074, Bias: -149818669676.148285, T: 10769, Avg. loss: 1946467741409588281344.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 45811008491395.74, NNZs: 1074, Bias: -133389060799.983109, T: 11748, Avg. loss: 1395175047989942812672.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 45807763730485.52, NNZs: 1074, Bias: -135069473825.459030, T: 12727, Avg. loss: 380468661239351672832.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 45807768138580.36, NNZs: 1074, Bias: -133345775580.381821, T: 13706, Avg. loss: 61843639525757665280.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 45807617704967.75, NNZs: 1074, Bias: -133611535653.304199, T: 14685, Avg. loss: 8003520961209359360.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 45807664818268.63, NNZs: 1074, Bias: -133442113753.440643, T: 15664, Avg. loss: 235153620905867872.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 45807649971664.27, NNZs: 1074, Bias: -133492474068.212891, T: 16643, Avg. loss: 16998665738238872.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 45807648967581.94, NNZs: 1074, Bias: -133495912105.919113, T: 17622, Avg. loss: 207602995067937.718750\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 45807649281947.75, NNZs: 1074, Bias: -133494833240.440079, T: 18601, Avg. loss: 4176397468109.568359\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 45807649271993.07, NNZs: 1074, Bias: -133494867398.676422, T: 19580, Avg. loss: 11126517018.204210\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 45807649272925.55, NNZs: 1074, Bias: -133494864198.922165, T: 20559, Avg. loss: 68682090.272132\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 45807649272872.49, NNZs: 1074, Bias: -133494864381.023956, T: 21538, Avg. loss: 472079.141321\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 45807649272871.66, NNZs: 1074, Bias: -133494864383.887527, T: 22517, Avg. loss: 16149.804704\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 45807649272872.76, NNZs: 1074, Bias: -133494864380.111923, T: 23496, Avg. loss: 664.344177\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 45807649272872.90, NNZs: 1074, Bias: -133494864379.646103, T: 24475, Avg. loss: 33.917163\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 45807649272872.80, NNZs: 1074, Bias: -133494864379.967743, T: 25454, Avg. loss: 3.313813\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 45807649272872.77, NNZs: 1074, Bias: -133494864380.064468, T: 26433, Avg. loss: 0.312740\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 45807649272872.77, NNZs: 1074, Bias: -133494864380.085663, T: 27412, Avg. loss: 0.059523\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 45807649272872.78, NNZs: 1074, Bias: -133494864380.047363, T: 28391, Avg. loss: 0.041386\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 45807649272872.77, NNZs: 1074, Bias: -133494864380.063400, T: 29370, Avg. loss: 0.044992\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 45807649272872.77, NNZs: 1074, Bias: -133494864380.085510, T: 30349, Avg. loss: 0.039905\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 45807649272872.77, NNZs: 1074, Bias: -133494864380.083603, T: 31328, Avg. loss: 0.039659\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 45807649272872.78, NNZs: 1074, Bias: -133494864380.069046, T: 32307, Avg. loss: 0.040397\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 45807649272872.77, NNZs: 1074, Bias: -133494864380.073029, T: 33286, Avg. loss: 0.037946\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 45807649272872.77, NNZs: 1074, Bias: -133494864380.083862, T: 34265, Avg. loss: 0.037538\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 45807649272872.77, NNZs: 1074, Bias: -133494864380.078934, T: 35244, Avg. loss: 0.037332\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 45807649272872.78, NNZs: 1074, Bias: -133494864380.072540, T: 36223, Avg. loss: 0.036502\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 45807649272872.78, NNZs: 1074, Bias: -133494864380.068405, T: 37202, Avg. loss: 0.036961\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 45807649272872.77, NNZs: 1074, Bias: -133494864380.085953, T: 38181, Avg. loss: 0.035627\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 39 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 94356569003166.23, NNZs: 748, Bias: -265058793761.004883, T: 979, Avg. loss: 1136921413417836413452288.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 111181370049507.64, NNZs: 939, Bias: -1078452404228.344482, T: 1958, Avg. loss: 3053883585570079034572800.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 113198022425180.27, NNZs: 961, Bias: -1293272592778.437012, T: 2937, Avg. loss: 634056803100656303865856.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 112865942844313.95, NNZs: 973, Bias: -1481344480866.071777, T: 3916, Avg. loss: 247075214980449654800384.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 112607253328038.42, NNZs: 973, Bias: -1617324975949.556885, T: 4895, Avg. loss: 81131444786259109609472.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 112680299355054.47, NNZs: 973, Bias: -1638092746712.952148, T: 5874, Avg. loss: 46858528341168347938816.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 112639752114113.41, NNZs: 973, Bias: -1620995123059.268799, T: 6853, Avg. loss: 60182194895183188852736.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 112484457507637.44, NNZs: 973, Bias: -1657280330100.059326, T: 7832, Avg. loss: 48425503004919112663040.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 112374544555978.08, NNZs: 973, Bias: -1703427701348.419434, T: 8811, Avg. loss: 17869858962154557800448.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 112435582875585.83, NNZs: 973, Bias: -1687215840621.347656, T: 9790, Avg. loss: 8682485512826443005952.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 112437962680130.30, NNZs: 973, Bias: -1681850481050.371338, T: 10769, Avg. loss: 11081101261829291638784.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 112386585457973.47, NNZs: 973, Bias: -1704850464159.386719, T: 11748, Avg. loss: 9700522763123832127488.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 112380528370693.80, NNZs: 973, Bias: -1705465925785.775146, T: 12727, Avg. loss: 7624680849345707245568.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 112335088939761.00, NNZs: 973, Bias: -1726807234681.623535, T: 13706, Avg. loss: 3522822856793752338432.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 112335897312801.97, NNZs: 973, Bias: -1725804199769.988281, T: 14685, Avg. loss: 603598942339624206336.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 112326083593762.05, NNZs: 973, Bias: -1731892790994.987793, T: 15664, Avg. loss: 431024878770877562880.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 112318105585580.31, NNZs: 973, Bias: -1736431640308.456787, T: 16643, Avg. loss: 358042724992516554752.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 112322098967787.16, NNZs: 973, Bias: -1733875184732.464844, T: 17622, Avg. loss: 64287343944961433600.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 112319568914089.17, NNZs: 973, Bias: -1735418678823.119385, T: 18601, Avg. loss: 43203308419028418560.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 112318082397644.73, NNZs: 973, Bias: -1736357310152.461182, T: 19580, Avg. loss: 9258374341656555520.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 112318209685122.92, NNZs: 973, Bias: -1736274990781.363037, T: 20559, Avg. loss: 45977556060334248.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 112318149489043.67, NNZs: 973, Bias: -1736313871704.792236, T: 21538, Avg. loss: 21810901159770004.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 112318132024995.97, NNZs: 973, Bias: -1736325166660.675537, T: 22517, Avg. loss: 823569538303921.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 112318131393604.88, NNZs: 973, Bias: -1736325575078.581543, T: 23496, Avg. loss: 5391004905510.310547\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 112318130793301.11, NNZs: 973, Bias: -1736325963395.888428, T: 24475, Avg. loss: 994379060188.524536\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 112318130841245.80, NNZs: 973, Bias: -1736325932381.712646, T: 25454, Avg. loss: 52428366309.351852\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 112318130769696.19, NNZs: 973, Bias: -1736325978665.132324, T: 26433, Avg. loss: 15463011107.350670\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 112318130756126.23, NNZs: 973, Bias: -1736325987443.166992, T: 27412, Avg. loss: 638232835.481287\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 112318130754234.48, NNZs: 973, Bias: -1736325988666.881836, T: 28391, Avg. loss: 43800698.710576\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 112318130753322.88, NNZs: 973, Bias: -1736325989256.580078, T: 29370, Avg. loss: 3359299.341784\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 112318130753059.27, NNZs: 973, Bias: -1736325989427.094238, T: 30349, Avg. loss: 321138.264779\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 112318130752982.70, NNZs: 973, Bias: -1736325989476.615234, T: 31328, Avg. loss: 34456.563357\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 112318130752960.11, NNZs: 973, Bias: -1736325989491.232422, T: 32307, Avg. loss: 4193.838313\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 112318130752951.25, NNZs: 973, Bias: -1736325989496.954590, T: 33286, Avg. loss: 585.127537\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 112318130752947.83, NNZs: 973, Bias: -1736325989499.170410, T: 34265, Avg. loss: 94.235382\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 112318130752947.06, NNZs: 973, Bias: -1736325989499.659668, T: 35244, Avg. loss: 17.022804\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 112318130752946.36, NNZs: 973, Bias: -1736325989500.113037, T: 36223, Avg. loss: 3.397699\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 112318130752946.28, NNZs: 973, Bias: -1736325989500.156250, T: 37202, Avg. loss: 0.668290\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 112318130752946.12, NNZs: 973, Bias: -1736325989500.251953, T: 38181, Avg. loss: 0.201030\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 112318130752946.06, NNZs: 973, Bias: -1736325989500.289062, T: 39160, Avg. loss: 0.065757\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 112318130752946.09, NNZs: 973, Bias: -1736325989500.271240, T: 40139, Avg. loss: 0.034595\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 112318130752946.09, NNZs: 973, Bias: -1736325989500.273438, T: 41118, Avg. loss: 0.045595\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 112318130752946.06, NNZs: 973, Bias: -1736325989500.284424, T: 42097, Avg. loss: 0.035725\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 112318130752946.06, NNZs: 973, Bias: -1736325989500.275146, T: 43076, Avg. loss: 0.036057\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 112318130752946.05, NNZs: 973, Bias: -1736325989500.291504, T: 44055, Avg. loss: 0.036843\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 112318130752946.02, NNZs: 973, Bias: -1736325989500.292480, T: 45034, Avg. loss: 0.032100\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 112318130752946.02, NNZs: 973, Bias: -1736325989500.282715, T: 46013, Avg. loss: 0.032519\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 112318130752946.02, NNZs: 972, Bias: -1736325989500.282715, T: 46992, Avg. loss: 0.035582\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 112318130752945.98, NNZs: 972, Bias: -1736325989500.284424, T: 47971, Avg. loss: 0.034037\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 112318130752945.97, NNZs: 972, Bias: -1736325989500.310059, T: 48950, Avg. loss: 0.029886\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 112318130752946.00, NNZs: 972, Bias: -1736325989500.275391, T: 49929, Avg. loss: 0.025688\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 112318130752945.98, NNZs: 972, Bias: -1736325989500.267090, T: 50908, Avg. loss: 0.032929\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 112318130752945.97, NNZs: 972, Bias: -1736325989500.300537, T: 51887, Avg. loss: 0.032824\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 112318130752945.97, NNZs: 972, Bias: -1736325989500.291260, T: 52866, Avg. loss: 0.025605\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 112318130752946.00, NNZs: 972, Bias: -1736325989500.273926, T: 53845, Avg. loss: 0.027113\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 112318130752945.98, NNZs: 972, Bias: -1736325989500.288574, T: 54824, Avg. loss: 0.032707\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 56 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 4858463779.20, NNZs: 907, Bias: -7908879.081954, T: 979, Avg. loss: 554804370579030.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 906437891862.81, NNZs: 1081, Bias: -3032748651.844269, T: 1958, Avg. loss: 47697698189400940544.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 9518833952101.01, NNZs: 1147, Bias: -76794345489.146973, T: 2937, Avg. loss: 17116036200767902711808.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 11848859103479.78, NNZs: 1186, Bias: -86921232416.839828, T: 3916, Avg. loss: 22178026967385650495488.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 12073615818796.43, NNZs: 1197, Bias: -35317668282.096405, T: 4895, Avg. loss: 9708496905801155215360.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 12102693567042.05, NNZs: 1203, Bias: -51130083327.003693, T: 5874, Avg. loss: 2343846556263414235136.000000\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 34096357677818.16, NNZs: 862, Bias: -39980101581.587112, T: 979, Avg. loss: 31467099841199710666752.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 45915296436998.63, NNZs: 993, Bias: 54614667131.962234, T: 1958, Avg. loss: 325516225166619143307264.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47094498256274.42, NNZs: 1027, Bias: -20145751699.132347, T: 2937, Avg. loss: 98815672101795696476160.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 47236672718087.18, NNZs: 1057, Bias: -193161651235.404083, T: 3916, Avg. loss: 40357895829195967168512.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 47362735890416.29, NNZs: 1062, Bias: -169279933699.975647, T: 4895, Avg. loss: 17037560829345762639872.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 47382566108660.05, NNZs: 1066, Bias: -104055578628.582870, T: 5874, Avg. loss: 11319635171062644211712.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 47357153130099.84, NNZs: 1066, Bias: -130419327102.523895, T: 6853, Avg. loss: 7657943893458381963264.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 47361342719632.29, NNZs: 1066, Bias: -118826803149.881134, T: 7832, Avg. loss: 4131238367101277700096.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 47357961795567.51, NNZs: 1066, Bias: -120085513389.947540, T: 8811, Avg. loss: 4028052064350137483264.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 47354334976211.14, NNZs: 1066, Bias: -105957814518.854218, T: 9790, Avg. loss: 2568463457416198488064.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 47339885293850.25, NNZs: 1066, Bias: -136867980242.092117, T: 10769, Avg. loss: 1609415526738216353792.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 47338584641789.55, NNZs: 1066, Bias: -122989226670.342529, T: 11748, Avg. loss: 974999949997752320000.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 47336355768173.11, NNZs: 1066, Bias: -123603809434.645416, T: 12727, Avg. loss: 296219274638934736896.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 47336401079835.62, NNZs: 1066, Bias: -121866749194.657135, T: 13706, Avg. loss: 51433240255586852864.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 47336198729779.59, NNZs: 1066, Bias: -122460402840.312363, T: 14685, Avg. loss: 5562477075536631808.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 47336258077894.40, NNZs: 1066, Bias: -122224388305.840805, T: 15664, Avg. loss: 177948533595370368.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 47336242024352.31, NNZs: 1066, Bias: -122285946970.304642, T: 16643, Avg. loss: 15649442010793162.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 47336242739283.42, NNZs: 1066, Bias: -122283173872.978409, T: 17622, Avg. loss: 145551742626530.156250\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 47336242750389.23, NNZs: 1064, Bias: -122283130829.045090, T: 18601, Avg. loss: 1347764126280.083740\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 47336242748796.00, NNZs: 1064, Bias: -122283136996.498184, T: 19580, Avg. loss: 248202083.870239\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 47336242748853.83, NNZs: 1062, Bias: -122283136772.625763, T: 20559, Avg. loss: 242155.965083\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 47336242748850.95, NNZs: 1062, Bias: -122283136783.782608, T: 21538, Avg. loss: 1614.294688\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.051804, T: 22517, Avg. loss: 14.323176\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 47336242748850.61, NNZs: 1061, Bias: -122283136785.112091, T: 23496, Avg. loss: 0.454578\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.059540, T: 24475, Avg. loss: 0.050133\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.052948, T: 25454, Avg. loss: 0.047530\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.086273, T: 26433, Avg. loss: 0.040878\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.095657, T: 27412, Avg. loss: 0.039765\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.057358, T: 28391, Avg. loss: 0.036919\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.065903, T: 29370, Avg. loss: 0.041306\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.093948, T: 30349, Avg. loss: 0.036149\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.085617, T: 31328, Avg. loss: 0.035689\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.078445, T: 32307, Avg. loss: 0.036645\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.084656, T: 33286, Avg. loss: 0.034371\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.086853, T: 34265, Avg. loss: 0.034080\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.088684, T: 35244, Avg. loss: 0.033642\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.083649, T: 36223, Avg. loss: 0.032681\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.078033, T: 37202, Avg. loss: 0.032782\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 47336242748850.62, NNZs: 1061, Bias: -122283136785.089508, T: 38181, Avg. loss: 0.032649\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 39 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 93260530543630.39, NNZs: 750, Bias: -141081135596.125458, T: 979, Avg. loss: 1142286439472262123880448.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 108708779574913.09, NNZs: 920, Bias: -1326456115394.234375, T: 1958, Avg. loss: 2871676785008449859616768.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 111154141730609.52, NNZs: 942, Bias: -1223852582643.806641, T: 2937, Avg. loss: 600076163951488682426368.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 111586282158724.98, NNZs: 962, Bias: -1433509145355.310303, T: 3916, Avg. loss: 261881302258523335294976.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 111485914412043.22, NNZs: 962, Bias: -1516075861182.147705, T: 4895, Avg. loss: 197185862007351259693056.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 111113969555253.08, NNZs: 962, Bias: -1612517160919.229004, T: 5874, Avg. loss: 174528372928317260562432.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 110797543798955.03, NNZs: 962, Bias: -1703336713583.597168, T: 6853, Avg. loss: 68322273501365718220800.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 110693779086490.91, NNZs: 962, Bias: -1745200116058.996582, T: 7832, Avg. loss: 24106619963540833828864.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 110749489762059.98, NNZs: 962, Bias: -1730785903340.547363, T: 8811, Avg. loss: 18425469175434831724544.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 110716669612567.05, NNZs: 962, Bias: -1734286141073.665771, T: 9790, Avg. loss: 34168047074806543155200.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 110680273131133.41, NNZs: 962, Bias: -1740450435304.012939, T: 10769, Avg. loss: 18468808871723919212544.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 110539680227499.12, NNZs: 962, Bias: -1804082930646.497559, T: 11748, Avg. loss: 10461178399284671807488.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 110600153648747.53, NNZs: 962, Bias: -1774960937917.053223, T: 12727, Avg. loss: 3823837940381202776064.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 110560461902346.33, NNZs: 962, Bias: -1792475077170.885742, T: 13706, Avg. loss: 8822040817815281205248.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 110548561803376.38, NNZs: 962, Bias: -1796298631049.420410, T: 14685, Avg. loss: 2432208665335864229888.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 110529630006193.67, NNZs: 962, Bias: -1806721940494.326660, T: 15664, Avg. loss: 849701717258100932608.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 110525005060700.12, NNZs: 962, Bias: -1809245300032.282715, T: 16643, Avg. loss: 374543504877827719168.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 110524154883298.59, NNZs: 962, Bias: -1809479168814.336426, T: 17622, Avg. loss: 204859152065315536896.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 110521377891436.52, NNZs: 962, Bias: -1811076671313.051025, T: 18601, Avg. loss: 52171726040409612288.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 110520040709343.72, NNZs: 962, Bias: -1811880609351.128906, T: 19580, Avg. loss: 4574525286065479168.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 110519830178727.41, NNZs: 962, Bias: -1812008741291.756592, T: 20559, Avg. loss: 105774111234039008.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 110519823552584.38, NNZs: 962, Bias: -1812012782186.923584, T: 21538, Avg. loss: 258316187575730.406250\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 110519822847470.91, NNZs: 962, Bias: -1812013212252.616943, T: 22517, Avg. loss: 1203833477790.963623\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 110519822816045.55, NNZs: 962, Bias: -1812013231419.797852, T: 23496, Avg. loss: 19154574150.633774\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 110519822775159.58, NNZs: 962, Bias: -1812013256357.296387, T: 24475, Avg. loss: 6302842180.270529\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 110519822770926.55, NNZs: 962, Bias: -1812013258939.134033, T: 25454, Avg. loss: 295638036.791620\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 110519822768919.67, NNZs: 962, Bias: -1812013260163.184570, T: 26433, Avg. loss: 12263858.125165\n",
            "Total training time: 0.02 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 28\n",
            "Norm: 110519822768565.91, NNZs: 962, Bias: -1812013260378.952637, T: 27412, Avg. loss: 678667.727634\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 110519822768501.25, NNZs: 962, Bias: -1812013260418.396973, T: 28391, Avg. loss: 50432.795375\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 110519822768454.52, NNZs: 962, Bias: -1812013260446.902100, T: 29370, Avg. loss: 6586.990188\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 110519822768445.03, NNZs: 962, Bias: -1812013260452.677734, T: 30349, Avg. loss: 585.321646\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 110519822768441.78, NNZs: 962, Bias: -1812013260454.660400, T: 31328, Avg. loss: 64.649866\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 110519822768440.16, NNZs: 962, Bias: -1812013260455.657227, T: 32307, Avg. loss: 8.338682\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 110519822768439.58, NNZs: 962, Bias: -1812013260456.005127, T: 33286, Avg. loss: 0.944623\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 110519822768439.48, NNZs: 962, Bias: -1812013260456.071777, T: 34265, Avg. loss: 0.133054\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 110519822768439.48, NNZs: 962, Bias: -1812013260456.076172, T: 35244, Avg. loss: 0.058316\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 110519822768439.39, NNZs: 962, Bias: -1812013260456.127197, T: 36223, Avg. loss: 0.047488\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 110519822768439.47, NNZs: 962, Bias: -1812013260456.083984, T: 37202, Avg. loss: 0.042860\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 110519822768439.48, NNZs: 962, Bias: -1812013260456.082764, T: 38181, Avg. loss: 0.045421\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 110519822768439.45, NNZs: 962, Bias: -1812013260456.101074, T: 39160, Avg. loss: 0.045314\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 110519822768439.48, NNZs: 962, Bias: -1812013260456.072021, T: 40139, Avg. loss: 0.029442\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 110519822768439.47, NNZs: 962, Bias: -1812013260456.085693, T: 41118, Avg. loss: 0.046546\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 110519822768439.45, NNZs: 962, Bias: -1812013260456.103516, T: 42097, Avg. loss: 0.035966\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 110519822768439.50, NNZs: 962, Bias: -1812013260456.069092, T: 43076, Avg. loss: 0.033927\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 110519822768439.47, NNZs: 962, Bias: -1812013260456.084473, T: 44055, Avg. loss: 0.039425\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 110519822768439.42, NNZs: 962, Bias: -1812013260456.095703, T: 45034, Avg. loss: 0.033797\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 46 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 37199630842.42, NNZs: 924, Bias: -96831745.118590, T: 979, Avg. loss: 29436160681796480.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 3591437264075.72, NNZs: 1091, Bias: -16112501402.421171, T: 1958, Avg. loss: 706495482129151688704.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 12368534959266.23, NNZs: 1159, Bias: -99028849776.149597, T: 2937, Avg. loss: 26279258394743950802944.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 14185122604240.77, NNZs: 1200, Bias: -114489969498.668793, T: 3916, Avg. loss: 32090532815587569041408.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 14485334296216.03, NNZs: 1209, Bias: -85798643706.482635, T: 4895, Avg. loss: 16406467702900354187264.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 14491611253938.23, NNZs: 1209, Bias: -91477087202.948486, T: 5874, Avg. loss: 5132638066584757207040.000000\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 37976202585134.35, NNZs: 852, Bias: 94419810919.060318, T: 979, Avg. loss: 91467121202877861724160.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 48955823545905.52, NNZs: 986, Bias: -89636099925.260010, T: 1958, Avg. loss: 321072526166718842667008.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 50764368067663.25, NNZs: 1027, Bias: -115070464490.469238, T: 2937, Avg. loss: 117854803287749750161408.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 50948256360149.76, NNZs: 1036, Bias: -199876893787.966187, T: 3916, Avg. loss: 45767537778739334086656.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 51057074335404.48, NNZs: 1036, Bias: -121000510813.608459, T: 4895, Avg. loss: 13755446124049101488128.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 51060497201051.56, NNZs: 1044, Bias: -140527380121.636383, T: 5874, Avg. loss: 9895095737762808070144.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 51049740884439.52, NNZs: 1052, Bias: -161526860405.667908, T: 6853, Avg. loss: 8789217905247706415104.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 51050479636009.93, NNZs: 1052, Bias: -138553417970.179474, T: 7832, Avg. loss: 4215075892830830854144.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 51051104960397.11, NNZs: 1052, Bias: -139689386650.977356, T: 8811, Avg. loss: 4093866256769800470528.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 51048172046585.41, NNZs: 1052, Bias: -128586477862.575424, T: 9790, Avg. loss: 3108836956127683411968.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 51029217379455.52, NNZs: 1052, Bias: -171018561498.354126, T: 10769, Avg. loss: 1829636216293421744128.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 51027672922978.05, NNZs: 1052, Bias: -155921790510.586334, T: 11748, Avg. loss: 1311174585706507403264.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 51025209901823.34, NNZs: 1052, Bias: -155671460855.427765, T: 12727, Avg. loss: 390576634162680496128.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 51024931936376.48, NNZs: 1052, Bias: -154843496028.820282, T: 13706, Avg. loss: 72639947808217071616.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 51024573015736.54, NNZs: 1052, Bias: -155802567225.606140, T: 14685, Avg. loss: 8067899764104195072.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 51024662862096.36, NNZs: 1052, Bias: -155496029119.612915, T: 15664, Avg. loss: 424605951029105920.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 51024652035304.74, NNZs: 1052, Bias: -155531107436.408813, T: 16643, Avg. loss: 14832314608671296.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 51024651275790.45, NNZs: 1052, Bias: -155533595431.150848, T: 17622, Avg. loss: 121357981538744.796875\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 51024651496294.64, NNZs: 1052, Bias: -155532871979.530884, T: 18601, Avg. loss: 1939421316880.750000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 51024651480940.96, NNZs: 1052, Bias: -155532922349.132721, T: 19580, Avg. loss: 6244255290.349359\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 51024651482442.46, NNZs: 1052, Bias: -155532917423.270233, T: 20559, Avg. loss: 64884355.195845\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 51024651482299.50, NNZs: 1052, Bias: -155532917892.247589, T: 21538, Avg. loss: 498540.375926\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 51024651482304.49, NNZs: 1052, Bias: -155532917875.849365, T: 22517, Avg. loss: 6095.394697\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 51024651482304.05, NNZs: 1052, Bias: -155532917877.299316, T: 23496, Avg. loss: 128.553683\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 51024651482303.91, NNZs: 1052, Bias: -155532917877.783081, T: 24475, Avg. loss: 4.922407\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 51024651482303.88, NNZs: 1052, Bias: -155532917877.851105, T: 25454, Avg. loss: 0.272885\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 51024651482303.87, NNZs: 1052, Bias: -155532917877.876801, T: 26433, Avg. loss: 0.048710\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 51024651482303.87, NNZs: 1052, Bias: -155532917877.873505, T: 27412, Avg. loss: 0.039849\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 51024651482303.88, NNZs: 1052, Bias: -155532917877.847412, T: 28391, Avg. loss: 0.036733\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 51024651482303.88, NNZs: 1052, Bias: -155532917877.854889, T: 29370, Avg. loss: 0.040162\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 51024651482303.88, NNZs: 1052, Bias: -155532917877.865143, T: 30349, Avg. loss: 0.037182\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 51024651482303.87, NNZs: 1052, Bias: -155532917877.865662, T: 31328, Avg. loss: 0.034854\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 51024651482303.86, NNZs: 1052, Bias: -155532917877.857727, T: 32307, Avg. loss: 0.036114\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 51024651482303.85, NNZs: 1052, Bias: -155532917877.864136, T: 33286, Avg. loss: 0.034805\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 51024651482303.85, NNZs: 1052, Bias: -155532917877.860535, T: 34265, Avg. loss: 0.034698\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 51024651482303.84, NNZs: 1052, Bias: -155532917877.876923, T: 35244, Avg. loss: 0.033658\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 51024651482303.84, NNZs: 1052, Bias: -155532917877.872070, T: 36223, Avg. loss: 0.032716\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 51024651482303.84, NNZs: 1052, Bias: -155532917877.865570, T: 37202, Avg. loss: 0.032987\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 51024651482303.84, NNZs: 1052, Bias: -155532917877.872223, T: 38181, Avg. loss: 0.032868\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 51024651482303.85, NNZs: 1052, Bias: -155532917877.853760, T: 39160, Avg. loss: 0.031065\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 51024651482303.85, NNZs: 1052, Bias: -155532917877.863647, T: 40139, Avg. loss: 0.032172\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 51024651482303.85, NNZs: 1052, Bias: -155532917877.859955, T: 41118, Avg. loss: 0.030775\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 51024651482303.85, NNZs: 1052, Bias: -155532917877.852997, T: 42097, Avg. loss: 0.030211\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 51024651482303.85, NNZs: 1052, Bias: -155532917877.851624, T: 43076, Avg. loss: 0.030289\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 51024651482303.85, NNZs: 1052, Bias: -155532917877.857422, T: 44055, Avg. loss: 0.029749\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 45 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 87684378150807.50, NNZs: 757, Bias: -213624793580.017914, T: 979, Avg. loss: 828602442557202794283008.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 109092495547234.50, NNZs: 946, Bias: -1177364110682.140381, T: 1958, Avg. loss: 2983108180052935671021568.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 110591106630206.84, NNZs: 961, Bias: -1419848417381.642334, T: 2937, Avg. loss: 743079699289464215961600.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 110938335383371.16, NNZs: 965, Bias: -1471952344193.702881, T: 3916, Avg. loss: 303729298825913382731776.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 110531318537404.41, NNZs: 965, Bias: -1580551305195.818115, T: 4895, Avg. loss: 228090112048785317167104.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 110266451106226.56, NNZs: 965, Bias: -1621558684203.013428, T: 5874, Avg. loss: 135435208265346358181888.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 110186065519110.77, NNZs: 966, Bias: -1648072469694.706787, T: 6853, Avg. loss: 68016544114156010209280.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 110107295325918.22, NNZs: 966, Bias: -1665931008717.357666, T: 7832, Avg. loss: 50545575580741444567040.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 109916818179682.11, NNZs: 966, Bias: -1738737141611.808350, T: 8811, Avg. loss: 27904242964819655786496.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 109946392446269.00, NNZs: 968, Bias: -1717881860304.444580, T: 9790, Avg. loss: 11486805231786367385600.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 109901078371457.44, NNZs: 968, Bias: -1739503420275.619629, T: 10769, Avg. loss: 7201414615719311572992.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 109898795959916.36, NNZs: 968, Bias: -1742358964592.338623, T: 11748, Avg. loss: 8384150234928466886656.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 109912411029667.69, NNZs: 968, Bias: -1732250091515.011719, T: 12727, Avg. loss: 4263112584095980847104.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 109862565709907.00, NNZs: 968, Bias: -1757675580688.410889, T: 13706, Avg. loss: 3669820528508630204416.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 109850872226744.05, NNZs: 968, Bias: -1763268786032.698486, T: 14685, Avg. loss: 1106028420031060639744.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 109843877275713.44, NNZs: 968, Bias: -1767179674902.643066, T: 15664, Avg. loss: 502541732077501874176.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 109836153004791.38, NNZs: 968, Bias: -1771498671403.988770, T: 16643, Avg. loss: 266561539215486812160.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 109838168495205.89, NNZs: 968, Bias: -1770161500949.476562, T: 17622, Avg. loss: 105580488412313387008.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 109834816330630.06, NNZs: 968, Bias: -1772127017160.457275, T: 18601, Avg. loss: 50494113843271786496.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 109833793544855.58, NNZs: 968, Bias: -1772753417826.190674, T: 19580, Avg. loss: 2932760852236535808.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 109833511692570.45, NNZs: 968, Bias: -1772927471773.631348, T: 20559, Avg. loss: 216529904806807712.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 109833474800310.03, NNZs: 968, Bias: -1772950315116.030762, T: 21538, Avg. loss: 4258430649749179.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 109833476919459.25, NNZs: 968, Bias: -1772949002156.714844, T: 22517, Avg. loss: 169290224805497.937500\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 109833473273617.75, NNZs: 968, Bias: -1772951260589.965088, T: 23496, Avg. loss: 56826223378457.164062\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 109833472825854.12, NNZs: 968, Bias: -1772951537972.926514, T: 24475, Avg. loss: 1897181818976.748779\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 109833472668106.36, NNZs: 968, Bias: -1772951635696.757568, T: 25454, Avg. loss: 52411777934.163345\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 109833472649793.02, NNZs: 968, Bias: -1772951647041.780518, T: 26433, Avg. loss: 2670664451.482315\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 109833472643817.11, NNZs: 968, Bias: -1772951650743.822021, T: 27412, Avg. loss: 115980976.562345\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 109833472642501.75, NNZs: 968, Bias: -1772951651558.677490, T: 28391, Avg. loss: 7386569.800298\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 109833472642169.66, NNZs: 968, Bias: -1772951651764.414062, T: 29370, Avg. loss: 628895.812380\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 109833472642026.20, NNZs: 968, Bias: -1772951651853.286133, T: 30349, Avg. loss: 60584.362727\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 109833472641979.56, NNZs: 968, Bias: -1772951651882.184326, T: 31328, Avg. loss: 6111.654216\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 109833472641967.77, NNZs: 968, Bias: -1772951651889.481201, T: 32307, Avg. loss: 768.710895\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 109833472641963.17, NNZs: 968, Bias: -1772951651892.325195, T: 33286, Avg. loss: 111.850315\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 109833472641961.28, NNZs: 968, Bias: -1772951651893.499268, T: 34265, Avg. loss: 16.981573\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 109833472641960.73, NNZs: 968, Bias: -1772951651893.837158, T: 35244, Avg. loss: 3.004871\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 109833472641960.41, NNZs: 968, Bias: -1772951651894.045410, T: 36223, Avg. loss: 0.595932\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 109833472641960.39, NNZs: 968, Bias: -1772951651894.060791, T: 37202, Avg. loss: 0.152240\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 109833472641960.36, NNZs: 968, Bias: -1772951651894.080566, T: 38181, Avg. loss: 0.075739\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 109833472641960.33, NNZs: 968, Bias: -1772951651894.093018, T: 39160, Avg. loss: 0.056733\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 109833472641960.36, NNZs: 968, Bias: -1772951651894.069092, T: 40139, Avg. loss: 0.044514\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 109833472641960.33, NNZs: 968, Bias: -1772951651894.091064, T: 41118, Avg. loss: 0.059839\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 109833472641960.28, NNZs: 968, Bias: -1772951651894.117676, T: 42097, Avg. loss: 0.049698\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 109833472641960.38, NNZs: 968, Bias: -1772951651894.053223, T: 43076, Avg. loss: 0.041310\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 109833472641960.30, NNZs: 968, Bias: -1772951651894.098633, T: 44055, Avg. loss: 0.057538\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 109833472641960.30, NNZs: 968, Bias: -1772951651894.085938, T: 45034, Avg. loss: 0.039685\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 109833472641960.27, NNZs: 968, Bias: -1772951651894.090332, T: 46013, Avg. loss: 0.046532\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 109833472641960.22, NNZs: 968, Bias: -1772951651894.099365, T: 46992, Avg. loss: 0.047274\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 109833472641960.25, NNZs: 968, Bias: -1772951651894.082031, T: 47971, Avg. loss: 0.043722\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 109833472641960.22, NNZs: 968, Bias: -1772951651894.102295, T: 48950, Avg. loss: 0.040511\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 109833472641960.23, NNZs: 968, Bias: -1772951651894.091553, T: 49929, Avg. loss: 0.039167\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 51 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2712140689.72, NNZs: 852, Bias: -7271118.597991, T: 979, Avg. loss: 169688905556767.968750\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1212482880144.52, NNZs: 1040, Bias: -1636152718.523950, T: 1958, Avg. loss: 77784884791546347520.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 12890204475679.71, NNZs: 1120, Bias: -103134813049.428040, T: 2937, Avg. loss: 30920633901201097752576.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 14310540053085.39, NNZs: 1148, Bias: -89539555392.574173, T: 3916, Avg. loss: 32315342539375197028352.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 14452544107059.79, NNZs: 1163, Bias: -26787643580.741730, T: 4895, Avg. loss: 9955659068144745447424.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 14469111284869.60, NNZs: 1163, Bias: -61720192272.917114, T: 5874, Avg. loss: 4084489502505159884800.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 30111189736124.87, NNZs: 852, Bias: -97510886571.039413, T: 980, Avg. loss: 68067019012624177168384.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 45086043903691.95, NNZs: 984, Bias: -27706194335.263649, T: 1960, Avg. loss: 327904917334646897770496.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 46710716929308.51, NNZs: 1040, Bias: -148948450867.976379, T: 2940, Avg. loss: 114500662670278604619776.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 47127397864657.23, NNZs: 1061, Bias: -151890181742.180786, T: 3920, Avg. loss: 54625682847901702684672.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 47246503278679.81, NNZs: 1064, Bias: -145944825099.264832, T: 4900, Avg. loss: 19283695586683887026176.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 47270932605624.16, NNZs: 1065, Bias: -132410886845.073059, T: 5880, Avg. loss: 11763056635726358642688.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 47283377436919.64, NNZs: 1066, Bias: -162744617064.587708, T: 6860, Avg. loss: 7785156261099218141184.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 47282424079874.74, NNZs: 1065, Bias: -131175479274.183060, T: 7840, Avg. loss: 5582881882261869821952.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 47281895048796.08, NNZs: 1065, Bias: -138394488344.806976, T: 8820, Avg. loss: 3733728997301510733824.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 47274794947152.61, NNZs: 1065, Bias: -135003968019.950928, T: 9800, Avg. loss: 3433717801638355271680.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 47264216912829.61, NNZs: 1065, Bias: -147216128027.600586, T: 10780, Avg. loss: 2229477217282058354688.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 47260806507263.79, NNZs: 1065, Bias: -142305395932.609253, T: 11760, Avg. loss: 942542511145924034560.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 47260166121458.06, NNZs: 1065, Bias: -136174314542.811523, T: 12740, Avg. loss: 361628975865253134336.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 47258220731963.50, NNZs: 1065, Bias: -140837667853.816193, T: 13720, Avg. loss: 67486261767019421696.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 47258549579022.37, NNZs: 1065, Bias: -139391508208.261230, T: 14700, Avg. loss: 11510178123213086720.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 47258475175035.84, NNZs: 1065, Bias: -139619093184.701904, T: 15680, Avg. loss: 759601150596046080.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 47258465142560.80, NNZs: 1065, Bias: -139652605521.691162, T: 16660, Avg. loss: 13194277520255026.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 47258467637680.99, NNZs: 1065, Bias: -139644151818.368805, T: 17640, Avg. loss: 293089260635498.187500\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 47258467481074.58, NNZs: 1065, Bias: -139644681608.197723, T: 18620, Avg. loss: 5733967566602.571289\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 47258467468019.67, NNZs: 1065, Bias: -139644725788.047729, T: 19600, Avg. loss: 13640940367.325521\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 47258467468361.71, NNZs: 1065, Bias: -139644724630.508087, T: 20580, Avg. loss: 21547787.467095\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 47258467468331.70, NNZs: 1065, Bias: -139644724732.063293, T: 21560, Avg. loss: 274292.968723\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 47258467468330.76, NNZs: 1065, Bias: -139644724735.264313, T: 22540, Avg. loss: 5855.955962\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 47258467468331.04, NNZs: 1065, Bias: -139644724734.321167, T: 23520, Avg. loss: 193.069552\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 47258467468331.04, NNZs: 1065, Bias: -139644724734.288391, T: 24500, Avg. loss: 8.575243\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 47258467468331.06, NNZs: 1065, Bias: -139644724734.226379, T: 25480, Avg. loss: 0.504359\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 47258467468331.07, NNZs: 1065, Bias: -139644724734.195099, T: 26460, Avg. loss: 0.065151\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 47258467468331.05, NNZs: 1065, Bias: -139644724734.246918, T: 27440, Avg. loss: 0.036978\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 47258467468331.06, NNZs: 1065, Bias: -139644724734.222565, T: 28420, Avg. loss: 0.040194\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 47258467468331.06, NNZs: 1065, Bias: -139644724734.223328, T: 29400, Avg. loss: 0.040125\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 47258467468331.06, NNZs: 1065, Bias: -139644724734.219696, T: 30380, Avg. loss: 0.038605\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 47258467468331.06, NNZs: 1065, Bias: -139644724734.226318, T: 31360, Avg. loss: 0.036361\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 47258467468331.07, NNZs: 1065, Bias: -139644724734.210022, T: 32340, Avg. loss: 0.035639\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 33 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 94548111374829.08, NNZs: 777, Bias: 207438992251.393494, T: 980, Avg. loss: 922556402785401591300096.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 110436755049000.02, NNZs: 940, Bias: -847281273351.541870, T: 1960, Avg. loss: 2695012275690175469715456.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 111620731876045.06, NNZs: 964, Bias: -1149493631148.215088, T: 2940, Avg. loss: 749508555696804238196736.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 111090001175514.80, NNZs: 964, Bias: -1435705867977.124512, T: 3920, Avg. loss: 261836382703276267667456.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 111304193078514.72, NNZs: 964, Bias: -1462371169676.793213, T: 4900, Avg. loss: 120475185051604434812928.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 111453706903308.86, NNZs: 966, Bias: -1456026657326.210693, T: 5880, Avg. loss: 100600237770921134260224.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 111333082117016.38, NNZs: 966, Bias: -1483338925184.773682, T: 6860, Avg. loss: 70056535642884806803456.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 111180447284876.83, NNZs: 966, Bias: -1534249064880.288086, T: 7840, Avg. loss: 60342585632599589257216.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm: 111171880662106.55, NNZs: 966, Bias: -1548028587362.632324, T: 8820, Avg. loss: 25520329339617406877696.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 111140288385722.92, NNZs: 966, Bias: -1550304200752.920166, T: 9800, Avg. loss: 27685966554220961202176.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 111050584433324.67, NNZs: 966, Bias: -1588617225654.631592, T: 10780, Avg. loss: 19132873558876867264512.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 110992464200357.39, NNZs: 966, Bias: -1609995733879.235840, T: 11760, Avg. loss: 13510213851518154571776.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 110992387909360.81, NNZs: 966, Bias: -1609918798565.267578, T: 12740, Avg. loss: 5092344054875627192320.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 110975941045771.77, NNZs: 966, Bias: -1615213156951.052002, T: 13720, Avg. loss: 3924153094454724001792.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 110943410136440.31, NNZs: 966, Bias: -1634040082108.901367, T: 14700, Avg. loss: 1861321606352306700288.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 110953018206089.20, NNZs: 966, Bias: -1628086346569.296875, T: 15680, Avg. loss: 399214331143958167552.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 110940501009083.81, NNZs: 966, Bias: -1635067912171.037598, T: 16660, Avg. loss: 718684388575927468032.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 110935681084190.91, NNZs: 966, Bias: -1638139881745.690918, T: 17640, Avg. loss: 128006220186695942144.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 110933852156658.08, NNZs: 966, Bias: -1639340947649.250000, T: 18620, Avg. loss: 20065453449145475072.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 110934655318164.97, NNZs: 966, Bias: -1638800999882.404297, T: 19600, Avg. loss: 602521229592442496.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 110934075254563.78, NNZs: 966, Bias: -1639188643841.231934, T: 20580, Avg. loss: 1792161245432604416.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 110933944690392.17, NNZs: 966, Bias: -1639276691245.661133, T: 21560, Avg. loss: 108687263081987760.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 110933936146873.45, NNZs: 966, Bias: -1639282461725.224121, T: 22540, Avg. loss: 5842030995588100.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 110933920261309.19, NNZs: 966, Bias: -1639293209995.562012, T: 23520, Avg. loss: 631534513515806.625000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 110933920386582.69, NNZs: 966, Bias: -1639293125173.529541, T: 24500, Avg. loss: 23395564442009.789062\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 110933919006716.02, NNZs: 966, Bias: -1639294058942.110352, T: 25480, Avg. loss: 4199293203642.066406\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 110933919030921.27, NNZs: 966, Bias: -1639294042561.594971, T: 26460, Avg. loss: 172281188136.286652\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 110933918880155.58, NNZs: 966, Bias: -1639294144587.271484, T: 27440, Avg. loss: 44217992894.507240\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 110933918856485.89, NNZs: 966, Bias: -1639294160604.956787, T: 28420, Avg. loss: 1914736352.361877\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 110933918850955.34, NNZs: 966, Bias: -1639294164347.580811, T: 29400, Avg. loss: 130734384.787240\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 110933918849282.88, NNZs: 966, Bias: -1639294165479.375977, T: 30380, Avg. loss: 11618836.211850\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 110933918848808.17, NNZs: 966, Bias: -1639294165800.606445, T: 31360, Avg. loss: 1171745.357925\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 110933918848662.78, NNZs: 966, Bias: -1639294165899.012207, T: 32340, Avg. loss: 142901.556100\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 110933918848616.28, NNZs: 966, Bias: -1639294165930.480713, T: 33320, Avg. loss: 21778.981838\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 110933918848598.66, NNZs: 966, Bias: -1639294165942.408691, T: 34300, Avg. loss: 3588.473006\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 110933918848590.39, NNZs: 966, Bias: -1639294165948.000732, T: 35280, Avg. loss: 633.259568\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 110933918848589.48, NNZs: 966, Bias: -1639294165948.609619, T: 36260, Avg. loss: 127.663736\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 110933918848587.56, NNZs: 966, Bias: -1639294165949.917236, T: 37240, Avg. loss: 28.026242\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 110933918848587.12, NNZs: 966, Bias: -1639294165950.214111, T: 38220, Avg. loss: 6.090842\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 110933918848586.86, NNZs: 966, Bias: -1639294165950.399902, T: 39200, Avg. loss: 1.381185\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 110933918848586.81, NNZs: 966, Bias: -1639294165950.443115, T: 40180, Avg. loss: 0.377731\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 110933918848586.77, NNZs: 966, Bias: -1639294165950.457520, T: 41160, Avg. loss: 0.133878\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 110933918848586.72, NNZs: 966, Bias: -1639294165950.477783, T: 42140, Avg. loss: 0.069449\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 110933918848586.64, NNZs: 966, Bias: -1639294165950.517578, T: 43120, Avg. loss: 0.052103\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 110933918848586.67, NNZs: 966, Bias: -1639294165950.483887, T: 44100, Avg. loss: 0.042123\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 110933918848586.66, NNZs: 966, Bias: -1639294165950.487549, T: 45080, Avg. loss: 0.049446\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 110933918848586.62, NNZs: 966, Bias: -1639294165950.509766, T: 46060, Avg. loss: 0.046623\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 110933918848586.62, NNZs: 966, Bias: -1639294165950.505127, T: 47040, Avg. loss: 0.044400\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 110933918848586.66, NNZs: 966, Bias: -1639294165950.489014, T: 48020, Avg. loss: 0.042492\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 110933918848586.66, NNZs: 966, Bias: -1639294165950.493164, T: 49000, Avg. loss: 0.047018\n",
            "Total training time: 0.06 seconds.\n",
            "Convergence after 50 epochs took 0.06 seconds\n",
            "-- Epoch 1\n",
            "Norm: 7193369135.31, NNZs: 841, Bias: -2445163.782297, T: 980, Avg. loss: 1214462537354479.500000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 6661913774021.83, NNZs: 1053, Bias: -69808783800.672180, T: 1960, Avg. loss: 2480756000374801301504.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 15185818950978.29, NNZs: 1135, Bias: -84199774260.567032, T: 2940, Avg. loss: 59493351247776250331136.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 16437261179241.09, NNZs: 1157, Bias: -95339170383.315628, T: 3920, Avg. loss: 37973488245045842673664.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 16338728213912.15, NNZs: 1164, Bias: -32575485638.899349, T: 4900, Avg. loss: 15631792029733461426176.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 16381099084444.17, NNZs: 1170, Bias: -64116370610.037849, T: 5880, Avg. loss: 4963919007790456111104.000000\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 47302162685775.86, NNZs: 844, Bias: -126492847048.000916, T: 980, Avg. loss: 215278657113967084699648.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 53598603153751.40, NNZs: 936, Bias: -49385727801.392555, T: 1960, Avg. loss: 344456744895993025134592.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 54403055097840.94, NNZs: 979, Bias: -177006010397.608154, T: 2940, Avg. loss: 141759872827018706944000.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 54472077093169.16, NNZs: 984, Bias: -136677533994.523315, T: 3920, Avg. loss: 50384123323155686096896.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 54525632347292.19, NNZs: 989, Bias: -195664682958.098450, T: 4900, Avg. loss: 16450158405959286784000.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 54555051077329.67, NNZs: 994, Bias: -129278988579.082001, T: 5880, Avg. loss: 11707755609166197030912.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 54531068608312.66, NNZs: 995, Bias: -203942486719.307281, T: 6860, Avg. loss: 6945678110285567623168.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 54535156341930.52, NNZs: 995, Bias: -140602942581.707764, T: 7840, Avg. loss: 5623295795368854290432.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 54536218660840.47, NNZs: 994, Bias: -139497848319.815948, T: 8820, Avg. loss: 3626922998345648570368.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 54526585187866.21, NNZs: 994, Bias: -139104846179.078430, T: 9800, Avg. loss: 3833152742229849669632.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 54516655213071.23, NNZs: 994, Bias: -154494471683.423340, T: 10780, Avg. loss: 2108306915502550941696.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 54511870014145.73, NNZs: 994, Bias: -153644423175.257416, T: 11760, Avg. loss: 1074551510491204681728.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 54512689485945.55, NNZs: 994, Bias: -144386807340.064392, T: 12740, Avg. loss: 290071705205156544512.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 54511004797839.68, NNZs: 994, Bias: -148971733418.475830, T: 13720, Avg. loss: 61188138057802506240.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 54511308865287.95, NNZs: 994, Bias: -147572257897.862854, T: 14700, Avg. loss: 9988918596671365120.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 54511185370533.51, NNZs: 994, Bias: -147994154007.998138, T: 15680, Avg. loss: 1107924726031331968.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 54511173195687.02, NNZs: 994, Bias: -148038350882.086334, T: 16660, Avg. loss: 20335931818545904.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 54511177107991.99, NNZs: 994, Bias: -148023931412.101166, T: 17640, Avg. loss: 399571378200970.125000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 54511176556123.37, NNZs: 994, Bias: -148025963364.210632, T: 18620, Avg. loss: 10443614406250.929688\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 54511176591356.24, NNZs: 994, Bias: -148025833615.071075, T: 19600, Avg. loss: 78410345217.827362\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 54511176587552.77, NNZs: 994, Bias: -148025847621.464233, T: 20580, Avg. loss: 975765196.227377\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 54511176587938.52, NNZs: 994, Bias: -148025846200.918243, T: 21560, Avg. loss: 12908323.886589\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 54511176587868.80, NNZs: 994, Bias: -148025846457.664764, T: 22540, Avg. loss: 523161.198744\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 54511176587865.51, NNZs: 994, Bias: -148025846469.812286, T: 23520, Avg. loss: 20525.933681\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 54511176587863.86, NNZs: 994, Bias: -148025846475.882507, T: 24500, Avg. loss: 1131.878749\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 54511176587863.84, NNZs: 994, Bias: -148025846475.941681, T: 25480, Avg. loss: 79.901748\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 54511176587863.76, NNZs: 994, Bias: -148025846476.215088, T: 26460, Avg. loss: 7.655224\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 54511176587863.75, NNZs: 994, Bias: -148025846476.292603, T: 27440, Avg. loss: 0.938731\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 54511176587863.75, NNZs: 994, Bias: -148025846476.298553, T: 28420, Avg. loss: 0.147447\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 54511176587863.75, NNZs: 994, Bias: -148025846476.308838, T: 29400, Avg. loss: 0.052124\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 54511176587863.74, NNZs: 994, Bias: -148025846476.310272, T: 30380, Avg. loss: 0.044189\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 54511176587863.75, NNZs: 994, Bias: -148025846476.315887, T: 31360, Avg. loss: 0.040537\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 54511176587863.75, NNZs: 994, Bias: -148025846476.300659, T: 32340, Avg. loss: 0.040009\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 54511176587863.77, NNZs: 994, Bias: -148025846476.289917, T: 33320, Avg. loss: 0.038112\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 54511176587863.75, NNZs: 994, Bias: -148025846476.319855, T: 34300, Avg. loss: 0.036794\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 54511176587863.76, NNZs: 994, Bias: -148025846476.302704, T: 35280, Avg. loss: 0.037219\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 54511176587863.75, NNZs: 994, Bias: -148025846476.309387, T: 36260, Avg. loss: 0.038496\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 54511176587863.76, NNZs: 994, Bias: -148025846476.304749, T: 37240, Avg. loss: 0.037003\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 54511176587863.76, NNZs: 994, Bias: -148025846476.297546, T: 38220, Avg. loss: 0.035621\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 54511176587863.76, NNZs: 994, Bias: -148025846476.316620, T: 39200, Avg. loss: 0.034923\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 54511176587863.76, NNZs: 994, Bias: -148025846476.317657, T: 40180, Avg. loss: 0.034687\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 54511176587863.76, NNZs: 994, Bias: -148025846476.313690, T: 41160, Avg. loss: 0.034047\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 54511176587863.76, NNZs: 994, Bias: -148025846476.307800, T: 42140, Avg. loss: 0.033783\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 54511176587863.76, NNZs: 994, Bias: -148025846476.316437, T: 43120, Avg. loss: 0.033477\n",
            "Total training time: 0.06 seconds.\n",
            "Convergence after 44 epochs took 0.06 seconds\n",
            "-- Epoch 1\n",
            "Norm: 92078435572644.27, NNZs: 718, Bias: 405886088388.704956, T: 980, Avg. loss: 1014131741489220926046208.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 106322253724825.92, NNZs: 938, Bias: -888293238068.202148, T: 1960, Avg. loss: 3263943125863722089709568.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 107766532127643.08, NNZs: 948, Bias: -1035643858334.249023, T: 2940, Avg. loss: 621750562480266248454144.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 107080664078446.38, NNZs: 953, Bias: -1471642400730.981201, T: 3920, Avg. loss: 359446586777552138797056.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 106789021816567.31, NNZs: 955, Bias: -1549570472883.954590, T: 4900, Avg. loss: 216370590186554974535680.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 106825948525603.20, NNZs: 956, Bias: -1534758532316.427979, T: 5880, Avg. loss: 115500962108859148664832.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 106642634777305.70, NNZs: 958, Bias: -1571034902289.338867, T: 6860, Avg. loss: 80473705657307827798016.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 106528419409094.25, NNZs: 958, Bias: -1618179321207.615234, T: 7840, Avg. loss: 45420666834228986511360.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 106598376225615.45, NNZs: 959, Bias: -1592066692778.383545, T: 8820, Avg. loss: 32400293849717232631808.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 106459284270841.58, NNZs: 959, Bias: -1640788140468.734131, T: 9800, Avg. loss: 31501436216133220827136.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 106388242440473.62, NNZs: 959, Bias: -1671799522569.787354, T: 10780, Avg. loss: 15231998045740567887872.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 106344708635029.78, NNZs: 959, Bias: -1686081766138.873291, T: 11760, Avg. loss: 10533372112280366350336.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 106337014912866.83, NNZs: 959, Bias: -1691573722645.055664, T: 12740, Avg. loss: 4533079773256603926528.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 106323705892840.06, NNZs: 959, Bias: -1696092717422.513428, T: 13720, Avg. loss: 2713677612828322168832.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 106301316207965.14, NNZs: 959, Bias: -1708501435878.087402, T: 14700, Avg. loss: 1664704673633553088512.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 106311446494195.67, NNZs: 959, Bias: -1702344890934.908447, T: 15680, Avg. loss: 666254419085746765824.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 106294099383175.91, NNZs: 959, Bias: -1711399535525.795898, T: 16660, Avg. loss: 881282147574651813888.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 106293065306338.53, NNZs: 959, Bias: -1712079544859.177734, T: 17640, Avg. loss: 155468337948722823168.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 106287162654599.55, NNZs: 959, Bias: -1715426462070.484863, T: 18620, Avg. loss: 119594268208639574016.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 106288296250398.42, NNZs: 959, Bias: -1714723966134.425781, T: 19600, Avg. loss: 4006443515099135488.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 106286924473656.81, NNZs: 959, Bias: -1715562796143.076660, T: 20580, Avg. loss: 4179215453705086976.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 106286742210173.56, NNZs: 959, Bias: -1715675195318.837891, T: 21560, Avg. loss: 189842768298282688.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 106286725940030.38, NNZs: 959, Bias: -1715685254593.037109, T: 22540, Avg. loss: 10292893419998844.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 106286703540239.59, NNZs: 959, Bias: -1715699128485.530518, T: 23520, Avg. loss: 992951936003343.875000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 106286703608458.00, NNZs: 959, Bias: -1715699086153.395508, T: 24500, Avg. loss: 36982650966954.265625\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 106286701753730.95, NNZs: 959, Bias: -1715700235130.644287, T: 25480, Avg. loss: 6147686360842.719727\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 106286701764385.83, NNZs: 959, Bias: -1715700228529.461670, T: 26460, Avg. loss: 244642477134.649231\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 106286701577031.72, NNZs: 959, Bias: -1715700344594.166016, T: 27440, Avg. loss: 57546529203.431351\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 106286701546890.20, NNZs: 959, Bias: -1715700363266.671387, T: 28420, Avg. loss: 2438526100.463892\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 106286701540751.64, NNZs: 959, Bias: -1715700367069.480713, T: 29400, Avg. loss: 164268201.075876\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 106286701538491.56, NNZs: 959, Bias: -1715700368469.585449, T: 30380, Avg. loss: 15084680.630628\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 106286701537812.92, NNZs: 959, Bias: -1715700368890.001221, T: 31360, Avg. loss: 1379211.531741\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 106286701537621.72, NNZs: 959, Bias: -1715700369008.443359, T: 32340, Avg. loss: 152084.305444\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 106286701537568.06, NNZs: 959, Bias: -1715700369041.684814, T: 33320, Avg. loss: 21155.134048\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 106286701537553.59, NNZs: 959, Bias: -1715700369050.635742, T: 34300, Avg. loss: 3630.508483\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 106286701537545.88, NNZs: 959, Bias: -1715700369055.426025, T: 35280, Avg. loss: 646.908415\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 106286701537544.33, NNZs: 959, Bias: -1715700369056.384277, T: 36260, Avg. loss: 126.050765\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 106286701537542.58, NNZs: 959, Bias: -1715700369057.467773, T: 37240, Avg. loss: 28.158798\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 106286701537541.98, NNZs: 959, Bias: -1715700369057.824463, T: 38220, Avg. loss: 6.047816\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 106286701537541.72, NNZs: 959, Bias: -1715700369057.997803, T: 39200, Avg. loss: 1.374227\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 106286701537541.62, NNZs: 959, Bias: -1715700369058.049316, T: 40180, Avg. loss: 0.378053\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 106286701537541.59, NNZs: 959, Bias: -1715700369058.061768, T: 41160, Avg. loss: 0.135096\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 106286701537541.56, NNZs: 959, Bias: -1715700369058.083008, T: 42140, Avg. loss: 0.070670\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 106286701537541.47, NNZs: 959, Bias: -1715700369058.122559, T: 43120, Avg. loss: 0.052363\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 106286701537541.52, NNZs: 959, Bias: -1715700369058.090088, T: 44100, Avg. loss: 0.042135\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 106286701537541.50, NNZs: 959, Bias: -1715700369058.093750, T: 45080, Avg. loss: 0.049381\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 106286701537541.45, NNZs: 959, Bias: -1715700369058.115967, T: 46060, Avg. loss: 0.046803\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 106286701537541.44, NNZs: 959, Bias: -1715700369058.112061, T: 47040, Avg. loss: 0.044282\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 106286701537541.45, NNZs: 959, Bias: -1715700369058.096191, T: 48020, Avg. loss: 0.042511\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 106286701537541.45, NNZs: 959, Bias: -1715700369058.100586, T: 49000, Avg. loss: 0.047021\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 50 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 12452698715.87, NNZs: 881, Bias: -41851006.098788, T: 980, Avg. loss: 3246330439356502.500000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1557746560236.35, NNZs: 1061, Bias: -7995512212.031862, T: 1960, Avg. loss: 125192716110628077568.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 10805191486962.86, NNZs: 1147, Bias: -57706563207.385773, T: 2940, Avg. loss: 20926881662265500631040.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 13583473891082.45, NNZs: 1175, Bias: -22249135932.273659, T: 3920, Avg. loss: 37128698489082692501504.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 13681070027929.85, NNZs: 1187, Bias: -3955161205.555468, T: 4900, Avg. loss: 20839186583309294501888.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 13739752115614.60, NNZs: 1190, Bias: -12273527484.068890, T: 5880, Avg. loss: 3322921169758752079872.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 16.66, NNZs: 1172, Bias: -0.155861, T: 979, Avg. loss: 0.534443\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 17.20, NNZs: 1175, Bias: -0.126667, T: 1958, Avg. loss: 0.337347\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 17.52, NNZs: 1176, Bias: -0.114928, T: 2937, Avg. loss: 0.300766\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 17.75, NNZs: 1179, Bias: -0.107075, T: 3916, Avg. loss: 0.281487\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 17.94, NNZs: 1179, Bias: -0.100155, T: 4895, Avg. loss: 0.268141\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 18.09, NNZs: 1182, Bias: -0.096310, T: 5874, Avg. loss: 0.258506\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 18.22, NNZs: 1182, Bias: -0.092848, T: 6853, Avg. loss: 0.251050\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 18.33, NNZs: 1182, Bias: -0.090217, T: 7832, Avg. loss: 0.245162\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 18.43, NNZs: 1184, Bias: -0.088898, T: 8811, Avg. loss: 0.240290\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 18.51, NNZs: 1184, Bias: -0.086800, T: 9790, Avg. loss: 0.236022\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 18.58, NNZs: 1184, Bias: -0.085493, T: 10769, Avg. loss: 0.232393\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 18.65, NNZs: 1184, Bias: -0.084094, T: 11748, Avg. loss: 0.229176\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 18.71, NNZs: 1185, Bias: -0.082545, T: 12727, Avg. loss: 0.226237\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 18.77, NNZs: 1186, Bias: -0.082073, T: 13706, Avg. loss: 0.223625\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 18.82, NNZs: 1186, Bias: -0.081084, T: 14685, Avg. loss: 0.221277\n",
            "Total training time: 0.02 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 16\n",
            "Norm: 18.88, NNZs: 1186, Bias: -0.080426, T: 15664, Avg. loss: 0.219171\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 18.92, NNZs: 1187, Bias: -0.079513, T: 16643, Avg. loss: 0.217197\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 18.97, NNZs: 1187, Bias: -0.079078, T: 17622, Avg. loss: 0.215397\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 19.01, NNZs: 1187, Bias: -0.077934, T: 18601, Avg. loss: 0.213681\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 19.04, NNZs: 1187, Bias: -0.076798, T: 19580, Avg. loss: 0.212105\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 19.09, NNZs: 1187, Bias: -0.077022, T: 20559, Avg. loss: 0.210724\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 19.12, NNZs: 1187, Bias: -0.075939, T: 21538, Avg. loss: 0.209267\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 19.15, NNZs: 1187, Bias: -0.075485, T: 22517, Avg. loss: 0.207994\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 19.19, NNZs: 1187, Bias: -0.075017, T: 23496, Avg. loss: 0.206751\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 19.22, NNZs: 1187, Bias: -0.074522, T: 24475, Avg. loss: 0.205575\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 19.25, NNZs: 1187, Bias: -0.074128, T: 25454, Avg. loss: 0.204454\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 19.28, NNZs: 1187, Bias: -0.073790, T: 26433, Avg. loss: 0.203386\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 19.30, NNZs: 1187, Bias: -0.073134, T: 27412, Avg. loss: 0.202362\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 19.33, NNZs: 1187, Bias: -0.072615, T: 28391, Avg. loss: 0.201387\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 19.35, NNZs: 1187, Bias: -0.072047, T: 29370, Avg. loss: 0.200451\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 19.38, NNZs: 1187, Bias: -0.072188, T: 30349, Avg. loss: 0.199596\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 19.41, NNZs: 1187, Bias: -0.071707, T: 31328, Avg. loss: 0.198740\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 19.43, NNZs: 1187, Bias: -0.071242, T: 32307, Avg. loss: 0.197930\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 33 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 22.74, NNZs: 1027, Bias: -0.362012, T: 979, Avg. loss: 0.390891\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 23.02, NNZs: 1043, Bias: -0.364995, T: 1958, Avg. loss: 0.223983\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 23.21, NNZs: 1045, Bias: -0.367758, T: 2937, Avg. loss: 0.196326\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 23.36, NNZs: 1047, Bias: -0.370524, T: 3916, Avg. loss: 0.182264\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 23.47, NNZs: 1050, Bias: -0.374970, T: 4895, Avg. loss: 0.174778\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 23.55, NNZs: 1057, Bias: -0.380165, T: 5874, Avg. loss: 0.168940\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 23.63, NNZs: 1058, Bias: -0.381269, T: 6853, Avg. loss: 0.162887\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 23.69, NNZs: 1059, Bias: -0.382533, T: 7832, Avg. loss: 0.158641\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 23.74, NNZs: 1059, Bias: -0.385750, T: 8811, Avg. loss: 0.155803\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 23.80, NNZs: 1059, Bias: -0.386669, T: 9790, Avg. loss: 0.152621\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 23.84, NNZs: 1060, Bias: -0.389613, T: 10769, Avg. loss: 0.150525\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 23.89, NNZs: 1060, Bias: -0.388956, T: 11748, Avg. loss: 0.148002\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 23.93, NNZs: 1060, Bias: -0.391302, T: 12727, Avg. loss: 0.146426\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 23.96, NNZs: 1060, Bias: -0.391879, T: 13706, Avg. loss: 0.144170\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 23.99, NNZs: 1060, Bias: -0.392989, T: 14685, Avg. loss: 0.142573\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 24.02, NNZs: 1063, Bias: -0.394039, T: 15664, Avg. loss: 0.141630\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 24.05, NNZs: 1063, Bias: -0.394876, T: 16643, Avg. loss: 0.140137\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 24.08, NNZs: 1063, Bias: -0.395733, T: 17622, Avg. loss: 0.139041\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 24.10, NNZs: 1063, Bias: -0.396435, T: 18601, Avg. loss: 0.138070\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 24.13, NNZs: 1064, Bias: -0.397413, T: 19580, Avg. loss: 0.136905\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 24.15, NNZs: 1064, Bias: -0.397967, T: 20559, Avg. loss: 0.135923\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 24.17, NNZs: 1064, Bias: -0.398478, T: 21538, Avg. loss: 0.135133\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 24.19, NNZs: 1064, Bias: -0.399531, T: 22517, Avg. loss: 0.134204\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 24.21, NNZs: 1064, Bias: -0.399922, T: 23496, Avg. loss: 0.133263\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 24.23, NNZs: 1064, Bias: -0.400647, T: 24475, Avg. loss: 0.132693\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 25 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 15.77, NNZs: 1221, Bias: -0.314340, T: 979, Avg. loss: 0.604913\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 16.33, NNZs: 1222, Bias: -0.323632, T: 1958, Avg. loss: 0.391018\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 16.63, NNZs: 1223, Bias: -0.327214, T: 2937, Avg. loss: 0.355865\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 16.84, NNZs: 1223, Bias: -0.328607, T: 3916, Avg. loss: 0.338673\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 17.00, NNZs: 1223, Bias: -0.329317, T: 4895, Avg. loss: 0.327111\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 17.12, NNZs: 1223, Bias: -0.329900, T: 5874, Avg. loss: 0.318230\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 17.23, NNZs: 1223, Bias: -0.331050, T: 6853, Avg. loss: 0.311130\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 17.32, NNZs: 1223, Bias: -0.331586, T: 7832, Avg. loss: 0.306089\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 17.40, NNZs: 1223, Bias: -0.331507, T: 8811, Avg. loss: 0.301562\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 17.46, NNZs: 1223, Bias: -0.331616, T: 9790, Avg. loss: 0.297714\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 17.52, NNZs: 1223, Bias: -0.332060, T: 10769, Avg. loss: 0.294200\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 17.58, NNZs: 1223, Bias: -0.332238, T: 11748, Avg. loss: 0.291358\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 17.63, NNZs: 1223, Bias: -0.332191, T: 12727, Avg. loss: 0.289023\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 17.67, NNZs: 1223, Bias: -0.332488, T: 13706, Avg. loss: 0.286719\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 17.71, NNZs: 1223, Bias: -0.332707, T: 14685, Avg. loss: 0.284670\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 17.75, NNZs: 1223, Bias: -0.332701, T: 15664, Avg. loss: 0.282670\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 17.79, NNZs: 1223, Bias: -0.332574, T: 16643, Avg. loss: 0.280879\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 17.82, NNZs: 1223, Bias: -0.332603, T: 17622, Avg. loss: 0.279237\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 17.85, NNZs: 1223, Bias: -0.332362, T: 18601, Avg. loss: 0.277757\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17.89, NNZs: 1223, Bias: -0.332174, T: 19580, Avg. loss: 0.276259\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17.92, NNZs: 1223, Bias: -0.332063, T: 20559, Avg. loss: 0.274972\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 17.94, NNZs: 1223, Bias: -0.331989, T: 21538, Avg. loss: 0.273607\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 17.97, NNZs: 1223, Bias: -0.331919, T: 22517, Avg. loss: 0.272408\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 17.99, NNZs: 1223, Bias: -0.331568, T: 23496, Avg. loss: 0.271454\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 18.02, NNZs: 1223, Bias: -0.331428, T: 24475, Avg. loss: 0.270282\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 18.04, NNZs: 1223, Bias: -0.331333, T: 25454, Avg. loss: 0.269183\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 18.06, NNZs: 1223, Bias: -0.331065, T: 26433, Avg. loss: 0.268304\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 18.09, NNZs: 1223, Bias: -0.330928, T: 27412, Avg. loss: 0.267311\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 18.11, NNZs: 1223, Bias: -0.330935, T: 28391, Avg. loss: 0.266362\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 18.13, NNZs: 1223, Bias: -0.330759, T: 29370, Avg. loss: 0.265574\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 18.15, NNZs: 1223, Bias: -0.330626, T: 30349, Avg. loss: 0.264776\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 31 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 16.27, NNZs: 1155, Bias: -0.122322, T: 979, Avg. loss: 0.530500\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 16.85, NNZs: 1161, Bias: -0.088071, T: 1958, Avg. loss: 0.333669\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 17.21, NNZs: 1169, Bias: -0.074992, T: 2937, Avg. loss: 0.296216\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 17.47, NNZs: 1169, Bias: -0.067902, T: 3916, Avg. loss: 0.277028\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 17.65, NNZs: 1169, Bias: -0.061014, T: 4895, Avg. loss: 0.264515\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 17.80, NNZs: 1171, Bias: -0.057506, T: 5874, Avg. loss: 0.255586\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 17.93, NNZs: 1173, Bias: -0.053955, T: 6853, Avg. loss: 0.248649\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 18.05, NNZs: 1173, Bias: -0.051202, T: 7832, Avg. loss: 0.242923\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 18.15, NNZs: 1173, Bias: -0.048928, T: 8811, Avg. loss: 0.238099\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 18.24, NNZs: 1175, Bias: -0.047092, T: 9790, Avg. loss: 0.233939\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 18.32, NNZs: 1175, Bias: -0.045755, T: 10769, Avg. loss: 0.230374\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 18.39, NNZs: 1175, Bias: -0.045052, T: 11748, Avg. loss: 0.227290\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 18.45, NNZs: 1175, Bias: -0.043329, T: 12727, Avg. loss: 0.224423\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 18.51, NNZs: 1175, Bias: -0.042265, T: 13706, Avg. loss: 0.221884\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 18.57, NNZs: 1175, Bias: -0.041884, T: 14685, Avg. loss: 0.219591\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 18.62, NNZs: 1175, Bias: -0.040874, T: 15664, Avg. loss: 0.217452\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 18.67, NNZs: 1175, Bias: -0.039766, T: 16643, Avg. loss: 0.215462\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 18.72, NNZs: 1175, Bias: -0.039392, T: 17622, Avg. loss: 0.213710\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 18.76, NNZs: 1175, Bias: -0.038572, T: 18601, Avg. loss: 0.212009\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 18.80, NNZs: 1175, Bias: -0.037596, T: 19580, Avg. loss: 0.210440\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 18.84, NNZs: 1175, Bias: -0.037337, T: 20559, Avg. loss: 0.209022\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 18.88, NNZs: 1175, Bias: -0.036812, T: 21538, Avg. loss: 0.207635\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 18.91, NNZs: 1175, Bias: -0.036490, T: 22517, Avg. loss: 0.206339\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 18.95, NNZs: 1175, Bias: -0.035812, T: 23496, Avg. loss: 0.205102\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 18.98, NNZs: 1175, Bias: -0.035648, T: 24475, Avg. loss: 0.203964\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 19.01, NNZs: 1175, Bias: -0.035256, T: 25454, Avg. loss: 0.202852\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 19.04, NNZs: 1176, Bias: -0.034893, T: 26433, Avg. loss: 0.201791\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 19.07, NNZs: 1176, Bias: -0.034607, T: 27412, Avg. loss: 0.200804\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 19.09, NNZs: 1176, Bias: -0.034333, T: 28391, Avg. loss: 0.199845\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 19.12, NNZs: 1176, Bias: -0.033940, T: 29370, Avg. loss: 0.198925\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 19.14, NNZs: 1176, Bias: -0.033813, T: 30349, Avg. loss: 0.198059\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 19.17, NNZs: 1176, Bias: -0.033281, T: 31328, Avg. loss: 0.197180\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 32 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 20.43, NNZs: 1037, Bias: -0.419258, T: 979, Avg. loss: 0.385508\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 20.70, NNZs: 1056, Bias: -0.422453, T: 1958, Avg. loss: 0.219884\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 20.93, NNZs: 1071, Bias: -0.419193, T: 2937, Avg. loss: 0.193000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 21.10, NNZs: 1082, Bias: -0.417880, T: 3916, Avg. loss: 0.180445\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 21.25, NNZs: 1093, Bias: -0.418034, T: 4895, Avg. loss: 0.172907\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 21.39, NNZs: 1099, Bias: -0.419236, T: 5874, Avg. loss: 0.169285\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 21.50, NNZs: 1100, Bias: -0.420313, T: 6853, Avg. loss: 0.163332\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 21.59, NNZs: 1100, Bias: -0.419540, T: 7832, Avg. loss: 0.158339\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 21.67, NNZs: 1102, Bias: -0.419564, T: 8811, Avg. loss: 0.155667\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 21.74, NNZs: 1102, Bias: -0.420507, T: 9790, Avg. loss: 0.152881\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 21.81, NNZs: 1102, Bias: -0.421921, T: 10769, Avg. loss: 0.151100\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 21.88, NNZs: 1102, Bias: -0.421040, T: 11748, Avg. loss: 0.148387\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 21.93, NNZs: 1102, Bias: -0.421609, T: 12727, Avg. loss: 0.146734\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 21.98, NNZs: 1102, Bias: -0.421852, T: 13706, Avg. loss: 0.144979\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 22.03, NNZs: 1102, Bias: -0.421601, T: 14685, Avg. loss: 0.143088\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 22.07, NNZs: 1102, Bias: -0.421749, T: 15664, Avg. loss: 0.142359\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 22.12, NNZs: 1102, Bias: -0.422163, T: 16643, Avg. loss: 0.140811\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 22.16, NNZs: 1102, Bias: -0.422287, T: 17622, Avg. loss: 0.139608\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 22.20, NNZs: 1102, Bias: -0.421826, T: 18601, Avg. loss: 0.138635\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 22.23, NNZs: 1102, Bias: -0.422455, T: 19580, Avg. loss: 0.137840\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 22.26, NNZs: 1102, Bias: -0.422831, T: 20559, Avg. loss: 0.136606\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 22.30, NNZs: 1102, Bias: -0.422867, T: 21538, Avg. loss: 0.135784\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 22.32, NNZs: 1102, Bias: -0.423487, T: 22517, Avg. loss: 0.134965\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 22.36, NNZs: 1102, Bias: -0.422926, T: 23496, Avg. loss: 0.134095\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 22.38, NNZs: 1102, Bias: -0.423325, T: 24475, Avg. loss: 0.133464\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 22.41, NNZs: 1102, Bias: -0.423701, T: 25454, Avg. loss: 0.132752\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 26 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 15.27, NNZs: 1216, Bias: -0.323904, T: 979, Avg. loss: 0.601915\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 15.87, NNZs: 1218, Bias: -0.322868, T: 1958, Avg. loss: 0.385207\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 16.17, NNZs: 1218, Bias: -0.324505, T: 2937, Avg. loss: 0.349869\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 16.39, NNZs: 1219, Bias: -0.326320, T: 3916, Avg. loss: 0.332004\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 16.55, NNZs: 1219, Bias: -0.326646, T: 4895, Avg. loss: 0.320642\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 16.67, NNZs: 1219, Bias: -0.327742, T: 5874, Avg. loss: 0.311678\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 16.78, NNZs: 1219, Bias: -0.328158, T: 6853, Avg. loss: 0.304903\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16.87, NNZs: 1219, Bias: -0.328514, T: 7832, Avg. loss: 0.299377\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 16.95, NNZs: 1219, Bias: -0.328527, T: 8811, Avg. loss: 0.295144\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 17.02, NNZs: 1219, Bias: -0.328676, T: 9790, Avg. loss: 0.291191\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 17.09, NNZs: 1220, Bias: -0.328832, T: 10769, Avg. loss: 0.287723\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 17.15, NNZs: 1220, Bias: -0.328533, T: 11748, Avg. loss: 0.284776\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 17.20, NNZs: 1220, Bias: -0.327952, T: 12727, Avg. loss: 0.282219\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 17.25, NNZs: 1222, Bias: -0.327605, T: 13706, Avg. loss: 0.279629\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 17.29, NNZs: 1222, Bias: -0.327275, T: 14685, Avg. loss: 0.277257\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 17.34, NNZs: 1222, Bias: -0.327022, T: 15664, Avg. loss: 0.275115\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 17.37, NNZs: 1223, Bias: -0.326870, T: 16643, Avg. loss: 0.273364\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 17.41, NNZs: 1223, Bias: -0.326640, T: 17622, Avg. loss: 0.271672\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 17.45, NNZs: 1223, Bias: -0.326599, T: 18601, Avg. loss: 0.270141\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17.48, NNZs: 1223, Bias: -0.326144, T: 19580, Avg. loss: 0.268597\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17.51, NNZs: 1223, Bias: -0.326081, T: 20559, Avg. loss: 0.267282\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 17.54, NNZs: 1223, Bias: -0.325957, T: 21538, Avg. loss: 0.265885\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 17.57, NNZs: 1223, Bias: -0.326046, T: 22517, Avg. loss: 0.264639\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 17.60, NNZs: 1223, Bias: -0.325662, T: 23496, Avg. loss: 0.263687\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 17.62, NNZs: 1223, Bias: -0.325529, T: 24475, Avg. loss: 0.262532\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 17.65, NNZs: 1223, Bias: -0.325440, T: 25454, Avg. loss: 0.261453\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 17.67, NNZs: 1223, Bias: -0.325099, T: 26433, Avg. loss: 0.260481\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 17.69, NNZs: 1223, Bias: -0.324987, T: 27412, Avg. loss: 0.259523\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 17.71, NNZs: 1223, Bias: -0.324941, T: 28391, Avg. loss: 0.258549\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 17.73, NNZs: 1223, Bias: -0.324798, T: 29370, Avg. loss: 0.257738\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 17.75, NNZs: 1223, Bias: -0.324553, T: 30349, Avg. loss: 0.256901\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 31 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 16.05, NNZs: 1162, Bias: -0.154695, T: 979, Avg. loss: 0.536419\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 16.66, NNZs: 1176, Bias: -0.116445, T: 1958, Avg. loss: 0.344111\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 17.05, NNZs: 1186, Bias: -0.101334, T: 2937, Avg. loss: 0.305000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 17.33, NNZs: 1189, Bias: -0.091619, T: 3916, Avg. loss: 0.284290\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 17.55, NNZs: 1189, Bias: -0.084686, T: 4895, Avg. loss: 0.270548\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 17.73, NNZs: 1191, Bias: -0.079376, T: 5874, Avg. loss: 0.260579\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 17.88, NNZs: 1194, Bias: -0.076150, T: 6853, Avg. loss: 0.253005\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 18.01, NNZs: 1194, Bias: -0.072952, T: 7832, Avg. loss: 0.246834\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 18.12, NNZs: 1194, Bias: -0.070794, T: 8811, Avg. loss: 0.241680\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 18.22, NNZs: 1194, Bias: -0.068660, T: 9790, Avg. loss: 0.237259\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 18.31, NNZs: 1194, Bias: -0.066599, T: 10769, Avg. loss: 0.233405\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 18.40, NNZs: 1194, Bias: -0.064967, T: 11748, Avg. loss: 0.230011\n",
            "Total training time: 0.01 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 13\n",
            "Norm: 18.47, NNZs: 1194, Bias: -0.063224, T: 12727, Avg. loss: 0.226891\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 18.54, NNZs: 1194, Bias: -0.062602, T: 13706, Avg. loss: 0.224172\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 18.61, NNZs: 1194, Bias: -0.061746, T: 14685, Avg. loss: 0.221604\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 18.67, NNZs: 1194, Bias: -0.060656, T: 15664, Avg. loss: 0.219261\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 18.72, NNZs: 1194, Bias: -0.059819, T: 16643, Avg. loss: 0.217150\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 18.78, NNZs: 1194, Bias: -0.059357, T: 17622, Avg. loss: 0.215182\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 18.82, NNZs: 1194, Bias: -0.058508, T: 18601, Avg. loss: 0.213313\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 18.87, NNZs: 1195, Bias: -0.058125, T: 19580, Avg. loss: 0.211609\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 18.92, NNZs: 1195, Bias: -0.057644, T: 20559, Avg. loss: 0.210023\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 18.96, NNZs: 1197, Bias: -0.057197, T: 21538, Avg. loss: 0.208493\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 19.00, NNZs: 1197, Bias: -0.056937, T: 22517, Avg. loss: 0.207065\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 19.03, NNZs: 1197, Bias: -0.056056, T: 23496, Avg. loss: 0.205691\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 19.07, NNZs: 1197, Bias: -0.055751, T: 24475, Avg. loss: 0.204422\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 19.11, NNZs: 1197, Bias: -0.055526, T: 25454, Avg. loss: 0.203196\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 19.14, NNZs: 1197, Bias: -0.055095, T: 26433, Avg. loss: 0.202034\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 19.17, NNZs: 1197, Bias: -0.054602, T: 27412, Avg. loss: 0.200915\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 19.20, NNZs: 1197, Bias: -0.054132, T: 28391, Avg. loss: 0.199841\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 19.23, NNZs: 1197, Bias: -0.053825, T: 29370, Avg. loss: 0.198839\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 19.26, NNZs: 1197, Bias: -0.053652, T: 30349, Avg. loss: 0.197878\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 19.29, NNZs: 1197, Bias: -0.053275, T: 31328, Avg. loss: 0.196931\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 19.31, NNZs: 1197, Bias: -0.053081, T: 32307, Avg. loss: 0.196062\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 19.34, NNZs: 1197, Bias: -0.052606, T: 33286, Avg. loss: 0.195205\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 19.36, NNZs: 1197, Bias: -0.052641, T: 34265, Avg. loss: 0.194416\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 35 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 20.06, NNZs: 1049, Bias: -0.444775, T: 979, Avg. loss: 0.387422\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 20.29, NNZs: 1078, Bias: -0.452237, T: 1958, Avg. loss: 0.227756\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 20.47, NNZs: 1080, Bias: -0.452300, T: 2937, Avg. loss: 0.199830\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 20.66, NNZs: 1081, Bias: -0.450517, T: 3916, Avg. loss: 0.188764\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 20.80, NNZs: 1083, Bias: -0.450190, T: 4895, Avg. loss: 0.180663\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 20.93, NNZs: 1086, Bias: -0.453270, T: 5874, Avg. loss: 0.175961\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 21.03, NNZs: 1086, Bias: -0.450914, T: 6853, Avg. loss: 0.169904\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 21.12, NNZs: 1087, Bias: -0.450474, T: 7832, Avg. loss: 0.165618\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 21.21, NNZs: 1088, Bias: -0.449330, T: 8811, Avg. loss: 0.162468\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 21.29, NNZs: 1090, Bias: -0.449789, T: 9790, Avg. loss: 0.159708\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 21.37, NNZs: 1090, Bias: -0.449439, T: 10769, Avg. loss: 0.157808\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 21.44, NNZs: 1090, Bias: -0.449251, T: 11748, Avg. loss: 0.155406\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 21.49, NNZs: 1092, Bias: -0.449960, T: 12727, Avg. loss: 0.153465\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 21.54, NNZs: 1095, Bias: -0.449803, T: 13706, Avg. loss: 0.151736\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 21.60, NNZs: 1097, Bias: -0.449509, T: 14685, Avg. loss: 0.150204\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 21.64, NNZs: 1097, Bias: -0.450703, T: 15664, Avg. loss: 0.149115\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 21.68, NNZs: 1097, Bias: -0.450328, T: 16643, Avg. loss: 0.147505\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 21.73, NNZs: 1097, Bias: -0.450479, T: 17622, Avg. loss: 0.146574\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 21.77, NNZs: 1097, Bias: -0.449859, T: 18601, Avg. loss: 0.145326\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 21.81, NNZs: 1097, Bias: -0.450080, T: 19580, Avg. loss: 0.144481\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 21.84, NNZs: 1097, Bias: -0.450507, T: 20559, Avg. loss: 0.143315\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 21.87, NNZs: 1097, Bias: -0.450706, T: 21538, Avg. loss: 0.142363\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 21.91, NNZs: 1098, Bias: -0.451014, T: 22517, Avg. loss: 0.141708\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 21.94, NNZs: 1098, Bias: -0.450737, T: 23496, Avg. loss: 0.140433\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 21.97, NNZs: 1099, Bias: -0.450889, T: 24475, Avg. loss: 0.139765\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 22.00, NNZs: 1099, Bias: -0.450915, T: 25454, Avg. loss: 0.139161\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 22.02, NNZs: 1099, Bias: -0.451601, T: 26433, Avg. loss: 0.138532\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 22.05, NNZs: 1099, Bias: -0.451368, T: 27412, Avg. loss: 0.137485\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 22.07, NNZs: 1101, Bias: -0.451745, T: 28391, Avg. loss: 0.137110\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 22.10, NNZs: 1101, Bias: -0.451676, T: 29370, Avg. loss: 0.136351\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 22.12, NNZs: 1101, Bias: -0.451946, T: 30349, Avg. loss: 0.136029\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 22.15, NNZs: 1101, Bias: -0.451856, T: 31328, Avg. loss: 0.135413\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 22.17, NNZs: 1101, Bias: -0.451972, T: 32307, Avg. loss: 0.134794\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 33 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 15.12, NNZs: 1241, Bias: -0.307033, T: 979, Avg. loss: 0.607333\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 15.70, NNZs: 1242, Bias: -0.307922, T: 1958, Avg. loss: 0.393689\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 16.02, NNZs: 1247, Bias: -0.308490, T: 2937, Avg. loss: 0.357490\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 16.24, NNZs: 1247, Bias: -0.308957, T: 3916, Avg. loss: 0.338990\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 16.40, NNZs: 1248, Bias: -0.309747, T: 4895, Avg. loss: 0.327876\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 16.53, NNZs: 1248, Bias: -0.310338, T: 5874, Avg. loss: 0.318898\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 16.64, NNZs: 1248, Bias: -0.310852, T: 6853, Avg. loss: 0.312270\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16.73, NNZs: 1248, Bias: -0.311561, T: 7832, Avg. loss: 0.306980\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 16.81, NNZs: 1248, Bias: -0.311777, T: 8811, Avg. loss: 0.302755\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 16.88, NNZs: 1248, Bias: -0.311855, T: 9790, Avg. loss: 0.299227\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 16.94, NNZs: 1248, Bias: -0.312003, T: 10769, Avg. loss: 0.295916\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 16.99, NNZs: 1248, Bias: -0.311889, T: 11748, Avg. loss: 0.293143\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 17.04, NNZs: 1248, Bias: -0.311086, T: 12727, Avg. loss: 0.290797\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 17.09, NNZs: 1248, Bias: -0.311079, T: 13706, Avg. loss: 0.288136\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 17.14, NNZs: 1248, Bias: -0.310708, T: 14685, Avg. loss: 0.286141\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 17.18, NNZs: 1248, Bias: -0.310676, T: 15664, Avg. loss: 0.284173\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 17.22, NNZs: 1248, Bias: -0.310237, T: 16643, Avg. loss: 0.282498\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 17.25, NNZs: 1248, Bias: -0.309877, T: 17622, Avg. loss: 0.280660\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 17.29, NNZs: 1248, Bias: -0.309682, T: 18601, Avg. loss: 0.279173\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17.32, NNZs: 1248, Bias: -0.309401, T: 19580, Avg. loss: 0.277637\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17.35, NNZs: 1248, Bias: -0.308971, T: 20559, Avg. loss: 0.276247\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 17.38, NNZs: 1248, Bias: -0.308811, T: 21538, Avg. loss: 0.274883\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 17.41, NNZs: 1248, Bias: -0.308621, T: 22517, Avg. loss: 0.273499\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 17.44, NNZs: 1248, Bias: -0.308299, T: 23496, Avg. loss: 0.272438\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 17.46, NNZs: 1248, Bias: -0.308071, T: 24475, Avg. loss: 0.271265\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 17.49, NNZs: 1248, Bias: -0.307801, T: 25454, Avg. loss: 0.270048\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 17.51, NNZs: 1248, Bias: -0.307338, T: 26433, Avg. loss: 0.269067\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 17.54, NNZs: 1248, Bias: -0.307026, T: 27412, Avg. loss: 0.267983\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 17.56, NNZs: 1248, Bias: -0.306849, T: 28391, Avg. loss: 0.266961\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 17.58, NNZs: 1248, Bias: -0.306522, T: 29370, Avg. loss: 0.266072\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 17.60, NNZs: 1248, Bias: -0.306377, T: 30349, Avg. loss: 0.265151\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 17.62, NNZs: 1248, Bias: -0.305966, T: 31328, Avg. loss: 0.264349\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 17.64, NNZs: 1248, Bias: -0.305650, T: 32307, Avg. loss: 0.263487\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 17.66, NNZs: 1248, Bias: -0.305380, T: 33286, Avg. loss: 0.262721\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 34 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 16.72, NNZs: 1174, Bias: -0.158625, T: 979, Avg. loss: 0.541188\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 17.24, NNZs: 1185, Bias: -0.119364, T: 1958, Avg. loss: 0.348634\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 17.58, NNZs: 1187, Bias: -0.102703, T: 2937, Avg. loss: 0.309322\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 17.84, NNZs: 1197, Bias: -0.096072, T: 3916, Avg. loss: 0.289291\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 18.04, NNZs: 1199, Bias: -0.089884, T: 4895, Avg. loss: 0.275701\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 18.21, NNZs: 1201, Bias: -0.085254, T: 5874, Avg. loss: 0.265521\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 18.35, NNZs: 1201, Bias: -0.081849, T: 6853, Avg. loss: 0.257452\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 18.48, NNZs: 1201, Bias: -0.077712, T: 7832, Avg. loss: 0.250699\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 18.59, NNZs: 1202, Bias: -0.074921, T: 8811, Avg. loss: 0.245084\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 18.69, NNZs: 1202, Bias: -0.072853, T: 9790, Avg. loss: 0.240306\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 18.77, NNZs: 1202, Bias: -0.070291, T: 10769, Avg. loss: 0.236209\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 18.85, NNZs: 1202, Bias: -0.068416, T: 11748, Avg. loss: 0.232553\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 18.93, NNZs: 1202, Bias: -0.066647, T: 12727, Avg. loss: 0.229195\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 19.00, NNZs: 1202, Bias: -0.065525, T: 13706, Avg. loss: 0.226201\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 19.06, NNZs: 1202, Bias: -0.064859, T: 14685, Avg. loss: 0.223448\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 19.12, NNZs: 1202, Bias: -0.063552, T: 15664, Avg. loss: 0.220933\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 19.18, NNZs: 1202, Bias: -0.062580, T: 16643, Avg. loss: 0.218592\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19.23, NNZs: 1202, Bias: -0.061985, T: 17622, Avg. loss: 0.216453\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 19.28, NNZs: 1203, Bias: -0.060857, T: 18601, Avg. loss: 0.214394\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 19.33, NNZs: 1203, Bias: -0.060498, T: 19580, Avg. loss: 0.212522\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 19.37, NNZs: 1203, Bias: -0.060141, T: 20559, Avg. loss: 0.210773\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 19.41, NNZs: 1203, Bias: -0.059125, T: 21538, Avg. loss: 0.209114\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 19.45, NNZs: 1203, Bias: -0.058738, T: 22517, Avg. loss: 0.207630\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 19.49, NNZs: 1203, Bias: -0.058031, T: 23496, Avg. loss: 0.206228\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 19.52, NNZs: 1203, Bias: -0.057512, T: 24475, Avg. loss: 0.204944\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 19.56, NNZs: 1203, Bias: -0.057085, T: 25454, Avg. loss: 0.203690\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 19.59, NNZs: 1203, Bias: -0.056468, T: 26433, Avg. loss: 0.202507\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 19.62, NNZs: 1203, Bias: -0.056016, T: 27412, Avg. loss: 0.201393\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 19.65, NNZs: 1203, Bias: -0.055482, T: 28391, Avg. loss: 0.200325\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 19.68, NNZs: 1203, Bias: -0.055100, T: 29370, Avg. loss: 0.199328\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 19.70, NNZs: 1203, Bias: -0.054697, T: 30349, Avg. loss: 0.198371\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 19.73, NNZs: 1203, Bias: -0.054190, T: 31328, Avg. loss: 0.197438\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 19.75, NNZs: 1203, Bias: -0.053810, T: 32307, Avg. loss: 0.196573\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 19.78, NNZs: 1203, Bias: -0.053600, T: 33286, Avg. loss: 0.195713\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 34 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 20.18, NNZs: 1064, Bias: -0.438423, T: 979, Avg. loss: 0.386107\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 20.44, NNZs: 1083, Bias: -0.449470, T: 1958, Avg. loss: 0.227672\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 20.63, NNZs: 1088, Bias: -0.448689, T: 2937, Avg. loss: 0.201174\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 20.79, NNZs: 1101, Bias: -0.446862, T: 3916, Avg. loss: 0.190716\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 20.94, NNZs: 1103, Bias: -0.448458, T: 4895, Avg. loss: 0.183727\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 21.04, NNZs: 1110, Bias: -0.450851, T: 5874, Avg. loss: 0.177540\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 21.14, NNZs: 1110, Bias: -0.450439, T: 6853, Avg. loss: 0.172048\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 21.23, NNZs: 1111, Bias: -0.451701, T: 7832, Avg. loss: 0.168288\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 21.30, NNZs: 1113, Bias: -0.451593, T: 8811, Avg. loss: 0.164681\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 21.38, NNZs: 1116, Bias: -0.450987, T: 9790, Avg. loss: 0.162210\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 21.43, NNZs: 1116, Bias: -0.452728, T: 10769, Avg. loss: 0.159987\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 21.49, NNZs: 1116, Bias: -0.452886, T: 11748, Avg. loss: 0.157092\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 21.54, NNZs: 1116, Bias: -0.453079, T: 12727, Avg. loss: 0.155300\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 21.58, NNZs: 1116, Bias: -0.454690, T: 13706, Avg. loss: 0.153710\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 21.63, NNZs: 1116, Bias: -0.454286, T: 14685, Avg. loss: 0.151932\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 21.67, NNZs: 1116, Bias: -0.455887, T: 15664, Avg. loss: 0.151071\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 21.70, NNZs: 1116, Bias: -0.456314, T: 16643, Avg. loss: 0.149525\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 21.73, NNZs: 1116, Bias: -0.456980, T: 17622, Avg. loss: 0.148096\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 21.76, NNZs: 1116, Bias: -0.457369, T: 18601, Avg. loss: 0.146961\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 21.79, NNZs: 1117, Bias: -0.458599, T: 19580, Avg. loss: 0.146154\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 21.82, NNZs: 1117, Bias: -0.458367, T: 20559, Avg. loss: 0.144843\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 21.84, NNZs: 1117, Bias: -0.458759, T: 21538, Avg. loss: 0.144147\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 21.87, NNZs: 1118, Bias: -0.459115, T: 22517, Avg. loss: 0.143514\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 21.89, NNZs: 1118, Bias: -0.459857, T: 23496, Avg. loss: 0.142634\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 21.92, NNZs: 1119, Bias: -0.460319, T: 24475, Avg. loss: 0.141945\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 21.94, NNZs: 1119, Bias: -0.460828, T: 25454, Avg. loss: 0.141162\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 26 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 15.14, NNZs: 1276, Bias: -0.290731, T: 979, Avg. loss: 0.609754\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 15.81, NNZs: 1278, Bias: -0.295803, T: 1958, Avg. loss: 0.388211\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 16.14, NNZs: 1279, Bias: -0.297466, T: 2937, Avg. loss: 0.352199\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 16.36, NNZs: 1280, Bias: -0.297524, T: 3916, Avg. loss: 0.335703\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 16.53, NNZs: 1280, Bias: -0.298058, T: 4895, Avg. loss: 0.325129\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 16.65, NNZs: 1280, Bias: -0.299107, T: 5874, Avg. loss: 0.317095\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 16.76, NNZs: 1280, Bias: -0.298749, T: 6853, Avg. loss: 0.311103\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16.85, NNZs: 1280, Bias: -0.298413, T: 7832, Avg. loss: 0.306101\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 16.93, NNZs: 1280, Bias: -0.298618, T: 8811, Avg. loss: 0.301773\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 17.00, NNZs: 1281, Bias: -0.298567, T: 9790, Avg. loss: 0.298094\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 17.06, NNZs: 1281, Bias: -0.298440, T: 10769, Avg. loss: 0.294868\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 17.12, NNZs: 1281, Bias: -0.298407, T: 11748, Avg. loss: 0.292002\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 17.17, NNZs: 1282, Bias: -0.297694, T: 12727, Avg. loss: 0.289804\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 17.22, NNZs: 1282, Bias: -0.297544, T: 13706, Avg. loss: 0.287414\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 17.26, NNZs: 1282, Bias: -0.296954, T: 14685, Avg. loss: 0.285558\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 17.30, NNZs: 1282, Bias: -0.296916, T: 15664, Avg. loss: 0.283605\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 17.34, NNZs: 1282, Bias: -0.296669, T: 16643, Avg. loss: 0.282010\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 17.37, NNZs: 1282, Bias: -0.296364, T: 17622, Avg. loss: 0.280464\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 17.41, NNZs: 1282, Bias: -0.295814, T: 18601, Avg. loss: 0.279042\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17.44, NNZs: 1283, Bias: -0.295690, T: 19580, Avg. loss: 0.277614\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17.47, NNZs: 1283, Bias: -0.295457, T: 20559, Avg. loss: 0.276422\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 17.50, NNZs: 1283, Bias: -0.295459, T: 21538, Avg. loss: 0.275176\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 17.53, NNZs: 1283, Bias: -0.295226, T: 22517, Avg. loss: 0.273979\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 17.55, NNZs: 1283, Bias: -0.294929, T: 23496, Avg. loss: 0.273037\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 17.58, NNZs: 1283, Bias: -0.294676, T: 24475, Avg. loss: 0.271961\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 17.60, NNZs: 1283, Bias: -0.294708, T: 25454, Avg. loss: 0.270887\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 17.63, NNZs: 1283, Bias: -0.294435, T: 26433, Avg. loss: 0.270041\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 17.65, NNZs: 1283, Bias: -0.294393, T: 27412, Avg. loss: 0.269063\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 17.67, NNZs: 1283, Bias: -0.294470, T: 28391, Avg. loss: 0.268234\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 17.69, NNZs: 1283, Bias: -0.294148, T: 29370, Avg. loss: 0.267441\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 17.71, NNZs: 1284, Bias: -0.293959, T: 30349, Avg. loss: 0.266660\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 31 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 16.61, NNZs: 1187, Bias: -0.193043, T: 979, Avg. loss: 0.542431\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 17.13, NNZs: 1202, Bias: -0.152333, T: 1958, Avg. loss: 0.348113\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 17.48, NNZs: 1207, Bias: -0.139957, T: 2937, Avg. loss: 0.308467\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 17.74, NNZs: 1216, Bias: -0.132018, T: 3916, Avg. loss: 0.287563\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 17.94, NNZs: 1216, Bias: -0.125720, T: 4895, Avg. loss: 0.273672\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 18.11, NNZs: 1221, Bias: -0.121459, T: 5874, Avg. loss: 0.263569\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 18.25, NNZs: 1221, Bias: -0.118467, T: 6853, Avg. loss: 0.255808\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 18.36, NNZs: 1221, Bias: -0.115285, T: 7832, Avg. loss: 0.249648\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 18.46, NNZs: 1221, Bias: -0.112357, T: 8811, Avg. loss: 0.244470\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 18.55, NNZs: 1221, Bias: -0.110475, T: 9790, Avg. loss: 0.240063\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 18.63, NNZs: 1221, Bias: -0.108690, T: 10769, Avg. loss: 0.236199\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 18.70, NNZs: 1221, Bias: -0.106735, T: 11748, Avg. loss: 0.232807\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 18.77, NNZs: 1221, Bias: -0.104767, T: 12727, Avg. loss: 0.229775\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 18.83, NNZs: 1221, Bias: -0.103700, T: 13706, Avg. loss: 0.227104\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 18.89, NNZs: 1221, Bias: -0.102514, T: 14685, Avg. loss: 0.224601\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 18.94, NNZs: 1221, Bias: -0.100972, T: 15664, Avg. loss: 0.222338\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 18.99, NNZs: 1221, Bias: -0.099675, T: 16643, Avg. loss: 0.220281\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19.04, NNZs: 1222, Bias: -0.098825, T: 17622, Avg. loss: 0.218410\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 19.08, NNZs: 1222, Bias: -0.097957, T: 18601, Avg. loss: 0.216597\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 19.13, NNZs: 1222, Bias: -0.097261, T: 19580, Avg. loss: 0.214956\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 19.17, NNZs: 1222, Bias: -0.096843, T: 20559, Avg. loss: 0.213399\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 19.20, NNZs: 1222, Bias: -0.095997, T: 21538, Avg. loss: 0.211926\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 19.24, NNZs: 1222, Bias: -0.095631, T: 22517, Avg. loss: 0.210594\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 19.27, NNZs: 1222, Bias: -0.094893, T: 23496, Avg. loss: 0.209306\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 19.30, NNZs: 1222, Bias: -0.094551, T: 24475, Avg. loss: 0.208118\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 19.33, NNZs: 1222, Bias: -0.094182, T: 25454, Avg. loss: 0.206981\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 19.36, NNZs: 1222, Bias: -0.093800, T: 26433, Avg. loss: 0.205883\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 19.39, NNZs: 1222, Bias: -0.093499, T: 27412, Avg. loss: 0.204847\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 19.42, NNZs: 1222, Bias: -0.092869, T: 28391, Avg. loss: 0.203864\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 19.44, NNZs: 1222, Bias: -0.092564, T: 29370, Avg. loss: 0.202944\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 19.47, NNZs: 1222, Bias: -0.092240, T: 30349, Avg. loss: 0.202040\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 19.49, NNZs: 1222, Bias: -0.091612, T: 31328, Avg. loss: 0.201159\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 19.52, NNZs: 1222, Bias: -0.091835, T: 32307, Avg. loss: 0.200359\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 33 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 20.01, NNZs: 1053, Bias: -0.416369, T: 979, Avg. loss: 0.363875\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 20.36, NNZs: 1107, Bias: -0.423619, T: 1958, Avg. loss: 0.231727\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 20.57, NNZs: 1113, Bias: -0.420339, T: 2937, Avg. loss: 0.201095\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 20.76, NNZs: 1124, Bias: -0.422042, T: 3916, Avg. loss: 0.190828\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 20.90, NNZs: 1127, Bias: -0.421994, T: 4895, Avg. loss: 0.183306\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 21.02, NNZs: 1129, Bias: -0.422625, T: 5874, Avg. loss: 0.176311\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 21.12, NNZs: 1134, Bias: -0.424154, T: 6853, Avg. loss: 0.171444\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 21.21, NNZs: 1136, Bias: -0.425163, T: 7832, Avg. loss: 0.167081\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 21.28, NNZs: 1136, Bias: -0.425773, T: 8811, Avg. loss: 0.163123\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 21.35, NNZs: 1136, Bias: -0.426847, T: 9790, Avg. loss: 0.159883\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 21.41, NNZs: 1136, Bias: -0.427942, T: 10769, Avg. loss: 0.157580\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 21.46, NNZs: 1136, Bias: -0.427537, T: 11748, Avg. loss: 0.154587\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 21.51, NNZs: 1136, Bias: -0.428259, T: 12727, Avg. loss: 0.152812\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 21.55, NNZs: 1136, Bias: -0.429859, T: 13706, Avg. loss: 0.150952\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 21.58, NNZs: 1136, Bias: -0.431290, T: 14685, Avg. loss: 0.149131\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 21.62, NNZs: 1136, Bias: -0.432589, T: 15664, Avg. loss: 0.147887\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 21.65, NNZs: 1136, Bias: -0.433316, T: 16643, Avg. loss: 0.146143\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 21.68, NNZs: 1136, Bias: -0.434409, T: 17622, Avg. loss: 0.144991\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 21.70, NNZs: 1136, Bias: -0.435092, T: 18601, Avg. loss: 0.143603\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 21.73, NNZs: 1136, Bias: -0.435595, T: 19580, Avg. loss: 0.142468\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 21.76, NNZs: 1136, Bias: -0.435742, T: 20559, Avg. loss: 0.141535\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 21.78, NNZs: 1136, Bias: -0.436516, T: 21538, Avg. loss: 0.140581\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 21.81, NNZs: 1136, Bias: -0.437331, T: 22517, Avg. loss: 0.139785\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 21.83, NNZs: 1136, Bias: -0.437784, T: 23496, Avg. loss: 0.138846\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 21.85, NNZs: 1139, Bias: -0.438153, T: 24475, Avg. loss: 0.137911\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 25 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 15.33, NNZs: 1283, Bias: -0.287435, T: 979, Avg. loss: 0.616157\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 15.93, NNZs: 1283, Bias: -0.285008, T: 1958, Avg. loss: 0.393992\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 16.26, NNZs: 1285, Bias: -0.286732, T: 2937, Avg. loss: 0.356861\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 16.49, NNZs: 1286, Bias: -0.287449, T: 3916, Avg. loss: 0.340233\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 16.64, NNZs: 1286, Bias: -0.287082, T: 4895, Avg. loss: 0.329474\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 16.77, NNZs: 1287, Bias: -0.287619, T: 5874, Avg. loss: 0.320743\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 16.88, NNZs: 1287, Bias: -0.286745, T: 6853, Avg. loss: 0.314886\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16.96, NNZs: 1287, Bias: -0.287041, T: 7832, Avg. loss: 0.309933\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 17.04, NNZs: 1287, Bias: -0.286837, T: 8811, Avg. loss: 0.305725\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 17.10, NNZs: 1287, Bias: -0.286689, T: 9790, Avg. loss: 0.302250\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 17.16, NNZs: 1287, Bias: -0.286548, T: 10769, Avg. loss: 0.299003\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 17.22, NNZs: 1287, Bias: -0.286848, T: 11748, Avg. loss: 0.296082\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 17.27, NNZs: 1287, Bias: -0.286407, T: 12727, Avg. loss: 0.293911\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 17.32, NNZs: 1287, Bias: -0.286686, T: 13706, Avg. loss: 0.291710\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 17.36, NNZs: 1287, Bias: -0.286190, T: 14685, Avg. loss: 0.289843\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 17.40, NNZs: 1287, Bias: -0.286181, T: 15664, Avg. loss: 0.287973\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 17.43, NNZs: 1287, Bias: -0.286032, T: 16643, Avg. loss: 0.286355\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 17.47, NNZs: 1287, Bias: -0.285883, T: 17622, Avg. loss: 0.284728\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 17.50, NNZs: 1287, Bias: -0.285834, T: 18601, Avg. loss: 0.283351\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17.53, NNZs: 1287, Bias: -0.285481, T: 19580, Avg. loss: 0.282036\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17.56, NNZs: 1287, Bias: -0.285056, T: 20559, Avg. loss: 0.280815\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 17.59, NNZs: 1287, Bias: -0.284947, T: 21538, Avg. loss: 0.279489\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 17.61, NNZs: 1287, Bias: -0.284939, T: 22517, Avg. loss: 0.278407\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 17.64, NNZs: 1287, Bias: -0.284538, T: 23496, Avg. loss: 0.277448\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 17.66, NNZs: 1287, Bias: -0.284577, T: 24475, Avg. loss: 0.276376\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 17.69, NNZs: 1287, Bias: -0.284385, T: 25454, Avg. loss: 0.275408\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 17.71, NNZs: 1287, Bias: -0.284150, T: 26433, Avg. loss: 0.274517\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 17.73, NNZs: 1287, Bias: -0.284190, T: 27412, Avg. loss: 0.273606\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 17.75, NNZs: 1287, Bias: -0.284019, T: 28391, Avg. loss: 0.272816\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 17.77, NNZs: 1287, Bias: -0.283931, T: 29370, Avg. loss: 0.271975\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 30 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 16.85, NNZs: 1197, Bias: -0.157521, T: 979, Avg. loss: 0.537026\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 17.32, NNZs: 1210, Bias: -0.124020, T: 1958, Avg. loss: 0.350969\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 17.63, NNZs: 1214, Bias: -0.111133, T: 2937, Avg. loss: 0.313538\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 17.86, NNZs: 1215, Bias: -0.103836, T: 3916, Avg. loss: 0.293786\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 18.03, NNZs: 1217, Bias: -0.097582, T: 4895, Avg. loss: 0.280601\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 18.18, NNZs: 1217, Bias: -0.093351, T: 5874, Avg. loss: 0.270740\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 18.31, NNZs: 1217, Bias: -0.089750, T: 6853, Avg. loss: 0.263137\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 18.42, NNZs: 1217, Bias: -0.086512, T: 7832, Avg. loss: 0.256854\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 18.51, NNZs: 1217, Bias: -0.083898, T: 8811, Avg. loss: 0.251658\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 18.59, NNZs: 1217, Bias: -0.081150, T: 9790, Avg. loss: 0.247187\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 18.67, NNZs: 1217, Bias: -0.079093, T: 10769, Avg. loss: 0.243326\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 18.74, NNZs: 1217, Bias: -0.077189, T: 11748, Avg. loss: 0.239869\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 18.80, NNZs: 1217, Bias: -0.074939, T: 12727, Avg. loss: 0.236800\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 18.86, NNZs: 1217, Bias: -0.074082, T: 13706, Avg. loss: 0.234071\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 18.92, NNZs: 1217, Bias: -0.072656, T: 14685, Avg. loss: 0.231588\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 18.97, NNZs: 1217, Bias: -0.070890, T: 15664, Avg. loss: 0.229276\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 19.01, NNZs: 1217, Bias: -0.069467, T: 16643, Avg. loss: 0.227184\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19.06, NNZs: 1219, Bias: -0.068684, T: 17622, Avg. loss: 0.225309\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 19.10, NNZs: 1219, Bias: -0.067638, T: 18601, Avg. loss: 0.223571\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 19.13, NNZs: 1219, Bias: -0.066891, T: 19580, Avg. loss: 0.222022\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 19.17, NNZs: 1219, Bias: -0.066240, T: 20559, Avg. loss: 0.220530\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 19.20, NNZs: 1219, Bias: -0.065203, T: 21538, Avg. loss: 0.219142\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 19.24, NNZs: 1220, Bias: -0.064866, T: 22517, Avg. loss: 0.217828\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm: 19.27, NNZs: 1220, Bias: -0.063907, T: 23496, Avg. loss: 0.216563\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 19.30, NNZs: 1220, Bias: -0.063020, T: 24475, Avg. loss: 0.215374\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 19.33, NNZs: 1220, Bias: -0.062688, T: 25454, Avg. loss: 0.214222\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 19.36, NNZs: 1220, Bias: -0.061757, T: 26433, Avg. loss: 0.213124\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 19.38, NNZs: 1220, Bias: -0.061241, T: 27412, Avg. loss: 0.212062\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 19.41, NNZs: 1220, Bias: -0.060453, T: 28391, Avg. loss: 0.211047\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 19.43, NNZs: 1220, Bias: -0.060312, T: 29370, Avg. loss: 0.210133\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 19.46, NNZs: 1220, Bias: -0.059519, T: 30349, Avg. loss: 0.209190\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 19.48, NNZs: 1220, Bias: -0.058885, T: 31328, Avg. loss: 0.208306\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 19.51, NNZs: 1220, Bias: -0.058637, T: 32307, Avg. loss: 0.207475\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 19.53, NNZs: 1220, Bias: -0.058330, T: 33286, Avg. loss: 0.206638\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 34 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 19.98, NNZs: 1002, Bias: -0.441589, T: 979, Avg. loss: 0.350883\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 20.27, NNZs: 1053, Bias: -0.435860, T: 1958, Avg. loss: 0.225760\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 20.47, NNZs: 1072, Bias: -0.428418, T: 2937, Avg. loss: 0.200860\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 20.63, NNZs: 1073, Bias: -0.425807, T: 3916, Avg. loss: 0.189651\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 20.79, NNZs: 1079, Bias: -0.424656, T: 4895, Avg. loss: 0.184053\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 20.90, NNZs: 1085, Bias: -0.423630, T: 5874, Avg. loss: 0.177035\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 21.01, NNZs: 1089, Bias: -0.422642, T: 6853, Avg. loss: 0.173785\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 21.10, NNZs: 1093, Bias: -0.421581, T: 7832, Avg. loss: 0.169069\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 21.20, NNZs: 1093, Bias: -0.421006, T: 8811, Avg. loss: 0.166057\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 21.28, NNZs: 1093, Bias: -0.421718, T: 9790, Avg. loss: 0.163563\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 21.35, NNZs: 1093, Bias: -0.421782, T: 10769, Avg. loss: 0.160964\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 21.42, NNZs: 1094, Bias: -0.421041, T: 11748, Avg. loss: 0.158686\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 21.49, NNZs: 1094, Bias: -0.419762, T: 12727, Avg. loss: 0.156667\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 21.54, NNZs: 1094, Bias: -0.421934, T: 13706, Avg. loss: 0.155518\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 21.60, NNZs: 1094, Bias: -0.420993, T: 14685, Avg. loss: 0.153421\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 21.64, NNZs: 1095, Bias: -0.421688, T: 15664, Avg. loss: 0.152647\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 21.69, NNZs: 1095, Bias: -0.422414, T: 16643, Avg. loss: 0.151282\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 21.73, NNZs: 1095, Bias: -0.423093, T: 17622, Avg. loss: 0.149952\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 21.76, NNZs: 1095, Bias: -0.423514, T: 18601, Avg. loss: 0.148486\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 21.80, NNZs: 1095, Bias: -0.424302, T: 19580, Avg. loss: 0.147884\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 21.84, NNZs: 1095, Bias: -0.424219, T: 20559, Avg. loss: 0.146443\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 21.87, NNZs: 1095, Bias: -0.424697, T: 21538, Avg. loss: 0.145778\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 21.90, NNZs: 1095, Bias: -0.424794, T: 22517, Avg. loss: 0.145032\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 21.93, NNZs: 1095, Bias: -0.425196, T: 23496, Avg. loss: 0.144087\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 21.95, NNZs: 1095, Bias: -0.425962, T: 24475, Avg. loss: 0.143120\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 21.98, NNZs: 1095, Bias: -0.426080, T: 25454, Avg. loss: 0.142266\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 26 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 15.12, NNZs: 1263, Bias: -0.287448, T: 979, Avg. loss: 0.576635\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 15.64, NNZs: 1264, Bias: -0.294031, T: 1958, Avg. loss: 0.381429\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 15.93, NNZs: 1264, Bias: -0.297429, T: 2937, Avg. loss: 0.349768\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 16.11, NNZs: 1264, Bias: -0.296379, T: 3916, Avg. loss: 0.335076\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 16.25, NNZs: 1264, Bias: -0.294722, T: 4895, Avg. loss: 0.325086\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 16.36, NNZs: 1264, Bias: -0.295119, T: 5874, Avg. loss: 0.317183\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 16.46, NNZs: 1264, Bias: -0.294250, T: 6853, Avg. loss: 0.311563\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16.54, NNZs: 1264, Bias: -0.293662, T: 7832, Avg. loss: 0.306955\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 16.61, NNZs: 1264, Bias: -0.293657, T: 8811, Avg. loss: 0.302740\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 16.67, NNZs: 1264, Bias: -0.293051, T: 9790, Avg. loss: 0.299289\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 16.73, NNZs: 1264, Bias: -0.292793, T: 10769, Avg. loss: 0.296198\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 16.79, NNZs: 1264, Bias: -0.292284, T: 11748, Avg. loss: 0.293088\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 16.84, NNZs: 1264, Bias: -0.292053, T: 12727, Avg. loss: 0.290772\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 16.89, NNZs: 1264, Bias: -0.291672, T: 13706, Avg. loss: 0.288345\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 16.93, NNZs: 1264, Bias: -0.291112, T: 14685, Avg. loss: 0.286390\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 16.97, NNZs: 1264, Bias: -0.290943, T: 15664, Avg. loss: 0.284409\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 17.01, NNZs: 1264, Bias: -0.290650, T: 16643, Avg. loss: 0.282740\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 17.05, NNZs: 1265, Bias: -0.290594, T: 17622, Avg. loss: 0.280999\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 17.08, NNZs: 1265, Bias: -0.290475, T: 18601, Avg. loss: 0.279590\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17.11, NNZs: 1265, Bias: -0.290350, T: 19580, Avg. loss: 0.278139\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17.14, NNZs: 1265, Bias: -0.290002, T: 20559, Avg. loss: 0.276929\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 17.17, NNZs: 1265, Bias: -0.290105, T: 21538, Avg. loss: 0.275555\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 17.20, NNZs: 1265, Bias: -0.289910, T: 22517, Avg. loss: 0.274351\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 17.23, NNZs: 1265, Bias: -0.289724, T: 23496, Avg. loss: 0.273305\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 17.25, NNZs: 1265, Bias: -0.289607, T: 24475, Avg. loss: 0.272171\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 17.28, NNZs: 1265, Bias: -0.289402, T: 25454, Avg. loss: 0.271140\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 17.30, NNZs: 1265, Bias: -0.289301, T: 26433, Avg. loss: 0.270135\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 17.33, NNZs: 1265, Bias: -0.289293, T: 27412, Avg. loss: 0.269132\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 17.35, NNZs: 1265, Bias: -0.289304, T: 28391, Avg. loss: 0.268253\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 17.37, NNZs: 1265, Bias: -0.288939, T: 29370, Avg. loss: 0.267405\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 17.39, NNZs: 1265, Bias: -0.288974, T: 30349, Avg. loss: 0.266531\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 17.41, NNZs: 1265, Bias: -0.288716, T: 31328, Avg. loss: 0.265799\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 17.43, NNZs: 1265, Bias: -0.288618, T: 32307, Avg. loss: 0.264960\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 33 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 17.39, NNZs: 1213, Bias: -0.155276, T: 979, Avg. loss: 0.536289\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 17.87, NNZs: 1227, Bias: -0.127398, T: 1958, Avg. loss: 0.345551\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 18.19, NNZs: 1236, Bias: -0.115142, T: 2937, Avg. loss: 0.309002\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 18.42, NNZs: 1237, Bias: -0.107990, T: 3916, Avg. loss: 0.289467\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 18.61, NNZs: 1240, Bias: -0.104612, T: 4895, Avg. loss: 0.276381\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 18.75, NNZs: 1240, Bias: -0.099777, T: 5874, Avg. loss: 0.266578\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 18.88, NNZs: 1240, Bias: -0.096406, T: 6853, Avg. loss: 0.259024\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 18.99, NNZs: 1240, Bias: -0.093728, T: 7832, Avg. loss: 0.252826\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 19.09, NNZs: 1240, Bias: -0.091389, T: 8811, Avg. loss: 0.247515\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 19.17, NNZs: 1240, Bias: -0.089315, T: 9790, Avg. loss: 0.242954\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 19.25, NNZs: 1240, Bias: -0.087590, T: 10769, Avg. loss: 0.239032\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 19.32, NNZs: 1240, Bias: -0.085970, T: 11748, Avg. loss: 0.235623\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 19.38, NNZs: 1240, Bias: -0.085054, T: 12727, Avg. loss: 0.232600\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 19.44, NNZs: 1240, Bias: -0.084227, T: 13706, Avg. loss: 0.229883\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 19.50, NNZs: 1240, Bias: -0.083260, T: 14685, Avg. loss: 0.227397\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 19.55, NNZs: 1240, Bias: -0.082460, T: 15664, Avg. loss: 0.225049\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 19.59, NNZs: 1240, Bias: -0.081358, T: 16643, Avg. loss: 0.222887\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19.64, NNZs: 1240, Bias: -0.081018, T: 17622, Avg. loss: 0.220969\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 19.68, NNZs: 1240, Bias: -0.080363, T: 18601, Avg. loss: 0.219190\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 19.71, NNZs: 1240, Bias: -0.079680, T: 19580, Avg. loss: 0.217555\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 19.75, NNZs: 1240, Bias: -0.079339, T: 20559, Avg. loss: 0.215994\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 19.78, NNZs: 1243, Bias: -0.078835, T: 21538, Avg. loss: 0.214545\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 19.82, NNZs: 1244, Bias: -0.078774, T: 22517, Avg. loss: 0.213206\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 19.85, NNZs: 1244, Bias: -0.078009, T: 23496, Avg. loss: 0.211934\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 19.88, NNZs: 1244, Bias: -0.077593, T: 24475, Avg. loss: 0.210756\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 19.90, NNZs: 1244, Bias: -0.077357, T: 25454, Avg. loss: 0.209623\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 19.93, NNZs: 1244, Bias: -0.076706, T: 26433, Avg. loss: 0.208569\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 19.96, NNZs: 1244, Bias: -0.076381, T: 27412, Avg. loss: 0.207548\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 19.98, NNZs: 1244, Bias: -0.075645, T: 28391, Avg. loss: 0.206547\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 20.01, NNZs: 1244, Bias: -0.075721, T: 29370, Avg. loss: 0.205680\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 20.03, NNZs: 1244, Bias: -0.075337, T: 30349, Avg. loss: 0.204788\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 20.05, NNZs: 1244, Bias: -0.074747, T: 31328, Avg. loss: 0.203935\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 20.07, NNZs: 1244, Bias: -0.074699, T: 32307, Avg. loss: 0.203173\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 20.09, NNZs: 1244, Bias: -0.074470, T: 33286, Avg. loss: 0.202409\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 34 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 18.85, NNZs: 1003, Bias: -0.439424, T: 979, Avg. loss: 0.350873\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 19.25, NNZs: 1102, Bias: -0.451166, T: 1958, Avg. loss: 0.242331\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 19.48, NNZs: 1105, Bias: -0.445097, T: 2937, Avg. loss: 0.206000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 19.68, NNZs: 1114, Bias: -0.444043, T: 3916, Avg. loss: 0.196840\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 19.84, NNZs: 1120, Bias: -0.445615, T: 4895, Avg. loss: 0.190344\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 19.97, NNZs: 1120, Bias: -0.446164, T: 5874, Avg. loss: 0.182034\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 20.07, NNZs: 1120, Bias: -0.447507, T: 6853, Avg. loss: 0.178626\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 20.17, NNZs: 1120, Bias: -0.447719, T: 7832, Avg. loss: 0.174131\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 20.26, NNZs: 1120, Bias: -0.446946, T: 8811, Avg. loss: 0.170477\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 20.33, NNZs: 1121, Bias: -0.447503, T: 9790, Avg. loss: 0.167908\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 20.40, NNZs: 1122, Bias: -0.449297, T: 10769, Avg. loss: 0.165358\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 20.47, NNZs: 1122, Bias: -0.448599, T: 11748, Avg. loss: 0.162768\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 20.53, NNZs: 1124, Bias: -0.448220, T: 12727, Avg. loss: 0.160665\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 20.58, NNZs: 1127, Bias: -0.449318, T: 13706, Avg. loss: 0.159374\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 20.63, NNZs: 1127, Bias: -0.449145, T: 14685, Avg. loss: 0.157666\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 20.68, NNZs: 1127, Bias: -0.449838, T: 15664, Avg. loss: 0.156473\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 20.72, NNZs: 1127, Bias: -0.450773, T: 16643, Avg. loss: 0.155205\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 20.76, NNZs: 1128, Bias: -0.451298, T: 17622, Avg. loss: 0.153782\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 20.79, NNZs: 1129, Bias: -0.450960, T: 18601, Avg. loss: 0.152331\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 20.83, NNZs: 1129, Bias: -0.451845, T: 19580, Avg. loss: 0.151831\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 20.86, NNZs: 1130, Bias: -0.451516, T: 20559, Avg. loss: 0.150500\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 20.89, NNZs: 1130, Bias: -0.452426, T: 21538, Avg. loss: 0.150087\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 20.93, NNZs: 1130, Bias: -0.452666, T: 22517, Avg. loss: 0.148967\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 20.96, NNZs: 1130, Bias: -0.452406, T: 23496, Avg. loss: 0.148340\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 20.99, NNZs: 1133, Bias: -0.452859, T: 24475, Avg. loss: 0.147555\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 21.02, NNZs: 1133, Bias: -0.452957, T: 25454, Avg. loss: 0.146712\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 21.05, NNZs: 1134, Bias: -0.453587, T: 26433, Avg. loss: 0.146242\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 21.07, NNZs: 1134, Bias: -0.453608, T: 27412, Avg. loss: 0.145243\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 28 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 15.34, NNZs: 1298, Bias: -0.288459, T: 979, Avg. loss: 0.600908\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 15.91, NNZs: 1299, Bias: -0.290930, T: 1958, Avg. loss: 0.395458\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 16.23, NNZs: 1300, Bias: -0.293207, T: 2937, Avg. loss: 0.360762\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 16.44, NNZs: 1301, Bias: -0.291272, T: 3916, Avg. loss: 0.344325\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 16.60, NNZs: 1301, Bias: -0.289377, T: 4895, Avg. loss: 0.333711\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 16.72, NNZs: 1301, Bias: -0.288391, T: 5874, Avg. loss: 0.324676\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 16.82, NNZs: 1301, Bias: -0.287270, T: 6853, Avg. loss: 0.318649\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16.91, NNZs: 1301, Bias: -0.285938, T: 7832, Avg. loss: 0.313555\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 16.99, NNZs: 1301, Bias: -0.285832, T: 8811, Avg. loss: 0.309072\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 17.05, NNZs: 1301, Bias: -0.285163, T: 9790, Avg. loss: 0.305600\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 17.12, NNZs: 1301, Bias: -0.284825, T: 10769, Avg. loss: 0.302375\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 17.18, NNZs: 1301, Bias: -0.284406, T: 11748, Avg. loss: 0.299260\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 17.23, NNZs: 1301, Bias: -0.283804, T: 12727, Avg. loss: 0.296902\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 17.27, NNZs: 1301, Bias: -0.283298, T: 13706, Avg. loss: 0.294340\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 17.32, NNZs: 1302, Bias: -0.282533, T: 14685, Avg. loss: 0.292434\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 17.36, NNZs: 1302, Bias: -0.282134, T: 15664, Avg. loss: 0.290400\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 17.40, NNZs: 1303, Bias: -0.281519, T: 16643, Avg. loss: 0.288641\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm: 17.43, NNZs: 1303, Bias: -0.281318, T: 17622, Avg. loss: 0.286761\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 17.47, NNZs: 1303, Bias: -0.280926, T: 18601, Avg. loss: 0.285203\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17.50, NNZs: 1303, Bias: -0.280658, T: 19580, Avg. loss: 0.283742\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17.53, NNZs: 1303, Bias: -0.280205, T: 20559, Avg. loss: 0.282536\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 17.56, NNZs: 1303, Bias: -0.279951, T: 21538, Avg. loss: 0.281170\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 17.59, NNZs: 1303, Bias: -0.279730, T: 22517, Avg. loss: 0.279936\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 17.61, NNZs: 1303, Bias: -0.279280, T: 23496, Avg. loss: 0.278834\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 17.64, NNZs: 1303, Bias: -0.278838, T: 24475, Avg. loss: 0.277707\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 17.66, NNZs: 1303, Bias: -0.278695, T: 25454, Avg. loss: 0.276678\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 17.69, NNZs: 1305, Bias: -0.278578, T: 26433, Avg. loss: 0.275685\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 17.71, NNZs: 1305, Bias: -0.278454, T: 27412, Avg. loss: 0.274656\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 17.73, NNZs: 1305, Bias: -0.278233, T: 28391, Avg. loss: 0.273786\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 17.75, NNZs: 1305, Bias: -0.277782, T: 29370, Avg. loss: 0.272911\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 17.78, NNZs: 1305, Bias: -0.277658, T: 30349, Avg. loss: 0.271987\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 17.80, NNZs: 1305, Bias: -0.277328, T: 31328, Avg. loss: 0.271179\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 17.81, NNZs: 1305, Bias: -0.277214, T: 32307, Avg. loss: 0.270392\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 33 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 17.56, NNZs: 1186, Bias: -0.133069, T: 979, Avg. loss: 0.524972\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 18.07, NNZs: 1192, Bias: -0.112666, T: 1958, Avg. loss: 0.340244\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 18.36, NNZs: 1198, Bias: -0.100008, T: 2937, Avg. loss: 0.305428\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 18.59, NNZs: 1201, Bias: -0.092368, T: 3916, Avg. loss: 0.286798\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 18.75, NNZs: 1201, Bias: -0.088567, T: 4895, Avg. loss: 0.274292\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 18.89, NNZs: 1201, Bias: -0.084914, T: 5874, Avg. loss: 0.265082\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 19.00, NNZs: 1201, Bias: -0.082768, T: 6853, Avg. loss: 0.257919\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 19.11, NNZs: 1202, Bias: -0.080172, T: 7832, Avg. loss: 0.251971\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 19.20, NNZs: 1206, Bias: -0.078376, T: 8811, Avg. loss: 0.246895\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 19.28, NNZs: 1206, Bias: -0.076253, T: 9790, Avg. loss: 0.242513\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 19.35, NNZs: 1206, Bias: -0.074346, T: 10769, Avg. loss: 0.238697\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 19.42, NNZs: 1206, Bias: -0.072902, T: 11748, Avg. loss: 0.235316\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 19.48, NNZs: 1206, Bias: -0.071871, T: 12727, Avg. loss: 0.232358\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 19.53, NNZs: 1206, Bias: -0.070701, T: 13706, Avg. loss: 0.229666\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 19.58, NNZs: 1206, Bias: -0.069897, T: 14685, Avg. loss: 0.227217\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 19.63, NNZs: 1206, Bias: -0.068460, T: 15664, Avg. loss: 0.224956\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 19.67, NNZs: 1206, Bias: -0.068116, T: 16643, Avg. loss: 0.222898\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19.71, NNZs: 1206, Bias: -0.067647, T: 17622, Avg. loss: 0.220995\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 19.75, NNZs: 1206, Bias: -0.067155, T: 18601, Avg. loss: 0.219169\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 19.79, NNZs: 1206, Bias: -0.066744, T: 19580, Avg. loss: 0.217499\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 19.83, NNZs: 1206, Bias: -0.066325, T: 20559, Avg. loss: 0.215934\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 19.86, NNZs: 1206, Bias: -0.066034, T: 21538, Avg. loss: 0.214479\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 19.89, NNZs: 1206, Bias: -0.066273, T: 22517, Avg. loss: 0.213145\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 19.92, NNZs: 1206, Bias: -0.065817, T: 23496, Avg. loss: 0.211868\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 19.94, NNZs: 1206, Bias: -0.065338, T: 24475, Avg. loss: 0.210690\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 19.97, NNZs: 1206, Bias: -0.065308, T: 25454, Avg. loss: 0.209545\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 20.00, NNZs: 1206, Bias: -0.064854, T: 26433, Avg. loss: 0.208445\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 20.02, NNZs: 1206, Bias: -0.064493, T: 27412, Avg. loss: 0.207391\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 20.05, NNZs: 1206, Bias: -0.063908, T: 28391, Avg. loss: 0.206376\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 20.07, NNZs: 1206, Bias: -0.063992, T: 29370, Avg. loss: 0.205436\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 20.09, NNZs: 1206, Bias: -0.063469, T: 30349, Avg. loss: 0.204494\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 20.11, NNZs: 1207, Bias: -0.063137, T: 31328, Avg. loss: 0.203601\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 20.14, NNZs: 1207, Bias: -0.062865, T: 32307, Avg. loss: 0.202752\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 20.16, NNZs: 1207, Bias: -0.062665, T: 33286, Avg. loss: 0.201918\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 34 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 19.16, NNZs: 965, Bias: -0.450533, T: 979, Avg. loss: 0.331225\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 19.52, NNZs: 1071, Bias: -0.470337, T: 1958, Avg. loss: 0.221538\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 19.77, NNZs: 1075, Bias: -0.464986, T: 2937, Avg. loss: 0.188874\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 19.92, NNZs: 1078, Bias: -0.467075, T: 3916, Avg. loss: 0.178988\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 20.05, NNZs: 1079, Bias: -0.469069, T: 4895, Avg. loss: 0.172625\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 20.16, NNZs: 1083, Bias: -0.469857, T: 5874, Avg. loss: 0.165767\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 20.26, NNZs: 1083, Bias: -0.471899, T: 6853, Avg. loss: 0.161943\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 20.35, NNZs: 1083, Bias: -0.472645, T: 7832, Avg. loss: 0.158296\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 20.43, NNZs: 1083, Bias: -0.473330, T: 8811, Avg. loss: 0.154466\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 20.49, NNZs: 1083, Bias: -0.474202, T: 9790, Avg. loss: 0.151956\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 20.55, NNZs: 1083, Bias: -0.475571, T: 10769, Avg. loss: 0.150211\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 20.61, NNZs: 1084, Bias: -0.475410, T: 11748, Avg. loss: 0.147630\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 20.66, NNZs: 1084, Bias: -0.475499, T: 12727, Avg. loss: 0.145697\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 20.70, NNZs: 1086, Bias: -0.476756, T: 13706, Avg. loss: 0.144630\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 20.76, NNZs: 1087, Bias: -0.476977, T: 14685, Avg. loss: 0.143072\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 20.80, NNZs: 1089, Bias: -0.477569, T: 15664, Avg. loss: 0.142004\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 20.84, NNZs: 1089, Bias: -0.478338, T: 16643, Avg. loss: 0.140923\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 20.88, NNZs: 1089, Bias: -0.478680, T: 17622, Avg. loss: 0.139682\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 20.93, NNZs: 1090, Bias: -0.478456, T: 18601, Avg. loss: 0.138585\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 20.96, NNZs: 1090, Bias: -0.479174, T: 19580, Avg. loss: 0.137854\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 21.00, NNZs: 1090, Bias: -0.479388, T: 20559, Avg. loss: 0.136938\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 21.03, NNZs: 1090, Bias: -0.479411, T: 21538, Avg. loss: 0.136176\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 21.07, NNZs: 1090, Bias: -0.479181, T: 22517, Avg. loss: 0.135421\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 21.10, NNZs: 1090, Bias: -0.479755, T: 23496, Avg. loss: 0.134771\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 24 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 15.37, NNZs: 1261, Bias: -0.294621, T: 979, Avg. loss: 0.567910\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 15.92, NNZs: 1261, Bias: -0.290672, T: 1958, Avg. loss: 0.374980\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 16.21, NNZs: 1261, Bias: -0.293365, T: 2937, Avg. loss: 0.344106\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 16.41, NNZs: 1261, Bias: -0.292740, T: 3916, Avg. loss: 0.328627\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 16.56, NNZs: 1262, Bias: -0.290930, T: 4895, Avg. loss: 0.318975\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 16.68, NNZs: 1262, Bias: -0.290104, T: 5874, Avg. loss: 0.310289\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 16.78, NNZs: 1262, Bias: -0.288913, T: 6853, Avg. loss: 0.304839\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16.86, NNZs: 1263, Bias: -0.288197, T: 7832, Avg. loss: 0.299844\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 16.94, NNZs: 1263, Bias: -0.288165, T: 8811, Avg. loss: 0.295243\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 17.01, NNZs: 1263, Bias: -0.287831, T: 9790, Avg. loss: 0.291832\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 17.07, NNZs: 1263, Bias: -0.287636, T: 10769, Avg. loss: 0.288763\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 17.13, NNZs: 1263, Bias: -0.287348, T: 11748, Avg. loss: 0.285955\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 17.18, NNZs: 1263, Bias: -0.286887, T: 12727, Avg. loss: 0.283424\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 17.22, NNZs: 1263, Bias: -0.286620, T: 13706, Avg. loss: 0.281010\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 17.27, NNZs: 1263, Bias: -0.286114, T: 14685, Avg. loss: 0.279208\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 17.31, NNZs: 1263, Bias: -0.285783, T: 15664, Avg. loss: 0.277204\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 17.34, NNZs: 1263, Bias: -0.285216, T: 16643, Avg. loss: 0.275509\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 17.38, NNZs: 1263, Bias: -0.284990, T: 17622, Avg. loss: 0.273737\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 17.41, NNZs: 1263, Bias: -0.284578, T: 18601, Avg. loss: 0.272320\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17.44, NNZs: 1264, Bias: -0.284269, T: 19580, Avg. loss: 0.270906\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17.47, NNZs: 1264, Bias: -0.284004, T: 20559, Avg. loss: 0.269610\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 17.50, NNZs: 1264, Bias: -0.283737, T: 21538, Avg. loss: 0.268293\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 17.53, NNZs: 1264, Bias: -0.283222, T: 22517, Avg. loss: 0.267060\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 17.55, NNZs: 1264, Bias: -0.283139, T: 23496, Avg. loss: 0.265973\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 17.58, NNZs: 1264, Bias: -0.282592, T: 24475, Avg. loss: 0.264866\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 17.60, NNZs: 1264, Bias: -0.282383, T: 25454, Avg. loss: 0.263853\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 17.62, NNZs: 1264, Bias: -0.282153, T: 26433, Avg. loss: 0.262875\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 17.65, NNZs: 1264, Bias: -0.281774, T: 27412, Avg. loss: 0.261932\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 17.67, NNZs: 1264, Bias: -0.281462, T: 28391, Avg. loss: 0.261021\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 17.69, NNZs: 1264, Bias: -0.280935, T: 29370, Avg. loss: 0.260187\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 17.71, NNZs: 1264, Bias: -0.280776, T: 30349, Avg. loss: 0.259304\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 31 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 15.75, NNZs: 1186, Bias: -0.148131, T: 980, Avg. loss: 0.546294\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 16.37, NNZs: 1202, Bias: -0.109098, T: 1960, Avg. loss: 0.353549\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 16.79, NNZs: 1209, Bias: -0.098000, T: 2940, Avg. loss: 0.315363\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 17.08, NNZs: 1215, Bias: -0.091410, T: 3920, Avg. loss: 0.294937\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 17.30, NNZs: 1216, Bias: -0.085142, T: 4900, Avg. loss: 0.281610\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 17.47, NNZs: 1216, Bias: -0.081142, T: 5880, Avg. loss: 0.271658\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 17.62, NNZs: 1216, Bias: -0.079193, T: 6860, Avg. loss: 0.264136\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 17.75, NNZs: 1217, Bias: -0.077446, T: 7840, Avg. loss: 0.257933\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 17.86, NNZs: 1217, Bias: -0.074901, T: 8820, Avg. loss: 0.252755\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 17.96, NNZs: 1219, Bias: -0.073851, T: 9800, Avg. loss: 0.248325\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 18.05, NNZs: 1222, Bias: -0.072672, T: 10780, Avg. loss: 0.244540\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 18.13, NNZs: 1222, Bias: -0.071167, T: 11760, Avg. loss: 0.241167\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 18.20, NNZs: 1222, Bias: -0.070827, T: 12740, Avg. loss: 0.238258\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 18.27, NNZs: 1222, Bias: -0.069740, T: 13720, Avg. loss: 0.235537\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 18.33, NNZs: 1222, Bias: -0.069120, T: 14700, Avg. loss: 0.233096\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 18.39, NNZs: 1223, Bias: -0.068390, T: 15680, Avg. loss: 0.230824\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 18.44, NNZs: 1223, Bias: -0.067655, T: 16660, Avg. loss: 0.228734\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 18.49, NNZs: 1223, Bias: -0.066979, T: 17640, Avg. loss: 0.226763\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 18.54, NNZs: 1225, Bias: -0.066501, T: 18620, Avg. loss: 0.224962\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 18.59, NNZs: 1225, Bias: -0.066161, T: 19600, Avg. loss: 0.223270\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 18.63, NNZs: 1225, Bias: -0.065509, T: 20580, Avg. loss: 0.221640\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 18.67, NNZs: 1225, Bias: -0.065609, T: 21560, Avg. loss: 0.220186\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 18.71, NNZs: 1225, Bias: -0.065216, T: 22540, Avg. loss: 0.218752\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 18.75, NNZs: 1225, Bias: -0.064650, T: 23520, Avg. loss: 0.217402\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 18.79, NNZs: 1225, Bias: -0.064711, T: 24500, Avg. loss: 0.216169\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 18.82, NNZs: 1225, Bias: -0.064489, T: 25480, Avg. loss: 0.214938\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 18.85, NNZs: 1225, Bias: -0.064064, T: 26460, Avg. loss: 0.213765\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 18.89, NNZs: 1225, Bias: -0.064011, T: 27440, Avg. loss: 0.212690\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 18.92, NNZs: 1225, Bias: -0.063581, T: 28420, Avg. loss: 0.211601\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 18.95, NNZs: 1225, Bias: -0.063681, T: 29400, Avg. loss: 0.210632\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 18.97, NNZs: 1225, Bias: -0.063440, T: 30380, Avg. loss: 0.209649\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 19.00, NNZs: 1227, Bias: -0.063400, T: 31360, Avg. loss: 0.208734\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 19.03, NNZs: 1227, Bias: -0.063140, T: 32340, Avg. loss: 0.207820\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 19.05, NNZs: 1228, Bias: -0.063022, T: 33320, Avg. loss: 0.206954\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 34 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 16.71, NNZs: 1044, Bias: -0.485232, T: 980, Avg. loss: 0.341632\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 17.27, NNZs: 1090, Bias: -0.472846, T: 1960, Avg. loss: 0.211610\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 17.68, NNZs: 1097, Bias: -0.464996, T: 2940, Avg. loss: 0.189666\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 17.95, NNZs: 1123, Bias: -0.464904, T: 3920, Avg. loss: 0.177126\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 18.18, NNZs: 1123, Bias: -0.462349, T: 4900, Avg. loss: 0.168964\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 18.36, NNZs: 1123, Bias: -0.458896, T: 5880, Avg. loss: 0.161039\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 18.50, NNZs: 1127, Bias: -0.460279, T: 6860, Avg. loss: 0.159492\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 18.64, NNZs: 1130, Bias: -0.460328, T: 7840, Avg. loss: 0.155114\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 18.76, NNZs: 1130, Bias: -0.460176, T: 8820, Avg. loss: 0.152313\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 18.87, NNZs: 1133, Bias: -0.458476, T: 9800, Avg. loss: 0.149152\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 18.97, NNZs: 1133, Bias: -0.458803, T: 10780, Avg. loss: 0.147729\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 19.06, NNZs: 1133, Bias: -0.458721, T: 11760, Avg. loss: 0.145401\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 19.14, NNZs: 1135, Bias: -0.460117, T: 12740, Avg. loss: 0.144367\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 19.22, NNZs: 1135, Bias: -0.459550, T: 13720, Avg. loss: 0.141952\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 19.28, NNZs: 1138, Bias: -0.459791, T: 14700, Avg. loss: 0.140819\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 19.35, NNZs: 1138, Bias: -0.459477, T: 15680, Avg. loss: 0.139697\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 19.41, NNZs: 1138, Bias: -0.460797, T: 16660, Avg. loss: 0.138322\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19.47, NNZs: 1138, Bias: -0.459725, T: 17640, Avg. loss: 0.136548\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 19.51, NNZs: 1138, Bias: -0.461524, T: 18620, Avg. loss: 0.136196\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 19.57, NNZs: 1138, Bias: -0.461221, T: 19600, Avg. loss: 0.134935\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 19.61, NNZs: 1138, Bias: -0.461793, T: 20580, Avg. loss: 0.134106\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 19.66, NNZs: 1138, Bias: -0.461958, T: 21560, Avg. loss: 0.133062\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 19.70, NNZs: 1138, Bias: -0.462079, T: 22540, Avg. loss: 0.132546\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 19.74, NNZs: 1138, Bias: -0.462285, T: 23520, Avg. loss: 0.131588\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 19.77, NNZs: 1138, Bias: -0.462970, T: 24500, Avg. loss: 0.130927\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 19.80, NNZs: 1138, Bias: -0.463634, T: 25480, Avg. loss: 0.130210\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 19.84, NNZs: 1138, Bias: -0.462956, T: 26460, Avg. loss: 0.129459\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 27 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 15.41, NNZs: 1258, Bias: -0.269021, T: 980, Avg. loss: 0.622957\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 15.91, NNZs: 1263, Bias: -0.265376, T: 1960, Avg. loss: 0.418038\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 16.22, NNZs: 1263, Bias: -0.268151, T: 2940, Avg. loss: 0.381418\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 16.44, NNZs: 1263, Bias: -0.268167, T: 3920, Avg. loss: 0.362993\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 16.61, NNZs: 1263, Bias: -0.268601, T: 4900, Avg. loss: 0.350450\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 16.74, NNZs: 1264, Bias: -0.269054, T: 5880, Avg. loss: 0.341049\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 16.85, NNZs: 1265, Bias: -0.269514, T: 6860, Avg. loss: 0.333997\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16.94, NNZs: 1265, Bias: -0.269412, T: 7840, Avg. loss: 0.328166\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 17.02, NNZs: 1265, Bias: -0.268761, T: 8820, Avg. loss: 0.323762\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 17.09, NNZs: 1265, Bias: -0.268475, T: 9800, Avg. loss: 0.319294\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 17.16, NNZs: 1265, Bias: -0.268359, T: 10780, Avg. loss: 0.315617\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 17.22, NNZs: 1265, Bias: -0.267936, T: 11760, Avg. loss: 0.312334\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 17.27, NNZs: 1267, Bias: -0.267393, T: 12740, Avg. loss: 0.309654\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 17.32, NNZs: 1269, Bias: -0.267041, T: 13720, Avg. loss: 0.306745\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 17.36, NNZs: 1269, Bias: -0.266438, T: 14700, Avg. loss: 0.304517\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 17.40, NNZs: 1269, Bias: -0.266064, T: 15680, Avg. loss: 0.302342\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 17.44, NNZs: 1269, Bias: -0.265953, T: 16660, Avg. loss: 0.300159\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 17.48, NNZs: 1269, Bias: -0.265556, T: 17640, Avg. loss: 0.298657\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 17.51, NNZs: 1269, Bias: -0.265553, T: 18620, Avg. loss: 0.296633\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17.54, NNZs: 1269, Bias: -0.265162, T: 19600, Avg. loss: 0.295212\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17.57, NNZs: 1269, Bias: -0.264684, T: 20580, Avg. loss: 0.293742\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 17.60, NNZs: 1269, Bias: -0.264629, T: 21560, Avg. loss: 0.292220\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 17.63, NNZs: 1269, Bias: -0.264228, T: 22540, Avg. loss: 0.290976\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 17.65, NNZs: 1269, Bias: -0.263866, T: 23520, Avg. loss: 0.289741\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 17.68, NNZs: 1269, Bias: -0.263692, T: 24500, Avg. loss: 0.288532\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 17.70, NNZs: 1269, Bias: -0.263480, T: 25480, Avg. loss: 0.287440\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 17.72, NNZs: 1271, Bias: -0.263523, T: 26460, Avg. loss: 0.286310\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 17.74, NNZs: 1271, Bias: -0.263104, T: 27440, Avg. loss: 0.285432\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 17.76, NNZs: 1271, Bias: -0.262689, T: 28420, Avg. loss: 0.284443\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 17.79, NNZs: 1271, Bias: -0.262509, T: 29400, Avg. loss: 0.283455\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 17.81, NNZs: 1271, Bias: -0.262415, T: 30380, Avg. loss: 0.282522\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 17.82, NNZs: 1271, Bias: -0.262104, T: 31360, Avg. loss: 0.281755\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 32 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 15.78, NNZs: 1210, Bias: -0.117690, T: 980, Avg. loss: 0.535777\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 16.45, NNZs: 1221, Bias: -0.077576, T: 1960, Avg. loss: 0.339413\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 16.87, NNZs: 1228, Bias: -0.066818, T: 2940, Avg. loss: 0.302227\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 17.17, NNZs: 1230, Bias: -0.059169, T: 3920, Avg. loss: 0.282049\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 17.38, NNZs: 1231, Bias: -0.054662, T: 4900, Avg. loss: 0.269253\n",
            "Total training time: 0.00 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 6\n",
            "Norm: 17.54, NNZs: 1231, Bias: -0.048793, T: 5880, Avg. loss: 0.260050\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 17.69, NNZs: 1234, Bias: -0.046925, T: 6860, Avg. loss: 0.253197\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 17.82, NNZs: 1239, Bias: -0.045184, T: 7840, Avg. loss: 0.247403\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 17.93, NNZs: 1239, Bias: -0.042646, T: 8820, Avg. loss: 0.242537\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 18.03, NNZs: 1239, Bias: -0.040601, T: 9800, Avg. loss: 0.238412\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 18.12, NNZs: 1239, Bias: -0.038408, T: 10780, Avg. loss: 0.234701\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 18.20, NNZs: 1240, Bias: -0.036604, T: 11760, Avg. loss: 0.231406\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 18.28, NNZs: 1240, Bias: -0.035386, T: 12740, Avg. loss: 0.228439\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 18.35, NNZs: 1240, Bias: -0.034536, T: 13720, Avg. loss: 0.225778\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 18.41, NNZs: 1241, Bias: -0.033673, T: 14700, Avg. loss: 0.223355\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 18.47, NNZs: 1242, Bias: -0.033279, T: 15680, Avg. loss: 0.221215\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 18.52, NNZs: 1242, Bias: -0.032739, T: 16660, Avg. loss: 0.219232\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 18.57, NNZs: 1242, Bias: -0.031627, T: 17640, Avg. loss: 0.217338\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 18.62, NNZs: 1242, Bias: -0.031327, T: 18620, Avg. loss: 0.215637\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 18.67, NNZs: 1242, Bias: -0.030851, T: 19600, Avg. loss: 0.213989\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 18.71, NNZs: 1242, Bias: -0.030301, T: 20580, Avg. loss: 0.212491\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 18.75, NNZs: 1242, Bias: -0.030175, T: 21560, Avg. loss: 0.211081\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 18.79, NNZs: 1242, Bias: -0.029876, T: 22540, Avg. loss: 0.209743\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 18.83, NNZs: 1242, Bias: -0.029140, T: 23520, Avg. loss: 0.208467\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 18.86, NNZs: 1242, Bias: -0.028969, T: 24500, Avg. loss: 0.207297\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 18.90, NNZs: 1242, Bias: -0.028723, T: 25480, Avg. loss: 0.206146\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 18.93, NNZs: 1242, Bias: -0.028466, T: 26460, Avg. loss: 0.205063\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 18.96, NNZs: 1242, Bias: -0.028279, T: 27440, Avg. loss: 0.204052\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 18.99, NNZs: 1242, Bias: -0.028115, T: 28420, Avg. loss: 0.203071\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 19.01, NNZs: 1242, Bias: -0.027878, T: 29400, Avg. loss: 0.202169\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 19.04, NNZs: 1242, Bias: -0.027657, T: 30380, Avg. loss: 0.201287\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 19.07, NNZs: 1242, Bias: -0.027764, T: 31360, Avg. loss: 0.200467\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 19.09, NNZs: 1242, Bias: -0.027588, T: 32340, Avg. loss: 0.199655\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 33 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 17.52, NNZs: 1034, Bias: -0.505298, T: 980, Avg. loss: 0.334610\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 18.02, NNZs: 1096, Bias: -0.500290, T: 1960, Avg. loss: 0.218552\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 18.34, NNZs: 1105, Bias: -0.493254, T: 2940, Avg. loss: 0.192409\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 18.59, NNZs: 1112, Bias: -0.493157, T: 3920, Avg. loss: 0.184004\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 18.78, NNZs: 1116, Bias: -0.490259, T: 4900, Avg. loss: 0.176737\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 18.93, NNZs: 1120, Bias: -0.490342, T: 5880, Avg. loss: 0.170530\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 19.07, NNZs: 1123, Bias: -0.490206, T: 6860, Avg. loss: 0.167087\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 19.19, NNZs: 1123, Bias: -0.490179, T: 7840, Avg. loss: 0.163151\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 19.30, NNZs: 1128, Bias: -0.490598, T: 8820, Avg. loss: 0.160997\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 19.40, NNZs: 1128, Bias: -0.488743, T: 9800, Avg. loss: 0.156928\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 19.48, NNZs: 1130, Bias: -0.490859, T: 10780, Avg. loss: 0.156450\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 19.56, NNZs: 1130, Bias: -0.490372, T: 11760, Avg. loss: 0.153393\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 19.63, NNZs: 1130, Bias: -0.491726, T: 12740, Avg. loss: 0.152169\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 19.70, NNZs: 1131, Bias: -0.491276, T: 13720, Avg. loss: 0.149789\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 19.77, NNZs: 1132, Bias: -0.492175, T: 14700, Avg. loss: 0.148435\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 19.83, NNZs: 1132, Bias: -0.492493, T: 15680, Avg. loss: 0.146988\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 19.88, NNZs: 1132, Bias: -0.493590, T: 16660, Avg. loss: 0.145579\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19.93, NNZs: 1132, Bias: -0.493613, T: 17640, Avg. loss: 0.143942\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 19.98, NNZs: 1132, Bias: -0.494344, T: 18620, Avg. loss: 0.143033\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 20.03, NNZs: 1132, Bias: -0.495024, T: 19600, Avg. loss: 0.142036\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 20.07, NNZs: 1134, Bias: -0.495311, T: 20580, Avg. loss: 0.140999\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 20.11, NNZs: 1134, Bias: -0.496027, T: 21560, Avg. loss: 0.140260\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 20.15, NNZs: 1134, Bias: -0.496012, T: 22540, Avg. loss: 0.139432\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 20.18, NNZs: 1134, Bias: -0.496662, T: 23520, Avg. loss: 0.138507\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 20.22, NNZs: 1135, Bias: -0.497338, T: 24500, Avg. loss: 0.137764\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 20.25, NNZs: 1135, Bias: -0.497806, T: 25480, Avg. loss: 0.137087\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 26 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 15.17, NNZs: 1287, Bias: -0.282835, T: 980, Avg. loss: 0.612313\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 15.79, NNZs: 1291, Bias: -0.279930, T: 1960, Avg. loss: 0.399854\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 16.13, NNZs: 1291, Bias: -0.282521, T: 2940, Avg. loss: 0.361727\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 16.35, NNZs: 1291, Bias: -0.281532, T: 3920, Avg. loss: 0.343079\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 16.52, NNZs: 1291, Bias: -0.281264, T: 4900, Avg. loss: 0.330365\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 16.66, NNZs: 1293, Bias: -0.280195, T: 5880, Avg. loss: 0.320825\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 16.77, NNZs: 1296, Bias: -0.279530, T: 6860, Avg. loss: 0.313644\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16.86, NNZs: 1296, Bias: -0.278698, T: 7840, Avg. loss: 0.307939\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 16.95, NNZs: 1296, Bias: -0.277768, T: 8820, Avg. loss: 0.303262\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 17.03, NNZs: 1296, Bias: -0.277243, T: 9800, Avg. loss: 0.299092\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 17.09, NNZs: 1296, Bias: -0.276419, T: 10780, Avg. loss: 0.295402\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 17.16, NNZs: 1296, Bias: -0.276373, T: 11760, Avg. loss: 0.292041\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 17.21, NNZs: 1296, Bias: -0.275734, T: 12740, Avg. loss: 0.289565\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 17.26, NNZs: 1296, Bias: -0.275517, T: 13720, Avg. loss: 0.286808\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 17.31, NNZs: 1299, Bias: -0.275097, T: 14700, Avg. loss: 0.284654\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 17.35, NNZs: 1299, Bias: -0.274572, T: 15680, Avg. loss: 0.282700\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 17.39, NNZs: 1299, Bias: -0.274497, T: 16660, Avg. loss: 0.280646\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 17.42, NNZs: 1299, Bias: -0.273929, T: 17640, Avg. loss: 0.279062\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 17.46, NNZs: 1299, Bias: -0.273889, T: 18620, Avg. loss: 0.277258\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17.49, NNZs: 1299, Bias: -0.273536, T: 19600, Avg. loss: 0.275803\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17.52, NNZs: 1299, Bias: -0.273215, T: 20580, Avg. loss: 0.274450\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 17.55, NNZs: 1299, Bias: -0.272957, T: 21560, Avg. loss: 0.273070\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 17.58, NNZs: 1299, Bias: -0.272829, T: 22540, Avg. loss: 0.271865\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 17.61, NNZs: 1299, Bias: -0.272454, T: 23520, Avg. loss: 0.270698\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 17.64, NNZs: 1299, Bias: -0.272234, T: 24500, Avg. loss: 0.269531\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 17.66, NNZs: 1299, Bias: -0.271930, T: 25480, Avg. loss: 0.268418\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 17.69, NNZs: 1299, Bias: -0.272068, T: 26460, Avg. loss: 0.267320\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 17.71, NNZs: 1299, Bias: -0.271712, T: 27440, Avg. loss: 0.266411\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 17.73, NNZs: 1299, Bias: -0.271400, T: 28420, Avg. loss: 0.265461\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 17.75, NNZs: 1299, Bias: -0.271121, T: 29400, Avg. loss: 0.264514\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 17.78, NNZs: 1299, Bias: -0.271044, T: 30380, Avg. loss: 0.263589\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 17.80, NNZs: 1299, Bias: -0.270656, T: 31360, Avg. loss: 0.262799\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 32 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 22449859492150.74, NNZs: 856, Bias: 2266634039.575268, T: 979, Avg. loss: 26273605603212784566272.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 32766983089107.25, NNZs: 982, Bias: -59065071635.109924, T: 1958, Avg. loss: 235567610375965660676096.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 29518108027470.61, NNZs: 1044, Bias: -8031244333.289127, T: 2937, Avg. loss: 85042936610441079554048.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 25494549110977.14, NNZs: 1059, Bias: -89377858662.619507, T: 3916, Avg. loss: 37280220216985303121920.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 22090304397854.29, NNZs: 1064, Bias: -65257488628.247238, T: 4895, Avg. loss: 17246926325957288525824.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 19529761169150.86, NNZs: 1068, Bias: -110197368547.712677, T: 5874, Avg. loss: 6340747956662149578752.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 17553184677160.40, NNZs: 1068, Bias: -48739621248.764511, T: 6853, Avg. loss: 3218483503490394488832.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 15874532141833.30, NNZs: 1069, Bias: -84325542016.080429, T: 7832, Avg. loss: 4645061848538194378752.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 14497006687038.85, NNZs: 1069, Bias: -103158855674.355362, T: 8811, Avg. loss: 3096967379587752263680.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13363693434386.55, NNZs: 1072, Bias: -95470953597.167816, T: 9790, Avg. loss: 2058923957838546993152.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 12406468937822.90, NNZs: 1072, Bias: -80675837132.461014, T: 10769, Avg. loss: 1343961703465939632128.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11582700384075.72, NNZs: 1072, Bias: -75775325902.245712, T: 11748, Avg. loss: 606431143053479903232.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 10868015428724.41, NNZs: 1072, Bias: -81263683576.648636, T: 12727, Avg. loss: 143702160878245511168.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 10251487472736.75, NNZs: 1072, Bias: -77627764987.982101, T: 13706, Avg. loss: 29130250605085188096.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 9704713256923.52, NNZs: 1073, Bias: -77778443863.812393, T: 14685, Avg. loss: 3403677595571063296.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 9218074817678.32, NNZs: 1073, Bias: -77525405249.059647, T: 15664, Avg. loss: 173995253791297760.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 8781787295873.64, NNZs: 1073, Bias: -77088265536.633987, T: 16643, Avg. loss: 130382057227564176.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 8388089032277.53, NNZs: 1075, Bias: -76712750894.035019, T: 17622, Avg. loss: 100229996303197088.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 8031069829419.20, NNZs: 1075, Bias: -76195441432.445374, T: 18601, Avg. loss: 156611448332729888.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 7705545433779.11, NNZs: 1075, Bias: -75763811786.685211, T: 19580, Avg. loss: 109711121955758944.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 7407484655693.05, NNZs: 1075, Bias: -75342269578.573517, T: 20559, Avg. loss: 93635095736950528.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 7133646665689.60, NNZs: 1075, Bias: -74764229470.373016, T: 21538, Avg. loss: 156795047115071616.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 6880940874414.37, NNZs: 1075, Bias: -74254326870.247406, T: 22517, Avg. loss: 136978901765829680.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 6646869605778.83, NNZs: 1075, Bias: -73877350634.680954, T: 23496, Avg. loss: 80783951937510832.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 6429600743172.13, NNZs: 1075, Bias: -73421299608.659164, T: 24475, Avg. loss: 117149445544329504.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 6227258652880.38, NNZs: 1075, Bias: -72986081543.613815, T: 25454, Avg. loss: 116299974615039968.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 6038345594031.73, NNZs: 1075, Bias: -72547800007.026657, T: 26433, Avg. loss: 118393652144776192.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 5861534448636.96, NNZs: 1075, Bias: -72112075963.892090, T: 27412, Avg. loss: 121792123731806288.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 5695633310552.13, NNZs: 1075, Bias: -71711192572.435532, T: 28391, Avg. loss: 104077454918429136.000000\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 29 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 93404961794841.20, NNZs: 709, Bias: -324558523896.922424, T: 979, Avg. loss: 1897633871668386212085760.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 80881685378846.98, NNZs: 853, Bias: -878888455103.775757, T: 1958, Avg. loss: 1577047983468912295542784.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 68180736396033.24, NNZs: 882, Bias: -794708221388.758911, T: 2937, Avg. loss: 296290487382058683334656.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 58263262298397.89, NNZs: 896, Bias: -838880505761.044189, T: 3916, Avg. loss: 222630817355345824841728.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 50592351098045.70, NNZs: 897, Bias: -911169366940.033325, T: 4895, Avg. loss: 80532380954619338031104.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 44620206753073.64, NNZs: 900, Bias: -923025294373.522461, T: 5874, Avg. loss: 59963161453529883738112.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 40206400732388.91, NNZs: 903, Bias: -894260489869.719727, T: 6853, Avg. loss: 27550715074352580132864.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 36369573251402.50, NNZs: 903, Bias: -894002880226.297607, T: 7832, Avg. loss: 19060580667534344192000.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 33310925104587.04, NNZs: 903, Bias: -892329482529.293213, T: 8811, Avg. loss: 11941649412297118973952.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 30841453692683.18, NNZs: 903, Bias: -874945595289.389038, T: 9790, Avg. loss: 11093356011183419686912.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 28461559818779.04, NNZs: 903, Bias: -895714859191.575073, T: 10769, Avg. loss: 15423265498344212922368.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 26581045510627.77, NNZs: 903, Bias: -893103499612.238403, T: 11748, Avg. loss: 4955342832419610820608.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 25029533018092.11, NNZs: 903, Bias: -882117345609.739014, T: 12727, Avg. loss: 2352404592670269767680.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 23587351959947.84, NNZs: 903, Bias: -880306046304.517212, T: 13706, Avg. loss: 2926457704156403073024.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 22262111396100.09, NNZs: 903, Bias: -894671961063.456299, T: 14685, Avg. loss: 1470111964876252905472.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 21186163829918.26, NNZs: 903, Bias: -884083055603.316895, T: 15664, Avg. loss: 803743683657103572992.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 20221682041615.20, NNZs: 903, Bias: -874947164369.308105, T: 16643, Avg. loss: 408668500301584596992.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19335330209119.44, NNZs: 903, Bias: -870133233259.067261, T: 17622, Avg. loss: 159894335986906693632.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 18518903735034.05, NNZs: 903, Bias: -868235094561.310913, T: 18601, Avg. loss: 132235278753543094272.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17789915802489.12, NNZs: 903, Bias: -863449348014.409180, T: 19580, Avg. loss: 10580496018571522048.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17121547006100.88, NNZs: 903, Bias: -859227702342.069458, T: 20559, Avg. loss: 19717589183089246208.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16509131211148.00, NNZs: 903, Bias: -854914923205.435791, T: 21538, Avg. loss: 3070001212369350656.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 15953313028415.32, NNZs: 903, Bias: -849175567251.626953, T: 22517, Avg. loss: 6483878660210586624.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 15436300647776.46, NNZs: 903, Bias: -844247644664.291992, T: 23496, Avg. loss: 3337286805707964928.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 14951802416607.29, NNZs: 903, Bias: -840401550661.049072, T: 24475, Avg. loss: 4129690702964843008.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 14513541580889.56, NNZs: 903, Bias: -834499383288.313232, T: 25454, Avg. loss: 6276451418664105984.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 14095202019747.08, NNZs: 903, Bias: -830512721406.113159, T: 26433, Avg. loss: 2027546453136990208.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 13718534051011.68, NNZs: 903, Bias: -824257863438.165405, T: 27412, Avg. loss: 5019872229140290560.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 13345263197052.54, NNZs: 903, Bias: -821591627739.513550, T: 28391, Avg. loss: 4547074280411593728.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 13015421169563.49, NNZs: 903, Bias: -815642774528.142334, T: 29370, Avg. loss: 4912313200405326848.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 12689734477846.46, NNZs: 903, Bias: -812373125081.704224, T: 30349, Avg. loss: 1972800723168018688.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 12389526885033.24, NNZs: 903, Bias: -808109579677.901245, T: 31328, Avg. loss: 2787245728936611840.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 12112051595659.86, NNZs: 903, Bias: -803103974100.711914, T: 32307, Avg. loss: 3096178950990848512.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 11842762836596.20, NNZs: 903, Bias: -799300863012.017212, T: 33286, Avg. loss: 2565082063122379264.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 11590054336192.53, NNZs: 903, Bias: -795267940224.216187, T: 34265, Avg. loss: 2457930434928495104.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 11347337179925.98, NNZs: 903, Bias: -791784362419.035278, T: 35244, Avg. loss: 2021820736890939136.000000\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 36 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 11655675982.66, NNZs: 823, Bias: -81762411.526893, T: 979, Avg. loss: 3152517013079834.500000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2934677535607.41, NNZs: 1041, Bias: -7146199097.048448, T: 1958, Avg. loss: 546856062930875580416.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 9905049249489.02, NNZs: 1086, Bias: -64632169759.291885, T: 2937, Avg. loss: 16107823209316609949696.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 10371223145365.77, NNZs: 1119, Bias: -60020889113.403343, T: 3916, Avg. loss: 31265221786198452731904.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 9316398147549.27, NNZs: 1125, Bias: -80902724519.455933, T: 4895, Avg. loss: 8262558003488505200640.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 8119993972738.85, NNZs: 1130, Bias: -54260359892.625572, T: 5874, Avg. loss: 4467178083415868047360.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10317491956468.65, NNZs: 813, Bias: -83517654951.005188, T: 979, Avg. loss: 1241068657749556199424.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 32794264851506.52, NNZs: 981, Bias: -29025079191.467739, T: 1958, Avg. loss: 179403477792412068216832.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 29193267701943.61, NNZs: 1028, Bias: 825064317.757477, T: 2937, Avg. loss: 91328655741358266384384.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 25448076382270.86, NNZs: 1041, Bias: -71783984298.506500, T: 3916, Avg. loss: 32144812273903203254272.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 22158782792825.00, NNZs: 1046, Bias: 22567140949.960747, T: 4895, Avg. loss: 13481411313361905254400.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 19592650252234.60, NNZs: 1048, Bias: -31858182084.869923, T: 5874, Avg. loss: 7270344144927997296640.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 97100704598796.08, NNZs: 734, Bias: 128111503237.167953, T: 979, Avg. loss: 1731269105878628483203072.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 82935567359731.55, NNZs: 874, Bias: -628974366159.806152, T: 1958, Avg. loss: 1733227977423307810013184.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 70148894931839.87, NNZs: 917, Bias: -643796702416.577148, T: 2937, Avg. loss: 317174634052822969090048.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 59584923835510.73, NNZs: 929, Bias: -723064283184.628784, T: 3916, Avg. loss: 217744677290780137619456.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 50718956615564.74, NNZs: 933, Bias: -890858184695.267456, T: 4895, Avg. loss: 88702120118298125271040.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 44750729791188.33, NNZs: 935, Bias: -888283267791.316162, T: 5874, Avg. loss: 31780421530053372805120.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 39880264306928.38, NNZs: 935, Bias: -904643720793.808838, T: 6853, Avg. loss: 37596718366781522051072.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 35962842684528.51, NNZs: 935, Bias: -908638740242.834351, T: 7832, Avg. loss: 32416426148273686839296.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 32821781484029.05, NNZs: 935, Bias: -912378515651.178589, T: 8811, Avg. loss: 13720761984456838873088.000000\n",
            "Total training time: 0.01 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 10\n",
            "Norm: 30454222488405.86, NNZs: 935, Bias: -879510481477.805664, T: 9790, Avg. loss: 10639743547768144134144.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 28231414897846.46, NNZs: 935, Bias: -881084750626.841919, T: 10769, Avg. loss: 6779392080979557351424.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 26342046406377.79, NNZs: 935, Bias: -883860898370.824829, T: 11748, Avg. loss: 2691941209128738226176.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 24819828642815.37, NNZs: 935, Bias: -863252361552.578003, T: 12727, Avg. loss: 1480080851257408094208.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 23426458237398.36, NNZs: 935, Bias: -860115953773.402954, T: 13706, Avg. loss: 915466498084956143616.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 22170316417695.40, NNZs: 935, Bias: -860534051564.394897, T: 14685, Avg. loss: 1077124923148436897792.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 21049664499854.42, NNZs: 935, Bias: -861433727552.751221, T: 15664, Avg. loss: 791737453077992308736.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 20101070354153.68, NNZs: 935, Bias: -849876113200.362183, T: 16643, Avg. loss: 301677070854366298112.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19208551407220.17, NNZs: 935, Bias: -847510025579.819458, T: 17622, Avg. loss: 178297091581351100416.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 18418397271408.33, NNZs: 935, Bias: -841026791994.416382, T: 18601, Avg. loss: 108229635149638287360.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17688715516523.79, NNZs: 935, Bias: -837148561805.737305, T: 19580, Avg. loss: 19671233146495844352.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17040179726185.35, NNZs: 935, Bias: -829564032730.432495, T: 20559, Avg. loss: 14003402179888723968.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16426980910637.86, NNZs: 935, Bias: -825885330267.046265, T: 21538, Avg. loss: 3766098144484971520.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 15878814436623.97, NNZs: 935, Bias: -819036818959.244263, T: 22517, Avg. loss: 6350607537900594176.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 15366775011095.47, NNZs: 935, Bias: -813485291931.943604, T: 23496, Avg. loss: 3896948643656615936.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 14883996527694.93, NNZs: 935, Bias: -809602405656.284668, T: 24475, Avg. loss: 2500948490980406272.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 14446425260380.59, NNZs: 935, Bias: -803739602117.339111, T: 25454, Avg. loss: 3189442037936945152.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 14032151126501.41, NNZs: 935, Bias: -799256068365.352905, T: 26433, Avg. loss: 2424979293657000448.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 13655740497214.44, NNZs: 935, Bias: -793032561333.468750, T: 27412, Avg. loss: 4353386623361288192.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 13290824503284.32, NNZs: 935, Bias: -789148723800.308838, T: 28391, Avg. loss: 2705792291149418496.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 12962653527286.50, NNZs: 935, Bias: -782990277212.240234, T: 29370, Avg. loss: 4679877706663666688.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 12639960787330.62, NNZs: 935, Bias: -779341927010.389038, T: 30349, Avg. loss: 2040772609882199808.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 12338870874033.75, NNZs: 935, Bias: -775300169032.673828, T: 31328, Avg. loss: 2189192907649242368.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 12058292632656.27, NNZs: 935, Bias: -770785900364.780518, T: 32307, Avg. loss: 2338894631598626304.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 11787937230252.88, NNZs: 935, Bias: -767195384598.901489, T: 33286, Avg. loss: 1654538038685705472.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 11538520425992.39, NNZs: 935, Bias: -762688361637.014160, T: 34265, Avg. loss: 2316976626028783616.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 11296344012271.13, NNZs: 935, Bias: -759162884228.310059, T: 35244, Avg. loss: 1770016899419317504.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 11066691577492.13, NNZs: 935, Bias: -755675686250.058838, T: 36223, Avg. loss: 1960273573491723776.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 10854204370189.57, NNZs: 935, Bias: -751407355282.035522, T: 37202, Avg. loss: 2153387383469787648.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 10645688922311.74, NNZs: 935, Bias: -748167980880.699097, T: 38181, Avg. loss: 1518808653322347008.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 10456062716377.74, NNZs: 935, Bias: -743705390561.798584, T: 39160, Avg. loss: 2829189988649147904.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 10262422541150.82, NNZs: 935, Bias: -741141345456.522949, T: 40139, Avg. loss: 1195216325733711616.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 10084425159432.62, NNZs: 935, Bias: -737642362100.035645, T: 41118, Avg. loss: 1808592628168635904.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 9916696704168.30, NNZs: 935, Bias: -733897217143.100342, T: 42097, Avg. loss: 1826672445819340800.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 9750122628685.06, NNZs: 935, Bias: -731055500651.122925, T: 43076, Avg. loss: 1203867407438206464.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 9595335610162.14, NNZs: 935, Bias: -727630187652.925171, T: 44055, Avg. loss: 1655820043573148416.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 9445582633185.48, NNZs: 935, Bias: -724455872996.659790, T: 45034, Avg. loss: 1533493778006002432.000000\n",
            "Total training time: 0.06 seconds.\n",
            "Convergence after 46 epochs took 0.06 seconds\n",
            "-- Epoch 1\n",
            "Norm: 133670072.00, NNZs: 831, Bias: -660412.967813, T: 979, Avg. loss: 411949192977.967773\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 9956311722.24, NNZs: 1002, Bias: -62326749.426894, T: 1958, Avg. loss: 6724580274265879.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 42090792365.27, NNZs: 1084, Bias: -18166199.872074, T: 2937, Avg. loss: 227956899672850976.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 96246794505.42, NNZs: 1124, Bias: -272597381.342233, T: 3916, Avg. loss: 2369532699889222656.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 106208005580.15, NNZs: 1141, Bias: -494712368.329554, T: 4895, Avg. loss: 2592068522218804224.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 97714079136.68, NNZs: 1143, Bias: -659144974.297906, T: 5874, Avg. loss: 1076924413729688448.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 11643350668722.39, NNZs: 836, Bias: -48267321335.392548, T: 979, Avg. loss: 1652285694186385309696.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 31629126392805.92, NNZs: 992, Bias: -103830588076.647583, T: 1958, Avg. loss: 198919709058141551329280.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 28805554814560.68, NNZs: 1052, Bias: -85812837054.574524, T: 2937, Avg. loss: 80745457086380946489344.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 25144482940372.68, NNZs: 1073, Bias: -140783917118.644226, T: 3916, Avg. loss: 27398158722061528203264.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 21806579343561.95, NNZs: 1077, Bias: -96211870057.282852, T: 4895, Avg. loss: 13714829159222608920576.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 19288216910030.43, NNZs: 1079, Bias: -97568540259.006821, T: 5874, Avg. loss: 6828416953790256644096.000000\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 92941868249031.47, NNZs: 742, Bias: -223826104262.096069, T: 979, Avg. loss: 2024545251766837494939648.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 80182048081871.45, NNZs: 886, Bias: -998574983948.187256, T: 1958, Avg. loss: 1779628147224707032154112.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 66962424119466.01, NNZs: 903, Bias: -1096460611130.143433, T: 2937, Avg. loss: 488515067472918662348800.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 58193786490608.10, NNZs: 913, Bias: -1042224828259.763916, T: 3916, Avg. loss: 110584188504056897994752.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 50730814873118.24, NNZs: 921, Bias: -1074660142636.329224, T: 4895, Avg. loss: 130774503336902676971520.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 44446299979756.53, NNZs: 921, Bias: -1158234564632.968750, T: 5874, Avg. loss: 55786143769252889362432.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 39922682536731.05, NNZs: 921, Bias: -1104732504669.075439, T: 6853, Avg. loss: 29919407922812614082560.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 36140759544204.03, NNZs: 921, Bias: -1129545483021.279541, T: 7832, Avg. loss: 16493842908251991048192.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 33066634652981.36, NNZs: 921, Bias: -1126617561692.662598, T: 8811, Avg. loss: 29733278279813293932544.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 30539312155987.66, NNZs: 921, Bias: -1121253558987.885254, T: 9790, Avg. loss: 14358523747104785956864.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 28502738498416.40, NNZs: 921, Bias: -1096012151349.001465, T: 10769, Avg. loss: 9504586752373242724352.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 26546375155277.93, NNZs: 921, Bias: -1107746850955.551514, T: 11748, Avg. loss: 6306479173143718002688.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 25028619871594.61, NNZs: 921, Bias: -1089218615098.240967, T: 12727, Avg. loss: 3858832780900453842944.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 23482042438549.05, NNZs: 921, Bias: -1108371519568.305420, T: 13706, Avg. loss: 2353538330749271277568.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 22308054255742.06, NNZs: 921, Bias: -1092435983340.358276, T: 14685, Avg. loss: 1526618368430462205952.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 21184054523139.94, NNZs: 923, Bias: -1093596142107.202271, T: 15664, Avg. loss: 515804605704032944128.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 20272959784337.00, NNZs: 923, Bias: -1076014385184.797852, T: 16643, Avg. loss: 422581601471081414656.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19371393814518.52, NNZs: 923, Bias: -1074174090208.478271, T: 17622, Avg. loss: 213324127394443362304.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 18622121853951.71, NNZs: 923, Bias: -1060843540681.645996, T: 18601, Avg. loss: 73274793333790932992.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17890638380317.70, NNZs: 923, Bias: -1056667809482.588745, T: 19580, Avg. loss: 28970595301877354496.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17252242707828.70, NNZs: 923, Bias: -1047540298613.993530, T: 20559, Avg. loss: 20998242749211275264.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16653610159149.16, NNZs: 923, Bias: -1040969673374.583984, T: 21538, Avg. loss: 6415310554242359296.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 16091396109201.30, NNZs: 923, Bias: -1036345152167.212280, T: 22517, Avg. loss: 5605655777828302848.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 15609941415231.32, NNZs: 924, Bias: -1026099169912.075073, T: 23496, Avg. loss: 16110298348280995840.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 15135689344394.79, NNZs: 924, Bias: -1020513823876.369263, T: 24475, Avg. loss: 4258954829356356608.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 14701046319914.58, NNZs: 924, Bias: -1014216215785.817139, T: 25454, Avg. loss: 6091449782913831936.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 14294038345967.30, NNZs: 924, Bias: -1008430700259.125854, T: 26433, Avg. loss: 5168822790025851904.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 13922594464106.08, NNZs: 924, Bias: -1001624176370.669678, T: 27412, Avg. loss: 6935153434904177664.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 13568093673963.68, NNZs: 924, Bias: -995979124413.029541, T: 28391, Avg. loss: 4693707041434362880.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 13239714283520.04, NNZs: 924, Bias: -989975793479.156128, T: 29370, Avg. loss: 3667217810016715264.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 12934237177251.05, NNZs: 924, Bias: -983727200120.130371, T: 30349, Avg. loss: 4306098258569110528.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 12640367808496.04, NNZs: 924, Bias: -978512785389.143799, T: 31328, Avg. loss: 3936631418264303104.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 12367580328371.10, NNZs: 924, Bias: -972875994082.382202, T: 32307, Avg. loss: 4184500790393849856.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 12112890644681.61, NNZs: 924, Bias: -967013103470.939697, T: 33286, Avg. loss: 3483542853647925248.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 11863409404848.65, NNZs: 924, Bias: -962402158132.465942, T: 34265, Avg. loss: 2620230902806292992.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 11643296733476.59, NNZs: 924, Bias: -955887265613.439453, T: 35244, Avg. loss: 4222238124087190528.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 11417311899234.80, NNZs: 924, Bias: -951645884745.948608, T: 36223, Avg. loss: 2172607075755155968.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 11215809862294.10, NNZs: 924, Bias: -945905100158.528931, T: 37202, Avg. loss: 3168589347233304064.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 11021462123494.12, NNZs: 924, Bias: -940647550808.096924, T: 38181, Avg. loss: 3097093247149939712.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 10837999431025.63, NNZs: 924, Bias: -935323287637.555420, T: 39160, Avg. loss: 3677887733249820160.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 10658830446007.26, NNZs: 924, Bias: -930625062809.428955, T: 40139, Avg. loss: 2764987498418412032.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 10494839146994.58, NNZs: 924, Bias: -925227367919.855103, T: 41118, Avg. loss: 3325279399188149248.000000\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 42 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 4592094441.96, NNZs: 859, Bias: -24082162.769463, T: 979, Avg. loss: 474343703101927.937500\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1464576991981.51, NNZs: 1023, Bias: -13987984518.106550, T: 1958, Avg. loss: 135677821839109931008.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 8725714858557.64, NNZs: 1096, Bias: -27635391829.808762, T: 2937, Avg. loss: 13568800217208941707264.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 9614538223278.83, NNZs: 1129, Bias: -53032675118.461777, T: 3916, Avg. loss: 23860445722882198208512.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 8520313498977.62, NNZs: 1130, Bias: -51995138963.000282, T: 4895, Avg. loss: 7423754420429310132224.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 7525599237077.49, NNZs: 1140, Bias: -31897886580.099266, T: 5874, Avg. loss: 2739941767104982155264.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 24300738313283.81, NNZs: 843, Bias: 69840221314.506866, T: 979, Avg. loss: 24515553910699275059200.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 33321175338818.46, NNZs: 1007, Bias: -57471149911.031494, T: 1958, Avg. loss: 220378058326123139301376.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 30281940733675.40, NNZs: 1059, Bias: -104760019960.893219, T: 2937, Avg. loss: 94071342085369845776384.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 26064549889861.53, NNZs: 1074, Bias: -167505521045.026398, T: 3916, Avg. loss: 37388825403313876893696.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 22682788825280.37, NNZs: 1074, Bias: -55299379985.576088, T: 4895, Avg. loss: 14711438329615652749312.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 20097593756996.91, NNZs: 1082, Bias: -61054646794.670906, T: 5874, Avg. loss: 9363914809265502552064.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 18020402382496.09, NNZs: 1082, Bias: -58891612184.432549, T: 6853, Avg. loss: 6737957024455924383744.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16305647149401.58, NNZs: 1082, Bias: -58998137038.061829, T: 7832, Avg. loss: 3814784492022571991040.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 14921087085384.94, NNZs: 1082, Bias: -63172162766.204018, T: 8811, Avg. loss: 3019190356150007627776.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13744843525450.57, NNZs: 1082, Bias: -56587680523.627510, T: 9790, Avg. loss: 2613232292241049911296.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 12744194707698.33, NNZs: 1082, Bias: -71387698250.602997, T: 10769, Avg. loss: 1533503356004837359616.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11898801861420.31, NNZs: 1082, Bias: -63055361620.902863, T: 11748, Avg. loss: 630550796614134923264.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11168469713068.49, NNZs: 1082, Bias: -63591358963.400360, T: 12727, Avg. loss: 165504275902815272960.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 10532061931724.12, NNZs: 1082, Bias: -63536422513.068398, T: 13706, Avg. loss: 34886199331850825728.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 9970677832120.82, NNZs: 1082, Bias: -63144006067.711639, T: 14685, Avg. loss: 3079819367995409408.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 9470799491091.03, NNZs: 1082, Bias: -62655014930.283539, T: 15664, Avg. loss: 241164210666052096.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 9022339843581.74, NNZs: 1082, Bias: -62404746770.338654, T: 16643, Avg. loss: 61711423463632040.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 8617771558542.97, NNZs: 1082, Bias: -62038568242.468674, T: 17622, Avg. loss: 80791062453049264.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 8250712092525.50, NNZs: 1082, Bias: -61733056539.602898, T: 18601, Avg. loss: 67366062958324368.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 7916087140844.45, NNZs: 1082, Bias: -61438367140.117943, T: 19580, Avg. loss: 63475486838952368.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 7609643410249.49, NNZs: 1085, Bias: -61203204179.533035, T: 20559, Avg. loss: 41351096444857472.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 7327989591495.58, NNZs: 1085, Bias: -60884067152.269249, T: 21538, Avg. loss: 80227058567722672.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 7068104132936.36, NNZs: 1085, Bias: -60581334005.306778, T: 22517, Avg. loss: 83363417958604256.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 6827457722250.69, NNZs: 1085, Bias: -60340714818.027702, T: 23496, Avg. loss: 55400938578615760.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 6604000565103.27, NNZs: 1085, Bias: -60082380220.668510, T: 24475, Avg. loss: 65855747947447056.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 6395906173457.61, NNZs: 1085, Bias: -59817873012.150177, T: 25454, Avg. loss: 63692596674555816.000000\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 26 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 104946299140508.48, NNZs: 798, Bias: -176235923743.588013, T: 979, Avg. loss: 2608797389589764475191296.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 85983499263516.95, NNZs: 925, Bias: -1164002500356.429932, T: 1958, Avg. loss: 1789962499901049362448384.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 71651518302470.72, NNZs: 938, Bias: -1271819050998.808838, T: 2937, Avg. loss: 589259785482327629496320.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 60196314949698.78, NNZs: 955, Bias: -1372155585678.006104, T: 3916, Avg. loss: 291395793184195644227584.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 52772268723439.25, NNZs: 959, Bias: -1323541798655.490723, T: 4895, Avg. loss: 105303426929152466354176.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 46517273187956.17, NNZs: 959, Bias: -1301950799127.319580, T: 5874, Avg. loss: 85372085464019654148096.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 41399823944300.24, NNZs: 959, Bias: -1311834601880.128418, T: 6853, Avg. loss: 57396348454780218114048.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 37173495146775.16, NNZs: 960, Bias: -1367537895251.980713, T: 7832, Avg. loss: 42277651579524370923520.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 34171496510285.87, NNZs: 960, Bias: -1349573679163.615967, T: 8811, Avg. loss: 23659276949982168481792.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 31679661646578.21, NNZs: 960, Bias: -1319887655427.023926, T: 9790, Avg. loss: 24668204636591058059264.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 29402900356504.25, NNZs: 960, Bias: -1314030811650.206787, T: 10769, Avg. loss: 11198260992439119511552.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 27409980279980.91, NNZs: 960, Bias: -1323914439436.250732, T: 11748, Avg. loss: 7265790473004684148736.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 25823225042126.35, NNZs: 960, Bias: -1305014700704.599854, T: 12727, Avg. loss: 4795380361482985275392.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 24361132959184.59, NNZs: 960, Bias: -1304064218626.634521, T: 13706, Avg. loss: 1522605992415891292160.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 23181058781248.80, NNZs: 960, Bias: -1282455196711.068115, T: 14685, Avg. loss: 2358034890711818567680.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 22008871165123.29, NNZs: 960, Bias: -1283616198402.155029, T: 15664, Avg. loss: 991975248193040023552.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 21082524061361.89, NNZs: 960, Bias: -1263645970774.794434, T: 16643, Avg. loss: 498191633591454990336.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 20186723593889.25, NNZs: 960, Bias: -1255483507302.290039, T: 17622, Avg. loss: 144351951583416320000.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 19382704624867.69, NNZs: 960, Bias: -1246496023885.886963, T: 18601, Avg. loss: 120978941782987816960.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 18665060325772.83, NNZs: 963, Bias: -1236152467073.646973, T: 19580, Avg. loss: 61905395578779385856.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17988728922275.62, NNZs: 963, Bias: -1229105067102.893799, T: 20559, Avg. loss: 29843582666456641536.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 17396981574038.47, NNZs: 963, Bias: -1218301658396.992188, T: 21538, Avg. loss: 15893809016646060032.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 16839768028627.20, NNZs: 963, Bias: -1209730265152.546143, T: 22517, Avg. loss: 6153605770639372288.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 16337240410639.67, NNZs: 963, Bias: -1199911537310.286865, T: 23496, Avg. loss: 9301341712361111552.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 15874101046171.99, NNZs: 963, Bias: -1190146796001.812500, T: 24475, Avg. loss: 19177410301241253888.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 15441420037984.84, NNZs: 963, Bias: -1181180888087.618896, T: 25454, Avg. loss: 12035330632309370880.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 15034031608498.61, NNZs: 963, Bias: -1173124705796.190674, T: 26433, Avg. loss: 8988026829771392000.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 14668450355440.44, NNZs: 963, Bias: -1163418059657.168945, T: 27412, Avg. loss: 9041400137408270336.000000\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 28 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 86968421.14, NNZs: 871, Bias: -495209.162210, T: 979, Avg. loss: 172915652488.811157\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 11039778068.20, NNZs: 1021, Bias: -78805634.014279, T: 1958, Avg. loss: 7079989897539360.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 84815542191.41, NNZs: 1111, Bias: -1014011842.215529, T: 2937, Avg. loss: 897020647077329664.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 134805145100.36, NNZs: 1150, Bias: -879823268.642464, T: 3916, Avg. loss: 3902870068566523392.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 140923121391.48, NNZs: 1168, Bias: -562927731.316085, T: 4895, Avg. loss: 3797148086225831936.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 137394797758.35, NNZs: 1185, Bias: -901151500.047797, T: 5874, Avg. loss: 4303146366550019584.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 21063914157866.45, NNZs: 869, Bias: 9671634118.589840, T: 979, Avg. loss: 14827365880947227492352.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 34098065792496.88, NNZs: 1022, Bias: -135203396088.721390, T: 1958, Avg. loss: 202425061118365195567104.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 30298389462035.07, NNZs: 1082, Bias: -131470958217.702560, T: 2937, Avg. loss: 90102877027289884786688.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 26036018013139.43, NNZs: 1088, Bias: -213430275314.768860, T: 3916, Avg. loss: 33627508735201790394368.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 22626409770294.68, NNZs: 1092, Bias: -175128194127.585175, T: 4895, Avg. loss: 24070239184231495368704.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 19972727438937.16, NNZs: 1097, Bias: -137367657761.633057, T: 5874, Avg. loss: 9690051258084893917184.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 17867226190328.37, NNZs: 1099, Bias: -162537328546.230377, T: 6853, Avg. loss: 6541622671159142121472.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16188706506401.66, NNZs: 1100, Bias: -153945886364.949402, T: 7832, Avg. loss: 4635774579977608495104.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 14820891543021.63, NNZs: 1103, Bias: -139309360597.694489, T: 8811, Avg. loss: 3254663645659722153984.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13667249892340.74, NNZs: 1105, Bias: -134532210587.440521, T: 9790, Avg. loss: 2242828064443055996928.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 12668490758161.55, NNZs: 1105, Bias: -145475301673.088257, T: 10769, Avg. loss: 1403341005071741353984.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11833791075666.41, NNZs: 1105, Bias: -134118232594.016754, T: 11748, Avg. loss: 784203951277571571712.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11104920880571.41, NNZs: 1105, Bias: -136257869828.401657, T: 12727, Avg. loss: 186468237960513781760.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 10473681392902.67, NNZs: 1105, Bias: -135131581244.319336, T: 13706, Avg. loss: 27817603427636682752.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 9916294392143.78, NNZs: 1105, Bias: -134276302861.824524, T: 14685, Avg. loss: 2801755132078113792.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 9420158928713.74, NNZs: 1105, Bias: -133301171792.691574, T: 15664, Avg. loss: 533600302942613440.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 8974754929213.29, NNZs: 1105, Bias: -132712763239.918976, T: 16643, Avg. loss: 247811068977386304.000000\n",
            "Total training time: 0.02 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 18\n",
            "Norm: 8573469268915.03, NNZs: 1112, Bias: -131748684739.481979, T: 17622, Avg. loss: 369128835108185984.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 8209704752555.20, NNZs: 1112, Bias: -130674070946.383270, T: 18601, Avg. loss: 401639173066298176.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 7878455371132.86, NNZs: 1112, Bias: -129447126843.588150, T: 19580, Avg. loss: 474200618840389376.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 7574850759983.93, NNZs: 1112, Bias: -128487624837.218567, T: 20559, Avg. loss: 319044869523712896.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 7296038053200.89, NNZs: 1112, Bias: -127407146561.597488, T: 21538, Avg. loss: 429374376017098304.000000\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 22 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 88299207055611.58, NNZs: 751, Bias: 193498440703.389923, T: 979, Avg. loss: 1023473083997123139076096.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 81517449781293.14, NNZs: 929, Bias: -627405743864.318115, T: 1958, Avg. loss: 2105911389209884044034048.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 68688889646182.66, NNZs: 955, Bias: -754631070997.687378, T: 2937, Avg. loss: 361539658083964104474624.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 58531058291343.41, NNZs: 966, Bias: -779593367671.810303, T: 3916, Avg. loss: 260085589003589696618496.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 50956523290018.92, NNZs: 973, Bias: -810636158626.038208, T: 4895, Avg. loss: 74009209232220118581248.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 44513141227783.84, NNZs: 973, Bias: -859260081189.360229, T: 5874, Avg. loss: 67889985374700540264448.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 40617867643615.54, NNZs: 973, Bias: -785376161710.077026, T: 6853, Avg. loss: 12137834471155038158848.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 36303907273997.79, NNZs: 973, Bias: -844091200367.245239, T: 7832, Avg. loss: 52155203157863243448320.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 33316860525455.95, NNZs: 973, Bias: -822228210636.677368, T: 8811, Avg. loss: 11453784699552487440384.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 30745101719789.24, NNZs: 973, Bias: -822568339798.836304, T: 9790, Avg. loss: 10632020942497456848896.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 28738124302863.22, NNZs: 973, Bias: -790253162298.262695, T: 10769, Avg. loss: 12033776816376934039552.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 26734702535707.17, NNZs: 973, Bias: -811346388275.171143, T: 11748, Avg. loss: 8221159972143258140672.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 25146892455200.27, NNZs: 973, Bias: -795797248745.008301, T: 12727, Avg. loss: 5127203453531908472832.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 23651439481175.07, NNZs: 973, Bias: -809542125860.983765, T: 13706, Avg. loss: 2564146303390271930368.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 22383992688155.43, NNZs: 973, Bias: -806949333764.529175, T: 14685, Avg. loss: 1832090093444243914752.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 21259106679128.99, NNZs: 973, Bias: -807626996760.989624, T: 15664, Avg. loss: 475000244131327180800.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 20273955711322.42, NNZs: 973, Bias: -801596814578.087158, T: 16643, Avg. loss: 370782527983104360448.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19389419398039.20, NNZs: 974, Bias: -795366589194.245361, T: 17622, Avg. loss: 106878207009031520256.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 18570621164808.83, NNZs: 974, Bias: -793360342519.502930, T: 18601, Avg. loss: 61393118784965230592.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17834161883090.24, NNZs: 974, Bias: -789450501546.884644, T: 19580, Avg. loss: 17989694943072782336.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17157915311369.00, NNZs: 974, Bias: -786245788557.719116, T: 20559, Avg. loss: 12221979909546549248.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16543015600064.48, NNZs: 974, Bias: -781767424137.604004, T: 21538, Avg. loss: 3758876365160714240.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 15972211236325.70, NNZs: 974, Bias: -778301740478.193115, T: 22517, Avg. loss: 3735456277478069760.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 15448588874605.12, NNZs: 974, Bias: -774122630428.561035, T: 23496, Avg. loss: 5318355424460996608.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 14968318536855.13, NNZs: 974, Bias: -769068241121.698608, T: 24475, Avg. loss: 6353575241331480576.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 14514094581334.73, NNZs: 974, Bias: -765630112752.562256, T: 25454, Avg. loss: 3841267204369055744.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 14092568703204.18, NNZs: 974, Bias: -761895643806.956909, T: 26433, Avg. loss: 3335619721723142144.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 13697656640048.92, NNZs: 974, Bias: -758426324484.692993, T: 27412, Avg. loss: 2855402387519707136.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 13333898429291.83, NNZs: 974, Bias: -753934219562.213623, T: 28391, Avg. loss: 4825883855917975552.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 12995534074656.07, NNZs: 974, Bias: -749038641653.442383, T: 29370, Avg. loss: 3379695731586675712.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 12664942891054.51, NNZs: 974, Bias: -746409224216.975342, T: 30349, Avg. loss: 1158127831095934464.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 12364346738803.26, NNZs: 974, Bias: -741981133589.154907, T: 31328, Avg. loss: 2904101626799962624.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 12079882223506.79, NNZs: 974, Bias: -737829349785.277100, T: 32307, Avg. loss: 2670843759422858240.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 11803159559702.26, NNZs: 974, Bias: -735027905691.897705, T: 33286, Avg. loss: 1434017653614064640.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 11544335736774.27, NNZs: 974, Bias: -731768697004.239624, T: 34265, Avg. loss: 1599928543724109056.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 11305724007627.39, NNZs: 974, Bias: -727511634423.859741, T: 35244, Avg. loss: 2346037295104121344.000000\n",
            "Total training time: 0.05 seconds.\n",
            "Convergence after 36 epochs took 0.05 seconds\n",
            "-- Epoch 1\n",
            "Norm: 5796364806.60, NNZs: 911, Bias: -34797791.356411, T: 979, Avg. loss: 756949942257507.250000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 742262197488.99, NNZs: 1066, Bias: -8518153892.928032, T: 1958, Avg. loss: 33844493911351721984.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5250881633372.02, NNZs: 1142, Bias: -36959560898.328316, T: 2937, Avg. loss: 3837566063770955415552.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 7737945783272.06, NNZs: 1172, Bias: -70242012000.613602, T: 3916, Avg. loss: 15460310981541571330048.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 7068274717033.38, NNZs: 1180, Bias: -43804798139.885941, T: 4895, Avg. loss: 7810573524106270998528.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6202995151388.26, NNZs: 1184, Bias: -41749055613.825653, T: 5874, Avg. loss: 2339906697044133150720.000000\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 21320725011751.67, NNZs: 839, Bias: 27724710288.487141, T: 979, Avg. loss: 8016769787897219907584.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 35169261551003.89, NNZs: 1026, Bias: -76756008062.695862, T: 1958, Avg. loss: 212051407283421452435456.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 30257527095218.02, NNZs: 1080, Bias: -30676603706.975086, T: 2937, Avg. loss: 81525714494083049717760.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 25836914616514.30, NNZs: 1093, Bias: -131234967125.076080, T: 3916, Avg. loss: 24016409121432228855808.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 22432137146351.26, NNZs: 1099, Bias: -71713363559.842834, T: 4895, Avg. loss: 15836313908685997867008.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 19787490196542.95, NNZs: 1107, Bias: -49084681318.640762, T: 5874, Avg. loss: 8096143006209126957056.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 89273656958709.12, NNZs: 746, Bias: 9094061409.767975, T: 979, Avg. loss: 958181909987388532195328.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 79832230043201.53, NNZs: 943, Bias: -1061483513403.593750, T: 1958, Avg. loss: 2208292544248293269438464.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 66957791356292.79, NNZs: 969, Bias: -891168066905.210083, T: 2937, Avg. loss: 297553842808633499320320.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 57092430433330.47, NNZs: 981, Bias: -940958786093.566895, T: 3916, Avg. loss: 200148459527925556838400.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 48541555609356.99, NNZs: 981, Bias: -1114054125001.624512, T: 4895, Avg. loss: 135725364586466043232256.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 43215027572810.33, NNZs: 981, Bias: -1048857682154.216431, T: 5874, Avg. loss: 43582869221413289459712.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 38890942310463.68, NNZs: 981, Bias: -1025676086682.676270, T: 6853, Avg. loss: 30496775297581734428672.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 35178983112905.72, NNZs: 981, Bias: -1033401783927.204834, T: 7832, Avg. loss: 25384487049005040140288.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 32411586905170.26, NNZs: 981, Bias: -1001943838692.224487, T: 8811, Avg. loss: 18216814456182264561664.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 29610557192377.96, NNZs: 981, Bias: -1042105754067.096313, T: 9790, Avg. loss: 15283931317604007804928.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 27692677545023.50, NNZs: 981, Bias: -1004251058129.584106, T: 10769, Avg. loss: 3709529760820014612480.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 25675514995517.04, NNZs: 981, Bias: -1039833431556.734863, T: 11748, Avg. loss: 5097479047310216265728.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 24307529569754.74, NNZs: 981, Bias: -1001018439478.762573, T: 12727, Avg. loss: 1817366050437632098304.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 22870350669863.32, NNZs: 981, Bias: -1008183729209.416626, T: 13706, Avg. loss: 1650427610329820168192.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 21692002554185.39, NNZs: 981, Bias: -999790822144.386719, T: 14685, Avg. loss: 311006149740556910592.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 20619211314900.78, NNZs: 981, Bias: -996976028801.881226, T: 15664, Avg. loss: 349628544607686688768.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 19650736367477.74, NNZs: 981, Bias: -994503651320.586060, T: 16643, Avg. loss: 313878459204024598528.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 18821353378822.27, NNZs: 981, Bias: -984563853599.472412, T: 17622, Avg. loss: 63076487849689874432.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 18047306025862.19, NNZs: 983, Bias: -979202149377.321899, T: 18601, Avg. loss: 77109772160230735872.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17348630497998.79, NNZs: 983, Bias: -973115862990.080200, T: 19580, Avg. loss: 20058475919453614080.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 16710041378713.91, NNZs: 983, Bias: -967233761952.350708, T: 20559, Avg. loss: 9937258492615004160.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16126281474438.62, NNZs: 983, Bias: -961285941744.387207, T: 21538, Avg. loss: 5685001462362381312.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 15601665601639.88, NNZs: 984, Bias: -953385507351.305054, T: 22517, Avg. loss: 9100755440615257088.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 15103791725110.63, NNZs: 984, Bias: -947942930253.646118, T: 23496, Avg. loss: 3320913564302134784.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 14651813484537.82, NNZs: 984, Bias: -941202934588.456299, T: 24475, Avg. loss: 7216676606842131456.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 14232993462262.91, NNZs: 984, Bias: -934494713436.967041, T: 25454, Avg. loss: 5087413144152985600.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 13843916391732.08, NNZs: 984, Bias: -927876076893.746948, T: 26433, Avg. loss: 6083338890500177920.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 13476123399637.62, NNZs: 984, Bias: -922129435261.406982, T: 27412, Avg. loss: 4532713042158594048.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 13138761641642.64, NNZs: 984, Bias: -915550737565.248169, T: 28391, Avg. loss: 5977775204065592320.000000\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 29 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2897070297.60, NNZs: 906, Bias: -5292702.544909, T: 979, Avg. loss: 216768914944468.687500\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 372330722276.02, NNZs: 1082, Bias: -1406511691.608411, T: 1958, Avg. loss: 8471215738316090368.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5321656459787.28, NNZs: 1157, Bias: -49894602350.958008, T: 2937, Avg. loss: 3881990987394436825088.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 7951491059767.43, NNZs: 1203, Bias: -64838918502.565056, T: 3916, Avg. loss: 15306928355885159809024.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 7346826560015.21, NNZs: 1215, Bias: -35196492002.908745, T: 4895, Avg. loss: 7905402412226432729088.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6595358362554.83, NNZs: 1221, Bias: -51160343334.778809, T: 5874, Avg. loss: 3838501595781676924928.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 30796605263531.59, NNZs: 872, Bias: 23283643902.169083, T: 979, Avg. loss: 27313358287005074587648.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 34830870590770.76, NNZs: 1021, Bias: 6952200460.948509, T: 1958, Avg. loss: 233592984548241706057728.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 29489608007419.63, NNZs: 1072, Bias: -76395243373.947601, T: 2937, Avg. loss: 63660844478506649780224.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 25083162546178.35, NNZs: 1084, Bias: -134901760571.452194, T: 3916, Avg. loss: 36671925207837584654336.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 21810371564237.58, NNZs: 1094, Bias: -90765558990.754898, T: 4895, Avg. loss: 16258356497122865446912.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 19294949645835.88, NNZs: 1102, Bias: -70415260362.447830, T: 5874, Avg. loss: 9685786630749593534464.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 17253261210422.00, NNZs: 1104, Bias: -83773941630.993042, T: 6853, Avg. loss: 7361993448545849442304.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 15620812625232.85, NNZs: 1104, Bias: -88194962983.494385, T: 7832, Avg. loss: 4868226613013150433280.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 14298883215156.40, NNZs: 1104, Bias: -85017685532.094238, T: 8811, Avg. loss: 3969470877677292879872.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 13182984630387.98, NNZs: 1104, Bias: -68903054169.139694, T: 9790, Avg. loss: 2526633264190342037504.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 12208705132465.03, NNZs: 1108, Bias: -94115383012.154205, T: 10769, Avg. loss: 1736713092094628462592.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 11396693603660.96, NNZs: 1108, Bias: -83016014410.951935, T: 11748, Avg. loss: 885794170770171494400.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 10694872677427.49, NNZs: 1106, Bias: -84199015459.973251, T: 12727, Avg. loss: 213044246688491438080.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 10086375397223.70, NNZs: 1106, Bias: -82887134897.040909, T: 13706, Avg. loss: 34167453080152244224.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 9548740012313.79, NNZs: 1106, Bias: -82625717007.723495, T: 14685, Avg. loss: 2937867048861322240.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 9070237881917.30, NNZs: 1106, Bias: -82026002232.776413, T: 15664, Avg. loss: 280593636834851808.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 8640986517132.91, NNZs: 1106, Bias: -81583966490.811386, T: 16643, Avg. loss: 136052384626091872.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 8253741262898.47, NNZs: 1106, Bias: -81086405563.264862, T: 17622, Avg. loss: 133658332106443312.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 7902462766698.41, NNZs: 1106, Bias: -80584683241.251236, T: 18601, Avg. loss: 133375489206434512.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 7582390727567.82, NNZs: 1106, Bias: -79953776839.443771, T: 19580, Avg. loss: 196941498334714016.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 7289089746039.36, NNZs: 1106, Bias: -79574466677.431915, T: 20559, Avg. loss: 73242010987496304.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 7019544658409.53, NNZs: 1106, Bias: -79122003730.815094, T: 21538, Avg. loss: 118173884845142032.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 6770910395950.17, NNZs: 1106, Bias: -78625311889.150253, T: 22517, Avg. loss: 156179380002420160.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 6540760865346.65, NNZs: 1107, Bias: -78124803250.809753, T: 23496, Avg. loss: 156431542466010304.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 6327065173813.57, NNZs: 1110, Bias: -77623407240.749100, T: 24475, Avg. loss: 144798351822463840.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 6127970105371.94, NNZs: 1112, Bias: -77210569858.078506, T: 25454, Avg. loss: 105720904567188704.000000\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 26 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 81663397772978.84, NNZs: 729, Bias: -111347726200.634171, T: 979, Avg. loss: 1033370107716592457482240.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 78486988625747.39, NNZs: 911, Bias: -1114049937519.216064, T: 1958, Avg. loss: 1859006319303872634421248.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 66433291591990.68, NNZs: 941, Bias: -915925871112.812378, T: 2937, Avg. loss: 473522399528464437739520.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 55423427646870.81, NNZs: 962, Bias: -1183205417923.478516, T: 3916, Avg. loss: 302054559312279332978688.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 47952534047617.19, NNZs: 965, Bias: -1234652730032.562256, T: 4895, Avg. loss: 69638240471434578100224.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 42228319895056.85, NNZs: 967, Bias: -1239002357907.848633, T: 5874, Avg. loss: 40397051638299045658624.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 38299448495236.59, NNZs: 967, Bias: -1148931446723.475342, T: 6853, Avg. loss: 23825567734474525900800.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 34326563421129.67, NNZs: 967, Bias: -1206495491846.205566, T: 7832, Avg. loss: 29913232412916441939968.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 31823112321111.85, NNZs: 971, Bias: -1161702706820.457520, T: 8811, Avg. loss: 15400303798280217690112.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 29461294970092.81, NNZs: 974, Bias: -1132178116732.908447, T: 9790, Avg. loss: 14041665344662463840256.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 27393518792230.18, NNZs: 974, Bias: -1120779869241.083740, T: 10769, Avg. loss: 8870166589123058466816.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 25357082066188.88, NNZs: 974, Bias: -1157002209927.581299, T: 11748, Avg. loss: 5568475649518196490240.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 23989281582614.01, NNZs: 977, Bias: -1123974188143.759766, T: 12727, Avg. loss: 404138861307127857152.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 22600991815611.48, NNZs: 977, Bias: -1126785779438.236328, T: 13706, Avg. loss: 876481923289129549824.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 21449818169086.13, NNZs: 977, Bias: -1116656472889.102539, T: 14685, Avg. loss: 139008376124321038336.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 20430923833482.39, NNZs: 977, Bias: -1106260891069.713379, T: 15664, Avg. loss: 269091869924694228992.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 19479632545040.60, NNZs: 977, Bias: -1103165063964.693848, T: 16643, Avg. loss: 200137231985372659712.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 18695218182134.70, NNZs: 977, Bias: -1087843021713.073364, T: 17622, Avg. loss: 66963663223403798528.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 17925804303239.73, NNZs: 977, Bias: -1083135700563.444946, T: 18601, Avg. loss: 57002599962247905280.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17247887905052.32, NNZs: 977, Bias: -1075124289940.483521, T: 19580, Avg. loss: 26272371172065615872.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 16619808052565.10, NNZs: 977, Bias: -1068765505426.518188, T: 20559, Avg. loss: 9834114273154355200.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16057763441305.75, NNZs: 977, Bias: -1060578980787.742188, T: 21538, Avg. loss: 7346368357755410432.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 15546473690609.68, NNZs: 977, Bias: -1051842784690.520142, T: 22517, Avg. loss: 9158957154369185792.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 15052001250617.88, NNZs: 977, Bias: -1046629149141.557861, T: 23496, Avg. loss: 4351686484208610304.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 14617629942422.57, NNZs: 977, Bias: -1038216555725.835449, T: 24475, Avg. loss: 11061457562470690816.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 14207898844074.02, NNZs: 977, Bias: -1031038428603.280762, T: 25454, Avg. loss: 5735602215408694272.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 13818575672389.94, NNZs: 977, Bias: -1025203172486.563110, T: 26433, Avg. loss: 5835523129446569984.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 13476630598947.18, NNZs: 977, Bias: -1016611133712.571777, T: 27412, Avg. loss: 8489278777687996416.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 13134867327566.76, NNZs: 977, Bias: -1011241807054.089966, T: 28391, Avg. loss: 4384944999495281152.000000\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 29 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 25134479800.19, NNZs: 922, Bias: -69980966.684682, T: 979, Avg. loss: 14404915279030472.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1773362386825.82, NNZs: 1089, Bias: -9600946093.416353, T: 1958, Avg. loss: 187514362627448799232.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 10667261689573.34, NNZs: 1163, Bias: -92943831252.535156, T: 2937, Avg. loss: 21909813536361407315968.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 11470173626656.88, NNZs: 1200, Bias: -108429294431.853806, T: 3916, Avg. loss: 29406739364065176453120.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 9970764695887.62, NNZs: 1217, Bias: -59978957811.870636, T: 4895, Avg. loss: 12059166560979597131776.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 8832287927089.58, NNZs: 1218, Bias: -69324854364.343262, T: 5874, Avg. loss: 2857776791664009936896.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 34651257562363.75, NNZs: 863, Bias: -46425745123.887024, T: 979, Avg. loss: 74214847151976451407872.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 36766281860616.58, NNZs: 986, Bias: -57948359974.630836, T: 1958, Avg. loss: 231018829452631533944832.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 31101758186687.70, NNZs: 1031, Bias: -150145623406.097992, T: 2937, Avg. loss: 88443759079702067675136.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 26594451103348.32, NNZs: 1053, Bias: -218686210315.679321, T: 3916, Avg. loss: 26207914835788940967936.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 23281861797292.53, NNZs: 1070, Bias: -136077863548.637253, T: 4895, Avg. loss: 12067893355953495474176.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 20598745047113.80, NNZs: 1077, Bias: -95561363414.280594, T: 5874, Avg. loss: 6805858652421936381952.000000\n",
            "Total training time: 0.01 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 7\n",
            "Norm: 18416315371155.93, NNZs: 1080, Bias: -130482631996.388046, T: 6853, Avg. loss: 6978617385133925203968.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16681135566456.32, NNZs: 1083, Bias: -117139518807.414993, T: 7832, Avg. loss: 3335083998120745369600.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 15278204727077.20, NNZs: 1083, Bias: -109724786117.208755, T: 8811, Avg. loss: 4151897962512784031744.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 14098732073363.15, NNZs: 1083, Bias: -94527960117.018616, T: 9790, Avg. loss: 2926512621502413144064.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 13040710213234.63, NNZs: 1083, Bias: -134139556830.865173, T: 10769, Avg. loss: 1739950776626932875264.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 12178917827803.41, NNZs: 1083, Bias: -118815712312.441666, T: 11748, Avg. loss: 1033176809165224804352.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11430716314357.14, NNZs: 1084, Bias: -117343312495.661224, T: 12727, Avg. loss: 271885324187105427456.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 10780207157778.79, NNZs: 1084, Bias: -116323150833.185394, T: 13706, Avg. loss: 43468730826167713792.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 10205621363725.99, NNZs: 1084, Bias: -116066154663.895645, T: 14685, Avg. loss: 3485429500302920704.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 9694836977300.57, NNZs: 1084, Bias: -115012582955.762985, T: 15664, Avg. loss: 690627682507643904.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 9236181414076.31, NNZs: 1084, Bias: -114524161709.340546, T: 16643, Avg. loss: 184822043168784960.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 8822627124336.00, NNZs: 1088, Bias: -113819435550.137131, T: 17622, Avg. loss: 271375985749302912.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 8447649113575.17, NNZs: 1088, Bias: -113012370282.196442, T: 18601, Avg. loss: 353523205137935168.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 8105717913403.43, NNZs: 1088, Bias: -112329457420.013794, T: 19580, Avg. loss: 257618593139986016.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 7792532009922.93, NNZs: 1088, Bias: -111756380756.909012, T: 20559, Avg. loss: 190101417451838432.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 7504869123890.54, NNZs: 1088, Bias: -111033766609.975998, T: 21538, Avg. loss: 297763408740134400.000000\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 22 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 80870031519471.11, NNZs: 750, Bias: -208483826929.705780, T: 979, Avg. loss: 718361331883656010006528.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 80790368317205.11, NNZs: 917, Bias: -983875221838.861328, T: 1958, Avg. loss: 2258647305266449075929088.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 68405798359934.86, NNZs: 957, Bias: -997903380022.640503, T: 2937, Avg. loss: 551262026843944248147968.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 57759194820598.24, NNZs: 963, Bias: -1065785251836.094360, T: 3916, Avg. loss: 198897743654124806733824.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 50390005475008.73, NNZs: 965, Bias: -1017770656861.694092, T: 4895, Avg. loss: 107527513353044776452096.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 44225914472990.92, NNZs: 965, Bias: -1065531669238.173462, T: 5874, Avg. loss: 109766729125956767711232.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 39671495994913.05, NNZs: 966, Bias: -1069781920349.029663, T: 6853, Avg. loss: 68462304529520810524672.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 35621437996071.19, NNZs: 966, Bias: -1106828285204.211182, T: 7832, Avg. loss: 47548522064494962147328.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 32605211005464.73, NNZs: 966, Bias: -1097803996209.718750, T: 8811, Avg. loss: 21757839550060565626880.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 30040632899980.01, NNZs: 966, Bias: -1104902501380.147949, T: 9790, Avg. loss: 12253864534187819663360.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 27939105087125.43, NNZs: 966, Bias: -1100396874756.056396, T: 10769, Avg. loss: 6049726921561546424320.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 26241106383632.96, NNZs: 966, Bias: -1074052045817.753418, T: 11748, Avg. loss: 9453373630334661296128.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 24719297645553.47, NNZs: 966, Bias: -1054753122610.207886, T: 12727, Avg. loss: 5803217154177246429184.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 23204758994697.06, NNZs: 966, Bias: -1067460812168.867676, T: 13706, Avg. loss: 4224406735661099909120.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 22005319898612.54, NNZs: 966, Bias: -1060313806914.310791, T: 14685, Avg. loss: 721828685679791308800.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 20900254734419.54, NNZs: 968, Bias: -1060423943487.528320, T: 15664, Avg. loss: 680872456258810544128.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 19943987404265.57, NNZs: 968, Bias: -1053592832991.340210, T: 16643, Avg. loss: 301195867132571090944.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19099418228168.20, NNZs: 968, Bias: -1044415298195.059814, T: 17622, Avg. loss: 171585462857359130624.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 18322835206990.18, NNZs: 968, Bias: -1037709146733.806763, T: 18601, Avg. loss: 92151927496169193472.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17603286709283.11, NNZs: 968, Bias: -1033546100829.555786, T: 19580, Avg. loss: 27108143760984813568.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 16977813941974.70, NNZs: 968, Bias: -1024123351595.010986, T: 20559, Avg. loss: 15352094464351176704.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16384251637617.62, NNZs: 968, Bias: -1018305623334.797119, T: 21538, Avg. loss: 9947436804173336576.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 15839915008281.50, NNZs: 969, Bias: -1012310739024.804565, T: 22517, Avg. loss: 11839381649392799744.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 15350464090984.96, NNZs: 969, Bias: -1004536731388.856201, T: 23496, Avg. loss: 12646541210083223552.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 14894518369698.47, NNZs: 969, Bias: -997450071782.512573, T: 24475, Avg. loss: 8905708162420909056.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 14462406310724.24, NNZs: 969, Bias: -991881695426.712646, T: 25454, Avg. loss: 8427319036563268608.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 14064937118197.53, NNZs: 969, Bias: -985765354730.961792, T: 26433, Avg. loss: 8411124695038249984.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 13697752024653.06, NNZs: 969, Bias: -979230791760.036499, T: 27412, Avg. loss: 9894019449069193216.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 13348677466118.71, NNZs: 969, Bias: -973710150063.557007, T: 28391, Avg. loss: 4832022091294863360.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 13022890568035.30, NNZs: 969, Bias: -968144826705.189941, T: 29370, Avg. loss: 5281398293546580992.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 12720918906893.39, NNZs: 969, Bias: -962152144081.341187, T: 30349, Avg. loss: 3953530574942709248.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 12434356374416.80, NNZs: 969, Bias: -956687553956.812744, T: 31328, Avg. loss: 4274317538033754112.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 12168811660508.99, NNZs: 969, Bias: -950758027704.316162, T: 32307, Avg. loss: 4726506815624439808.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 11908190720624.49, NNZs: 969, Bias: -946246980516.132324, T: 33286, Avg. loss: 2809215134107604480.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 11667600238225.67, NNZs: 969, Bias: -941092400927.521484, T: 34265, Avg. loss: 3197748944529449472.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 11444599606760.11, NNZs: 969, Bias: -935483080420.695801, T: 35244, Avg. loss: 4213181169007681536.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 11220670520258.78, NNZs: 969, Bias: -931517445711.560547, T: 36223, Avg. loss: 2519098003742290432.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 11023548109674.77, NNZs: 969, Bias: -925749683728.935425, T: 37202, Avg. loss: 3378551193744189440.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 10827975081412.32, NNZs: 969, Bias: -921118151529.092529, T: 38181, Avg. loss: 2272999756396596480.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 10636940637648.60, NNZs: 969, Bias: -917150389180.208374, T: 39160, Avg. loss: 2118115295068909056.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 10462249732827.32, NNZs: 969, Bias: -912393743189.652710, T: 40139, Avg. loss: 2693976347586288128.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 10296744317663.67, NNZs: 969, Bias: -907621482838.266235, T: 41118, Avg. loss: 3070752459690941952.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 10129671774989.85, NNZs: 969, Bias: -903971852300.594971, T: 42097, Avg. loss: 1993554225575396864.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 9983487743768.73, NNZs: 969, Bias: -898868338012.336182, T: 43076, Avg. loss: 3125480960716015104.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 9828213491036.79, NNZs: 969, Bias: -895597736922.900391, T: 44055, Avg. loss: 1741590074396604928.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 9693971825755.09, NNZs: 969, Bias: -890783747278.391968, T: 45034, Avg. loss: 2995548131048327680.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 9552207261833.41, NNZs: 969, Bias: -887506509128.174316, T: 46013, Avg. loss: 1821142064095046656.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 9426028967034.04, NNZs: 969, Bias: -883212887457.047729, T: 46992, Avg. loss: 2640773397014783488.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 9297717550296.58, NNZs: 969, Bias: -879777612799.754517, T: 47971, Avg. loss: 1776754918296849152.000000\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 9182933475656.99, NNZs: 969, Bias: -875504456536.770874, T: 48950, Avg. loss: 2598336175317715968.000000\n",
            "Total training time: 0.07 seconds.\n",
            "Convergence after 50 epochs took 0.07 seconds\n",
            "-- Epoch 1\n",
            "Norm: 1672916094.84, NNZs: 856, Bias: -4752438.818159, T: 979, Avg. loss: 68547722810928.453125\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 533849596216.78, NNZs: 1045, Bias: -773519819.346812, T: 1958, Avg. loss: 16300365889053360128.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 10759554795124.17, NNZs: 1130, Bias: -78709297871.337265, T: 2937, Avg. loss: 25323165980145179688960.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 11196466883680.94, NNZs: 1164, Bias: -126763528844.933746, T: 3916, Avg. loss: 22270094982016567083008.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 9817005602153.77, NNZs: 1177, Bias: -56937605604.336845, T: 4895, Avg. loss: 7758486993030781861888.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 8688336374911.59, NNZs: 1180, Bias: -83194302054.110992, T: 5874, Avg. loss: 4437224094998204514304.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 28908196059239.27, NNZs: 847, Bias: -78507723353.350311, T: 980, Avg. loss: 60760119194877612261376.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 35975195918317.73, NNZs: 990, Bias: -29135170054.948174, T: 1960, Avg. loss: 267747313442020646715392.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 31931339010831.25, NNZs: 1064, Bias: -58917071541.006699, T: 2940, Avg. loss: 85907382880386194341888.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 27374500870705.26, NNZs: 1085, Bias: -68999429653.394653, T: 3920, Avg. loss: 29208929961131477303296.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 23580765694827.45, NNZs: 1091, Bias: -93764441487.325729, T: 4900, Avg. loss: 16644694284256373899264.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 20877882940668.36, NNZs: 1094, Bias: -91077560106.950485, T: 5880, Avg. loss: 10201716536489185116160.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 18688932247536.87, NNZs: 1094, Bias: -123062762126.717880, T: 6860, Avg. loss: 6228741612682781655040.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16929856727824.57, NNZs: 1098, Bias: -83181486579.186142, T: 7840, Avg. loss: 4093153892905889300480.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 15496666288500.64, NNZs: 1098, Bias: -84566611966.740570, T: 8820, Avg. loss: 3394332528024505810944.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 14277613353673.50, NNZs: 1098, Bias: -82782190568.138855, T: 9800, Avg. loss: 3623129945238131441664.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 13230362465594.96, NNZs: 1097, Bias: -97274300311.476562, T: 10780, Avg. loss: 2145714857140398587904.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 12349363889216.21, NNZs: 1097, Bias: -90061160414.599960, T: 11760, Avg. loss: 918514874524406120448.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11593818669401.72, NNZs: 1097, Bias: -84484207487.169708, T: 12740, Avg. loss: 259690663784410152960.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 10931078829786.01, NNZs: 1097, Bias: -86957643779.475098, T: 13720, Avg. loss: 38199108181376589824.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 10349029337226.34, NNZs: 1097, Bias: -85704011774.630371, T: 14700, Avg. loss: 6213733210278656000.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 9830046107196.18, NNZs: 1097, Bias: -85441591620.087158, T: 15680, Avg. loss: 485052146075876224.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 9364690456152.48, NNZs: 1097, Bias: -85079896508.319672, T: 16660, Avg. loss: 107728358078948608.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 8944830507968.81, NNZs: 1097, Bias: -84697252771.247375, T: 17640, Avg. loss: 113779691164071984.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 8564004199505.92, NNZs: 1101, Bias: -84259452972.956100, T: 18620, Avg. loss: 152990487250210656.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 8216855700480.56, NNZs: 1101, Bias: -83818629666.761887, T: 19600, Avg. loss: 124694359388454320.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 7899001770975.57, NNZs: 1101, Bias: -83386478142.268234, T: 20580, Avg. loss: 149169775080463328.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 7606901527193.93, NNZs: 1102, Bias: -82868952982.397797, T: 21560, Avg. loss: 168044277488520576.000000\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 22 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 87775197827013.48, NNZs: 784, Bias: 194424844826.380798, T: 980, Avg. loss: 648405766332846138458112.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 81155694638123.39, NNZs: 948, Bias: -479616988280.585205, T: 1960, Avg. loss: 2102022349158767040397312.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 68339593848076.91, NNZs: 975, Bias: -544985046032.743652, T: 2940, Avg. loss: 461454340530951086407680.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 57671533216240.45, NNZs: 978, Bias: -823375332317.497803, T: 3920, Avg. loss: 280862164565652748107776.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 49759583706306.67, NNZs: 981, Bias: -971233378507.491333, T: 4900, Avg. loss: 97766304678713690685440.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 43858538573257.23, NNZs: 981, Bias: -988061140840.193726, T: 5880, Avg. loss: 76157666976731242692608.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 39451607340277.93, NNZs: 984, Bias: -947090766017.585815, T: 6860, Avg. loss: 46962726462764422791168.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 36175160721044.59, NNZs: 984, Bias: -907095009784.564819, T: 7840, Avg. loss: 26633132434328704253952.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 32864925615829.49, NNZs: 984, Bias: -929372983070.390259, T: 8820, Avg. loss: 39181169196004680925184.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 30210589110644.07, NNZs: 984, Bias: -943400123151.358887, T: 9800, Avg. loss: 12423721544093009969152.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 28049653868985.18, NNZs: 984, Bias: -949030568498.751709, T: 10780, Avg. loss: 6714184918008796807168.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 26405978589062.73, NNZs: 985, Bias: -919394691343.843872, T: 11760, Avg. loss: 3807975519343869952000.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 24749131146044.76, NNZs: 985, Bias: -922051615008.953735, T: 12740, Avg. loss: 6242874133372239609856.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 23331619737803.25, NNZs: 985, Bias: -918304148133.214233, T: 13720, Avg. loss: 2742678643533909852160.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 22063599323270.83, NNZs: 985, Bias: -922848218416.651489, T: 14700, Avg. loss: 1583102480293997576192.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 21005505263856.66, NNZs: 984, Bias: -910604635218.005005, T: 15680, Avg. loss: 643825548326997721088.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 20022063372808.50, NNZs: 984, Bias: -908142066675.135498, T: 16660, Avg. loss: 367576565154282209280.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19147082418618.80, NNZs: 983, Bias: -902531387668.111938, T: 17640, Avg. loss: 322952293375184666624.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 18340226410606.04, NNZs: 983, Bias: -900378720369.069092, T: 18620, Avg. loss: 73048164764548366336.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17636590211680.91, NNZs: 983, Bias: -892305693922.245850, T: 19600, Avg. loss: 19622227905916248064.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 16981348232200.56, NNZs: 983, Bias: -886920757105.335938, T: 20580, Avg. loss: 30813661413707390976.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16393342931918.48, NNZs: 984, Bias: -879199969801.412842, T: 21560, Avg. loss: 6498922253783185408.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 15835456081982.36, NNZs: 985, Bias: -874819235105.983154, T: 22540, Avg. loss: 11598729989700206592.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 15333373735894.44, NNZs: 985, Bias: -868143556498.735718, T: 23520, Avg. loss: 6778126443452298240.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 14852087229970.55, NNZs: 985, Bias: -864475262059.265747, T: 24500, Avg. loss: 5709525792881051648.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 14428062350144.73, NNZs: 985, Bias: -856901467051.421997, T: 25480, Avg. loss: 8045602107103215616.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 14022378520118.62, NNZs: 985, Bias: -851355328382.966553, T: 26460, Avg. loss: 3564044123566883328.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 13640847165099.44, NNZs: 985, Bias: -846421248273.978149, T: 27440, Avg. loss: 3504478744066482176.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 13287739493458.95, NNZs: 985, Bias: -840953467987.811890, T: 28420, Avg. loss: 3776628976303507968.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 12960225017858.10, NNZs: 985, Bias: -835072983545.536133, T: 29400, Avg. loss: 4223179752463320576.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 12641773596669.96, NNZs: 985, Bias: -830979839917.818604, T: 30380, Avg. loss: 2471453191009465856.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 12355771319156.89, NNZs: 985, Bias: -824870610018.489136, T: 31360, Avg. loss: 5515271634916639744.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 12075492277672.84, NNZs: 985, Bias: -820503049022.992188, T: 32340, Avg. loss: 2415221458254025216.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 11817098583956.80, NNZs: 985, Bias: -815287301541.741943, T: 33320, Avg. loss: 2707368546997381120.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 11572495661532.78, NNZs: 985, Bias: -810248873990.582275, T: 34300, Avg. loss: 2622546396099848704.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 11336842776050.90, NNZs: 985, Bias: -805900028787.830811, T: 35280, Avg. loss: 2673673789731991552.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 11116761105079.07, NNZs: 985, Bias: -801134520767.124512, T: 36260, Avg. loss: 2693258427400845312.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 10906837895459.97, NNZs: 985, Bias: -796604682392.799194, T: 37240, Avg. loss: 2697533580152824320.000000\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 38 epochs took 0.04 seconds\n",
            "-- Epoch 1\n",
            "Norm: 4178457243.17, NNZs: 857, Bias: -2605531.630226, T: 980, Avg. loss: 431797198611871.812500\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2763934786093.27, NNZs: 1054, Bias: -31052888324.071327, T: 1960, Avg. loss: 393661489534176526336.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 11833443526894.93, NNZs: 1135, Bias: -78479918709.991379, T: 2940, Avg. loss: 33193676068324676468736.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 11826529903492.54, NNZs: 1181, Bias: -61449278218.480835, T: 3920, Avg. loss: 30275052188620535365632.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 10189195776557.26, NNZs: 1184, Bias: -36977940001.098961, T: 4900, Avg. loss: 13504277127396621025280.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 8952736258216.64, NNZs: 1194, Bias: -42623521730.588699, T: 5880, Avg. loss: 3128422421021688070144.000000\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 40715921725454.01, NNZs: 836, Bias: -305933124362.221497, T: 980, Avg. loss: 180266844713296779542528.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 37271004186643.02, NNZs: 961, Bias: -68972959438.796494, T: 1960, Avg. loss: 261892302019733433090048.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 31326928431698.55, NNZs: 1008, Bias: -178687995180.320618, T: 2940, Avg. loss: 96004997169624203132928.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 26781999374918.39, NNZs: 1029, Bias: -137843776331.605560, T: 3920, Avg. loss: 33408538283988985315328.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 23187528608439.27, NNZs: 1039, Bias: -140493206099.924591, T: 4900, Avg. loss: 18979417441198422360064.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 20513388735899.01, NNZs: 1046, Bias: -109164311511.337402, T: 5880, Avg. loss: 8799040429308302589952.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 18319835879246.99, NNZs: 1046, Bias: -152745656750.430511, T: 6860, Avg. loss: 5999350166667372003328.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 16604391056400.06, NNZs: 1046, Bias: -115865670062.141754, T: 7840, Avg. loss: 4603435253856036454400.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 15207168779568.46, NNZs: 1050, Bias: -110536867621.544098, T: 8820, Avg. loss: 3074171886168875991040.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 14008093996055.51, NNZs: 1051, Bias: -108770742162.243439, T: 9800, Avg. loss: 3423655972522879352832.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 12980405552966.68, NNZs: 1051, Bias: -120972540222.184265, T: 10780, Avg. loss: 1901996434108706193408.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 12113806655318.08, NNZs: 1051, Bias: -118940922325.258896, T: 11760, Avg. loss: 831974778348010209280.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 11376781163465.92, NNZs: 1051, Bias: -111262483876.950119, T: 12740, Avg. loss: 221581329277225631744.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 10726087691905.21, NNZs: 1051, Bias: -113608221054.235138, T: 13720, Avg. loss: 31811737200210862080.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 10155660869945.92, NNZs: 1051, Bias: -112054698758.580414, T: 14700, Avg. loss: 5141290516843925504.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 9646628127163.31, NNZs: 1052, Bias: -111631383552.991806, T: 15680, Avg. loss: 562158160661822464.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 9190628993139.68, NNZs: 1052, Bias: -110777275488.200302, T: 16660, Avg. loss: 399117842948948032.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 8778948096868.26, NNZs: 1052, Bias: -110179247577.761475, T: 17640, Avg. loss: 205952599867146656.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 8405814586447.09, NNZs: 1052, Bias: -109347280455.456070, T: 18620, Avg. loss: 334459150593564736.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 8065592947357.53, NNZs: 1052, Bias: -108624595814.019470, T: 19600, Avg. loss: 244508554464831136.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 7754109416931.61, NNZs: 1051, Bias: -107916753784.991104, T: 20580, Avg. loss: 272873517555558880.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 7467829869768.42, NNZs: 1055, Bias: -107191931147.759048, T: 21560, Avg. loss: 249509454063046752.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 7203990812709.62, NNZs: 1055, Bias: -106297344624.391312, T: 22540, Avg. loss: 380094962773832832.000000\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 23 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 84271293676012.94, NNZs: 738, Bias: 486744167134.356995, T: 980, Avg. loss: 837878732693709533478912.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 80344829316353.17, NNZs: 964, Bias: -617672687914.797363, T: 1960, Avg. loss: 2281100380160450694742016.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 68947781600348.72, NNZs: 981, Bias: -777607780399.704102, T: 2940, Avg. loss: 350853149514558313857024.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 57690469242320.48, NNZs: 985, Bias: -1021274689736.480591, T: 3920, Avg. loss: 269153528043741686792192.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 49958428940875.27, NNZs: 986, Bias: -1051736201029.905151, T: 4900, Avg. loss: 83849192177825363263488.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 44007127309333.07, NNZs: 986, Bias: -1110015289667.395020, T: 5880, Avg. loss: 63103882342944791855104.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 40088522983747.16, NNZs: 986, Bias: -1026301050326.035400, T: 6860, Avg. loss: 59893450027192903794688.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 35838336374603.86, NNZs: 988, Bias: -1086016457266.863403, T: 7840, Avg. loss: 45566520389562401816576.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 33027933213948.65, NNZs: 989, Bias: -1045089627796.627808, T: 8820, Avg. loss: 9971155633249500266496.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 30609494862606.86, NNZs: 989, Bias: -1020199985245.746094, T: 9800, Avg. loss: 15798704291393967226880.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 28148022069296.86, NNZs: 989, Bias: -1061202890077.100098, T: 10780, Avg. loss: 13618327608049035902976.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 26453530783172.95, NNZs: 992, Bias: -1027837347012.110352, T: 11760, Avg. loss: 4312499143508452442112.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 24826902584123.77, NNZs: 992, Bias: -1033109734842.030762, T: 12740, Avg. loss: 2944827975258571014144.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 23516656223155.32, NNZs: 992, Bias: -1010375928509.284302, T: 13720, Avg. loss: 2100312744528336388096.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 22212129534668.82, NNZs: 992, Bias: -1020473751802.849121, T: 14700, Avg. loss: 2674952677428850851840.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 21179376206172.76, NNZs: 992, Bias: -1003856067292.439819, T: 15680, Avg. loss: 830439880933642076160.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 20204124991501.89, NNZs: 992, Bias: -997854575245.503296, T: 16660, Avg. loss: 702464733275489304576.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 19327591292841.96, NNZs: 992, Bias: -991110996758.921021, T: 17640, Avg. loss: 515491673163013029888.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 18519005285857.64, NNZs: 993, Bias: -988025575313.577881, T: 18620, Avg. loss: 83489322439983267840.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 17805781159613.55, NNZs: 993, Bias: -980982021531.349243, T: 19600, Avg. loss: 26526238317034049536.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 17145029092565.70, NNZs: 994, Bias: -975818464198.276367, T: 20580, Avg. loss: 32787032467092832256.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 16547612579487.74, NNZs: 994, Bias: -969357608678.220703, T: 21560, Avg. loss: 4803200734472742912.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 15996087435257.47, NNZs: 994, Bias: -963335328405.479614, T: 22540, Avg. loss: 20744687573973127168.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 15498124369036.70, NNZs: 994, Bias: -955672880906.852539, T: 23520, Avg. loss: 8048368613470814208.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 15012874240714.91, NNZs: 994, Bias: -952043775511.834717, T: 24500, Avg. loss: 6092702251348990976.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 14577850387119.37, NNZs: 994, Bias: -945961642871.531860, T: 25480, Avg. loss: 6747370267458534400.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 14179578974787.53, NNZs: 994, Bias: -939047942289.381592, T: 26460, Avg. loss: 4374041656675097600.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 13804489275328.45, NNZs: 994, Bias: -932883799240.315186, T: 27440, Avg. loss: 5172705764057874432.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 13453374944095.85, NNZs: 994, Bias: -926878576018.869263, T: 28420, Avg. loss: 4021303824368667136.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 13115699088607.52, NNZs: 994, Bias: -922264272223.320923, T: 29400, Avg. loss: 2576570512165018624.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 12809570188094.31, NNZs: 994, Bias: -916168739469.350830, T: 30380, Avg. loss: 4563756496071129600.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 12524359437281.59, NNZs: 994, Bias: -909842478298.719116, T: 31360, Avg. loss: 5317532041117822976.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 12243892365166.31, NNZs: 994, Bias: -905284844138.505859, T: 32340, Avg. loss: 2567196680586803712.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 11985507556479.05, NNZs: 994, Bias: -899937727642.057739, T: 33320, Avg. loss: 2643699029741861376.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 11747523792767.46, NNZs: 994, Bias: -893897195207.478394, T: 34300, Avg. loss: 3454765526403250176.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 11507235683577.89, NNZs: 994, Bias: -889961735796.625610, T: 35280, Avg. loss: 2488972620994121216.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 11289595757742.43, NNZs: 994, Bias: -884763293901.403809, T: 36260, Avg. loss: 3412574195090832384.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 11081802488189.94, NNZs: 994, Bias: -879834375475.634155, T: 37240, Avg. loss: 3167623952322605056.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 10880933439980.18, NNZs: 994, Bias: -875439804417.520142, T: 38220, Avg. loss: 2455433559325100032.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 10689894562606.12, NNZs: 994, Bias: -871102959446.332031, T: 39200, Avg. loss: 2670194254191695360.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 10508496602880.13, NNZs: 994, Bias: -866770045070.069336, T: 40180, Avg. loss: 2125606853707857152.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 10335662168436.08, NNZs: 994, Bias: -862505860841.830566, T: 41160, Avg. loss: 2468045827753883648.000000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 10169108912967.45, NNZs: 994, Bias: -858506200193.390625, T: 42140, Avg. loss: 2456897756163335168.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 10009345952060.63, NNZs: 994, Bias: -854635051590.395508, T: 43120, Avg. loss: 2236078620088808704.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 9861595844684.21, NNZs: 994, Bias: -850249227843.219482, T: 44100, Avg. loss: 2260850603831562240.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 9707781849494.98, NNZs: 994, Bias: -847374821704.275757, T: 45080, Avg. loss: 1483199219728016896.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 9572654259524.57, NNZs: 994, Bias: -843116740466.662109, T: 46060, Avg. loss: 2383929318058974720.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 9437517930178.25, NNZs: 994, Bias: -839577270117.575562, T: 47040, Avg. loss: 1827976360490667008.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 9307750596525.33, NNZs: 994, Bias: -836096273754.828735, T: 48020, Avg. loss: 1893518581214787072.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 9187066156936.78, NNZs: 994, Bias: -832236166433.445190, T: 49000, Avg. loss: 2150962714703090432.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 9063845671429.64, NNZs: 994, Bias: -829236432466.701294, T: 49980, Avg. loss: 1449752574361619456.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 8954286370004.79, NNZs: 995, Bias: -825296855935.374756, T: 50960, Avg. loss: 2223204672834758656.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 8838958539833.29, NNZs: 995, Bias: -822495836776.146362, T: 51940, Avg. loss: 1262781457957786368.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 8735435484151.57, NNZs: 995, Bias: -818912104275.618896, T: 52920, Avg. loss: 1946513163074933504.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 8632810356913.25, NNZs: 995, Bias: -815688543540.748047, T: 53900, Avg. loss: 1603752864436357888.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 8534686960461.95, NNZs: 995, Bias: -812420354436.536865, T: 54880, Avg. loss: 1588477497348227584.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 8441832495140.21, NNZs: 995, Bias: -809010180480.706055, T: 55860, Avg. loss: 1651131322736961024.000000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 8347415878569.75, NNZs: 995, Bias: -806148291770.019653, T: 56840, Avg. loss: 1393458685634423552.000000\n",
            "Total training time: 0.06 seconds.\n",
            "Convergence after 58 epochs took 0.06 seconds\n",
            "-- Epoch 1\n",
            "Norm: 7022114799.29, NNZs: 885, Bias: -24922743.881250, T: 980, Avg. loss: 1101896583376621.250000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 590471468465.51, NNZs: 1062, Bias: -3220873410.209390, T: 1960, Avg. loss: 19991010917767598080.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 4623086738137.20, NNZs: 1149, Bias: -26102312745.439152, T: 2940, Avg. loss: 2940191195887542730752.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 7712275489861.29, NNZs: 1190, Bias: -10526215233.034523, T: 3920, Avg. loss: 17815867861739876909056.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 6948201231948.89, NNZs: 1207, Bias: 10627161200.493036, T: 4900, Avg. loss: 6799093847385595445248.000000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 6115648742121.50, NNZs: 1213, Bias: -11193858379.593575, T: 5880, Avg. loss: 3972788570536427388928.000000\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.26, NNZs: 4, Bias: -0.099617, T: 979, Avg. loss: 0.675429\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.22, NNZs: 1, Bias: -0.109654, T: 1958, Avg. loss: 0.672805\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.115518, T: 2937, Avg. loss: 0.671727\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.119574, T: 3916, Avg. loss: 0.671045\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.122748, T: 4895, Avg. loss: 0.670443\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.125342, T: 5874, Avg. loss: 0.670026\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.127523, T: 6853, Avg. loss: 0.669683\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.129401, T: 7832, Avg. loss: 0.669394\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.32, NNZs: 7, Bias: -0.126288, T: 979, Avg. loss: 0.650563\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.25, NNZs: 2, Bias: -0.143199, T: 1958, Avg. loss: 0.638107\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.22, NNZs: 1, Bias: -0.152973, T: 2937, Avg. loss: 0.633476\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.159849, T: 3916, Avg. loss: 0.630488\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.165021, T: 4895, Avg. loss: 0.628527\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.169207, T: 5874, Avg. loss: 0.626862\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.172736, T: 6853, Avg. loss: 0.625482\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.175814, T: 7832, Avg. loss: 0.624251\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.178517, T: 8811, Avg. loss: 0.623194\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.180915, T: 9790, Avg. loss: 0.622361\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.183076, T: 10769, Avg. loss: 0.621541\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.185051, T: 11748, Avg. loss: 0.620779\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.186863, T: 12727, Avg. loss: 0.620115\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.188537, T: 13706, Avg. loss: 0.619483\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 14 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.15, NNZs: 6, Bias: -0.035827, T: 979, Avg. loss: 0.696297\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.10, NNZs: 0, Bias: -0.037787, T: 1958, Avg. loss: 0.696530\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.08, NNZs: 0, Bias: -0.039000, T: 2937, Avg. loss: 0.696701\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.039865, T: 3916, Avg. loss: 0.696805\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.040586, T: 4895, Avg. loss: 0.696914\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.06, NNZs: 0, Bias: -0.041170, T: 5874, Avg. loss: 0.696976\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.27, NNZs: 4, Bias: -0.099910, T: 979, Avg. loss: 0.674826\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.23, NNZs: 1, Bias: -0.110078, T: 1958, Avg. loss: 0.672370\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.115986, T: 2937, Avg. loss: 0.671295\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.120039, T: 3916, Avg. loss: 0.670624\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.123225, T: 4895, Avg. loss: 0.670026\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.125808, T: 5874, Avg. loss: 0.669627\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.127988, T: 6853, Avg. loss: 0.669270\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.129860, T: 7832, Avg. loss: 0.668997\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.33, NNZs: 6, Bias: -0.146159, T: 979, Avg. loss: 0.642247\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.26, NNZs: 1, Bias: -0.162729, T: 1958, Avg. loss: 0.630406\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.24, NNZs: 1, Bias: -0.172238, T: 2937, Avg. loss: 0.625938\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.22, NNZs: 1, Bias: -0.178912, T: 3916, Avg. loss: 0.623113\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.183953, T: 4895, Avg. loss: 0.621155\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.188028, T: 5874, Avg. loss: 0.619583\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.191459, T: 6853, Avg. loss: 0.618268\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.194458, T: 7832, Avg. loss: 0.617055\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.197092, T: 8811, Avg. loss: 0.616060\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.199429, T: 9790, Avg. loss: 0.615249\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.201527, T: 10769, Avg. loss: 0.614509\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.203448, T: 11748, Avg. loss: 0.613758\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.205212, T: 12727, Avg. loss: 0.613110\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 13 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.15, NNZs: 6, Bias: -0.031711, T: 979, Avg. loss: 0.695871\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.10, NNZs: 0, Bias: -0.033746, T: 1958, Avg. loss: 0.696139\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.08, NNZs: 0, Bias: -0.035041, T: 2937, Avg. loss: 0.696329\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.035926, T: 3916, Avg. loss: 0.696418\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.036672, T: 4895, Avg. loss: 0.696535\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.06, NNZs: 0, Bias: -0.037263, T: 5874, Avg. loss: 0.696587\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.28, NNZs: 5, Bias: -0.111945, T: 979, Avg. loss: 0.672254\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.24, NNZs: 1, Bias: -0.121828, T: 1958, Avg. loss: 0.670037\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.22, NNZs: 1, Bias: -0.127541, T: 2937, Avg. loss: 0.669003\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.22, NNZs: 1, Bias: -0.131521, T: 3916, Avg. loss: 0.668388\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.134626, T: 4895, Avg. loss: 0.667788\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.137149, T: 5874, Avg. loss: 0.667416\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.139275, T: 6853, Avg. loss: 0.667081\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.141107, T: 7832, Avg. loss: 0.666811\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.33, NNZs: 9, Bias: -0.150269, T: 979, Avg. loss: 0.640477\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.26, NNZs: 1, Bias: -0.166759, T: 1958, Avg. loss: 0.628831\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.24, NNZs: 1, Bias: -0.176303, T: 2937, Avg. loss: 0.624295\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.22, NNZs: 1, Bias: -0.182926, T: 3916, Avg. loss: 0.621563\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.187958, T: 4895, Avg. loss: 0.619560\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.192001, T: 5874, Avg. loss: 0.618058\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.195410, T: 6853, Avg. loss: 0.616768\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.198387, T: 7832, Avg. loss: 0.615571\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.200999, T: 8811, Avg. loss: 0.614585\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.203320, T: 9790, Avg. loss: 0.613765\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.205412, T: 10769, Avg. loss: 0.613008\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.207323, T: 11748, Avg. loss: 0.612273\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.209080, T: 12727, Avg. loss: 0.611618\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 13 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.15, NNZs: 7, Bias: -0.030437, T: 979, Avg. loss: 0.695766\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.10, NNZs: 0, Bias: -0.032603, T: 1958, Avg. loss: 0.696047\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.08, NNZs: 0, Bias: -0.033927, T: 2937, Avg. loss: 0.696228\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.034829, T: 3916, Avg. loss: 0.696316\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.035577, T: 4895, Avg. loss: 0.696427\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.06, NNZs: 0, Bias: -0.036169, T: 5874, Avg. loss: 0.696478\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.26, NNZs: 7, Bias: -0.095265, T: 979, Avg. loss: 0.676128\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.22, NNZs: 1, Bias: -0.105478, T: 1958, Avg. loss: 0.673425\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.111433, T: 2937, Avg. loss: 0.672297\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.115549, T: 3916, Avg. loss: 0.671616\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.118743, T: 4895, Avg. loss: 0.670984\n",
            "Total training time: 0.01 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 6\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.121350, T: 5874, Avg. loss: 0.670594\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.123553, T: 6853, Avg. loss: 0.670215\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.125442, T: 7832, Avg. loss: 0.669940\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.32, NNZs: 10, Bias: -0.149469, T: 979, Avg. loss: 0.641297\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.25, NNZs: 1, Bias: -0.165956, T: 1958, Avg. loss: 0.629815\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.22, NNZs: 1, Bias: -0.175432, T: 2937, Avg. loss: 0.625438\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.182037, T: 3916, Avg. loss: 0.622693\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.187044, T: 4895, Avg. loss: 0.620699\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.191132, T: 5874, Avg. loss: 0.619054\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.194547, T: 6853, Avg. loss: 0.617817\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.197521, T: 7832, Avg. loss: 0.616642\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.200142, T: 8811, Avg. loss: 0.615618\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.202476, T: 9790, Avg. loss: 0.614781\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.204575, T: 10769, Avg. loss: 0.614018\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.206494, T: 11748, Avg. loss: 0.613285\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.208258, T: 12727, Avg. loss: 0.612623\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.209886, T: 13706, Avg. loss: 0.612037\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 14 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.15, NNZs: 7, Bias: -0.030257, T: 979, Avg. loss: 0.695732\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.10, NNZs: 0, Bias: -0.032345, T: 1958, Avg. loss: 0.696009\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.08, NNZs: 0, Bias: -0.033646, T: 2937, Avg. loss: 0.696193\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.034547, T: 3916, Avg. loss: 0.696287\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.035291, T: 4895, Avg. loss: 0.696396\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.06, NNZs: 0, Bias: -0.035876, T: 5874, Avg. loss: 0.696444\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.27, NNZs: 11, Bias: -0.094634, T: 979, Avg. loss: 0.676439\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.23, NNZs: 1, Bias: -0.104843, T: 1958, Avg. loss: 0.673239\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.110788, T: 2937, Avg. loss: 0.672084\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.114922, T: 3916, Avg. loss: 0.671424\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.118101, T: 4895, Avg. loss: 0.670799\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.120721, T: 5874, Avg. loss: 0.670370\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.122916, T: 6853, Avg. loss: 0.670014\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.124805, T: 7832, Avg. loss: 0.669736\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.32, NNZs: 10, Bias: -0.153649, T: 979, Avg. loss: 0.639604\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.25, NNZs: 1, Bias: -0.169452, T: 1958, Avg. loss: 0.628578\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.22, NNZs: 1, Bias: -0.178955, T: 2937, Avg. loss: 0.624025\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.185546, T: 3916, Avg. loss: 0.621266\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.190514, T: 4895, Avg. loss: 0.619359\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.194596, T: 5874, Avg. loss: 0.617638\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.197985, T: 6853, Avg. loss: 0.616465\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.200948, T: 7832, Avg. loss: 0.615267\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.203554, T: 8811, Avg. loss: 0.614278\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.205868, T: 9790, Avg. loss: 0.613465\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.207960, T: 10769, Avg. loss: 0.612668\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.209871, T: 11748, Avg. loss: 0.611955\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.211628, T: 12727, Avg. loss: 0.611287\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 13 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.15, NNZs: 7, Bias: -0.027127, T: 979, Avg. loss: 0.695439\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.10, NNZs: 0, Bias: -0.029397, T: 1958, Avg. loss: 0.695746\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.08, NNZs: 0, Bias: -0.030656, T: 2937, Avg. loss: 0.695885\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.031568, T: 3916, Avg. loss: 0.695997\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.032333, T: 4895, Avg. loss: 0.696115\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.06, NNZs: 0, Bias: -0.032914, T: 5874, Avg. loss: 0.696147\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.26, NNZs: 10, Bias: -0.081289, T: 979, Avg. loss: 0.678993\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.22, NNZs: 1, Bias: -0.091693, T: 1958, Avg. loss: 0.675678\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.097781, T: 2937, Avg. loss: 0.674499\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.101983, T: 3916, Avg. loss: 0.673736\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.105252, T: 4895, Avg. loss: 0.673151\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.107917, T: 5874, Avg. loss: 0.672700\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.110157, T: 6853, Avg. loss: 0.672335\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.112084, T: 7832, Avg. loss: 0.672021\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.31, NNZs: 10, Bias: -0.150987, T: 979, Avg. loss: 0.641584\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.24, NNZs: 1, Bias: -0.167071, T: 1958, Avg. loss: 0.629619\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.176654, T: 2937, Avg. loss: 0.624969\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.183344, T: 3916, Avg. loss: 0.622168\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.188385, T: 4895, Avg. loss: 0.620191\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.192538, T: 5874, Avg. loss: 0.618465\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.195975, T: 6853, Avg. loss: 0.617275\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.198979, T: 7832, Avg. loss: 0.616031\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.201635, T: 8811, Avg. loss: 0.615006\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.203984, T: 9790, Avg. loss: 0.614202\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.206108, T: 10769, Avg. loss: 0.613378\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.208044, T: 11748, Avg. loss: 0.612669\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.209831, T: 12727, Avg. loss: 0.611970\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.211471, T: 13706, Avg. loss: 0.611414\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 14 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.15, NNZs: 5, Bias: -0.010313, T: 979, Avg. loss: 0.693796\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.10, NNZs: 0, Bias: -0.012867, T: 1958, Avg. loss: 0.694204\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.08, NNZs: 0, Bias: -0.014308, T: 2937, Avg. loss: 0.694363\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.015328, T: 3916, Avg. loss: 0.694474\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.016161, T: 4895, Avg. loss: 0.694587\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.06, NNZs: 0, Bias: -0.016803, T: 5874, Avg. loss: 0.694626\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.24, NNZs: 10, Bias: -0.077974, T: 979, Avg. loss: 0.680348\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.088474, T: 1958, Avg. loss: 0.676507\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.094598, T: 2937, Avg. loss: 0.675397\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.098869, T: 3916, Avg. loss: 0.674525\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.102152, T: 4895, Avg. loss: 0.673958\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.104839, T: 5874, Avg. loss: 0.673456\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.107088, T: 6853, Avg. loss: 0.673119\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.109032, T: 7832, Avg. loss: 0.672786\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.30, NNZs: 10, Bias: -0.157563, T: 979, Avg. loss: 0.639675\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.23, NNZs: 1, Bias: -0.173308, T: 1958, Avg. loss: 0.627810\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.182825, T: 2937, Avg. loss: 0.623086\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.189447, T: 3916, Avg. loss: 0.620348\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.194455, T: 4895, Avg. loss: 0.618401\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.198583, T: 5874, Avg. loss: 0.616677\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.201996, T: 6853, Avg. loss: 0.615518\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.204970, T: 7832, Avg. loss: 0.614293\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.15, NNZs: 1, Bias: -0.207606, T: 8811, Avg. loss: 0.613267\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.15, NNZs: 1, Bias: -0.209940, T: 9790, Avg. loss: 0.612455\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.15, NNZs: 1, Bias: -0.212046, T: 10769, Avg. loss: 0.611652\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.15, NNZs: 1, Bias: -0.213968, T: 11748, Avg. loss: 0.610951\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.14, NNZs: 1, Bias: -0.215739, T: 12727, Avg. loss: 0.610278\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.14, NNZs: 1, Bias: -0.217370, T: 13706, Avg. loss: 0.609677\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 14 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.15, NNZs: 6, Bias: -0.011817, T: 979, Avg. loss: 0.693899\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.10, NNZs: 0, Bias: -0.014363, T: 1958, Avg. loss: 0.694344\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.08, NNZs: 0, Bias: -0.015804, T: 2937, Avg. loss: 0.694505\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.016782, T: 3916, Avg. loss: 0.694594\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.017642, T: 4895, Avg. loss: 0.694744\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.06, NNZs: 0, Bias: -0.018282, T: 5874, Avg. loss: 0.694766\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.26, NNZs: 9, Bias: -0.072284, T: 979, Avg. loss: 0.680160\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.22, NNZs: 1, Bias: -0.082960, T: 1958, Avg. loss: 0.677063\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.089146, T: 2937, Avg. loss: 0.675912\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.093440, T: 3916, Avg. loss: 0.675085\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.096742, T: 4895, Avg. loss: 0.674492\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.099448, T: 5874, Avg. loss: 0.674014\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.101718, T: 6853, Avg. loss: 0.673656\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.103675, T: 7832, Avg. loss: 0.673324\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.32, NNZs: 10, Bias: -0.158116, T: 979, Avg. loss: 0.638220\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.25, NNZs: 1, Bias: -0.173693, T: 1958, Avg. loss: 0.626955\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.22, NNZs: 1, Bias: -0.183182, T: 2937, Avg. loss: 0.622277\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.189820, T: 3916, Avg. loss: 0.619505\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.194822, T: 4895, Avg. loss: 0.617605\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.198942, T: 5874, Avg. loss: 0.615907\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.202358, T: 6853, Avg. loss: 0.614715\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.205323, T: 7832, Avg. loss: 0.613519\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.207956, T: 8811, Avg. loss: 0.612483\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.210291, T: 9790, Avg. loss: 0.611669\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.212389, T: 10769, Avg. loss: 0.610919\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.214307, T: 11748, Avg. loss: 0.610190\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.216074, T: 12727, Avg. loss: 0.609514\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.217702, T: 13706, Avg. loss: 0.608924\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 14 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.15, NNZs: 6, Bias: -0.013281, T: 979, Avg. loss: 0.694072\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.10, NNZs: 0, Bias: -0.015783, T: 1958, Avg. loss: 0.694471\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.08, NNZs: 0, Bias: -0.017215, T: 2937, Avg. loss: 0.694636\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.018195, T: 3916, Avg. loss: 0.694729\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.019030, T: 4895, Avg. loss: 0.694862\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.06, NNZs: 0, Bias: -0.019655, T: 5874, Avg. loss: 0.694886\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.25, NNZs: 5, Bias: -0.113890, T: 980, Avg. loss: 0.673651\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.22, NNZs: 1, Bias: -0.123907, T: 1960, Avg. loss: 0.670634\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.129606, T: 2940, Avg. loss: 0.669402\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.133621, T: 3920, Avg. loss: 0.668698\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.136671, T: 4900, Avg. loss: 0.668274\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.139201, T: 5880, Avg. loss: 0.667849\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.141319, T: 6860, Avg. loss: 0.667501\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.143142, T: 7840, Avg. loss: 0.667223\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.31, NNZs: 3, Bias: -0.171421, T: 980, Avg. loss: 0.633302\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.24, NNZs: 1, Bias: -0.187293, T: 1960, Avg. loss: 0.622305\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.196345, T: 2940, Avg. loss: 0.618149\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.202727, T: 3920, Avg. loss: 0.615419\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.207644, T: 4900, Avg. loss: 0.613468\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.211652, T: 5880, Avg. loss: 0.611942\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.215007, T: 6860, Avg. loss: 0.610692\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.217910, T: 7840, Avg. loss: 0.609602\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.220461, T: 8820, Avg. loss: 0.608657\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.222747, T: 9800, Avg. loss: 0.607780\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.224798, T: 10780, Avg. loss: 0.607058\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.16, NNZs: 1, Bias: -0.226675, T: 11760, Avg. loss: 0.606382\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.15, NNZs: 1, Bias: -0.228381, T: 12740, Avg. loss: 0.605808\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 13 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.15, NNZs: 2, Bias: -0.018744, T: 980, Avg. loss: 0.694522\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.10, NNZs: 0, Bias: -0.021505, T: 1960, Avg. loss: 0.695041\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.08, NNZs: 0, Bias: -0.022940, T: 2940, Avg. loss: 0.695166\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.023959, T: 3920, Avg. loss: 0.695280\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.024755, T: 4900, Avg. loss: 0.695369\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.06, NNZs: 0, Bias: -0.025419, T: 5880, Avg. loss: 0.695449\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 6 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.26, NNZs: 4, Bias: -0.109326, T: 980, Avg. loss: 0.673920\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.23, NNZs: 1, Bias: -0.119423, T: 1960, Avg. loss: 0.670994\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.125175, T: 2940, Avg. loss: 0.669708\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.129181, T: 3920, Avg. loss: 0.669017\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.132260, T: 4900, Avg. loss: 0.668562\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.134811, T: 5880, Avg. loss: 0.668125\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.136942, T: 6860, Avg. loss: 0.667779\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.138776, T: 7840, Avg. loss: 0.667485\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.32, NNZs: 3, Bias: -0.168920, T: 980, Avg. loss: 0.633931\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.25, NNZs: 1, Bias: -0.184455, T: 1960, Avg. loss: 0.622802\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.23, NNZs: 1, Bias: -0.193582, T: 2940, Avg. loss: 0.618544\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.21, NNZs: 1, Bias: -0.199955, T: 3920, Avg. loss: 0.615824\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.20, NNZs: 1, Bias: -0.204898, T: 4900, Avg. loss: 0.613873\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.208906, T: 5880, Avg. loss: 0.612377\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.19, NNZs: 1, Bias: -0.212267, T: 6860, Avg. loss: 0.611111\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.215189, T: 7840, Avg. loss: 0.609976\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.217735, T: 8820, Avg. loss: 0.609078\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.220031, T: 9800, Avg. loss: 0.608173\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.18, NNZs: 1, Bias: -0.222088, T: 10780, Avg. loss: 0.607452\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.223967, T: 11760, Avg. loss: 0.606773\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.17, NNZs: 1, Bias: -0.225679, T: 12740, Avg. loss: 0.606182\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 13 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.15, NNZs: 4, Bias: -0.017108, T: 980, Avg. loss: 0.694427\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.10, NNZs: 0, Bias: -0.019861, T: 1960, Avg. loss: 0.694881\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.08, NNZs: 0, Bias: -0.021321, T: 2940, Avg. loss: 0.695018\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.022350, T: 3920, Avg. loss: 0.695129\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.07, NNZs: 0, Bias: -0.023164, T: 4900, Avg. loss: 0.695226\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.06, NNZs: 0, Bias: -0.023831, T: 5880, Avg. loss: 0.695298\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.46, NNZs: 1280, Bias: -0.094796, T: 979, Avg. loss: 0.658943\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.46, NNZs: 1280, Bias: -0.104106, T: 1958, Avg. loss: 0.654669\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.46, NNZs: 1280, Bias: -0.109559, T: 2937, Avg. loss: 0.653667\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.46, NNZs: 1280, Bias: -0.113329, T: 3916, Avg. loss: 0.653023\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.46, NNZs: 1280, Bias: -0.116281, T: 4895, Avg. loss: 0.652409\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.46, NNZs: 1280, Bias: -0.118695, T: 5874, Avg. loss: 0.651955\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.46, NNZs: 1280, Bias: -0.120723, T: 6853, Avg. loss: 0.651705\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.46, NNZs: 1280, Bias: -0.122472, T: 7832, Avg. loss: 0.651446\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.51, NNZs: 1280, Bias: -0.119717, T: 979, Avg. loss: 0.629399\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.51, NNZs: 1280, Bias: -0.135734, T: 1958, Avg. loss: 0.616274\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.50, NNZs: 1280, Bias: -0.145003, T: 2937, Avg. loss: 0.611787\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.50, NNZs: 1280, Bias: -0.151531, T: 3916, Avg. loss: 0.609106\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.50, NNZs: 1280, Bias: -0.156445, T: 4895, Avg. loss: 0.607565\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.50, NNZs: 1280, Bias: -0.160422, T: 5874, Avg. loss: 0.606067\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.50, NNZs: 1280, Bias: -0.163777, T: 6853, Avg. loss: 0.604813\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.50, NNZs: 1280, Bias: -0.166703, T: 7832, Avg. loss: 0.603554\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.50, NNZs: 1280, Bias: -0.169274, T: 8811, Avg. loss: 0.602628\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.50, NNZs: 1280, Bias: -0.171555, T: 9790, Avg. loss: 0.601951\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.49, NNZs: 1280, Bias: -0.173611, T: 10769, Avg. loss: 0.601245\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.49, NNZs: 1280, Bias: -0.175489, T: 11748, Avg. loss: 0.600518\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.49, NNZs: 1280, Bias: -0.177214, T: 12727, Avg. loss: 0.599938\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 13 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.27, NNZs: 1280, Bias: -0.036020, T: 979, Avg. loss: 0.687596\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.27, NNZs: 1280, Bias: -0.038112, T: 1958, Avg. loss: 0.685378\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.27, NNZs: 1280, Bias: -0.039395, T: 2937, Avg. loss: 0.685388\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.27, NNZs: 1280, Bias: -0.040311, T: 3916, Avg. loss: 0.685354\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.27, NNZs: 1280, Bias: -0.041068, T: 4895, Avg. loss: 0.685480\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.27, NNZs: 1280, Bias: -0.041683, T: 5874, Avg. loss: 0.685509\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.27, NNZs: 1280, Bias: -0.042194, T: 6853, Avg. loss: 0.685532\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.47, NNZs: 1274, Bias: -0.095025, T: 979, Avg. loss: 0.657768\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.47, NNZs: 1274, Bias: -0.104450, T: 1958, Avg. loss: 0.653683\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.47, NNZs: 1274, Bias: -0.109939, T: 2937, Avg. loss: 0.652693\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.47, NNZs: 1274, Bias: -0.113702, T: 3916, Avg. loss: 0.652064\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.47, NNZs: 1274, Bias: -0.116663, T: 4895, Avg. loss: 0.651470\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.47, NNZs: 1274, Bias: -0.119064, T: 5874, Avg. loss: 0.651054\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.47, NNZs: 1274, Bias: -0.121089, T: 6853, Avg. loss: 0.650747\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.50, NNZs: 1274, Bias: -0.139378, T: 979, Avg. loss: 0.621736\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.50, NNZs: 1274, Bias: -0.155077, T: 1958, Avg. loss: 0.609542\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.50, NNZs: 1274, Bias: -0.164098, T: 2937, Avg. loss: 0.605312\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.49, NNZs: 1274, Bias: -0.170437, T: 3916, Avg. loss: 0.602793\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.49, NNZs: 1274, Bias: -0.175226, T: 4895, Avg. loss: 0.601165\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.49, NNZs: 1274, Bias: -0.179099, T: 5874, Avg. loss: 0.599781\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.49, NNZs: 1274, Bias: -0.182360, T: 6853, Avg. loss: 0.598599\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.49, NNZs: 1274, Bias: -0.185212, T: 7832, Avg. loss: 0.597343\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.49, NNZs: 1274, Bias: -0.187717, T: 8811, Avg. loss: 0.596445\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.49, NNZs: 1274, Bias: -0.189940, T: 9790, Avg. loss: 0.595800\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.49, NNZs: 1274, Bias: -0.191937, T: 10769, Avg. loss: 0.595202\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.49, NNZs: 1274, Bias: -0.193765, T: 11748, Avg. loss: 0.594479\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.49, NNZs: 1274, Bias: -0.195444, T: 12727, Avg. loss: 0.593903\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 13 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.28, NNZs: 1274, Bias: -0.032196, T: 979, Avg. loss: 0.686594\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.28, NNZs: 1274, Bias: -0.034360, T: 1958, Avg. loss: 0.684528\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.28, NNZs: 1274, Bias: -0.035725, T: 2937, Avg. loss: 0.684521\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.28, NNZs: 1274, Bias: -0.036661, T: 3916, Avg. loss: 0.684466\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.28, NNZs: 1274, Bias: -0.037443, T: 4895, Avg. loss: 0.684588\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.28, NNZs: 1274, Bias: -0.038065, T: 5874, Avg. loss: 0.684579\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.28, NNZs: 1274, Bias: -0.038589, T: 6853, Avg. loss: 0.684620\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.47, NNZs: 1291, Bias: -0.106755, T: 979, Avg. loss: 0.655618\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.47, NNZs: 1291, Bias: -0.115916, T: 1958, Avg. loss: 0.651885\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.47, NNZs: 1291, Bias: -0.121226, T: 2937, Avg. loss: 0.650997\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.47, NNZs: 1291, Bias: -0.124924, T: 3916, Avg. loss: 0.650361\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.47, NNZs: 1291, Bias: -0.127811, T: 4895, Avg. loss: 0.649762\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.47, NNZs: 1291, Bias: -0.130156, T: 5874, Avg. loss: 0.649372\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.47, NNZs: 1291, Bias: -0.132132, T: 6853, Avg. loss: 0.649072\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.49, NNZs: 1291, Bias: -0.143338, T: 979, Avg. loss: 0.620985\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.48, NNZs: 1291, Bias: -0.158993, T: 1958, Avg. loss: 0.609271\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.48, NNZs: 1291, Bias: -0.168068, T: 2937, Avg. loss: 0.604872\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.48, NNZs: 1291, Bias: -0.174371, T: 3916, Avg. loss: 0.602527\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.48, NNZs: 1291, Bias: -0.179163, T: 4895, Avg. loss: 0.600765\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.48, NNZs: 1291, Bias: -0.183015, T: 5874, Avg. loss: 0.599546\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.48, NNZs: 1291, Bias: -0.186264, T: 6853, Avg. loss: 0.598328\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.48, NNZs: 1291, Bias: -0.189102, T: 7832, Avg. loss: 0.597136\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.48, NNZs: 1291, Bias: -0.191591, T: 8811, Avg. loss: 0.596235\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.48, NNZs: 1291, Bias: -0.193805, T: 9790, Avg. loss: 0.595541\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.47, NNZs: 1291, Bias: -0.195800, T: 10769, Avg. loss: 0.594865\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.47, NNZs: 1291, Bias: -0.197622, T: 11748, Avg. loss: 0.594195\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.47, NNZs: 1291, Bias: -0.199299, T: 12727, Avg. loss: 0.593583\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 13 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.28, NNZs: 1291, Bias: -0.030834, T: 979, Avg. loss: 0.686829\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.28, NNZs: 1291, Bias: -0.033133, T: 1958, Avg. loss: 0.684563\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.28, NNZs: 1291, Bias: -0.034529, T: 2937, Avg. loss: 0.684509\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.28, NNZs: 1291, Bias: -0.035483, T: 3916, Avg. loss: 0.684487\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.28, NNZs: 1291, Bias: -0.036270, T: 4895, Avg. loss: 0.684595\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.28, NNZs: 1291, Bias: -0.036894, T: 5874, Avg. loss: 0.684585\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.28, NNZs: 1291, Bias: -0.037424, T: 6853, Avg. loss: 0.684625\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.46, NNZs: 1328, Bias: -0.090802, T: 979, Avg. loss: 0.660024\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.45, NNZs: 1328, Bias: -0.100310, T: 1958, Avg. loss: 0.655926\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.45, NNZs: 1328, Bias: -0.105864, T: 2937, Avg. loss: 0.654844\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.45, NNZs: 1328, Bias: -0.109702, T: 3916, Avg. loss: 0.654174\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.45, NNZs: 1328, Bias: -0.112681, T: 4895, Avg. loss: 0.653577\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.45, NNZs: 1328, Bias: -0.115114, T: 5874, Avg. loss: 0.653176\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.45, NNZs: 1328, Bias: -0.117170, T: 6853, Avg. loss: 0.652808\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.45, NNZs: 1328, Bias: -0.118933, T: 7832, Avg. loss: 0.652613\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.48, NNZs: 1328, Bias: -0.142649, T: 979, Avg. loss: 0.622157\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.48, NNZs: 1328, Bias: -0.158344, T: 1958, Avg. loss: 0.610788\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.47, NNZs: 1328, Bias: -0.167378, T: 2937, Avg. loss: 0.606589\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.47, NNZs: 1328, Bias: -0.173681, T: 3916, Avg. loss: 0.604127\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.47, NNZs: 1328, Bias: -0.178461, T: 4895, Avg. loss: 0.602437\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.47, NNZs: 1328, Bias: -0.182365, T: 5874, Avg. loss: 0.600925\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.47, NNZs: 1328, Bias: -0.185629, T: 6853, Avg. loss: 0.599801\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.47, NNZs: 1328, Bias: -0.188470, T: 7832, Avg. loss: 0.598676\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.47, NNZs: 1328, Bias: -0.190975, T: 8811, Avg. loss: 0.597669\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.47, NNZs: 1328, Bias: -0.193206, T: 9790, Avg. loss: 0.596947\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.47, NNZs: 1328, Bias: -0.195213, T: 10769, Avg. loss: 0.596271\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.47, NNZs: 1328, Bias: -0.197047, T: 11748, Avg. loss: 0.595598\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.47, NNZs: 1328, Bias: -0.198735, T: 12727, Avg. loss: 0.594964\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.46, NNZs: 1328, Bias: -0.200292, T: 13706, Avg. loss: 0.594457\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 14 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.26, NNZs: 1328, Bias: -0.030662, T: 979, Avg. loss: 0.687759\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.26, NNZs: 1328, Bias: -0.032860, T: 1958, Avg. loss: 0.685611\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.26, NNZs: 1328, Bias: -0.034221, T: 2937, Avg. loss: 0.685568\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.27, NNZs: 1328, Bias: -0.035166, T: 3916, Avg. loss: 0.685534\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.27, NNZs: 1328, Bias: -0.035941, T: 4895, Avg. loss: 0.685621\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.27, NNZs: 1328, Bias: -0.036551, T: 5874, Avg. loss: 0.685627\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.27, NNZs: 1328, Bias: -0.037078, T: 6853, Avg. loss: 0.685663\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.46, NNZs: 1329, Bias: -0.090005, T: 979, Avg. loss: 0.660186\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.46, NNZs: 1329, Bias: -0.099524, T: 1958, Avg. loss: 0.655942\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.45, NNZs: 1329, Bias: -0.105079, T: 2937, Avg. loss: 0.654822\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.45, NNZs: 1329, Bias: -0.108940, T: 3916, Avg. loss: 0.654121\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.45, NNZs: 1329, Bias: -0.111910, T: 4895, Avg. loss: 0.653584\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.45, NNZs: 1329, Bias: -0.114359, T: 5874, Avg. loss: 0.653094\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.45, NNZs: 1329, Bias: -0.116410, T: 6853, Avg. loss: 0.652796\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.45, NNZs: 1329, Bias: -0.118176, T: 7832, Avg. loss: 0.652571\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.48, NNZs: 1329, Bias: -0.146642, T: 979, Avg. loss: 0.619550\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.48, NNZs: 1329, Bias: -0.161699, T: 1958, Avg. loss: 0.610144\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.47, NNZs: 1329, Bias: -0.170758, T: 2937, Avg. loss: 0.605131\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.47, NNZs: 1329, Bias: -0.177045, T: 3916, Avg. loss: 0.602739\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.47, NNZs: 1329, Bias: -0.181788, T: 4895, Avg. loss: 0.601154\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.47, NNZs: 1329, Bias: -0.185687, T: 5874, Avg. loss: 0.599532\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.47, NNZs: 1329, Bias: -0.188924, T: 6853, Avg. loss: 0.598505\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.47, NNZs: 1329, Bias: -0.191755, T: 7832, Avg. loss: 0.597332\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.47, NNZs: 1329, Bias: -0.194244, T: 8811, Avg. loss: 0.596364\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.47, NNZs: 1329, Bias: -0.196456, T: 9790, Avg. loss: 0.595697\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.47, NNZs: 1329, Bias: -0.198456, T: 10769, Avg. loss: 0.594954\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.47, NNZs: 1329, Bias: -0.200283, T: 11748, Avg. loss: 0.594286\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.47, NNZs: 1329, Bias: -0.201962, T: 12727, Avg. loss: 0.593647\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 13 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.27, NNZs: 1329, Bias: -0.027599, T: 979, Avg. loss: 0.687433\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.27, NNZs: 1329, Bias: -0.029973, T: 1958, Avg. loss: 0.685389\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.27, NNZs: 1329, Bias: -0.031295, T: 2937, Avg. loss: 0.685147\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.27, NNZs: 1329, Bias: -0.032250, T: 3916, Avg. loss: 0.685232\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.27, NNZs: 1329, Bias: -0.033046, T: 4895, Avg. loss: 0.685312\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.27, NNZs: 1329, Bias: -0.033654, T: 5874, Avg. loss: 0.685304\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.27, NNZs: 1329, Bias: -0.034187, T: 6853, Avg. loss: 0.685349\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.46, NNZs: 1327, Bias: -0.076843, T: 979, Avg. loss: 0.662533\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.46, NNZs: 1327, Bias: -0.086541, T: 1958, Avg. loss: 0.657918\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.46, NNZs: 1327, Bias: -0.092223, T: 2937, Avg. loss: 0.656662\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.46, NNZs: 1327, Bias: -0.096144, T: 3916, Avg. loss: 0.655921\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.46, NNZs: 1327, Bias: -0.099196, T: 4895, Avg. loss: 0.655409\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.46, NNZs: 1327, Bias: -0.101686, T: 5874, Avg. loss: 0.654952\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.45, NNZs: 1327, Bias: -0.103777, T: 6853, Avg. loss: 0.654618\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.45, NNZs: 1327, Bias: -0.105576, T: 7832, Avg. loss: 0.654349\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.48, NNZs: 1327, Bias: -0.144426, T: 979, Avg. loss: 0.620942\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.48, NNZs: 1327, Bias: -0.159721, T: 1958, Avg. loss: 0.610397\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.47, NNZs: 1327, Bias: -0.168838, T: 2937, Avg. loss: 0.605445\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.47, NNZs: 1327, Bias: -0.175209, T: 3916, Avg. loss: 0.602995\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.47, NNZs: 1327, Bias: -0.180013, T: 4895, Avg. loss: 0.601390\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.47, NNZs: 1327, Bias: -0.183972, T: 5874, Avg. loss: 0.599736\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.47, NNZs: 1327, Bias: -0.187248, T: 6853, Avg. loss: 0.598739\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.47, NNZs: 1327, Bias: -0.190114, T: 7832, Avg. loss: 0.597470\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.47, NNZs: 1327, Bias: -0.192646, T: 8811, Avg. loss: 0.596472\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.47, NNZs: 1327, Bias: -0.194887, T: 9790, Avg. loss: 0.595834\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.47, NNZs: 1327, Bias: -0.196913, T: 10769, Avg. loss: 0.595072\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.47, NNZs: 1327, Bias: -0.198761, T: 11748, Avg. loss: 0.594463\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.47, NNZs: 1327, Bias: -0.200467, T: 12727, Avg. loss: 0.593732\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 13 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.26, NNZs: 1327, Bias: -0.010993, T: 979, Avg. loss: 0.685637\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.26, NNZs: 1327, Bias: -0.013641, T: 1958, Avg. loss: 0.683957\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.26, NNZs: 1327, Bias: -0.015136, T: 2937, Avg. loss: 0.683870\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.26, NNZs: 1327, Bias: -0.016194, T: 3916, Avg. loss: 0.683852\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.26, NNZs: 1327, Bias: -0.017055, T: 4895, Avg. loss: 0.683951\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.26, NNZs: 1327, Bias: -0.017722, T: 5874, Avg. loss: 0.683960\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.26, NNZs: 1327, Bias: -0.018293, T: 6853, Avg. loss: 0.683946\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.45, NNZs: 1343, Bias: -0.074058, T: 979, Avg. loss: 0.664211\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.45, NNZs: 1343, Bias: -0.083849, T: 1958, Avg. loss: 0.659152\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.45, NNZs: 1343, Bias: -0.089569, T: 2937, Avg. loss: 0.658014\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.45, NNZs: 1343, Bias: -0.093556, T: 3916, Avg. loss: 0.657191\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.45, NNZs: 1343, Bias: -0.096624, T: 4895, Avg. loss: 0.656746\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.45, NNZs: 1343, Bias: -0.099134, T: 5874, Avg. loss: 0.656224\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.45, NNZs: 1343, Bias: -0.101235, T: 6853, Avg. loss: 0.655917\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.44, NNZs: 1343, Bias: -0.103051, T: 7832, Avg. loss: 0.655624\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.47, NNZs: 1343, Bias: -0.150452, T: 979, Avg. loss: 0.619509\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.47, NNZs: 1343, Bias: -0.165460, T: 1958, Avg. loss: 0.609802\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.46, NNZs: 1343, Bias: -0.174534, T: 2937, Avg. loss: 0.604536\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.46, NNZs: 1343, Bias: -0.180850, T: 3916, Avg. loss: 0.602042\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.46, NNZs: 1343, Bias: -0.185631, T: 4895, Avg. loss: 0.600456\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.46, NNZs: 1343, Bias: -0.189573, T: 5874, Avg. loss: 0.598795\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.46, NNZs: 1343, Bias: -0.192833, T: 6853, Avg. loss: 0.597795\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.46, NNZs: 1343, Bias: -0.195674, T: 7832, Avg. loss: 0.596599\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.46, NNZs: 1343, Bias: -0.198192, T: 8811, Avg. loss: 0.595568\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.46, NNZs: 1343, Bias: -0.200422, T: 9790, Avg. loss: 0.594913\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.46, NNZs: 1343, Bias: -0.202436, T: 10769, Avg. loss: 0.594195\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.46, NNZs: 1343, Bias: -0.204273, T: 11748, Avg. loss: 0.593573\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.46, NNZs: 1343, Bias: -0.205967, T: 12727, Avg. loss: 0.592881\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.45, NNZs: 1343, Bias: -0.207525, T: 13706, Avg. loss: 0.592360\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 14 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.25, NNZs: 1343, Bias: -0.012353, T: 979, Avg. loss: 0.686501\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.25, NNZs: 1343, Bias: -0.014988, T: 1958, Avg. loss: 0.684868\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.25, NNZs: 1343, Bias: -0.016478, T: 2937, Avg. loss: 0.684812\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.25, NNZs: 1343, Bias: -0.017494, T: 3916, Avg. loss: 0.684750\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.26, NNZs: 1343, Bias: -0.018378, T: 4895, Avg. loss: 0.684968\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.26, NNZs: 1343, Bias: -0.019040, T: 5874, Avg. loss: 0.684911\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.26, NNZs: 1343, Bias: -0.019613, T: 6853, Avg. loss: 0.684919\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.46, NNZs: 1325, Bias: -0.068023, T: 979, Avg. loss: 0.663122\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.46, NNZs: 1325, Bias: -0.077968, T: 1958, Avg. loss: 0.658797\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.46, NNZs: 1325, Bias: -0.083742, T: 2937, Avg. loss: 0.657727\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.46, NNZs: 1325, Bias: -0.087746, T: 3916, Avg. loss: 0.656868\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.46, NNZs: 1325, Bias: -0.090828, T: 4895, Avg. loss: 0.656407\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.46, NNZs: 1325, Bias: -0.093353, T: 5874, Avg. loss: 0.655897\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.46, NNZs: 1325, Bias: -0.095472, T: 6853, Avg. loss: 0.655561\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.46, NNZs: 1325, Bias: -0.097298, T: 7832, Avg. loss: 0.655275\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.49, NNZs: 1325, Bias: -0.151531, T: 979, Avg. loss: 0.618002\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.48, NNZs: 1325, Bias: -0.166376, T: 1958, Avg. loss: 0.608295\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.48, NNZs: 1325, Bias: -0.175417, T: 2937, Avg. loss: 0.602899\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.48, NNZs: 1325, Bias: -0.181744, T: 3916, Avg. loss: 0.600342\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.48, NNZs: 1325, Bias: -0.186517, T: 4895, Avg. loss: 0.598858\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.48, NNZs: 1325, Bias: -0.190449, T: 5874, Avg. loss: 0.597207\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.48, NNZs: 1325, Bias: -0.193710, T: 6853, Avg. loss: 0.596180\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.48, NNZs: 1325, Bias: -0.196542, T: 7832, Avg. loss: 0.595077\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.48, NNZs: 1325, Bias: -0.199056, T: 8811, Avg. loss: 0.593985\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.47, NNZs: 1325, Bias: -0.201286, T: 9790, Avg. loss: 0.593308\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.47, NNZs: 1325, Bias: -0.203290, T: 10769, Avg. loss: 0.592687\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.47, NNZs: 1325, Bias: -0.205123, T: 11748, Avg. loss: 0.592029\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.47, NNZs: 1325, Bias: -0.206811, T: 12727, Avg. loss: 0.591340\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.47, NNZs: 1325, Bias: -0.208368, T: 13706, Avg. loss: 0.590819\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 14 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.27, NNZs: 1325, Bias: -0.013829, T: 979, Avg. loss: 0.685393\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.27, NNZs: 1325, Bias: -0.016423, T: 1958, Avg. loss: 0.683720\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.27, NNZs: 1325, Bias: -0.017909, T: 2937, Avg. loss: 0.683710\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.27, NNZs: 1325, Bias: -0.018927, T: 3916, Avg. loss: 0.683660\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.27, NNZs: 1325, Bias: -0.019789, T: 4895, Avg. loss: 0.683825\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.27, NNZs: 1325, Bias: -0.020437, T: 5874, Avg. loss: 0.683761\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.27, NNZs: 1325, Bias: -0.021010, T: 6853, Avg. loss: 0.683804\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.45, NNZs: 1320, Bias: -0.108593, T: 980, Avg. loss: 0.657914\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.45, NNZs: 1320, Bias: -0.117964, T: 1960, Avg. loss: 0.654118\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.45, NNZs: 1320, Bias: -0.123294, T: 2940, Avg. loss: 0.652890\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.44, NNZs: 1320, Bias: -0.127050, T: 3920, Avg. loss: 0.652190\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.44, NNZs: 1320, Bias: -0.129900, T: 4900, Avg. loss: 0.651763\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.44, NNZs: 1320, Bias: -0.132268, T: 5880, Avg. loss: 0.651347\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.44, NNZs: 1320, Bias: -0.134250, T: 6860, Avg. loss: 0.651062\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.44, NNZs: 1320, Bias: -0.135956, T: 7840, Avg. loss: 0.650801\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.47, NNZs: 1320, Bias: -0.165299, T: 980, Avg. loss: 0.614913\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.47, NNZs: 1320, Bias: -0.180372, T: 1960, Avg. loss: 0.603459\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.47, NNZs: 1320, Bias: -0.188980, T: 2940, Avg. loss: 0.599471\n",
            "Total training time: 0.00 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 4\n",
            "Norm: 0.46, NNZs: 1320, Bias: -0.195054, T: 3920, Avg. loss: 0.597005\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.46, NNZs: 1320, Bias: -0.199735, T: 4900, Avg. loss: 0.595204\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.46, NNZs: 1320, Bias: -0.203553, T: 5880, Avg. loss: 0.593714\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.46, NNZs: 1320, Bias: -0.206749, T: 6860, Avg. loss: 0.592610\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.46, NNZs: 1320, Bias: -0.209515, T: 7840, Avg. loss: 0.591594\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.46, NNZs: 1320, Bias: -0.211946, T: 8820, Avg. loss: 0.590739\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.46, NNZs: 1320, Bias: -0.214125, T: 9800, Avg. loss: 0.589910\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.46, NNZs: 1320, Bias: -0.216081, T: 10780, Avg. loss: 0.589296\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.46, NNZs: 1320, Bias: -0.217871, T: 11760, Avg. loss: 0.588673\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.46, NNZs: 1320, Bias: -0.219498, T: 12740, Avg. loss: 0.588239\n",
            "Total training time: 0.02 seconds.\n",
            "Convergence after 13 epochs took 0.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.26, NNZs: 1320, Bias: -0.019235, T: 980, Avg. loss: 0.686881\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.26, NNZs: 1320, Bias: -0.022072, T: 1960, Avg. loss: 0.685070\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.26, NNZs: 1320, Bias: -0.023560, T: 2940, Avg. loss: 0.684716\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.26, NNZs: 1320, Bias: -0.024615, T: 3920, Avg. loss: 0.684804\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.26, NNZs: 1320, Bias: -0.025439, T: 4900, Avg. loss: 0.684808\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.26, NNZs: 1320, Bias: -0.026125, T: 5880, Avg. loss: 0.684866\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.26, NNZs: 1320, Bias: -0.026679, T: 6860, Avg. loss: 0.684825\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.46, NNZs: 1338, Bias: -0.103911, T: 980, Avg. loss: 0.657481\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.46, NNZs: 1338, Bias: -0.113334, T: 1960, Avg. loss: 0.653801\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.46, NNZs: 1338, Bias: -0.118698, T: 2940, Avg. loss: 0.652569\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.45, NNZs: 1338, Bias: -0.122435, T: 3920, Avg. loss: 0.651947\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.45, NNZs: 1338, Bias: -0.125306, T: 4900, Avg. loss: 0.651459\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.45, NNZs: 1338, Bias: -0.127687, T: 5880, Avg. loss: 0.651020\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.45, NNZs: 1338, Bias: -0.129676, T: 6860, Avg. loss: 0.650771\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.45, NNZs: 1338, Bias: -0.131388, T: 7840, Avg. loss: 0.650488\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.48, NNZs: 1338, Bias: -0.162504, T: 980, Avg. loss: 0.614761\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.48, NNZs: 1338, Bias: -0.177263, T: 1960, Avg. loss: 0.604056\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.48, NNZs: 1338, Bias: -0.185943, T: 2940, Avg. loss: 0.599573\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.47, NNZs: 1338, Bias: -0.192008, T: 3920, Avg. loss: 0.597172\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.47, NNZs: 1338, Bias: -0.196715, T: 4900, Avg. loss: 0.595349\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.47, NNZs: 1338, Bias: -0.200533, T: 5880, Avg. loss: 0.593942\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.47, NNZs: 1338, Bias: -0.203735, T: 6860, Avg. loss: 0.592796\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.47, NNZs: 1338, Bias: -0.206519, T: 7840, Avg. loss: 0.591676\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.47, NNZs: 1338, Bias: -0.208947, T: 8820, Avg. loss: 0.590953\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.47, NNZs: 1338, Bias: -0.211135, T: 9800, Avg. loss: 0.590036\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.47, NNZs: 1338, Bias: -0.213096, T: 10780, Avg. loss: 0.589468\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.47, NNZs: 1338, Bias: -0.214889, T: 11760, Avg. loss: 0.588826\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.47, NNZs: 1338, Bias: -0.216522, T: 12740, Avg. loss: 0.588352\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 13 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 0.27, NNZs: 1338, Bias: -0.017757, T: 980, Avg. loss: 0.686293\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.27, NNZs: 1338, Bias: -0.020600, T: 1960, Avg. loss: 0.684370\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.27, NNZs: 1338, Bias: -0.022119, T: 2940, Avg. loss: 0.684087\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.27, NNZs: 1338, Bias: -0.023189, T: 3920, Avg. loss: 0.684124\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.27, NNZs: 1338, Bias: -0.024033, T: 4900, Avg. loss: 0.684159\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.27, NNZs: 1338, Bias: -0.024725, T: 5880, Avg. loss: 0.684204\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.27, NNZs: 1338, Bias: -0.025285, T: 6860, Avg. loss: 0.684163\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 7 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 16.52, NNZs: 1262, Bias: -0.173974, T: 1088, Avg. loss: 0.534957\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 17.12, NNZs: 1283, Bias: -0.145432, T: 2176, Avg. loss: 0.341549\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 17.49, NNZs: 1287, Bias: -0.132416, T: 3264, Avg. loss: 0.307129\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 17.73, NNZs: 1290, Bias: -0.123823, T: 4352, Avg. loss: 0.288929\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 17.93, NNZs: 1291, Bias: -0.120754, T: 5440, Avg. loss: 0.277168\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 18.09, NNZs: 1291, Bias: -0.116314, T: 6528, Avg. loss: 0.268564\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 18.22, NNZs: 1291, Bias: -0.111915, T: 7616, Avg. loss: 0.261643\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 18.34, NNZs: 1291, Bias: -0.108728, T: 8704, Avg. loss: 0.255986\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 18.44, NNZs: 1291, Bias: -0.107628, T: 9792, Avg. loss: 0.251472\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 18.52, NNZs: 1291, Bias: -0.105813, T: 10880, Avg. loss: 0.247468\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 18.60, NNZs: 1291, Bias: -0.104456, T: 11968, Avg. loss: 0.243995\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 18.67, NNZs: 1291, Bias: -0.103219, T: 13056, Avg. loss: 0.240962\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 18.73, NNZs: 1291, Bias: -0.101878, T: 14144, Avg. loss: 0.238186\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 18.79, NNZs: 1291, Bias: -0.101080, T: 15232, Avg. loss: 0.235733\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 18.85, NNZs: 1291, Bias: -0.100652, T: 16320, Avg. loss: 0.233536\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 18.90, NNZs: 1291, Bias: -0.099589, T: 17408, Avg. loss: 0.231503\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 18.94, NNZs: 1291, Bias: -0.098782, T: 18496, Avg. loss: 0.229636\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 18.99, NNZs: 1291, Bias: -0.098455, T: 19584, Avg. loss: 0.227898\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 19.03, NNZs: 1291, Bias: -0.097691, T: 20672, Avg. loss: 0.226228\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 19.07, NNZs: 1291, Bias: -0.097313, T: 21760, Avg. loss: 0.224694\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 19.11, NNZs: 1291, Bias: -0.096831, T: 22848, Avg. loss: 0.223215\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 19.15, NNZs: 1291, Bias: -0.096181, T: 23936, Avg. loss: 0.221811\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 19.18, NNZs: 1291, Bias: -0.095687, T: 25024, Avg. loss: 0.220504\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 19.22, NNZs: 1291, Bias: -0.095408, T: 26112, Avg. loss: 0.219259\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 19.25, NNZs: 1291, Bias: -0.094908, T: 27200, Avg. loss: 0.218084\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 19.28, NNZs: 1291, Bias: -0.094647, T: 28288, Avg. loss: 0.216960\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 19.31, NNZs: 1291, Bias: -0.094158, T: 29376, Avg. loss: 0.215899\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 19.34, NNZs: 1291, Bias: -0.093891, T: 30464, Avg. loss: 0.214917\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 19.36, NNZs: 1291, Bias: -0.093631, T: 31552, Avg. loss: 0.213975\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 19.39, NNZs: 1291, Bias: -0.093347, T: 32640, Avg. loss: 0.213058\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 19.41, NNZs: 1291, Bias: -0.093073, T: 33728, Avg. loss: 0.212184\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 19.44, NNZs: 1291, Bias: -0.092870, T: 34816, Avg. loss: 0.211361\n",
            "Total training time: 0.03 seconds.\n",
            "Convergence after 32 epochs took 0.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 24.28, NNZs: 1165, Bias: -0.489115, T: 1088, Avg. loss: 0.427232\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 24.48, NNZs: 1176, Bias: -0.480668, T: 2176, Avg. loss: 0.214930\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 24.64, NNZs: 1186, Bias: -0.482210, T: 3264, Avg. loss: 0.196480\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 24.74, NNZs: 1193, Bias: -0.483434, T: 4352, Avg. loss: 0.185990\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 24.83, NNZs: 1195, Bias: -0.486978, T: 5440, Avg. loss: 0.179672\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 24.90, NNZs: 1195, Bias: -0.488094, T: 6528, Avg. loss: 0.172183\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 24.96, NNZs: 1195, Bias: -0.488403, T: 7616, Avg. loss: 0.168200\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 25.01, NNZs: 1198, Bias: -0.489870, T: 8704, Avg. loss: 0.164410\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 25.06, NNZs: 1198, Bias: -0.490078, T: 9792, Avg. loss: 0.162218\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 25.10, NNZs: 1198, Bias: -0.490982, T: 10880, Avg. loss: 0.160253\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 25.14, NNZs: 1198, Bias: -0.491625, T: 11968, Avg. loss: 0.157649\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 25.18, NNZs: 1198, Bias: -0.492272, T: 13056, Avg. loss: 0.156111\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 25.22, NNZs: 1199, Bias: -0.493217, T: 14144, Avg. loss: 0.154696\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 25.25, NNZs: 1199, Bias: -0.493442, T: 15232, Avg. loss: 0.153001\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 25.29, NNZs: 1200, Bias: -0.493647, T: 16320, Avg. loss: 0.151954\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 25.32, NNZs: 1200, Bias: -0.493990, T: 17408, Avg. loss: 0.150318\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 25.35, NNZs: 1200, Bias: -0.493936, T: 18496, Avg. loss: 0.149119\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 25.38, NNZs: 1200, Bias: -0.494337, T: 19584, Avg. loss: 0.148196\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 25.40, NNZs: 1200, Bias: -0.495422, T: 20672, Avg. loss: 0.147406\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 25.43, NNZs: 1200, Bias: -0.495807, T: 21760, Avg. loss: 0.146444\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 25.45, NNZs: 1200, Bias: -0.496130, T: 22848, Avg. loss: 0.145587\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 25.48, NNZs: 1200, Bias: -0.495879, T: 23936, Avg. loss: 0.144781\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 22 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 16.23, NNZs: 1322, Bias: -0.306744, T: 1088, Avg. loss: 0.595379\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 16.73, NNZs: 1324, Bias: -0.306606, T: 2176, Avg. loss: 0.411181\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 17.00, NNZs: 1325, Bias: -0.306159, T: 3264, Avg. loss: 0.378508\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 17.18, NNZs: 1325, Bias: -0.304915, T: 4352, Avg. loss: 0.362630\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 17.31, NNZs: 1326, Bias: -0.304648, T: 5440, Avg. loss: 0.350055\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 17.42, NNZs: 1326, Bias: -0.303530, T: 6528, Avg. loss: 0.342544\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 17.52, NNZs: 1328, Bias: -0.302681, T: 7616, Avg. loss: 0.335914\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 17.60, NNZs: 1328, Bias: -0.301923, T: 8704, Avg. loss: 0.331137\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 17.67, NNZs: 1329, Bias: -0.301631, T: 9792, Avg. loss: 0.326729\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 17.73, NNZs: 1330, Bias: -0.301127, T: 10880, Avg. loss: 0.323137\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 17.78, NNZs: 1330, Bias: -0.300796, T: 11968, Avg. loss: 0.319959\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 17.83, NNZs: 1330, Bias: -0.300526, T: 13056, Avg. loss: 0.317164\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 17.88, NNZs: 1330, Bias: -0.300523, T: 14144, Avg. loss: 0.314635\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 17.92, NNZs: 1330, Bias: -0.300253, T: 15232, Avg. loss: 0.312502\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 17.96, NNZs: 1330, Bias: -0.300181, T: 16320, Avg. loss: 0.310406\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 18.00, NNZs: 1330, Bias: -0.300183, T: 17408, Avg. loss: 0.308483\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 18.03, NNZs: 1330, Bias: -0.300103, T: 18496, Avg. loss: 0.306740\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 18.07, NNZs: 1330, Bias: -0.299896, T: 19584, Avg. loss: 0.305041\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 18.10, NNZs: 1330, Bias: -0.299937, T: 20672, Avg. loss: 0.303591\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 18.13, NNZs: 1330, Bias: -0.299592, T: 21760, Avg. loss: 0.302165\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 18.16, NNZs: 1330, Bias: -0.299725, T: 22848, Avg. loss: 0.300726\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 18.18, NNZs: 1330, Bias: -0.299451, T: 23936, Avg. loss: 0.299574\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 18.21, NNZs: 1335, Bias: -0.299415, T: 25024, Avg. loss: 0.298321\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 18.23, NNZs: 1335, Bias: -0.299315, T: 26112, Avg. loss: 0.297171\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 18.26, NNZs: 1335, Bias: -0.299279, T: 27200, Avg. loss: 0.296060\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 18.28, NNZs: 1335, Bias: -0.299169, T: 28288, Avg. loss: 0.295003\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 18.30, NNZs: 1335, Bias: -0.299095, T: 29376, Avg. loss: 0.294014\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 18.32, NNZs: 1335, Bias: -0.298919, T: 30464, Avg. loss: 0.293140\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 18.34, NNZs: 1335, Bias: -0.298719, T: 31552, Avg. loss: 0.292236\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 18.36, NNZs: 1335, Bias: -0.298625, T: 32640, Avg. loss: 0.291344\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 18.38, NNZs: 1335, Bias: -0.298680, T: 33728, Avg. loss: 0.290457\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 31 epochs took 0.04 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=10,\n",
              "                   estimator=SGDClassifier(class_weight={'negative': 1.051,\n",
              "                                                         'neutral': 3.521,\n",
              "                                                         'positive': 0.566},\n",
              "                                           random_state=29),\n",
              "                   param_distributions={'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
              "                                        'loss': ['hinge', 'log',\n",
              "                                                 'squared_hinge',\n",
              "                                                 'modified_huber'],\n",
              "                                        'penalty': ['l2', 'l1', 'elasticnet',\n",
              "                                                    'none'],\n",
              "                                        'verbose': [1]},\n",
              "                   random_state=29, scoring='f1_macro')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "\n",
        "svm = SGDClassifier(random_state=29, class_weight =w)\n",
        "svm_rs=RandomizedSearchCV(svm, params, cv=10, random_state = 29,scoring='f1_macro')\n",
        "svm_rs.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3X_b7M-lC2f",
        "outputId": "0bf5ace0-c026-45ad-d731-d9933bf8dfee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tuned hpyerparameters :(best parameters)  {'verbose': 1, 'penalty': 'none', 'loss': 'hinge', 'alpha': 0.01}\n",
            "f1_macro : 0.5769497838124436\n"
          ]
        }
      ],
      "source": [
        "print(\"tuned hpyerparameters :(best parameters) \",svm_rs.best_params_)\n",
        "print(\"f1_macro :\",svm_rs.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xnYAvSJrWeW"
      },
      "outputs": [],
      "source": [
        "predict_result_aspect = svm_rs.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCoqWsdyraN6",
        "outputId": "6be1b75e-1505-4ab2-d6a1-d0f34cf59488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "0.7130620985010707\n",
            "f1 score\n",
            "0.7198824794165235\n",
            "recall\n",
            "0.7130620985010707\n",
            "precision\n",
            "0.72802744417775\n"
          ]
        }
      ],
      "source": [
        "print('accuracy')\n",
        "print(accuracy_score(y_test, predict_result_aspect))\n",
        "print('f1 score')\n",
        "print(f1_score(y_test, predict_result_aspect,average='weighted'))\n",
        "print('recall')\n",
        "print(recall_score(y_test, predict_result_aspect,average='weighted'))\n",
        "print('precision')\n",
        "print(precision_score(y_test, predict_result_aspect,average='weighted'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0vyLF88sbqF",
        "outputId": "941bbc56-9e6f-450d-c464-1866897cd745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.58      0.60      0.59       126\n",
            "     neutral       0.31      0.23      0.27        48\n",
            "    positive       0.82      0.84      0.83       293\n",
            "\n",
            "    accuracy                           0.71       467\n",
            "   macro avg       0.57      0.56      0.56       467\n",
            "weighted avg       0.70      0.71      0.71       467\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(predict_result_aspect, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWm6tDTUXyje"
      },
      "source": [
        "### CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_CEGltHwMdU"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=29)\n",
        "X_train_svd = svd.fit_transform(X_train)\n",
        "X_test_svd = svd.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beDClnT1Qibx",
        "outputId": "86e2fc5a-e51d-462c-c8ab-1dd9fd0f7cd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1088, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "X_train_svd.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbuoWTTFw9ha"
      },
      "outputs": [],
      "source": [
        "y_train_dict = pd.DataFrame(y_train, columns=[\"SP\"])\n",
        "y_test_dict = pd.DataFrame(y_test, columns=[\"SP\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuG0R8yEZtbV"
      },
      "outputs": [],
      "source": [
        "cols = []\n",
        "for i in range(X_train_svd.shape[1]):\n",
        "  cols.append(str(i))\n",
        "\n",
        "X_train_crf = pd.DataFrame(X_train_svd,columns = cols)\n",
        "X_test_crf = pd.DataFrame(X_test_svd,columns = cols)#['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxOA9zDqwS_T"
      },
      "outputs": [],
      "source": [
        "#train data\n",
        "\n",
        "X_train_crf_dict = []\n",
        "for i in (X_train_crf.index):\n",
        "  dic = X_train_crf.iloc[i].to_dict()\n",
        "  base = []\n",
        "  base.append(dic)\n",
        "  X_train_crf_dict.append(base)\n",
        "\n",
        "  \n",
        "y_train_crf_dict = []\n",
        "for i in (y_train_dict.index):\n",
        "  dic = y_train_dict.iloc[i].tolist()\n",
        "  y_train_crf_dict.append(dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNzV-Vvrd2_K"
      },
      "outputs": [],
      "source": [
        "#test data\n",
        "X_test_crf_dict = []\n",
        "for i in (X_test_crf.index):\n",
        "  dic = X_test_crf.iloc[i].to_dict()\n",
        "  base = []\n",
        "  base.append(dic)\n",
        "  X_test_crf_dict.append(base)\n",
        "\n",
        "\n",
        "y_test_crf_dict = []\n",
        "for i in (y_test_dict.index):\n",
        "  dic = y_test_dict.iloc[i].tolist()\n",
        "  y_test_crf_dict.append(dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsPgj-7Se72Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "params = {\n",
        "    'c1': scipy.stats.expon(scale=0.5),\n",
        "    'c2': scipy.stats.expon(scale=0.05),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-unfXyuOfY8m"
      },
      "outputs": [],
      "source": [
        "# use the same metric for evaluation\n",
        "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
        "                        average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60yBBvT1qTH5"
      },
      "outputs": [],
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcU7wurzex2g",
        "outputId": "74a669d7-351f-47c5-f8f8-f7df0295928d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  warnings.warn('From version 0.24, get_params will raise an '\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.1s\n",
            "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    8.9s\n",
            "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   18.3s\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   20.3s finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=10,\n",
              "                   estimator=CRF(algorithm='lbfgs',\n",
              "                                 all_possible_transitions=True, c1=0.1, c2=0.1,\n",
              "                                 keep_tempfiles=None, max_iterations=100),\n",
              "                   n_iter=50, n_jobs=-1,\n",
              "                   param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f0040afcee0>,\n",
              "                                        'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f0040afca60>},\n",
              "                   scoring=make_scorer(flat_f1_score, average=weighted),\n",
              "                   verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "crf_rs = RandomizedSearchCV(crf, params,\n",
        "                        cv=10,\n",
        "                        verbose=1,\n",
        "                        n_jobs=-1,\n",
        "                        n_iter=50,\n",
        "                        scoring=f1_scorer) #Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
        "crf_rs.fit(X_train_crf_dict, y_train_crf_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jng4wEdKfNNO",
        "outputId": "1287f30e-52cd-4c22-d7f1-7c6cd5e83ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tuned hpyerparameters :(best parameters)  {'c1': 0.06941325169741289, 'c2': 0.057558947127530714}\n",
            "f1_macro : 0.72424301368396\n"
          ]
        }
      ],
      "source": [
        "print(\"tuned hpyerparameters :(best parameters) \",crf_rs.best_params_)\n",
        "print(\"f1_macro :\",crf_rs.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RMBdiyZI6GR"
      },
      "outputs": [],
      "source": [
        "predict_result_aspect = crf_rs.predict(X_test_crf_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSe0-swsJJn7",
        "outputId": "440461b8-4590-49ec-e30e-f088c7bd6bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative      0.661     0.615     0.637       130\n",
            "     neutral      0.286     0.114     0.163        35\n",
            "    positive      0.819     0.901     0.858       302\n",
            "\n",
            "    accuracy                          0.762       467\n",
            "   macro avg      0.589     0.543     0.553       467\n",
            "weighted avg      0.735     0.762     0.745       467\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:67: FutureWarning: Pass labels=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(metrics.flat_classification_report(\n",
        "y_test_crf_dict, predict_result_aspect, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmLwisJVf1oc"
      },
      "source": [
        "#DL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN COC beforehand"
      ],
      "metadata": {
        "id": "83JvEMZEPBwv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMKVoOvebfx0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FI4y1mNf2TZ"
      },
      "source": [
        "## CNN_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKy7Fi4Iw3iJ"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bVXzRunkIr5",
        "outputId": "ab57621c-b2b0-4d03-e818-b9b9efb4693e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 200, 50)           50000     \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 200, 50)           0         \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 198, 128)          19328     \n",
            "                                                                 \n",
            " global_max_pooling1d_4 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 250)               32250     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 250)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 251       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 101,829\n",
            "Trainable params: 101,829\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "num_filters = 8\n",
        "filter_size = 3\n",
        "pool_size = 2\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(1000, 50, input_length=200))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv1D(128, 3, padding='valid', activation='relu', strides=1,))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-lf9Rj6m45_",
        "outputId": "5b692ed0-adec-4e8b-d11d-ae712c60d17a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train seq: 25000\n",
            "Test seq: 25000\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import imdb\n",
        "n_words = 1000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=n_words)\n",
        "print('Train seq: {}'.format(len(X_train)))\n",
        "print('Test seq: {}'.format(len(X_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "NZBXY8TCk8NN",
        "outputId": "94d2c09e-1e65-493b-e278-b4955e56d947"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-2e705f171227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Pad sequences with max_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m     \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m       raise ValueError(f'Shape of sample {trunc.shape[1:]} of sequence at '\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'index_s'"
          ]
        }
      ],
      "source": [
        "# Pad sequences with max_len\n",
        "max_len = 200\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhq8V1mfCFGS"
      },
      "outputs": [],
      "source": [
        "X_train_cnn = pad_sequences(X_train_cnn, maxlen=max_len)\n",
        "X_test_cnn = pad_sequences(X_test_cnn, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPLhcS7CoMrm"
      },
      "outputs": [],
      "source": [
        "callbacks = [EarlyStopping(monitor='loss', patience=3)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrGkzYiysOXn"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "n_epochs = 100\n",
        "\n",
        "model.fit(X_train_cnn, y_train, batch_size=batch_size, epochs=n_epochs, validation_split=0.2, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "R1nIbbGesSjN",
        "outputId": "1730dcac-f4a4-4d69-9a7b-560c6c4b4a55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "204/204 [==============================] - 1s 6ms/step - loss: 0.6965\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-93204e393be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nAccuracy on test set: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Accuracy on test set: 0.873\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "print('\\nAccuracy on test set: {}'.format(model.evaluate(X_test_cnn, y_test)[1]))\n",
        "\n",
        "# Accuracy on test set: 0.873"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h6UG992Flod",
        "outputId": "930abaaa-79d1-4309-cdf6-381e486ad0a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1], dtype=int32)"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_cnn[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "BVZfNCSG6OCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "T54oSZnA97GT",
        "outputId": "6ad6ebc1-0ef9-4030-cb47-a22815814f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                               text   aspectTerm  \\\n",
              "0     142087  I am very satisfied with it ... I have used th...        price   \n",
              "1     142089  Cheap processing, wobbling unstable unstable l...   processing   \n",
              "2     142089  Cheap processing, wobbling unstable unstable l...         lids   \n",
              "3     142089  Cheap processing, wobbling unstable unstable l...  baking time   \n",
              "4     142091  The iron does not remain, the waffles are too ...      waffles   \n",
              "...      ...                                                ...          ...   \n",
              "1083  145299  When I used the machine to make triangular san...        bakes   \n",
              "1084  145303           The device is very bad in terms of price        price   \n",
              "1085  145309                 Its size is small than I expected.         size   \n",
              "1086  145316                   The products come out delicious.     products   \n",
              "1087  145355  If the plates could be removed, it would be fa...       plates   \n",
              "\n",
              "      From   To        SP                                         text_wo_sw  \\\n",
              "0       82   87  positive  satisfied ... device times ... price device .....   \n",
              "1        6   16  negative  cheap processing, wobbling unstable unstable l...   \n",
              "2       45   49  negative  cheap processing, wobbling unstable unstable l...   \n",
              "3      142  153  negative  cheap processing, wobbling unstable unstable l...   \n",
              "4       30   37  negative     iron remain, waffles small waffles brown, six.   \n",
              "...    ...  ...       ...                                                ...   \n",
              "1083   106  111  positive  machine triangular sandwiches, handle broke. d...   \n",
              "1084    35   40  negative                             device bad terms price   \n",
              "1085     4    8  negative                               size small expected.   \n",
              "1086     4   12  positive                           products come delicious.   \n",
              "1087     7   13   neutral                         plates removed, fantastic!   \n",
              "\n",
              "                                                    coc  \n",
              "0     satisfied...device times...device...can't expe...  \n",
              "1                cheap, wobbling unstable unstable lids  \n",
              "2     cheap processing, wobbling unstable unstable f...  \n",
              "3     independently time open incredible unreasonabl...  \n",
              "4                iron remain, small waffles brown, six.  \n",
              "...                                                 ...  \n",
              "1083       sandwiches, handle broke. device good, well.  \n",
              "1084                                   device bad terms  \n",
              "1085                                    small expected.  \n",
              "1086                                    come delicious.  \n",
              "1087                                removed, fantastic!  \n",
              "\n",
              "[1088 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c5656005-cb7c-4182-baf4-53004a17d75e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>aspectTerm</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>SP</th>\n",
              "      <th>text_wo_sw</th>\n",
              "      <th>coc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142087</td>\n",
              "      <td>I am very satisfied with it ... I have used th...</td>\n",
              "      <td>price</td>\n",
              "      <td>82</td>\n",
              "      <td>87</td>\n",
              "      <td>positive</td>\n",
              "      <td>satisfied ... device times ... price device .....</td>\n",
              "      <td>satisfied...device times...device...can't expe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>processing</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap, wobbling unstable unstable lids</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>lids</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap processing, wobbling unstable unstable f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>baking time</td>\n",
              "      <td>142</td>\n",
              "      <td>153</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>independently time open incredible unreasonabl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142091</td>\n",
              "      <td>The iron does not remain, the waffles are too ...</td>\n",
              "      <td>waffles</td>\n",
              "      <td>30</td>\n",
              "      <td>37</td>\n",
              "      <td>negative</td>\n",
              "      <td>iron remain, waffles small waffles brown, six.</td>\n",
              "      <td>iron remain, small waffles brown, six.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>145299</td>\n",
              "      <td>When I used the machine to make triangular san...</td>\n",
              "      <td>bakes</td>\n",
              "      <td>106</td>\n",
              "      <td>111</td>\n",
              "      <td>positive</td>\n",
              "      <td>machine triangular sandwiches, handle broke. d...</td>\n",
              "      <td>sandwiches, handle broke. device good, well.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1084</th>\n",
              "      <td>145303</td>\n",
              "      <td>The device is very bad in terms of price</td>\n",
              "      <td>price</td>\n",
              "      <td>35</td>\n",
              "      <td>40</td>\n",
              "      <td>negative</td>\n",
              "      <td>device bad terms price</td>\n",
              "      <td>device bad terms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085</th>\n",
              "      <td>145309</td>\n",
              "      <td>Its size is small than I expected.</td>\n",
              "      <td>size</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>negative</td>\n",
              "      <td>size small expected.</td>\n",
              "      <td>small expected.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1086</th>\n",
              "      <td>145316</td>\n",
              "      <td>The products come out delicious.</td>\n",
              "      <td>products</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>positive</td>\n",
              "      <td>products come delicious.</td>\n",
              "      <td>come delicious.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1087</th>\n",
              "      <td>145355</td>\n",
              "      <td>If the plates could be removed, it would be fa...</td>\n",
              "      <td>plates</td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>neutral</td>\n",
              "      <td>plates removed, fantastic!</td>\n",
              "      <td>removed, fantastic!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1088 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5656005-cb7c-4182-baf4-53004a17d75e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c5656005-cb7c-4182-baf4-53004a17d75e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c5656005-cb7c-4182-baf4-53004a17d75e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sp.loc[train_sp['SP'] == 'positive', 'SP'] = 1\n",
        "train_sp.loc[train_sp['SP'] == 'negative', 'SP'] = -1\n",
        "train_sp.loc[train_sp['SP'] == 'neutral', 'SP'] = 0\n",
        "\n",
        "test_sp.loc[test_sp['SP'] == 'positive', 'SP'] = 1\n",
        "test_sp.loc[test_sp['SP'] == 'negative', 'SP'] = -1\n",
        "test_sp.loc[test_sp['SP'] == 'neutral', 'SP'] = 0\n",
        "\n",
        "train_sp[\"SP\"] = train_sp[\"SP\"].astype(np.int64)\n",
        "test_sp[\"SP\"] = test_sp[\"SP\"].astype(np.int64)\n"
      ],
      "metadata": {
        "id": "wK-npHovFHmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "NgDs944iFPp5",
        "outputId": "e5e6b17f-be13-4c9b-9d89-51bcf27c9e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                               text   aspectTerm  \\\n",
              "0     142087  I am very satisfied with it ... I have used th...        price   \n",
              "1     142089  Cheap processing, wobbling unstable unstable l...   processing   \n",
              "2     142089  Cheap processing, wobbling unstable unstable l...         lids   \n",
              "3     142089  Cheap processing, wobbling unstable unstable l...  baking time   \n",
              "4     142091  The iron does not remain, the waffles are too ...      waffles   \n",
              "...      ...                                                ...          ...   \n",
              "1083  145299  When I used the machine to make triangular san...        bakes   \n",
              "1084  145303           The device is very bad in terms of price        price   \n",
              "1085  145309                 Its size is small than I expected.         size   \n",
              "1086  145316                   The products come out delicious.     products   \n",
              "1087  145355  If the plates could be removed, it would be fa...       plates   \n",
              "\n",
              "      From   To  SP                                         text_wo_sw  \\\n",
              "0       82   87   1  satisfied ... device times ... price device .....   \n",
              "1        6   16  -1  cheap processing, wobbling unstable unstable l...   \n",
              "2       45   49  -1  cheap processing, wobbling unstable unstable l...   \n",
              "3      142  153  -1  cheap processing, wobbling unstable unstable l...   \n",
              "4       30   37  -1     iron remain, waffles small waffles brown, six.   \n",
              "...    ...  ...  ..                                                ...   \n",
              "1083   106  111   1  machine triangular sandwiches, handle broke. d...   \n",
              "1084    35   40  -1                             device bad terms price   \n",
              "1085     4    8  -1                               size small expected.   \n",
              "1086     4   12   1                           products come delicious.   \n",
              "1087     7   13   0                         plates removed, fantastic!   \n",
              "\n",
              "                                                    coc  \n",
              "0     satisfied...device times...device...can't expe...  \n",
              "1                cheap, wobbling unstable unstable lids  \n",
              "2     cheap processing, wobbling unstable unstable f...  \n",
              "3     independently time open incredible unreasonabl...  \n",
              "4                iron remain, small waffles brown, six.  \n",
              "...                                                 ...  \n",
              "1083       sandwiches, handle broke. device good, well.  \n",
              "1084                                   device bad terms  \n",
              "1085                                    small expected.  \n",
              "1086                                    come delicious.  \n",
              "1087                                removed, fantastic!  \n",
              "\n",
              "[1088 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-639212d1-80d7-4368-bb08-eafe85260faf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>aspectTerm</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>SP</th>\n",
              "      <th>text_wo_sw</th>\n",
              "      <th>coc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142087</td>\n",
              "      <td>I am very satisfied with it ... I have used th...</td>\n",
              "      <td>price</td>\n",
              "      <td>82</td>\n",
              "      <td>87</td>\n",
              "      <td>1</td>\n",
              "      <td>satisfied ... device times ... price device .....</td>\n",
              "      <td>satisfied...device times...device...can't expe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>processing</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>-1</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap, wobbling unstable unstable lids</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>lids</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "      <td>-1</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap processing, wobbling unstable unstable f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>baking time</td>\n",
              "      <td>142</td>\n",
              "      <td>153</td>\n",
              "      <td>-1</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>independently time open incredible unreasonabl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142091</td>\n",
              "      <td>The iron does not remain, the waffles are too ...</td>\n",
              "      <td>waffles</td>\n",
              "      <td>30</td>\n",
              "      <td>37</td>\n",
              "      <td>-1</td>\n",
              "      <td>iron remain, waffles small waffles brown, six.</td>\n",
              "      <td>iron remain, small waffles brown, six.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>145299</td>\n",
              "      <td>When I used the machine to make triangular san...</td>\n",
              "      <td>bakes</td>\n",
              "      <td>106</td>\n",
              "      <td>111</td>\n",
              "      <td>1</td>\n",
              "      <td>machine triangular sandwiches, handle broke. d...</td>\n",
              "      <td>sandwiches, handle broke. device good, well.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1084</th>\n",
              "      <td>145303</td>\n",
              "      <td>The device is very bad in terms of price</td>\n",
              "      <td>price</td>\n",
              "      <td>35</td>\n",
              "      <td>40</td>\n",
              "      <td>-1</td>\n",
              "      <td>device bad terms price</td>\n",
              "      <td>device bad terms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085</th>\n",
              "      <td>145309</td>\n",
              "      <td>Its size is small than I expected.</td>\n",
              "      <td>size</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>-1</td>\n",
              "      <td>size small expected.</td>\n",
              "      <td>small expected.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1086</th>\n",
              "      <td>145316</td>\n",
              "      <td>The products come out delicious.</td>\n",
              "      <td>products</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>products come delicious.</td>\n",
              "      <td>come delicious.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1087</th>\n",
              "      <td>145355</td>\n",
              "      <td>If the plates could be removed, it would be fa...</td>\n",
              "      <td>plates</td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>plates removed, fantastic!</td>\n",
              "      <td>removed, fantastic!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1088 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-639212d1-80d7-4368-bb08-eafe85260faf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-639212d1-80d7-4368-bb08-eafe85260faf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-639212d1-80d7-4368-bb08-eafe85260faf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=train_sp['SP']\n",
        "y_test=test_sp['SP']\n",
        "X_train=train_sp['coc']\n",
        "X_test=test_sp['coc']"
      ],
      "metadata": {
        "id": "0vrC3qM_-NN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRTalnsxpqZJ",
        "outputId": "5cefd92f-a36d-4017-ecfc-b4397a5e9d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       satisfied...device times...device...can't expe...\n",
              "1                  cheap, wobbling unstable unstable lids\n",
              "2       cheap processing, wobbling unstable unstable f...\n",
              "3       independently time open incredible unreasonabl...\n",
              "4                  iron remain, small waffles brown, six.\n",
              "                              ...                        \n",
              "1083         sandwiches, handle broke. device good, well.\n",
              "1084                                     device bad terms\n",
              "1085                                      small expected.\n",
              "1086                                      come delicious.\n",
              "1087                                  removed, fantastic!\n",
              "Name: coc, Length: 1088, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_words=10000\n",
        "tokenizer=Tokenizer(max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "sequence_train=tokenizer.texts_to_sequences(X_train)\n",
        "sequence_test=tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "bESZbg-X-_MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec=tokenizer.word_index\n",
        "V=len(word2vec)\n",
        "print('dataset has %s number of independent tokens' %V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH8qS4rL_S0y",
        "outputId": "b9c32c84-3d18-41eb-e5db-aee3b129e63a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 1437 number of independent tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=pad_sequences(sequence_train)\n",
        "T=data_train.shape[1]\n",
        "data_test=pad_sequences(sequence_test,maxlen=T)\n"
      ],
      "metadata": {
        "id": "NnJssgDW_Xgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xJCk91VPps2",
        "outputId": "a450954f-01dd-4435-e8bd-d5e550674461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1437"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D=40\n",
        "i=Input((T,))\n",
        "x=Embedding(V+1,D)(i)\n",
        "x=Conv1D(64,2,activation='relu')(x)\n",
        "x=MaxPooling1D(2)(x)\n",
        "x=Conv1D(128,2,activation='relu')(x)\n",
        "x=MaxPooling1D(2)(x)\n",
        "x=Conv1D(256,2,activation='relu')(x)\n",
        "x=GlobalMaxPooling1D()(x)\n",
        "x=Dense(5,activation='softmax')(x)\n",
        "model=Model(i,x)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mILzFW8uAZ7P",
        "outputId": "9077c6d4-76d9-4f4c-95f8-39858c8321cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 12)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 12, 40)            57520     \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 11, 64)            5184      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 5, 64)            0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 4, 128)            16512     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 2, 128)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 1, 256)            65792     \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 256)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 146,293\n",
            "Trainable params: 146,293\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "cnn_senti=model.fit(data_train,y_train,validation_data=(data_test,y_test),epochs=5,batch_size=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF_xNqomJn5v",
        "outputId": "5af5084a-d63f-4617-ca59-5f50e831d487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "11/11 [==============================] - 8s 43ms/step - loss: nan - accuracy: 0.1167 - val_loss: nan - val_accuracy: 0.0749\n",
            "Epoch 2/5\n",
            "11/11 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0947 - val_loss: nan - val_accuracy: 0.0749\n",
            "Epoch 3/5\n",
            "11/11 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0947 - val_loss: nan - val_accuracy: 0.0749\n",
            "Epoch 4/5\n",
            "11/11 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0947 - val_loss: nan - val_accuracy: 0.0749\n",
            "Epoch 5/5\n",
            "11/11 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0947 - val_loss: nan - val_accuracy: 0.0749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVzxjmx9L4cX",
        "outputId": "a7c71b05-c466-4ead-9170-041cf588deba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1088, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam = k.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n",
        "model.compile(optimizer=adam,loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "cyexrVrKBTKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the best only\n",
        "\n",
        "filepath=\"/content/drive/MyDrive/Colab Notebooks/MA/cnn/checkpoint_sp/\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 10, verbose = 1,restore_best_weights = True)\n",
        "callbacks_list = [earlystop, checkpoint]"
      ],
      "metadata": {
        "id": "PK7fjalxBnU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aaa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "-a_JSHSuQaH6",
        "outputId": "7fa67abb-0eba-4842-8972-64bb42af6940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-972a1a11f199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'aaa' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history = model.fit(data_train,y_train,validation_data=(data_test,y_test),epochs=100,batch_size=32)"
      ],
      "metadata": {
        "id": "jRPcdzpqBM2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict(data_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "Ic44ieWQIgci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.shape"
      ],
      "metadata": {
        "id": "Ppgh-74iEd2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN 3"
      ],
      "metadata": {
        "id": "cYG7HaglZj73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_train = train_sp.coc.tolist()\n",
        "label_train = train_sp.SP.tolist()\n",
        "\n",
        "text_test = test_sp.coc.tolist()\n",
        "label_test = test_sp.SP.tolist()"
      ],
      "metadata": {
        "id": "1zMBXo_DZlcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(text_train)\n",
        "X_test = np.array(text_test)"
      ],
      "metadata": {
        "id": "By4J3Kz9bHzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words = 40000\n",
        "tokenizer = Tokenizer(num_words=top_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "XHIyH6_XbR1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 100\n",
        "X_train = pad_sequences(X_train, maxlen=max_words, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_words, padding='post')"
      ],
      "metadata": {
        "id": "TXEqfksbbfvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(label_train)\n",
        "y_test = np.array(label_test)"
      ],
      "metadata": {
        "id": "tzFzSlxPblnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFAOue_LbpbQ",
        "outputId": "9a7668f1-4078-4553-f213-f002c26411ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1088, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FihChsSUcAGA",
        "outputId": "0199f571-510f-494b-e97e-eca713030436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 94,   9, 132, ...,   0,   0,   0],\n",
              "       [117, 336, 337, ...,   0,   0,   0],\n",
              "       [117, 279, 336, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [ 23, 217,   0, ...,   0,   0,   0],\n",
              "       [ 47,  70,   0, ...,   0,   0,   0],\n",
              "       [577, 143,   0, ...,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(20000,32, input_length=100))\n",
        "model.add(Conv1D(256, 3, activation='relu', padding='same'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(128, 3, activation='relu', padding='same'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(250, activation='relu'))\n",
        "\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[get_f1])"
      ],
      "metadata": {
        "id": "m154j4QXZrYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGqo6PCGbxKh",
        "outputId": "c4c8ea25-cd63-4fca-d7dc-dadd01a8b8e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_44 (Embedding)    (None, 100, 32)           640000    \n",
            "                                                                 \n",
            " conv1d_95 (Conv1D)          (None, 100, 256)          24832     \n",
            "                                                                 \n",
            " max_pooling1d_70 (MaxPoolin  (None, 50, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 50, 256)           0         \n",
            "                                                                 \n",
            " conv1d_96 (Conv1D)          (None, 50, 128)           98432     \n",
            "                                                                 \n",
            " max_pooling1d_71 (MaxPoolin  (None, 25, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 25, 128)           0         \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 3200)              0         \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 250)               800250    \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 1)                 251       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,563,765\n",
            "Trainable params: 1,563,765\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=20, batch_size=128, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "xvJo5ZzbZ6N_",
        "outputId": "e1be56a5-6b43-4321-80bd-4c46a4f34f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-383-f791711d0d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1653\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[1;32m   1654\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1244\n  y sizes: 1088\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluate Model Performance on Test set\")\n",
        "result = model.evaluate(X_test,y_test)\n",
        "print(dict(zip(model.metrics_names, result)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuBsZiXRcGDE",
        "outputId": "59f227ad-8699-49ca-e92d-bfa359c639ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate Model Performance on Test set\n",
            "15/15 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "{'loss': nan, 'accuracy': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA9qL4_of3oY"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full sequence w/out Time Distributed. TimeDistributedDense applies a same dense to every time step during GRU/LSTM Cell unrolling."
      ],
      "metadata": {
        "id": "tJSF1T6dL2ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple RNN"
      ],
      "metadata": {
        "id": "xh0GRs_nlPig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = train_sp.append(test_sp, ignore_index=True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "b7d1df8c-16d8-469d-9d5f-7c0fa35eeabc",
        "id": "ja9TvDi0lBjg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                               text   aspectTerm  \\\n",
              "0     142087  I am very satisfied with it ... I have used th...        price   \n",
              "1     142089  Cheap processing, wobbling unstable unstable l...   processing   \n",
              "2     142089  Cheap processing, wobbling unstable unstable l...         lids   \n",
              "3     142089  Cheap processing, wobbling unstable unstable l...  baking time   \n",
              "4     142091  The iron does not remain, the waffles are too ...      waffles   \n",
              "...      ...                                                ...          ...   \n",
              "1550  145346  It is very simple to handle and the price qual...      coating   \n",
              "1551  145357  Easy to clean, heat enough and the materials s...        clean   \n",
              "1552  145357  Easy to clean, heat enough and the materials s...         heat   \n",
              "1553  145357  Easy to clean, heat enough and the materials s...    materials   \n",
              "1554  145357  Easy to clean, heat enough and the materials s...        price   \n",
              "\n",
              "      From   To        SP                                         text_wo_sw  \\\n",
              "0       82   87  positive  satisfied ... device times ... price device .....   \n",
              "1        6   16  negative  cheap processing, wobbling unstable unstable l...   \n",
              "2       45   49  negative  cheap processing, wobbling unstable unstable l...   \n",
              "3      142  153  negative  cheap processing, wobbling unstable unstable l...   \n",
              "4       30   37  negative     iron remain, waffles small waffles brown, six.   \n",
              "...    ...  ...       ...                                                ...   \n",
              "1550    81   88  positive  simple handle price quality relationship good....   \n",
              "1551     8   13  positive  easy clean, heat materials good quality plasti...   \n",
              "1552    15   19  positive  easy clean, heat materials good quality plasti...   \n",
              "1553    35   44  positive  easy clean, heat materials good quality plasti...   \n",
              "1554   123  128  positive  easy clean, heat materials good quality plasti...   \n",
              "\n",
              "                                                    coc  \n",
              "0     satisfied...device times...device...can't expe...  \n",
              "1                cheap, wobbling unstable unstable lids  \n",
              "2     cheap processing, wobbling unstable unstable f...  \n",
              "3     independently time open incredible unreasonabl...  \n",
              "4                iron remain, small waffles brown, six.  \n",
              "...                                                 ...  \n",
              "1550  handle price quality relationship good. non -s...  \n",
              "1551                  easy, heat materials good quality  \n",
              "1552  easy clean, materials good quality plasticucho...  \n",
              "1553  easy clean, heat good quality plasticuchos. go...  \n",
              "1554         materials good quality plasticuchos. good.  \n",
              "\n",
              "[1555 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e17b8914-9b0b-4390-a070-777d07056bd0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>aspectTerm</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>SP</th>\n",
              "      <th>text_wo_sw</th>\n",
              "      <th>coc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142087</td>\n",
              "      <td>I am very satisfied with it ... I have used th...</td>\n",
              "      <td>price</td>\n",
              "      <td>82</td>\n",
              "      <td>87</td>\n",
              "      <td>positive</td>\n",
              "      <td>satisfied ... device times ... price device .....</td>\n",
              "      <td>satisfied...device times...device...can't expe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>processing</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap, wobbling unstable unstable lids</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>lids</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap processing, wobbling unstable unstable f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>baking time</td>\n",
              "      <td>142</td>\n",
              "      <td>153</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>independently time open incredible unreasonabl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142091</td>\n",
              "      <td>The iron does not remain, the waffles are too ...</td>\n",
              "      <td>waffles</td>\n",
              "      <td>30</td>\n",
              "      <td>37</td>\n",
              "      <td>negative</td>\n",
              "      <td>iron remain, waffles small waffles brown, six.</td>\n",
              "      <td>iron remain, small waffles brown, six.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1550</th>\n",
              "      <td>145346</td>\n",
              "      <td>It is very simple to handle and the price qual...</td>\n",
              "      <td>coating</td>\n",
              "      <td>81</td>\n",
              "      <td>88</td>\n",
              "      <td>positive</td>\n",
              "      <td>simple handle price quality relationship good....</td>\n",
              "      <td>handle price quality relationship good. non -s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1551</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>clean</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy, heat materials good quality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1552</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>heat</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy clean, materials good quality plasticucho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1553</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>materials</td>\n",
              "      <td>35</td>\n",
              "      <td>44</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy clean, heat good quality plasticuchos. go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1554</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>price</td>\n",
              "      <td>123</td>\n",
              "      <td>128</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>materials good quality plasticuchos. good.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1555 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e17b8914-9b0b-4390-a070-777d07056bd0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e17b8914-9b0b-4390-a070-777d07056bd0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e17b8914-9b0b-4390-a070-777d07056bd0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 1000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 10\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(df['coc'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd122e53-a434-4c99-97eb-4580f11648a0",
        "id": "QXduAn2PlBjg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1765 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = tokenizer.texts_to_sequences(df['coc'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81009ca7-3359-4304-b146-caad47c62dc4",
        "id": "ZcgwKgLslBjh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data tensor: (1555, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(df['SP']).values\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9fec20c-4500-4845-c7c0-f170d880857a",
        "id": "CJ3h16BflBjh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (1555, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e58a7c6-31aa-445f-e6ac-d32004d45993",
        "id": "THNxfyillBji"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 29)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d0c2a3-b290-4a2c-8124-c0198e782806",
        "id": "sSpdAj2GlBji"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1244, 10) (1244, 3)\n",
            "(311, 10) (311, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first layer is the embedded layer that uses 100 length vectors to represent each word.\n",
        "SpatialDropout1D performs variational dropout in NLP models.\n",
        "The next layer is the LSTM layer with 100 memory units.\n",
        "The output layer must create 3 output values, one for each class.\n",
        "Activation function is softmax for multi-class classification.\n",
        "Because it is a multi-class classification problem, categorical_crossentropy is used as the loss function.\n"
      ],
      "metadata": {
        "id": "yu5em4v3lBjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(SimpleRNN(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aabcbdb-cee3-468a-ad59-7b78d8a46a8e",
        "id": "C5Xdr1H7lBjj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 10, 100)           100000    \n",
            "                                                                 \n",
            " spatial_dropout1d_7 (Spatia  (None, 10, 100)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,403\n",
            "Trainable params: 120,403\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam = k.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999)\n",
        "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=[get_f1])"
      ],
      "metadata": {
        "id": "_27b6K6ilBjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the best only\n",
        "\n",
        "filepath=\"/content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 5, verbose = 1,restore_best_weights = True)\n",
        "callbacks_list = [earlystop, checkpoint]"
      ],
      "metadata": {
        "id": "z_6yJdk8lBjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=callbacks_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8266fa-5778-48d2-be50-49542c97dd5d",
        "id": "YJvBeY3OlBjm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 1.1015 - get_f1: 0.0462\n",
            "Epoch 1: val_loss improved from inf to 1.03805, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 8s 439ms/step - loss: 1.0989 - get_f1: 0.0492 - val_loss: 1.0380 - val_get_f1: 0.0000e+00\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0263 - get_f1: 0.1320\n",
            "Epoch 2: val_loss improved from 1.03805 to 0.97673, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 1.0263 - get_f1: 0.1320 - val_loss: 0.9767 - val_get_f1: 0.1245\n",
            "Epoch 3/100\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.9637 - get_f1: 0.3413\n",
            "Epoch 3: val_loss improved from 0.97673 to 0.93341, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 141ms/step - loss: 0.9590 - get_f1: 0.3554 - val_loss: 0.9334 - val_get_f1: 0.5164\n",
            "Epoch 4/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.9338 - get_f1: 0.4810\n",
            "Epoch 4: val_loss improved from 0.93341 to 0.91164, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 3s 191ms/step - loss: 0.9402 - get_f1: 0.4707 - val_loss: 0.9116 - val_get_f1: 0.5525\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9078 - get_f1: 0.5349\n",
            "Epoch 5: val_loss improved from 0.91164 to 0.90096, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 143ms/step - loss: 0.9078 - get_f1: 0.5349 - val_loss: 0.9010 - val_get_f1: 0.5632\n",
            "Epoch 6/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.8997 - get_f1: 0.5529\n",
            "Epoch 6: val_loss improved from 0.90096 to 0.89627, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.9022 - get_f1: 0.5470 - val_loss: 0.8963 - val_get_f1: 0.5774\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8729 - get_f1: 0.5845\n",
            "Epoch 7: val_loss improved from 0.89627 to 0.89143, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 0.8729 - get_f1: 0.5845 - val_loss: 0.8914 - val_get_f1: 0.5846\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8747 - get_f1: 0.5845\n",
            "Epoch 8: val_loss improved from 0.89143 to 0.88794, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.8747 - get_f1: 0.5845 - val_loss: 0.8879 - val_get_f1: 0.5864\n",
            "Epoch 9/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.8674 - get_f1: 0.5894\n",
            "Epoch 9: val_loss improved from 0.88794 to 0.88144, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.8673 - get_f1: 0.5848 - val_loss: 0.8814 - val_get_f1: 0.5864\n",
            "Epoch 10/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.8596 - get_f1: 0.5883\n",
            "Epoch 10: val_loss improved from 0.88144 to 0.87598, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.8604 - get_f1: 0.5881 - val_loss: 0.8760 - val_get_f1: 0.5850\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8422 - get_f1: 0.5860\n",
            "Epoch 11: val_loss improved from 0.87598 to 0.87466, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 0.8422 - get_f1: 0.5860 - val_loss: 0.8747 - val_get_f1: 0.5908\n",
            "Epoch 12/100\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.8266 - get_f1: 0.6179\n",
            "Epoch 12: val_loss improved from 0.87466 to 0.87350, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.8385 - get_f1: 0.6028 - val_loss: 0.8735 - val_get_f1: 0.5897\n",
            "Epoch 13/100\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.8139 - get_f1: 0.6251\n",
            "Epoch 13: val_loss improved from 0.87350 to 0.86623, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 0.8268 - get_f1: 0.6058 - val_loss: 0.8662 - val_get_f1: 0.5897\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8306 - get_f1: 0.5869\n",
            "Epoch 14: val_loss improved from 0.86623 to 0.85822, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 0.8306 - get_f1: 0.5869 - val_loss: 0.8582 - val_get_f1: 0.5897\n",
            "Epoch 15/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.8063 - get_f1: 0.6056\n",
            "Epoch 15: val_loss improved from 0.85822 to 0.85523, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 0.8097 - get_f1: 0.5975 - val_loss: 0.8552 - val_get_f1: 0.5897\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8108 - get_f1: 0.6139\n",
            "Epoch 16: val_loss improved from 0.85523 to 0.85379, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 143ms/step - loss: 0.8108 - get_f1: 0.6139 - val_loss: 0.8538 - val_get_f1: 0.5928\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8011 - get_f1: 0.6240\n",
            "Epoch 17: val_loss improved from 0.85379 to 0.84584, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 0.8011 - get_f1: 0.6240 - val_loss: 0.8458 - val_get_f1: 0.5928\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7867 - get_f1: 0.6274\n",
            "Epoch 18: val_loss improved from 0.84584 to 0.83893, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 0.7867 - get_f1: 0.6274 - val_loss: 0.8389 - val_get_f1: 0.5967\n",
            "Epoch 19/100\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.7784 - get_f1: 0.6266\n",
            "Epoch 19: val_loss improved from 0.83893 to 0.83755, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 147ms/step - loss: 0.7846 - get_f1: 0.6245 - val_loss: 0.8375 - val_get_f1: 0.5942\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7570 - get_f1: 0.6529\n",
            "Epoch 20: val_loss improved from 0.83755 to 0.82894, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 3s 176ms/step - loss: 0.7570 - get_f1: 0.6529 - val_loss: 0.8289 - val_get_f1: 0.5966\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7598 - get_f1: 0.6403\n",
            "Epoch 21: val_loss improved from 0.82894 to 0.82070, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 0.7598 - get_f1: 0.6403 - val_loss: 0.8207 - val_get_f1: 0.6001\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7366 - get_f1: 0.6600\n",
            "Epoch 22: val_loss improved from 0.82070 to 0.81529, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 150ms/step - loss: 0.7366 - get_f1: 0.6600 - val_loss: 0.8153 - val_get_f1: 0.5988\n",
            "Epoch 23/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.7177 - get_f1: 0.6665\n",
            "Epoch 23: val_loss improved from 0.81529 to 0.80606, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.7168 - get_f1: 0.6659 - val_loss: 0.8061 - val_get_f1: 0.6088\n",
            "Epoch 24/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.7125 - get_f1: 0.6800\n",
            "Epoch 24: val_loss improved from 0.80606 to 0.79437, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 146ms/step - loss: 0.7156 - get_f1: 0.6738 - val_loss: 0.7944 - val_get_f1: 0.6153\n",
            "Epoch 25/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6966 - get_f1: 0.6891\n",
            "Epoch 25: val_loss improved from 0.79437 to 0.78795, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.6958 - get_f1: 0.6904 - val_loss: 0.7880 - val_get_f1: 0.6143\n",
            "Epoch 26/100\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.6771 - get_f1: 0.6865\n",
            "Epoch 26: val_loss improved from 0.78795 to 0.77648, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 3s 180ms/step - loss: 0.6835 - get_f1: 0.6769 - val_loss: 0.7765 - val_get_f1: 0.6226\n",
            "Epoch 27/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6590 - get_f1: 0.7137\n",
            "Epoch 27: val_loss improved from 0.77648 to 0.76177, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 0.6584 - get_f1: 0.7101 - val_loss: 0.7618 - val_get_f1: 0.6448\n",
            "Epoch 28/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6445 - get_f1: 0.7265\n",
            "Epoch 28: val_loss improved from 0.76177 to 0.75279, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 147ms/step - loss: 0.6498 - get_f1: 0.7156 - val_loss: 0.7528 - val_get_f1: 0.6414\n",
            "Epoch 29/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6298 - get_f1: 0.7373\n",
            "Epoch 29: val_loss improved from 0.75279 to 0.73407, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 146ms/step - loss: 0.6293 - get_f1: 0.7361 - val_loss: 0.7341 - val_get_f1: 0.6519\n",
            "Epoch 30/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6248 - get_f1: 0.7287\n",
            "Epoch 30: val_loss improved from 0.73407 to 0.72964, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 147ms/step - loss: 0.6232 - get_f1: 0.7295 - val_loss: 0.7296 - val_get_f1: 0.6584\n",
            "Epoch 31/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5817 - get_f1: 0.7729\n",
            "Epoch 31: val_loss improved from 0.72964 to 0.70656, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 3s 176ms/step - loss: 0.5860 - get_f1: 0.7676 - val_loss: 0.7066 - val_get_f1: 0.6723\n",
            "Epoch 32/100\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.5780 - get_f1: 0.7722\n",
            "Epoch 32: val_loss improved from 0.70656 to 0.69703, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 149ms/step - loss: 0.5781 - get_f1: 0.7721 - val_loss: 0.6970 - val_get_f1: 0.6806\n",
            "Epoch 33/100\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.5475 - get_f1: 0.7984\n",
            "Epoch 33: val_loss improved from 0.69703 to 0.68422, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 0.5474 - get_f1: 0.7940 - val_loss: 0.6842 - val_get_f1: 0.6954\n",
            "Epoch 34/100\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.5399 - get_f1: 0.8012\n",
            "Epoch 34: val_loss improved from 0.68422 to 0.68078, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 148ms/step - loss: 0.5333 - get_f1: 0.8053 - val_loss: 0.6808 - val_get_f1: 0.7087\n",
            "Epoch 35/100\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.5194 - get_f1: 0.8027\n",
            "Epoch 35: val_loss improved from 0.68078 to 0.66322, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.5217 - get_f1: 0.7951 - val_loss: 0.6632 - val_get_f1: 0.7069\n",
            "Epoch 36/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5093 - get_f1: 0.8101\n",
            "Epoch 36: val_loss improved from 0.66322 to 0.65679, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.5125 - get_f1: 0.8069 - val_loss: 0.6568 - val_get_f1: 0.7050\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4947 - get_f1: 0.8143\n",
            "Epoch 37: val_loss improved from 0.65679 to 0.64774, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 148ms/step - loss: 0.4947 - get_f1: 0.8143 - val_loss: 0.6477 - val_get_f1: 0.7210\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4760 - get_f1: 0.8079\n",
            "Epoch 38: val_loss improved from 0.64774 to 0.64141, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 148ms/step - loss: 0.4760 - get_f1: 0.8079 - val_loss: 0.6414 - val_get_f1: 0.7199\n",
            "Epoch 39/100\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.4720 - get_f1: 0.8246\n",
            "Epoch 39: val_loss improved from 0.64141 to 0.63660, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 147ms/step - loss: 0.4631 - get_f1: 0.8328 - val_loss: 0.6366 - val_get_f1: 0.7262\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4584 - get_f1: 0.8237\n",
            "Epoch 40: val_loss improved from 0.63660 to 0.63322, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 149ms/step - loss: 0.4584 - get_f1: 0.8237 - val_loss: 0.6332 - val_get_f1: 0.7263\n",
            "Epoch 41/100\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.4487 - get_f1: 0.8425\n",
            "Epoch 41: val_loss improved from 0.63322 to 0.61733, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 147ms/step - loss: 0.4448 - get_f1: 0.8392 - val_loss: 0.6173 - val_get_f1: 0.7420\n",
            "Epoch 42/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4305 - get_f1: 0.8423\n",
            "Epoch 42: val_loss did not improve from 0.61733\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.4318 - get_f1: 0.8389 - val_loss: 0.6260 - val_get_f1: 0.7388\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4119 - get_f1: 0.8525\n",
            "Epoch 43: val_loss did not improve from 0.61733\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.4119 - get_f1: 0.8525 - val_loss: 0.6185 - val_get_f1: 0.7427\n",
            "Epoch 44/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4225 - get_f1: 0.8306\n",
            "Epoch 44: val_loss improved from 0.61733 to 0.61168, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 3s 176ms/step - loss: 0.4197 - get_f1: 0.8347 - val_loss: 0.6117 - val_get_f1: 0.7461\n",
            "Epoch 45/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3897 - get_f1: 0.8599\n",
            "Epoch 45: val_loss improved from 0.61168 to 0.60795, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 147ms/step - loss: 0.3922 - get_f1: 0.8608 - val_loss: 0.6080 - val_get_f1: 0.7513\n",
            "Epoch 46/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3991 - get_f1: 0.8490\n",
            "Epoch 46: val_loss did not improve from 0.60795\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.3992 - get_f1: 0.8503 - val_loss: 0.6179 - val_get_f1: 0.7392\n",
            "Epoch 47/100\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3638 - get_f1: 0.8721\n",
            "Epoch 47: val_loss did not improve from 0.60795\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.3723 - get_f1: 0.8645 - val_loss: 0.6085 - val_get_f1: 0.7464\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3664 - get_f1: 0.8684\n",
            "Epoch 48: val_loss improved from 0.60795 to 0.60524, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 149ms/step - loss: 0.3664 - get_f1: 0.8684 - val_loss: 0.6052 - val_get_f1: 0.7472\n",
            "Epoch 49/100\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.3627 - get_f1: 0.8643\n",
            "Epoch 49: val_loss did not improve from 0.60524\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.3662 - get_f1: 0.8549 - val_loss: 0.6107 - val_get_f1: 0.7470\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3525 - get_f1: 0.8678\n",
            "Epoch 50: val_loss did not improve from 0.60524\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.3525 - get_f1: 0.8678 - val_loss: 0.6061 - val_get_f1: 0.7428\n",
            "Epoch 51/100\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3569 - get_f1: 0.8687\n",
            "Epoch 51: val_loss did not improve from 0.60524\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.3515 - get_f1: 0.8710 - val_loss: 0.6081 - val_get_f1: 0.7481\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3468 - get_f1: 0.8713\n",
            "Epoch 52: val_loss improved from 0.60524 to 0.59905, saving model to /content/drive/MyDrive/Colab Notebooks/MA/rnn/checkpoint_sp/\n",
            "16/16 [==============================] - 2s 149ms/step - loss: 0.3468 - get_f1: 0.8713 - val_loss: 0.5991 - val_get_f1: 0.7498\n",
            "Epoch 53/100\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3357 - get_f1: 0.8820\n",
            "Epoch 53: val_loss did not improve from 0.59905\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.3329 - get_f1: 0.8825 - val_loss: 0.6126 - val_get_f1: 0.7481\n",
            "Epoch 54/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3245 - get_f1: 0.8875\n",
            "Epoch 54: val_loss did not improve from 0.59905\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.3262 - get_f1: 0.8851 - val_loss: 0.6010 - val_get_f1: 0.7534\n",
            "Epoch 55/100\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3218 - get_f1: 0.8810\n",
            "Epoch 55: val_loss did not improve from 0.59905\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.3210 - get_f1: 0.8777 - val_loss: 0.5998 - val_get_f1: 0.7495\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3140 - get_f1: 0.8885\n",
            "Epoch 56: val_loss did not improve from 0.59905\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.3140 - get_f1: 0.8885 - val_loss: 0.6033 - val_get_f1: 0.7545\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3155 - get_f1: 0.8881Restoring model weights from the end of the best epoch: 52.\n",
            "\n",
            "Epoch 57: val_loss did not improve from 0.59905\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.3155 - get_f1: 0.8881 - val_loss: 0.6122 - val_get_f1: 0.7490\n",
            "Epoch 57: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 42: val_loss did not improve from 0.52517\n",
        "16/16 [==============================] - 1s 58ms/step - loss: 0.3515 - get_f1: 0.8703 - val_loss: 0.5355 - val_get_f1: 0.7946\n",
        "Epoch 42: early stopping"
      ],
      "metadata": {
        "id": "22x5g6Q6lBjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "plot_graphs(history, 'get_f1')\n",
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "e2a1d94d-14b5-4a01-edfe-480f227cc0e2",
        "id": "gcCFwjvflBjo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xV9fnA8c+TTRaQDYQQRpgJBIkMlaWyHOBmuBVRq6KtWrXa/ijVWm3raq2zVLQoIliLgqBscABh77AhQBaQkJCd+/39cS6QQggBcnNyc5/363VfN2fcc58TLnnud4sxBqWUUp7Ly+4AlFJK2UsTgVJKeThNBEop5eE0ESillIfTRKCUUh7Ox+4AzldERISJj4+3OwyllHIrq1atyjHGRFZ1zO0SQXx8PKmpqXaHoZRSbkVE9p7tmFYNKaWUh9NEoJRSHk4TgVJKeTi3ayNQSnmmsrIy0tPTKS4utjuUei0gIIDY2Fh8fX1r/BpNBEopt5Cenk5ISAjx8fGIiN3h1EvGGA4fPkx6ejqtW7eu8etcVjUkIpNEJEtENp7leEcR+UlESkTkKVfFoZRqGIqLiwkPD9ckUA0RITw8/LxLTa5sI/gIGFrN8SPAeOAvLoxBKdWAaBI4twv5HbksERhjlmD9sT/b8SxjzEqgzFUxVJaWmc8fvtlMSXlFXbydUkq5DbfoNSQi40QkVURSs7OzL+ga6UcL+eey3SzfddbcpJRS1QoODrY7BJdwi0RgjHnfGJNijEmJjKxyhPQ5XdY2ggBfLxZszarl6JRSyr25RSKoDQG+3lzRLoL5WzPRVdmUUhfDGMPTTz9NYmIiSUlJfP755wAcOnSIfv36kZycTGJiIkuXLqWiooJ77rnn5Lmvv/66zdGfyaO6j17ZMZp5W7LYkVVAQnSI3eEopS7Q77/exOaDx2r1mp2bh/J/13ep0blffvkla9euZd26deTk5HDppZfSr18/Pv30U4YMGcLzzz9PRUUFhYWFrF27lgMHDrBxo9WBMjc3t1bjrg0uSwQi8hkwAIgQkXTg/wBfAGPMuyISA6QCoYBDRJ4AOhtjavdft5IrO0YBMG9LliYCpdQFW7ZsGaNHj8bb25vo6Gj69+/PypUrufTSS7nvvvsoKyvjhhtuIDk5mTZt2rBr1y4ee+wxrr32WgYPHmx3+GdwWSIwxow+x/EMINZV71+VmMYBdGkeyoKtmTw8oG1dvrVSqhbV9Jt7XevXrx9Llixh1qxZ3HPPPfzqV7/irrvuYt26dcydO5d3332XadOmMWnSJLtD/R8e00ZwwlUdo1i19yhHj5faHYpSyk317duXzz//nIqKCrKzs1myZAk9e/Zk7969REdH88ADDzB27FhWr15NTk4ODoeDm2++mRdffJHVq1fbHf4ZPKqNAODKTtG8tWAHi9OyuaF7C7vDUUq5oRtvvJGffvqJbt26ISK8+uqrxMTEMHnyZP785z/j6+tLcHAwH3/8MQcOHODee+/F4XAA8PLLL9sc/ZnE3XrQpKSkmItZmMbhMPT84zz6tI3gb6O712JkSilX2rJlC506dbI7DLdQ1e9KRFYZY1KqOt/jqoa8vISBHaJYvC2LsgqH3eEopZTtPC4RAFzVKZpjxeWs2nvU7lCUUsp2HpkIrkiIwM9bRxkrpRR4aCII9vehV5sw5m3JtDsUpZSynUcmArC6ke7KPs7unON2h6KUUrbynETgqIB9P4Ozl9SVHaMBtHpIKeXxPCcRrJ0Ck4ZApjXfR1x4IAlRwSzYqtVDSinP5jmJoMO14OUDG744uevKTlEs33WE/OI6WRtHKeVBqlu7YM+ePSQmJtZhNNXznEQQFA5tr4INM8A5wu+qjtGUOwxLt+fYHJxSStnHs6aY6HobzLgf9v0E8ZdzSVwTmgT68v3mTK5JamZ3dEqpmvr2WcjYULvXjEmCYX866+Fnn32Wli1b8sgjjwAwYcIEfHx8WLhwIUePHqWsrIwXX3yRESNGnNfbFhcX8/DDD5OamoqPjw+vvfYaAwcOZNOmTdx7772UlpbicDiYMWMGzZs357bbbiM9PZ2Kigp++9vfMnLkyIu6bfC0RNBhGPgGWtVD8Zfj4+3FVR2j+X5zBqXlDvx8PKeApJQ6PyNHjuSJJ544mQimTZvG3LlzGT9+PKGhoeTk5NC7d2+GDx9+XgvIv/3224gIGzZsYOvWrQwePJi0tDTeffddHn/8cW6//XZKS0upqKhg9uzZNG/enFmzZgGQl5dXK/fmWYnALwg6Xgubv4Jhr4KPH0MTY5ixOp2fdx2mX/sLWwZTKVXHqvnm7irdu3cnKyuLgwcPkp2dTdOmTYmJieGXv/wlS5YswcvLiwMHDpCZmUlMTEyNr7ts2TIee+wxADp27EirVq1IS0ujT58+vPTSS6Snp3PTTTeRkJBAUlISTz75JM888wzXXXcdffv2rZV787yvwEm3QtFR2LkAgL4JEQT6eTNnU4bNgSml6rtbb72V6dOn8/nnnzNy5EimTJlCdnY2q1atYu3atURHR1NcXFwr7zVmzBhmzpxJo0aNuOaaa1iwYAHt27dn9erVJCUl8cILLzBx4sRaeS+XJQIRmSQiWSKy8SzHRUTeEpEdIrJeRC5xVSz/o+2V0CjsZO+hAF9vBnaI4rtNmVQ43GsmVqVU3Ro5ciRTp05l+vTp3HrrreTl5REVFYWvry8LFy5k7969533Nvn37MmXKFADS0tLYt28fHTp0YNeuXbRp04bx48czYsQI1q9fz8GDBwkMDOSOO+7g6aefrrW1DVxZIvgIGFrN8WFAgvMxDnjHhbGc4u0LXW6AbbOhpACAoYkx5BSUsHqfTkKnlDq7Ll26kJ+fT4sWLWjWrBm33347qampJCUl8fHHH9OxY8fzvuYvfvELHA4HSUlJjBw5ko8++gh/f3+mTZtGYmIiycnJbNy4kbvuuosNGzbQs2dPkpOT+f3vf88LL7xQK/fl0vUIRCQe+MYYc0aHWRF5D1hkjPnMub0NGGCMOVTdNS92PQIA9v4I/xoGN30AXW+joKScSyZ+z519WvHb6zpf3LWVUi6h6xHUnDutR9AC2F9pO9257wwiMk5EUkUkNTs7++LfuWVvCI09WT0U7O9D34QI5mzMwN0W6lFKqYvlFo3Fxpj3jTEpxpiUyMha6Nnj5QVJN8OO+XDcGkw2JDGGA7lFbDp47OKvr5RSwIYNG0hOTv6fR69evewO6wx2dh89ALSstB3r3Fc3km6DH960upJeOparO0Xj7SXM2ZhBYovGdRaGUqrmjDHn1UffbklJSaxdu7ZO3/NCajXsLBHMBO5y9h7qDeSdq32gVkV3gchOsGE6AGFBfvRqHabdSJWqpwICAjh8+LBW31bDGMPhw4cJCAg4r9e5rEQgIp8BA4AIEUkH/g/wBTDGvAvMBq4BdgCFwL2uiuUsAULSLbDgD5C7D5rEMTQxht/9dxM7svJpFxVSp+EopaoXGxtLeno6tdJO2IAFBAQQGxt7Xq9xWSIwxow+x3EDPOKq96+RE4lg/TTo9xSDO1uJYM7GDB69UhOBUvWJr68vrVu3tjuMBsktGotdpmk8tO4HK96HsiJiGgdwSVwTrR5SSnkUz04EAP2fgYJMWDUZsAaXbTxwjP1HCm0OTCml6oYmgvgrIL4vLHsdyooZ0sWaLGqulgqUUh5CEwE4SwUZsHoyrcKD6NQsVBOBUspjaCIAaN0XWl0BS1+DsmKu69qMlXuO8s6indpVTSnV4GkiOGHAqVLB2L6tGZHcnFfmbOX5rzZSXuGwOzqllHIZTQQnxPeFVpfDstfxN2W8flsyjwxsy6fL9zH241QKSsrtjlAppVxCE8EJIlZbQf4hWP0xXl7C00M68scbk1i6PYeR7/1E5rHaWXBCKaXqE00ElbXuB3GXnexBBDCmVxwf3p3C7pzj3Pj2D+zKLrA5SKWUql2aCCoTsdoK8g/Cmk9O7h7YIYppD/ahuNzB+KlrKNM2A6VUA6KJ4HSt+0NcH1j6VyjIOrk7sUVj/nhjIhsPHOO9xTttDFAppWqXJoLTicCQl6A4DyZfDwWnJrgamtiM67o2483529mWkW9jkEopVXs0EVSlRQ8Y8zkc3WslA+fiNQC/H96F0ABfnvpinXYrVUo1CJoIzqZ1P2cy2A2Th8PxwwCEB/vzhxsS2XAgj/eW7LI5SKWUuniaCKrTpj+MngpHdsLHw6HwCADXJDXj2q7NeGNemlYRKaXcniaCc2k7EEZ/BjnbrWSQnwnAxOFdCAnw5enpWkWklHJvLk0EIjJURLaJyA4RebaK461EZL6IrBeRRSJyfsvq1JW2V8LoTyE7Dd7sBnOfJ9zk8ocRiaxPz+Mv36VxrLjM7iiVUuqCiKsmVRMRbyANGASkAyuB0caYzZXO+QL4xhgzWUSuBO41xtxZ3XVTUlJMamqqS2I+p5wdsPQvsP5z8PaDHvfyfM5VTNlUipdAUmwTLmsbzmVtw0lpFUYjP2974lRKqdOIyCpjTEqVx1yYCPoAE4wxQ5zbzwEYY16udM4mYKgxZr+ICNYC9qHVXdfWRHDC4Z3WOIN1UzFePmTHX8caOvFtbiyzDoVQ5hD8fbx4e8wlXN052t5YlVIK+xLBLVh/5Mc6t+8EehljHq10zqfAcmPMmyJyEzADiDDGHD7tWuOAcQBxcXE99u7d65KYz9uR3VZC2DLTGncAGP8QjjbtytzcWJYVt+H5h+6ieUxzmwNVSnm6+pwImgN/B1oDS4CbgURjTO7ZrlsvSgSnczisnkXpK08+TOZmxFQAYMLbIy17Qsue0Lw7RCSAbyObg1ZKeZLqEoGPC9/3ANCy0nasc99JxpiDwE0AIhIM3FxdEqi3vLysP+4RCZA8BgApPc6yxd/x46JvuclxgHbbZsPafztfINC0FUR0gMj2ENUZ2gyE0Gb23YNSymO5MhGsBBJEpDVWAhgFjKl8gohEAEeMMQ7gOWCSC+OpW35BXDHoRr7Oa8ugVfv55N6eXBGWB5kbIHub9chJg12LoKLEek2zZOgwDNoPhWbdrOkulFLKxVxWNQQgItcAbwDewCRjzEsiMhFINcbMdFYfvQwYrKqhR4wxJdVds15WDVWjsLSc4X//gbyiMmaP70tkiP//nuCogKwtsH0ubJtjVS1hIKQ5tLsS4vtZS2mGajuDUurC2dJG4CrulggAtmXkM/zvy+jZOozJ9/bEy+t/v+lXOAzeJ/YVZMP27yBtDuxeAsXOmrKwtlZCiO0JQRHgHwoBjZ2PUPAL1hKEUuqsNBHUA58u38dv/rOBvgkR+HgJR46XklNQypHjpZRWOLisbTjXd23OkC4xNA70tV7kqIDMjbBnGexeCnt/hJK8qt9AvMAvBPxDwD/Yem4cC52uh4Qh1j6llMfSRFAPGGP4/debmbspg/BgP8KC/IkI8iMsyA+A7zZnsu9IIb7eQr+ESK7v1pyrO0cT7F+pGcdRAUf3QFGulRCKKz1KCqAk3/k4Zj1nbYGCDPBpBO0HQ5cbIWEw+AXZ80tQStlGE4EbMMaw4UAeX687yKz1hziYV4yftxeXtwtnSJcYru4cTUSw/7kvVJnDAft+gk3/gc3/heNZ4BsIHa+D5NHWIjxeOvpZKU+gicDNOByG1fuOMmdjBnM3Z7D/SBFeAinxYVzfrTljesadalOo8UUrrKqljdOtxFCcB6EtoOtt0G2M1Y1VKdVgaSJwY8YYthzKZ86mDOZuzGBbZj6DOkfz5qhkAv0usPdvWTFsmw3rPoMd88FUQHg7q0E6rA2Etbaem8ZDYLjVIK0lB6XcmiaCBuSjH3Yz8ZvNdGoWyj/vvpSYxgEXd8H8TNjwBexfbk2ZcWQXlB0/8zz/UAhoAo0aQ0w36DYKWl1uDaZTStV7mggamIVbs3jsszUE+Xvzz7svJbFF49q7uDFwPNtKCEf3QtFRqwtrUa71XHjEqmIqzYfGLaHrSOg2GiLa1V4MSqlap4mgAdqacYz7P0rlyPFS3hiVzJAuMXX35qWFp6qWdi4A44AWKdB5hNVdNax13cWilKoRTQQNVFZ+MQ98vIr16blc1jac9tEhlR7BhAT4uj6I/AxYP82qXspYb+2LTrISQqfrIaqTDnRTqh7QRNCAFZdV8NfvtrF89xG2ZxZQVFZx8lj76GCeuLo9wxJjkLr4Y3x0D2z5BrZ8bbU5YKyBbidHQDexnpvEWdVJrS7TJKFUHdFE4CEcDkP60SLSMvPZlpnPV2sOsD2rgOSWTXhuWEd6tQmvu2DyM6xpMnL3Vxr4lms9Z22xBr2FJ0CPe6wZWwPD6i42pTyQJgIPVV7h4MvVB3jt+zQyjhVzVcconhrSgQBfb7ZlHGNbRgHbMo+xNSOfNhFBvDYymdC6qE4qLbTGMqz6CNJXWMt+dh4B3e+E+L7aE0kpF9BE4OGKSiv414+7eWfRTvKLy0/uF4FWYYG0jQxmcVo2XZqH8vF9vU7NdVQXMjfBqsmwfqpVWmgSB8l3WCOfm8TVXRxKNXCaCBQAR4+XMmN1OqEBvnSICSEhOvjkoLR5mzP5xZTVtIsK5pP7exJ+vtNZXKyyItg6C9Z8Yq3RgECb/pB0qzUVRpOW57qCUqoamghUjSxOy2bcx6m0Cg/k32N7ERVykYPVLtTRvVbX1LVTIHefta9JK6vaKP4K66GJQanzoolA1diPO3MYOzmVmNAApjzQi2aNbVxb2eGArM3WNNx7lsLeH6wBbgCt+0HPB60V3XT6C6XOybZEICJDgTexVij70Bjzp9OOxwGTgSbOc541xsyu7pqaCFwvdc8R7vnXSkICfGgXFUxBSTnHS8o5XlJBQUk5CVHB3H9FawZ3iTn/ye8uxonEkDYHUv8Fx9KtdoRLx1oNzdrzSKmzsiURiIg3kAYMAtKx1jAebYzZXOmc94E1xph3RKQzMNsYE1/ddTUR1I21+3P5wzebcRhDsL8PQX4+BPn70MjPiyVpOew7UkhcWCD3XR7PrSktCfJ35fLXVagot0Y3r3jfKi34NILud0D/X0NwVN3GopQbsCsR9AEmGGOGOLefAzDGvFzpnPeAXcaYV5zn/9UYc1l119VEYL8Kh+G7TRl8sHQXq/fl0riRL3f0juMXA9rVfUIAyNgIy9+12hW8/eGyx+CyR61V2pRSgH2J4BZgqDFmrHP7TqCXMebRSuc0A74DmgJBwNXGmFXVXVcTQf2yau8RPliym7mbM4ht2ohXb+5Gn7Z1OHCtspwdsGCitQhPUCT0f8YasOZdh91hlaqnqksEdo/cGQ18ZIyJBa4BPhGRM2ISkXEikioiqdnZ2XUepDq7Hq3CePfOHnw+rg9eIoz+4Gd+99+NHC8pP/eLa1tEO7jtYxg7HyLaw+yn4G89YN4E2LfcWpxHKXUGu6uGNmGVGvY7t3cBvY0xWWe7rpYI6q+i0gpenbuVj37cY3/pwBjY/h38+Ddr2mxTAY3CrDWbOwzVtZuVx7GrasgHq7H4KuAAVmPxGGPMpkrnfAt8boz5SEQ6AfOBFqaaoDQR1H8rdh/h6enr2Hu4kI4xIUSG+BMVEkBUqD9RIf50jW1Mj1Z12MOnKBd2zoe0uVZyKDpqVR31+7VVdeTjV3exKGUTO7uPXgO8gdU1dJIx5iURmQikGmNmOnsKfQAEAwb4tTHmu+quqYnAPRSVVvDekp1sPHCM7PxisvNLyC4ooazCIAKT7r6UgR1t6N3jqLDGIyx+1ept1KQVDHzeGsGscxypBkwHlKl6weEwHD5eyt2TVpB+tJBvHutLXHigPcEYY5US5v3eWkchqgtcPQHaD7YnHqVcrD43FisP4uUlRIb48+4dPRARHvz3KopKbWrAFYF2V8O4xXDLJCgvgk9vhRkPnBq9rJSH0ESg6lxceCBvjEpma8Yxnv9qA7aWSr28IPFmeGQFDHgONn0J/+gD2+fZF5NSdUwTgbLFwA5RPH5VAl+uPsC/l++zOxxrrMGAZ2HsPGsVtSk3w9dPQEm+3ZEp5XKaCJRtxl+ZwMAOkUz8ehOr951ZHVNUWkF5haNug2re3aouumy8tXDOO5fDhulQXlq3cShVh7SxWNkqr7CM6/++jNJyBzf3aMH+I0XsP1rI/iNF5BSUENu0EVPH9Sa2qQ2Nynt/gpmPweHtEBQFPe6GHvdC4xZ1H4tSF0l7Dal6bdPBPMZ8sJzjJeU0b9KI2KaNaNk0kJjGAfzrh92EB/vzxUN9iKjrxXLAmvF053xY+aE1DkG8oOM10OcxiOtV9/EodYE0Eah6r6S8Am8RfLz/t7Zy1d4j3P7hctpEBDP1wd51s6by2RzdY01/vfpjKM6Foa9Ar3H2xaPUedDuo6re8/fxPiMJgDWX0Xt3prA9K5+xH6Xa190UoGk8DPo9/HIjtB8G3z4Nc36jcxgpt6eJQNV7/dtH8vrIZFbuPcIvpqyirK4bkE/nFwQjP4FeD8PPb8O0u6C00N6YlLoImgiUW7iua3NeuiGJhduyeXLaOhwOm6s0vbxh2J9g6J9g6yyYfB0UnHWuRKXqNRtWEVHqwozpFUdeURmvzNkKwF9u7Yafj83fZXo/bC2XOf1++PAqa9nM5t2hWTIEhNobm1I1pIlAuZWHB7RFBP707VaOFpby7h097FkVrbKO18K9s+A/D8H3vzu1PzwBWlwCSbdBwtX2xafUOdTo65SIPC4ioWL5p4isFhGdnUvZ4qH+bXn1lq78sCOH2z9cztHj9WCwV4se8OhKeHoX3D7DmtE0IgF2LrDmMNo80+4IlTqrmpar7zPGHAMGYy0reSfwJ5dFpdQ53JbSkvfuTGHzoWPc+t5PHMwtsjskS1C49e2//69h9Gcwfq2VJGbcDzsX2h2dUlWqaSIQ5/M1wCfOxWWkmvOVcrlBnaP55L6eZOYVc/M7PzJ/Sybr03PZnXOc7PwSissq7J3QDsA/GG7/wqommno77F9hbzxKVaFGA8pE5F9AC6A10A1roZlFxpgerg3vTDqgTJ1u88Fj3DVpBTkFJWccaxLoy+sjkxnYwYZFcCrLz4RJQ6DoCNwzG2IS7Y1HeZyLHlnsXFA+GdhljMkVkTAg1hizvnZDPTdNBKoqeYVlbMk4RkFxOQUl5eQXl5FfUs7X6w6xPTOfv97WjRHJNs8RdHQvTBoKjnK4bw6Et7U3HuVRaiMRXA6sNcYcF5E7gEuAN40xe8/xuqHAm1gliA+NMX867fjrwEDnZiAQZYxpUt01NRGo83GsuIwHJqeyfPcRJlzfmXsub21vQNnbrGTgFwRDX4YO11hjEpRysdqYYuIdoFBEugFPAjuBj8/xpt7A28AwoDMw2rlG8UnGmF8aY5KNMcnA34AvaxiPUjUSGuDL5Pt6MrhzNBO+3sxr322zt90gsgPc+SUg8Pkd8LdL4Od3dd0DZauaJoJyY/3vGQH83RjzNhByjtf0BHYYY3YZY0qBqc7Xn81o4LMaxqNUjQX4evOP2y9hZEpL3lqwgxe+2kiFnSOTm3eH8Wvg1skQHA1znoHXusDc5+HYQfviUh6rpokgX0Sew+o2OsvZZnCuaSBbAPsrbac7951BRFphNUQvOMvxcSKSKiKp2dnZNQxZqVN8vL34081JPNS/LVOW7+P2D39my6Fj9gXk7QNdboD7v4OxC6wupz+/A3/rAUv/CuVnNnwr5So1TQQjgRKs8QQZQCzw51qMYxQw3RhT5TSOxpj3jTEpxpiUyMjIWnxb5UlEhGeHdeSVm5PYlpHPtW8t5bkvN1TZ26hOxfaAWybB+NXQ9kqYPxH+0Ru2f29vXMpj1CgROP/4TwEai8h1QLExpto2AuAA0LLSdqxzX1VGodVCqo6MvDSORU8N5J7LWvNF6n4G/nkRHyzZRWm5zbOaNo2HUVPgjhnWAjhTboHPRsORXfbGpRq8mvYaug2rBLAIayBZX+BpY8z0al7jA6QBV2ElgJXAGOdgtMrndQTmAK1NDYLRXkOqNu3IKuClWZtZuC2btpFBfDauN1EhAXaHZa2R/PM/YPGrUHYcQppZU1ZEdLAanCPaQ1xv8LFh1Tbllmqj++g6YJAxJsu5HQnMM8Z0O8frrgHewOo+OskY85KITARSjTEznedMAAKMMc/W5GY0EShXWLA1k0emrKF9dDBTx/WhkV896dJ57CCsn2Z1O83ZBtlpUOrsYdSsG4z6FBrH2hujcgu1kQg2GGOSKm17Aesq76srmgiUq3y3KYMH/72KwZ2j+cftPfD2qoezqBgD+Ydg9xKY9RT4NrIWyYnrbXdkqp6rjXEEc0RkrojcIyL3ALOA2bUVoFL1weAuMfz22s7M3ZTJy7O32B1O1UQgtDl0GwVj51lzGX10HayabHdkyo3VtLH4aeB9oKvz8b4x5hlXBqaUHe69PJ67+7Tiw2W7+eSnPXaHU72ojvDAAmjdF74eb5UQKsrsjkq5oRqv6GGMmQHMcGEsStlORPjd9V1IP1rE/83cRGzTQAZ2tHnCuuo0agpjvoD5E+DHv8GBVOg6CtoPgTCbp9NQbqPaNgIRyQeqOkEAY4yp87X4tI1A1YXjJeXc9t5P7M45zqu3dGVIlxh8vev5Et/rv4Alf7YalcHqYdRhKLQfCi17g1c9j1+51EU3FtcnmghUXck8Vszo939mV85xIoL9uS0lllGXxhEXHmh3aNU7sgvS5kLaHNjzAzjKoGlruPR+SL4dAsPsjlDZQBOBUheowmFYnJbFp8v3s2BrJg4DfRMiuP+K1gywe42Dmig+ZiWF1H/Cvp/ApxEk3QI9H7C6nyqPoYlAqVpwKK+IL1LTmbpiHwfzipk4ogt39Ym3O6yay9gAKz6wxiWUF1mJoP0waD8YmnXXqqMGThOBUrWopLyCRz9dw/ebM/n10A78YkA7u0M6P0VHYe2nsOkrSF8JGAiKgoTBkDAIWl0GwW5Q2lHnRROBUrWsrMLBU1+s479rD/LIwLY8NbgDIvVwANq5HM+BHfOs6qOd86E4z9rfNB5ie0LLnhCbAtGJ4H2uCYdVfVZdIqhx91Gl1Cm+3l68dlsyjXy9eXvhTo6XVPC76zrjVR9HI1cnKMIanNZtFFSUw4FVsH85pK+wRi9vmGad5+VjNThHtGXheqoAABgkSURBVHfOedQemnWFmDqfXEC5gCYCpS6Qt5fw8k1JBPn78M9luyksLWfC8C4E+rnpfytvH4jrZT3Ams4iL91KCpmbISfNemz/zuqJBJByHwz5ozXVhXJbbvqJVap+EBFeuLYTQf4+vDV/O1+tOUivNmH0bx/JgA6RtI0Mds8qI7Cms2jS0nok3nxqf0U55O6F1Enw099h/wprPYXIDlVf50QSaTsQAhrXTezqvGgbgVK1ZPmuw3y/OZNFadnsyCoAoEWTRjzUvw13ulPvovOx/Xv4z4NQVgTX/NkapyBibW/+r5Us9i+3zvX2t3ooJd5ijXzWUkSd0sZipepY+tFCFqdlM2NVOmv35zL78b50jKnzgfh149gh+PIB2LMUkm6FoEirV1JxLoS3gx73WOs0b/kaNn4Jx7PALwQ6Xmst19lmwIUlhYNrAWNdW52TJgKlbHL0eCkD/rKILs1DmTK2l/tWE52LowKWvgaL/gjiDZ2uh5R7Ib6vVUKofN7uJbBxOmz+GkrywDcI2l1lvSZhMDRqUs37OKw2ih/ehH0/WvtapEDvh6HziIbds8nhsNpmLnAxIk0EStno45/28Lv/buLdO3owNDHG7nBcK3c/+ARAcA3WFi8vhT1LYOss61GQCV6+EHspRHeB6M4Q1RkiO4JvIGz4An58C7K3QuOW0OcRK+ksfxeO7ISQ5tBzLPS4t/ppNIyBo3usEszRvVbvpxYp0LjFxd///pUw+0koyYf+z1olpIsdqHc8B9Z8Aqn/shrnr3jigi5jWyIQkaHAm1grlH1ojPlTFefcBkzAmtxunTFmTHXX1ESg3E15hYNr31pGYVk53/+yPwG+9WT1s/rE4bBmTt3yNez7GbK2nFqJDaxEUFZojWe4/HHocuOpb/8OB+z4Hn5+B3YtdHZ1jYfwBAhva3V3DWtr9YDas9QqkeTtd15YODmvZkgzaNHDegAcz7aSU0GW9fALhEvugqTbrHUgKivOg/kTYeU/rfUiAsMhYz1EJ8HVE6wSz/mUBo2xfg+p/7TaWipKodUVVhJIGHTev16wKRGIiDfWmsWDgHSsNYtHG2M2VzonAZgGXGmMOSoiUSeWwzwbTQTKHf24I4cxHy7nqcHtefTKBLvDqf+Msf5YZ22BrM2Quw86XHvuP6iZm2HjDKuX0uEdcHgnVJScOt4oDOKvgNb9rGqrpvGQuRHSU61ElJ4KR3db5/oFWyOsg6Otdo+ju61pOvxDIXkMXDrWagPZMhO+fQbyM6DXQ3Dl81Z116YvYcEfrNJHfF+48gWrdOMfAl6nfRk4nmMtR5q91Xres9S6b/9Q6DbaKglEdbyoX6ldiaAPMMEYM8S5/RyAMeblSue8CqQZYz6s6XU1ESh39dAnq1icls3CpwYQ0zjA7nA8g8NhJZQjO60/5lFdzl1VU5RrlTb8gv53vzFWV9mVH1jTczjKrFLH4e3WwLrr3zxVmjihvBRWfQSLX4HCnFP7/YKthOAfAoWHrUflYzFJVgJIuuXMOC6QXYngFmCoMWasc/tOoJcx5tFK53yFVWq4HKv6aIIxZk4V1xoHjAOIi4vrsXfvXpfErJQr7T9SyFWvLeaaxBjeGKU9XdxaQRasngzbvrUaqXs/Yg3IO5uSfNjyDRQdsX4uPgYlzkdAY4jsZI3DiOwAoS3OrxqphurzFBM+QAIwAIgFlohIkjEmt/JJxpj3sZbKJCUlxb1at5VyahkWyLi+bfj7wh3c2acVPVrpugBuKzgK+j1tPWrCPwSSR7s2povgynlnDwAtK23HOvdVlg7MNMaUGWN2Y5UOtAJVNVgPD2hLdKg/E2ZupqCk3O5wlAJcmwhWAgki0lpE/IBRwMzTzvkKqzSAiEQA7YFdLoxJKVsF+fvw/LWd2XAgj0tfnMcTU9ewOC2b8gqH3aEpD+ayqiFjTLmIPArMxar/n2SM2SQiE4FUY8xM57HBIrIZqACeNsYcPvtVlXJ/w7s1p0WTRsxYnc6s9Yf4au1BIkP8GdGtOfde0ZoWTXTqBVW3dECZUjYqKa9g4dYsvlx9gIXbsohtGsis8Ve47wymqt6qrrFY16ZTykb+Pt4MTWzG+3el8PF9vdhz+Dgvztpid1jKw2giUKqe6NM2nHF92/Dp8n3M25xpdzjKg2giUKoe+dXg9nRqFsozM9aTnV9y7hcoVQs0EShVj/j7ePPmqGTyS8p5ZsZ63K0NT7knTQRK1TPto0N4blhHFmzN4tMV++wOR3kATQRK1UN394mnb0IEf/hmMzuzC+wORzVwmgiUqoe8vIS/3NqNAF9vnpi6lsMF2l6gXEcTgVL1VHRoAK/e3JWtGccY9PoSZq47qG0GyiU0EShVjw3uEsOs8X1pGRbI+M/WMO6TVWQeK7Y7LNXAaCJQqp5rHx3Clw9fxvPXdGJJWjZXv7aYaan7tXSgao0mAqXcgLeX8EC/Nsx5oh+dmoXy6+nrufXdn1i554jdoakGQBOBUm6kdUQQUx/ozcs3JbHvSCG3vvsT93+0kq0Zx+wOTbkxnXROKTdVVFrBpB928+7inRSUlHNj9xb88ur2tAwLtDs0VQ/ZslSlq2giUOp/5RaW8o9FO/noxz1UOAxDukRzz2WtuTS+KeKCJQ+Ve9JEoJQHOJRXxEc/7GHqyv3kFZXRqVko91zWihHJLQjw9bY7PGUzTQRKeZCi0gq+WnuAyT/uYWtGPk0CfZlwfRdu6N7C7tCUjWxbj0BEhorINhHZISLPVnH8HhHJFpG1zsdYV8ajlCdo5OfN6J5xfPt4Xz4f15uEqGCe+Hwtr8zZisPhXl/8VN1wWSIQEW/gbWAY0BkYLSKdqzj1c2NMsvPxoaviUcrTiAi92oQzZWxvxvSK451FOxn3SSoFJeV2h6bqGVeWCHoCO4wxu4wxpcBUYIQL308pVQU/Hy9euiGRiSO6sHBbNjf94wf2HS60OyxVj7gyEbQA9lfaTnfuO93NIrJeRKaLSMuqLiQi40QkVURSs7OzXRGrUg2aiHBXn3g+vq8nmcdKGPH2Mn7aedjusFQ9YfeAsq+BeGNMV+B7YHJVJxlj3jfGpBhjUiIjI+s0QKUaksvbRfDfRy4nPNifuyYtZ8aqdLtDUvWAKxPBAaDyN/xY576TjDGHjTEn5tf9EOjhwniUUkB8RBAzHr6MS+PDePKLdbw5b7vOW+ThXJkIVgIJItJaRPyAUcDMyieISLNKm8OBLS6MRynl1LiRLx/d25ObLmnB6/PSeHr6ekrLHXaHpWzi46oLG2PKReRRYC7gDUwyxmwSkYlAqjFmJjBeRIYD5cAR4B5XxaOU+l9+Pl789dZuxIUF8sa87WTkFfOPOy4hNMDX7tBUHdMBZUoppq9K59kZ62kTGcSbo7rTqVmo3SGpWmbbgDKllHu4pUcsk+/rSXZ+Cdf9bRkTZm4ir6jM7rBUHdFEoJQCrB5FC58awOieLZn80x6u+usivkjdr6ORPYAmAqXUSU0C/XjxhiS+fvQKWoYF8vT09dzy7o8s2JpJbmGp3eEpF9E2AqVUlRwOw/TV6bzy7VYOH7eSQOuIILq3bEJyXBN6tg6jY4y2JbgLnX1UKXXBikorWLs/lzX7j7J2Xy5r9ueSnW8N/7khuTkvXNeZiGB/m6NU51JdInBZ91GlVMPQyM+bPm3D6dM2HABjDAdyi5i2cj/vLN7JorRsfnNNJ27tEasL4bgpbSNQSp0XESG2aSC/GtyB2eP7khAVzK+nr2f0Bz+zM7vA7vDUBdBEoJS6YAnRIXw+rg8v35TEpoPHGPbGUl7/Po3isgq7Q1PnQROBUuqieHkJo3vGMf/J/gxJjOHN+dsZ/PoSFmzNtDs0VUOaCJRStSIqJIC/je7Op2N74efjxX0fpTJ2cir7j+jaB/WdJgKlVK26rF0Es8f35blhHflxZw5Xv7aYtxfu0IFp9ZgmAqVUrfPz8eLB/m2Z/2R/ruoUxZ/nbuPp6espr9AZTusjTQRKKZdp1rgRb4+5hF8Nas+M1ek8PGW1NiTXQ5oIlFIuJSKMvyqBiSO6MG9LJndPWkF+8ZkT2lU4DEvSsvlhR44NUXo2HVCmlKoTd/WJp3EjX56cto7RH/zM5Ht7Ehbkx+ZDx/jP6gP8d91BsvNL8PYSPrm/J5e1jbA7ZI+hU0woperUwq1ZPDxlFdGhAQT4eLMtMx9fb2FAhyiGd2vOG/PSyC0s4+vHrqB5k0Z2h9tg2LYegYgMFZFtIrJDRJ6t5rybRcSISJVBKqUajoEdo/jk/l4UFJcTHODDH25IZMVvruaDu1K4vltz3rszheKyCh6espqScm1PqAsuKxGIiDeQBgwC0rHWMB5tjNl82nkhwCzAD3jUGFPt130tESjVMBhjzjo30bcbDvHwlNWM6RXHH29MquPIGia7SgQ9gR3GmF3GmFJgKjCiivP+ALwCFLswFqVUPVPdBHXDkprxUP+2fLp8H9NS959x3BjD1oxjHMwtcmWIHsOVjcUtgMr/gulAr8oniMglQEtjzCwRefpsFxKRccA4gLi4OBeEqpSqb54a3J4NB3J54auNdIoJJbFFKBsO5PHtxgy+3XCIPYcLiQzx55vHriA6NMDucN2abd1HRcQLeA148lznGmPeN8akGGNSIiMjXR+cUsp2Pt5evDWqOxFBftw/eSVXvLKQ4X//gfeX7KJlWCDPDevI8ZJyHvr3Km1LuEiuLBEcAFpW2o517jshBEgEFjmLiDHATBEZfq52AqWUZwgP9uedO3rw4Cer6BATwhNXJzCoczRNAv0AiG0ayCOfrmbCzE28fFNXm6N1X65MBCuBBBFpjZUARgFjThw0xuQBJzsKi8gi4ClNAkqpyrq1bMLPv7mqymPXdm3GpoNt+ceinSS2aMztvVrVcXQNg8uqhowx5cCjwFxgCzDNGLNJRCaKyHBXva9SyrM8ObgDAzpEMmHmJlL3HLE7HLekA8qUUm4vr7CM4W8vo7C0QhuPz8K2AWVKKVUXGgf68v6dKRwvKWfcx6n8uDNHG5DPg841pJRqEDrEhPDabcmMn7qGMR8sp5GvN73ahNE3IZJ+CRG0DAvESwQvwXr2Ovs4Bk+jVUNKqQaloKScn3ceZun2bJZuz2FXzvGznhsR7M9vrunIjd1bVDvArSGormpIE4FSqkFLP1rIjzsOk3O8BGOs6a4dxuAwsHR7Nmv25dI3IYKXbkgiLjzQ7nBdRhOBUkpVocJh+PfPe3l1zlYqjOFXg9pz3+Wt8fFueM2nmgiUUqoaB3OL+N1/NzJvSxaJLUIZ3DmGQD9vgvx9rGc/HyJD/OnYLAR/H2+7w70gmgiUUuocjDHM3pDBi7M2cyiv6jkw/by96NQ8lO4tm5DcsgmXxDV1m+okTQRKKXUeyiscFJZVUFhSQUFJOYWl5Rw4WsTa/bms2Z/LhvQ8ipxrL1+b1IwJw7sQGeJvc9TVqy4RaPdRpZQ6jY+3F6HeXoQG+J7c1zW2CcOSmgFWokjLLGDupgzeWbSTZTty+O11nbn5EvfsfdTwWkSUUsrFfLy96Nw8lF8Oas/sx/uSEBXMU1+s465JK9h/pNDu8M6bJgKllLoI7aKCmfZgHyaO6MLqvUcZ8sYSXv8+jbTMfNyl6l3bCJRSqpYcyC3id19tZP7WLABahQcyqFM0gzpHkxIfhreNo5m1sVgppepQRl4x87Zk8v3mTH7aeZjSCgdNAn3p1TqMS+PD6NU6nE7NQup0vIImAqWUsklBSTmLt2WzcFsWK3YfYZ+zDSHIz5tLWjWlX0IkQxNjaBnm2m6omgiUUqqeyMgrZsWeI6zYfZgVu4+QllkAQFKLxgxNjGFYYgxtIoMBa2zD8dIK8orKyCsso0mgL82bNLqg99VEoJRS9dTew8eZszGDbzdmsHZ/LgDNGwdQUu4gr6iMcsepv9EP9W/Ls8M6XtD72DaOQESGAm8C3sCHxpg/nXb8IeARoAIoAMYZYza7MiallKpPWoUH8WD/tjzYvy0Hc4uYuymDdftzCfL3oXEj35OPJoG+tIsKcUkMLisRiIg3kAYMAtKx1jAeXfkPvYiEGmOOOX8eDvzCGDO0uutqiUAppc6fXSuU9QR2GGN2GWNKganAiMonnEgCTkGAe9VTKaVUA+DKqqEWwP5K2+lAr9NPEpFHgF8BfsCVVV1IRMYB4wDi4uJqPVCllPJkto8sNsa8bYxpCzwDvHCWc943xqQYY1IiIyPrNkCllGrgXJkIDgAtK23HOvedzVTgBhfGo5RSqgquTAQrgQQRaS0ifsAoYGblE0QkodLmtcB2F8ajlFKqCi5rIzDGlIvIo8BcrO6jk4wxm0RkIpBqjJkJPCoiVwNlwFHgblfFo5RSqmouHUdgjJkNzD5t3+8q/fy4K99fKaXUudneWKyUUspebjfFhIhkA3sv8OURQE4thlOfNNR70/tyPw313tz9vloZY6rsdul2ieBiiEjq2UbWubuGem96X+6nod5bQ70v0KohpZTyeJoIlFLKw3laInjf7gBcqKHem96X+2mo99ZQ78uz2giUUkqdydNKBEoppU6jiUAppTycxyQCERkqIttEZIeIPGt3PBdDRCaJSJaIbKy0L0xEvheR7c7npnbGeCFEpKWILBSRzSKySUQed+5363sTkQARWSEi65z39Xvn/tYistz5mfzcOSeX2xERbxFZIyLfOLcbyn3tEZENIrJWRFKd+9z6s3g2HpEInKulvQ0MAzoDo0Wks71RXZSPgNNXcnsWmG+MSQDmO7fdTTnwpDGmM9AbeMT57+Tu91YCXGmM6QYkA0NFpDfwCvC6MaYd1lxb99sY48V4HNhSabuh3BfAQGNMcqXxA+7+WaySRyQCarBamjsxxiwBjpy2ewQw2fnzZNxwSm9jzCFjzGrnz/lYf1xa4Ob3ZiwFzk1f58NgLcQ03bnf7e4LQERisWYO/tC5LTSA+6qGW38Wz8ZTEkFVq6W1sCkWV4k2xhxy/pwBRNsZzMUSkXigO7CcBnBvzuqTtUAW8D2wE8g1xpQ7T3HXz+QbwK8Bh3M7nIZxX2Al6+9EZJVzlURoAJ/Fqrh09lFlD2OMERG37RcsIsHADOAJY8wx60umxV3vzRhTASSLSBPgP0BHm0O6aCJyHZBljFklIgPsjscFrjDGHBCRKOB7Edla+aC7fhar4iklgvNdLc0dZYpIMwDnc5bN8VwQEfHFSgJTjDFfOnc3iHsDMMbkAguBPkATETnxZcwdP5OXA8NFZA9WdeuVwJu4/30BYIw54HzOwkrePWlAn8XKPCURnHO1tAZgJqcW9rkb+K+NsVwQZ/3yP4EtxpjXKh1y63sTkUhnSQARaQQMwmr/WAjc4jzN7e7LGPOcMSbWGBOP9X9qgTHmdtz8vgBEJEhEQk78DAwGNuLmn8Wz8ZiRxSJyDVZ95onV0l6yOaQLJiKfAQOwpsXNBP4P+AqYBsRhTdN9mzHm9Ablek1ErgCWAhs4Vef8G6x2Are9NxHpitWw6I315WuaMWaiiLTB+iYdBqwB7jDGlNgX6YVzVg09ZYy5riHcl/Me/uPc9AE+Nca8JCLhuPFn8Ww8JhEopZSqmqdUDSmllDoLTQRKKeXhNBEopZSH00SglFIeThOBUkp5OE0ESjmJSIVzpskTj1qbUExE4ivPFqtUfaJTTCh1SpExJtnuIJSqa1oiUOocnPPSv+qcm36FiLRz7o8XkQUisl5E5otInHN/tIj8x7n+wDoRucx5KW8R+cC5JsF3zlHGiMh45xoM60Vkqk23qTyYJgKlTml0WtXQyErH8owxScDfsUaoA/wNmGyM6QpMAd5y7n8LWOxcf+ASYJNzfwLwtjGmC5AL3Ozc/yzQ3Xmdh1x1c0qdjY4sVspJRAqMMcFV7N+DtbDMLuekeBnGmHARyQGaGWPKnPsPGWMiRCQbiK08rYJzWu3vnQuaICLPAL7GmBdFZA5QgDVNyFeV1i5Qqk5oiUCpmjFn+fl8VJ5vp4JTbXTXYq2gdwmwstLMnUrVCU0EStXMyErPPzl//hFr1k2A27EmzANrCcOH4eSCNI3PdlER8QJaGmMWAs8AjYEzSiVKuZJ+81DqlEbOVcROmGOMOdGFtKmIrMf6Vj/aue8x4F8i8jSQDdzr3P848L6I3I/1zf9h4BBV8wb+7UwWArzlXLNAqTqjbQRKnYOzjSDFGJNjdyxKuYJWDSmllIfTEoFSSnk4LREopZSH00SglFIeThOBUkp5OE0ESinl4TQRKKWUh/t/VDtMUEP4aSAAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/Colab Notebooks/MA/rnn/rnn_sp.h5\")"
      ],
      "metadata": {
        "id": "R1AJPXZ3lBjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a = keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/MA/rnn/rnn_sp.h5\",  custom_objects={\"get_f1\": get_f1})"
      ],
      "metadata": {
        "id": "-oskWrIwlBjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3236e30-9f75-44ae-e7bb-65a964dc5969",
        "id": "287E9p8wlBjp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "710842e7-31d8-46e6-f37e-d408e6c56ffa",
        "id": "GQDtpF1ylBjp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.64      0.65        96\n",
            "           1       0.25      0.03      0.06        32\n",
            "           2       0.79      0.92      0.85       183\n",
            "\n",
            "    accuracy                           0.74       311\n",
            "   macro avg       0.57      0.53      0.52       311\n",
            "weighted avg       0.69      0.74      0.71       311\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy')\n",
        "print(accuracy_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1)))\n",
        "print('f1 score')\n",
        "print(f1_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n",
        "print('recall')\n",
        "print(recall_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n",
        "print('precision')\n",
        "print(precision_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67632a6a-8d82-4877-8d79-fd98e9c279a5",
        "id": "58_mPnaGlBjp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "0.7427652733118971\n",
            "f1 score\n",
            "0.70594713245786\n",
            "recall\n",
            "0.7427652733118971\n",
            "precision\n",
            "0.6928824229076171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM\n"
      ],
      "metadata": {
        "id": "vNvQsxEP3aQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = train_sp.append(test_sp, ignore_index=True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "sAn6n9zf3i-a",
        "outputId": "d34b7dd5-c58f-4892-e75d-17c19da62f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                               text   aspectTerm  \\\n",
              "0     142087  I am very satisfied with it ... I have used th...        price   \n",
              "1     142089  Cheap processing, wobbling unstable unstable l...   processing   \n",
              "2     142089  Cheap processing, wobbling unstable unstable l...         lids   \n",
              "3     142089  Cheap processing, wobbling unstable unstable l...  baking time   \n",
              "4     142091  The iron does not remain, the waffles are too ...      waffles   \n",
              "...      ...                                                ...          ...   \n",
              "1550  145346  It is very simple to handle and the price qual...      coating   \n",
              "1551  145357  Easy to clean, heat enough and the materials s...        clean   \n",
              "1552  145357  Easy to clean, heat enough and the materials s...         heat   \n",
              "1553  145357  Easy to clean, heat enough and the materials s...    materials   \n",
              "1554  145357  Easy to clean, heat enough and the materials s...        price   \n",
              "\n",
              "      From   To        SP                                         text_wo_sw  \\\n",
              "0       82   87  positive  satisfied ... device times ... price device .....   \n",
              "1        6   16  negative  cheap processing, wobbling unstable unstable l...   \n",
              "2       45   49  negative  cheap processing, wobbling unstable unstable l...   \n",
              "3      142  153  negative  cheap processing, wobbling unstable unstable l...   \n",
              "4       30   37  negative     iron remain, waffles small waffles brown, six.   \n",
              "...    ...  ...       ...                                                ...   \n",
              "1550    81   88  positive  simple handle price quality relationship good....   \n",
              "1551     8   13  positive  easy clean, heat materials good quality plasti...   \n",
              "1552    15   19  positive  easy clean, heat materials good quality plasti...   \n",
              "1553    35   44  positive  easy clean, heat materials good quality plasti...   \n",
              "1554   123  128  positive  easy clean, heat materials good quality plasti...   \n",
              "\n",
              "                                                    coc  \n",
              "0     satisfied...device times...device...can't expe...  \n",
              "1                cheap, wobbling unstable unstable lids  \n",
              "2     cheap processing, wobbling unstable unstable f...  \n",
              "3     independently time open incredible unreasonabl...  \n",
              "4                iron remain, small waffles brown, six.  \n",
              "...                                                 ...  \n",
              "1550  handle price quality relationship good. non -s...  \n",
              "1551                  easy, heat materials good quality  \n",
              "1552  easy clean, materials good quality plasticucho...  \n",
              "1553  easy clean, heat good quality plasticuchos. go...  \n",
              "1554         materials good quality plasticuchos. good.  \n",
              "\n",
              "[1555 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72cdd33a-d73e-4249-abb2-bf75f2f08946\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>aspectTerm</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>SP</th>\n",
              "      <th>text_wo_sw</th>\n",
              "      <th>coc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142087</td>\n",
              "      <td>I am very satisfied with it ... I have used th...</td>\n",
              "      <td>price</td>\n",
              "      <td>82</td>\n",
              "      <td>87</td>\n",
              "      <td>positive</td>\n",
              "      <td>satisfied ... device times ... price device .....</td>\n",
              "      <td>satisfied...device times...device...can't expe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>processing</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap, wobbling unstable unstable lids</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>lids</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap processing, wobbling unstable unstable f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>baking time</td>\n",
              "      <td>142</td>\n",
              "      <td>153</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>independently time open incredible unreasonabl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142091</td>\n",
              "      <td>The iron does not remain, the waffles are too ...</td>\n",
              "      <td>waffles</td>\n",
              "      <td>30</td>\n",
              "      <td>37</td>\n",
              "      <td>negative</td>\n",
              "      <td>iron remain, waffles small waffles brown, six.</td>\n",
              "      <td>iron remain, small waffles brown, six.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1550</th>\n",
              "      <td>145346</td>\n",
              "      <td>It is very simple to handle and the price qual...</td>\n",
              "      <td>coating</td>\n",
              "      <td>81</td>\n",
              "      <td>88</td>\n",
              "      <td>positive</td>\n",
              "      <td>simple handle price quality relationship good....</td>\n",
              "      <td>handle price quality relationship good. non -s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1551</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>clean</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy, heat materials good quality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1552</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>heat</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy clean, materials good quality plasticucho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1553</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>materials</td>\n",
              "      <td>35</td>\n",
              "      <td>44</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy clean, heat good quality plasticuchos. go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1554</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>price</td>\n",
              "      <td>123</td>\n",
              "      <td>128</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>materials good quality plasticuchos. good.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1555 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72cdd33a-d73e-4249-abb2-bf75f2f08946')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72cdd33a-d73e-4249-abb2-bf75f2f08946 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72cdd33a-d73e-4249-abb2-bf75f2f08946');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 1000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 10\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM =100\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(df['coc'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQCsI4n13bs2",
        "outputId": "48695e65-40f4-42fb-94c0-faf98dcd2801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1765 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = tokenizer.texts_to_sequences(df['coc'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veUSx2WZ3nr8",
        "outputId": "7b6d2f7e-06af-4bda-9e37-1c713313fb23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data tensor: (1555, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(df['SP']).values\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqde77Ei3ts9",
        "outputId": "d4a5608c-2b9d-48df-8cde-47f27da400e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (1555, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANTXbjQcUy3o",
        "outputId": "9be7422c-cbdc-4c5f-b0af-ad2efda9f48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 29)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9FrcD3F33JT",
        "outputId": "26b78328-3d3a-421b-c7bb-4b64154b70e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1244, 10) (1244, 3)\n",
            "(311, 10) (311, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first layer is the embedded layer that uses 100 length vectors to represent each word.\n",
        "SpatialDropout1D performs variational dropout in NLP models.\n",
        "The next layer is the LSTM layer with 100 memory units.\n",
        "The output layer must create 3 output values, one for each class.\n",
        "Activation function is softmax for multi-class classification.\n",
        "Because it is a multi-class classification problem, categorical_crossentropy is used as the loss function.\n"
      ],
      "metadata": {
        "id": "Utp_m7Uy4Ala"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shhG5TKz37H1",
        "outputId": "fd988935-6834-48df-f67d-85777b9c6c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, 10, 100)           100000    \n",
            "                                                                 \n",
            " spatial_dropout1d_8 (Spatia  (None, 10, 100)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 180,703\n",
            "Trainable params: 180,703\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam = k.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999)\n",
        "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=[get_f1])"
      ],
      "metadata": {
        "id": "JCQ5Yoa04spP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the best only\n",
        "\n",
        "filepath=\"/content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 5, verbose = 1,restore_best_weights = True)\n",
        "callbacks_list = [earlystop, checkpoint]"
      ],
      "metadata": {
        "id": "SVqAnKPH433p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=callbacks_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cx6wGtH4uOq",
        "outputId": "907e0b30-c455-4eb0-bcb0-7766223eede9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0887 - get_f1: 0.0000e+00\n",
            "Epoch 1: val_loss improved from inf to 1.07854, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 8s 335ms/step - loss: 1.0887 - get_f1: 0.0000e+00 - val_loss: 1.0785 - val_get_f1: 0.0000e+00\n",
            "Epoch 2/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.0703 - get_f1: 0.0000e+00\n",
            "Epoch 2: val_loss improved from 1.07854 to 1.05808, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 274ms/step - loss: 1.0706 - get_f1: 0.0000e+00 - val_loss: 1.0581 - val_get_f1: 0.0000e+00\n",
            "Epoch 3/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.0464 - get_f1: 0.0000e+00\n",
            "Epoch 3: val_loss improved from 1.05808 to 1.02983, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 267ms/step - loss: 1.0457 - get_f1: 0.0000e+00 - val_loss: 1.0298 - val_get_f1: 0.0000e+00\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0109 - get_f1: 0.0000e+00\n",
            "Epoch 4: val_loss improved from 1.02983 to 0.98769, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.0109 - get_f1: 0.0000e+00 - val_loss: 0.9877 - val_get_f1: 0.0000e+00\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9609 - get_f1: 0.1510\n",
            "Epoch 5: val_loss improved from 0.98769 to 0.93180, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 272ms/step - loss: 0.9609 - get_f1: 0.1510 - val_loss: 0.9318 - val_get_f1: 0.4430\n",
            "Epoch 6/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.9025 - get_f1: 0.5237\n",
            "Epoch 6: val_loss improved from 0.93180 to 0.89703, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 268ms/step - loss: 0.8986 - get_f1: 0.5312 - val_loss: 0.8970 - val_get_f1: 0.5640\n",
            "Epoch 7/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.8673 - get_f1: 0.6138\n",
            "Epoch 7: val_loss improved from 0.89703 to 0.88760, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 297ms/step - loss: 0.8757 - get_f1: 0.6053 - val_loss: 0.8876 - val_get_f1: 0.5814\n",
            "Epoch 8/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.8572 - get_f1: 0.6296\n",
            "Epoch 8: val_loss improved from 0.88760 to 0.87076, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 268ms/step - loss: 0.8604 - get_f1: 0.6257 - val_loss: 0.8708 - val_get_f1: 0.5835\n",
            "Epoch 9/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.8499 - get_f1: 0.6299\n",
            "Epoch 9: val_loss improved from 0.87076 to 0.86138, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 267ms/step - loss: 0.8488 - get_f1: 0.6315 - val_loss: 0.8614 - val_get_f1: 0.5882\n",
            "Epoch 10/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.8376 - get_f1: 0.6351\n",
            "Epoch 10: val_loss improved from 0.86138 to 0.85419, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 266ms/step - loss: 0.8377 - get_f1: 0.6327 - val_loss: 0.8542 - val_get_f1: 0.6005\n",
            "Epoch 11/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.8246 - get_f1: 0.6326\n",
            "Epoch 11: val_loss improved from 0.85419 to 0.84337, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 0.8242 - get_f1: 0.6365 - val_loss: 0.8434 - val_get_f1: 0.5997\n",
            "Epoch 12/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.8052 - get_f1: 0.6369\n",
            "Epoch 12: val_loss improved from 0.84337 to 0.83340, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 268ms/step - loss: 0.8092 - get_f1: 0.6333 - val_loss: 0.8334 - val_get_f1: 0.6021\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7926 - get_f1: 0.6386\n",
            "Epoch 13: val_loss improved from 0.83340 to 0.82047, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 265ms/step - loss: 0.7926 - get_f1: 0.6386 - val_loss: 0.8205 - val_get_f1: 0.6021\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7741 - get_f1: 0.6380\n",
            "Epoch 14: val_loss improved from 0.82047 to 0.80642, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 293ms/step - loss: 0.7741 - get_f1: 0.6380 - val_loss: 0.8064 - val_get_f1: 0.6062\n",
            "Epoch 15/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.7548 - get_f1: 0.6460\n",
            "Epoch 15: val_loss improved from 0.80642 to 0.79107, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 270ms/step - loss: 0.7541 - get_f1: 0.6455 - val_loss: 0.7911 - val_get_f1: 0.6059\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7287 - get_f1: 0.6571\n",
            "Epoch 16: val_loss improved from 0.79107 to 0.77133, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 268ms/step - loss: 0.7287 - get_f1: 0.6571 - val_loss: 0.7713 - val_get_f1: 0.6176\n",
            "Epoch 17/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.7050 - get_f1: 0.6574\n",
            "Epoch 17: val_loss improved from 0.77133 to 0.75395, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 360ms/step - loss: 0.7054 - get_f1: 0.6580 - val_loss: 0.7540 - val_get_f1: 0.6232\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6795 - get_f1: 0.6660\n",
            "Epoch 18: val_loss improved from 0.75395 to 0.73560, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 7s 458ms/step - loss: 0.6795 - get_f1: 0.6660 - val_loss: 0.7356 - val_get_f1: 0.6409\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6620 - get_f1: 0.6838\n",
            "Epoch 19: val_loss improved from 0.73560 to 0.72017, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 264ms/step - loss: 0.6620 - get_f1: 0.6838 - val_loss: 0.7202 - val_get_f1: 0.6409\n",
            "Epoch 20/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6261 - get_f1: 0.6902\n",
            "Epoch 20: val_loss improved from 0.72017 to 0.70129, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 296ms/step - loss: 0.6294 - get_f1: 0.6881 - val_loss: 0.7013 - val_get_f1: 0.6424\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6143 - get_f1: 0.6974\n",
            "Epoch 21: val_loss improved from 0.70129 to 0.68644, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 270ms/step - loss: 0.6143 - get_f1: 0.6974 - val_loss: 0.6864 - val_get_f1: 0.6441\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5900 - get_f1: 0.7140\n",
            "Epoch 22: val_loss improved from 0.68644 to 0.67179, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 260ms/step - loss: 0.5900 - get_f1: 0.7140 - val_loss: 0.6718 - val_get_f1: 0.6727\n",
            "Epoch 23/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5789 - get_f1: 0.7417\n",
            "Epoch 23: val_loss improved from 0.67179 to 0.65713, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 272ms/step - loss: 0.5779 - get_f1: 0.7427 - val_loss: 0.6571 - val_get_f1: 0.6888\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5544 - get_f1: 0.7769\n",
            "Epoch 24: val_loss improved from 0.65713 to 0.63953, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 0.5544 - get_f1: 0.7769 - val_loss: 0.6395 - val_get_f1: 0.7291\n",
            "Epoch 25/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5362 - get_f1: 0.8018\n",
            "Epoch 25: val_loss improved from 0.63953 to 0.63102, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 268ms/step - loss: 0.5358 - get_f1: 0.8024 - val_loss: 0.6310 - val_get_f1: 0.7432\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5230 - get_f1: 0.8142\n",
            "Epoch 26: val_loss improved from 0.63102 to 0.61174, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 271ms/step - loss: 0.5230 - get_f1: 0.8142 - val_loss: 0.6117 - val_get_f1: 0.7856\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5123 - get_f1: 0.8214\n",
            "Epoch 27: val_loss improved from 0.61174 to 0.60530, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 0.5123 - get_f1: 0.8214 - val_loss: 0.6053 - val_get_f1: 0.7716\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4899 - get_f1: 0.8162\n",
            "Epoch 28: val_loss improved from 0.60530 to 0.59220, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 263ms/step - loss: 0.4899 - get_f1: 0.8162 - val_loss: 0.5922 - val_get_f1: 0.7936\n",
            "Epoch 29/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4687 - get_f1: 0.8351\n",
            "Epoch 29: val_loss improved from 0.59220 to 0.58355, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 265ms/step - loss: 0.4722 - get_f1: 0.8329 - val_loss: 0.5836 - val_get_f1: 0.7945\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4640 - get_f1: 0.8339\n",
            "Epoch 30: val_loss did not improve from 0.58355\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.4640 - get_f1: 0.8339 - val_loss: 0.5840 - val_get_f1: 0.7896\n",
            "Epoch 31/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4552 - get_f1: 0.8348\n",
            "Epoch 31: val_loss improved from 0.58355 to 0.56846, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 295ms/step - loss: 0.4506 - get_f1: 0.8377 - val_loss: 0.5685 - val_get_f1: 0.7994\n",
            "Epoch 32/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4471 - get_f1: 0.8411\n",
            "Epoch 32: val_loss improved from 0.56846 to 0.56517, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 266ms/step - loss: 0.4479 - get_f1: 0.8403 - val_loss: 0.5652 - val_get_f1: 0.7965\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4305 - get_f1: 0.8444\n",
            "Epoch 33: val_loss improved from 0.56517 to 0.56312, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 270ms/step - loss: 0.4305 - get_f1: 0.8444 - val_loss: 0.5631 - val_get_f1: 0.7946\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4161 - get_f1: 0.8536\n",
            "Epoch 34: val_loss improved from 0.56312 to 0.56072, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 261ms/step - loss: 0.4161 - get_f1: 0.8536 - val_loss: 0.5607 - val_get_f1: 0.7956\n",
            "Epoch 35/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4065 - get_f1: 0.8541\n",
            "Epoch 35: val_loss did not improve from 0.56072\n",
            "16/16 [==============================] - 1s 50ms/step - loss: 0.4040 - get_f1: 0.8584 - val_loss: 0.5651 - val_get_f1: 0.7925\n",
            "Epoch 36/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4021 - get_f1: 0.8550\n",
            "Epoch 36: val_loss improved from 0.56072 to 0.54595, saving model to /content/drive/MyDrive/Colab Notebooks/MA/lstm/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 0.4012 - get_f1: 0.8567 - val_loss: 0.5459 - val_get_f1: 0.8074\n",
            "Epoch 37/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3902 - get_f1: 0.8526\n",
            "Epoch 37: val_loss did not improve from 0.54595\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.3909 - get_f1: 0.8527 - val_loss: 0.5504 - val_get_f1: 0.8089\n",
            "Epoch 38/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3763 - get_f1: 0.8601\n",
            "Epoch 38: val_loss did not improve from 0.54595\n",
            "16/16 [==============================] - 1s 50ms/step - loss: 0.3806 - get_f1: 0.8578 - val_loss: 0.5561 - val_get_f1: 0.8021\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3732 - get_f1: 0.8521\n",
            "Epoch 39: val_loss did not improve from 0.54595\n",
            "16/16 [==============================] - 1s 51ms/step - loss: 0.3732 - get_f1: 0.8521 - val_loss: 0.5477 - val_get_f1: 0.8070\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3663 - get_f1: 0.8669\n",
            "Epoch 40: val_loss did not improve from 0.54595\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.3663 - get_f1: 0.8669 - val_loss: 0.5485 - val_get_f1: 0.8098\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3681 - get_f1: 0.8595Restoring model weights from the end of the best epoch: 36.\n",
            "\n",
            "Epoch 41: val_loss did not improve from 0.54595\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.3681 - get_f1: 0.8595 - val_loss: 0.5509 - val_get_f1: 0.8098\n",
            "Epoch 41: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 42: val_loss did not improve from 0.52517\n",
        "16/16 [==============================] - 1s 58ms/step - loss: 0.3515 - get_f1: 0.8703 - val_loss: 0.5355 - val_get_f1: 0.7946\n",
        "Epoch 42: early stopping"
      ],
      "metadata": {
        "id": "5xsh_swa8245"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "plot_graphs(history, 'get_f1')\n",
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "rfQnBbHK4bm7",
        "outputId": "a416affa-6327-4edb-8611-3bb6337ba33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/Colab Notebooks/MA/lstm/lstm_sp.h5\")"
      ],
      "metadata": {
        "id": "gVwwYkmFN6mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a = keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/MA/lstm/lstm_sp.h5\",  custom_objects={\"get_f1\": get_f1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vijp3cT-N8R-",
        "outputId": "6da7f636-c9d3-49fd-b61d-506f02ca8856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9sCJmyRPsTi",
        "outputId": "b8ee69f9-9ce6-4a28-a33c-bd1d921c013c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5tEPvxpP1HI",
        "outputId": "db1923b6-9391-46df-b798-1f8ef885dde0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.76      0.72        96\n",
            "           1       0.00      0.00      0.00        32\n",
            "           2       0.81      0.90      0.85       183\n",
            "\n",
            "    accuracy                           0.77       311\n",
            "   macro avg       0.50      0.55      0.52       311\n",
            "weighted avg       0.69      0.77      0.72       311\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy')\n",
        "print(accuracy_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1)))\n",
        "print('f1 score')\n",
        "print(f1_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n",
        "print('recall')\n",
        "print(recall_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n",
        "print('precision')\n",
        "print(precision_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07pGEi3zWfTi",
        "outputId": "0a98c027-7581-4121-fd2b-840ecf5f0cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "0.7652733118971061\n",
            "f1 score\n",
            "0.7239763972830023\n",
            "recall\n",
            "0.7652733118971061\n",
            "precision\n",
            "0.6869219654450833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bi-LSTM\n"
      ],
      "metadata": {
        "id": "yZ8r9cfKbN5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = train_sp.append(test_sp, ignore_index=True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "89f3ded3-2986-42fe-8790-96b964b89431",
        "id": "6-YDL3N6bN5V"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                               text   aspectTerm  \\\n",
              "0     142087  I am very satisfied with it ... I have used th...        price   \n",
              "1     142089  Cheap processing, wobbling unstable unstable l...   processing   \n",
              "2     142089  Cheap processing, wobbling unstable unstable l...         lids   \n",
              "3     142089  Cheap processing, wobbling unstable unstable l...  baking time   \n",
              "4     142091  The iron does not remain, the waffles are too ...      waffles   \n",
              "...      ...                                                ...          ...   \n",
              "1550  145346  It is very simple to handle and the price qual...      coating   \n",
              "1551  145357  Easy to clean, heat enough and the materials s...        clean   \n",
              "1552  145357  Easy to clean, heat enough and the materials s...         heat   \n",
              "1553  145357  Easy to clean, heat enough and the materials s...    materials   \n",
              "1554  145357  Easy to clean, heat enough and the materials s...        price   \n",
              "\n",
              "      From   To        SP                                         text_wo_sw  \\\n",
              "0       82   87  positive  satisfied ... device times ... price device .....   \n",
              "1        6   16  negative  cheap processing, wobbling unstable unstable l...   \n",
              "2       45   49  negative  cheap processing, wobbling unstable unstable l...   \n",
              "3      142  153  negative  cheap processing, wobbling unstable unstable l...   \n",
              "4       30   37  negative     iron remain, waffles small waffles brown, six.   \n",
              "...    ...  ...       ...                                                ...   \n",
              "1550    81   88  positive  simple handle price quality relationship good....   \n",
              "1551     8   13  positive  easy clean, heat materials good quality plasti...   \n",
              "1552    15   19  positive  easy clean, heat materials good quality plasti...   \n",
              "1553    35   44  positive  easy clean, heat materials good quality plasti...   \n",
              "1554   123  128  positive  easy clean, heat materials good quality plasti...   \n",
              "\n",
              "                                                    coc  \n",
              "0     satisfied...device times...device...can't expe...  \n",
              "1                cheap, wobbling unstable unstable lids  \n",
              "2     cheap processing, wobbling unstable unstable f...  \n",
              "3     independently time open incredible unreasonabl...  \n",
              "4                iron remain, small waffles brown, six.  \n",
              "...                                                 ...  \n",
              "1550  handle price quality relationship good. non -s...  \n",
              "1551                  easy, heat materials good quality  \n",
              "1552  easy clean, materials good quality plasticucho...  \n",
              "1553  easy clean, heat good quality plasticuchos. go...  \n",
              "1554         materials good quality plasticuchos. good.  \n",
              "\n",
              "[1555 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f99d50f-e03f-45ee-98c2-18e2bb184404\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>aspectTerm</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>SP</th>\n",
              "      <th>text_wo_sw</th>\n",
              "      <th>coc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142087</td>\n",
              "      <td>I am very satisfied with it ... I have used th...</td>\n",
              "      <td>price</td>\n",
              "      <td>82</td>\n",
              "      <td>87</td>\n",
              "      <td>positive</td>\n",
              "      <td>satisfied ... device times ... price device .....</td>\n",
              "      <td>satisfied...device times...device...can't expe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>processing</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap, wobbling unstable unstable lids</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>lids</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap processing, wobbling unstable unstable f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>baking time</td>\n",
              "      <td>142</td>\n",
              "      <td>153</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>independently time open incredible unreasonabl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142091</td>\n",
              "      <td>The iron does not remain, the waffles are too ...</td>\n",
              "      <td>waffles</td>\n",
              "      <td>30</td>\n",
              "      <td>37</td>\n",
              "      <td>negative</td>\n",
              "      <td>iron remain, waffles small waffles brown, six.</td>\n",
              "      <td>iron remain, small waffles brown, six.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1550</th>\n",
              "      <td>145346</td>\n",
              "      <td>It is very simple to handle and the price qual...</td>\n",
              "      <td>coating</td>\n",
              "      <td>81</td>\n",
              "      <td>88</td>\n",
              "      <td>positive</td>\n",
              "      <td>simple handle price quality relationship good....</td>\n",
              "      <td>handle price quality relationship good. non -s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1551</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>clean</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy, heat materials good quality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1552</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>heat</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy clean, materials good quality plasticucho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1553</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>materials</td>\n",
              "      <td>35</td>\n",
              "      <td>44</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy clean, heat good quality plasticuchos. go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1554</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>price</td>\n",
              "      <td>123</td>\n",
              "      <td>128</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>materials good quality plasticuchos. good.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1555 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f99d50f-e03f-45ee-98c2-18e2bb184404')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f99d50f-e03f-45ee-98c2-18e2bb184404 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f99d50f-e03f-45ee-98c2-18e2bb184404');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 1000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 10\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(df['coc'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e6dab9a-68c1-41ce-91cb-be9cea437bd8",
        "id": "obLobZQ1bN5W"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1765 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = tokenizer.texts_to_sequences(df['coc'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "083fb8bc-0170-4a24-ca31-9565745e0de9",
        "id": "UundcFMFbN5W"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data tensor: (1555, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(df['SP']).values\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3ff4288-10be-4ea8-fffb-26e7605fa1e7",
        "id": "-9CJ6H16bN5W"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (1555, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec81050c-494c-4534-acfb-eb3bf051540c",
        "id": "bL56O-SQbN5X"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 29)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db08a8f9-ee52-42cf-ce25-79ecc195b3da",
        "id": "W0Mo7MB3bN5X"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1244, 10) (1244, 3)\n",
            "(311, 10) (311, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first layer is the embedded layer that uses 100 length vectors to represent each word.\n",
        "SpatialDropout1D performs variational dropout in NLP models.\n",
        "The next layer is the LSTM layer with 100 memory units.\n",
        "The output layer must create 3 output values, one for each class.\n",
        "Activation function is softmax for multi-class classification.\n",
        "Because it is a multi-class classification problem, categorical_crossentropy is used as the loss function.\n"
      ],
      "metadata": {
        "id": "rNXYgWb6bN5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d48945-67e2-4524-f765-96813ef3a16b",
        "id": "b8B5Ll4CbN5X"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (None, 10, 100)           100000    \n",
            "                                                                 \n",
            " spatial_dropout1d_9 (Spatia  (None, 10, 100)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 200)              160800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 3)                 603       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 261,403\n",
            "Trainable params: 261,403\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam = k.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999)\n",
        "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=[get_f1])"
      ],
      "metadata": {
        "id": "YmQdhsLibN5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the best only\n",
        "\n",
        "filepath=\"/content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 5, verbose = 1,restore_best_weights = True)\n",
        "callbacks_list = [earlystop, checkpoint]"
      ],
      "metadata": {
        "id": "fXm1-G2JbN5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=callbacks_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05549aac-690b-4129-ca07-6ed101b56245",
        "id": "m1RcIeQkbN5Y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0845 - get_f1: 0.0000e+00\n",
            "Epoch 1: val_loss improved from inf to 1.06998, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 16s 751ms/step - loss: 1.0845 - get_f1: 0.0000e+00 - val_loss: 1.0700 - val_get_f1: 0.0000e+00\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0585 - get_f1: 0.0000e+00\n",
            "Epoch 2: val_loss improved from 1.06998 to 1.04331, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 718ms/step - loss: 1.0585 - get_f1: 0.0000e+00 - val_loss: 1.0433 - val_get_f1: 0.0000e+00\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0287 - get_f1: 0.0000e+00\n",
            "Epoch 3: val_loss improved from 1.04331 to 1.01018, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 689ms/step - loss: 1.0287 - get_f1: 0.0000e+00 - val_loss: 1.0102 - val_get_f1: 0.0000e+00\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9901 - get_f1: 0.0057\n",
            "Epoch 4: val_loss improved from 1.01018 to 0.96797, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 746ms/step - loss: 0.9901 - get_f1: 0.0057 - val_loss: 0.9680 - val_get_f1: 0.0615\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9424 - get_f1: 0.2687\n",
            "Epoch 5: val_loss improved from 0.96797 to 0.92115, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 727ms/step - loss: 0.9424 - get_f1: 0.2687 - val_loss: 0.9211 - val_get_f1: 0.4755\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8925 - get_f1: 0.5324\n",
            "Epoch 6: val_loss improved from 0.92115 to 0.89086, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 683ms/step - loss: 0.8925 - get_f1: 0.5324 - val_loss: 0.8909 - val_get_f1: 0.5622\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8686 - get_f1: 0.6189\n",
            "Epoch 7: val_loss improved from 0.89086 to 0.87888, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 736ms/step - loss: 0.8686 - get_f1: 0.6189 - val_loss: 0.8789 - val_get_f1: 0.5944\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8534 - get_f1: 0.6286\n",
            "Epoch 8: val_loss improved from 0.87888 to 0.86599, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 724ms/step - loss: 0.8534 - get_f1: 0.6286 - val_loss: 0.8660 - val_get_f1: 0.6072\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8395 - get_f1: 0.6317\n",
            "Epoch 9: val_loss improved from 0.86599 to 0.85408, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 724ms/step - loss: 0.8395 - get_f1: 0.6317 - val_loss: 0.8541 - val_get_f1: 0.6061\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8248 - get_f1: 0.6317\n",
            "Epoch 10: val_loss improved from 0.85408 to 0.84341, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 683ms/step - loss: 0.8248 - get_f1: 0.6317 - val_loss: 0.8434 - val_get_f1: 0.6061\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8049 - get_f1: 0.6363\n",
            "Epoch 11: val_loss improved from 0.84341 to 0.82955, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 746ms/step - loss: 0.8049 - get_f1: 0.6363 - val_loss: 0.8296 - val_get_f1: 0.6050\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7771 - get_f1: 0.6361\n",
            "Epoch 12: val_loss improved from 0.82955 to 0.80953, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 735ms/step - loss: 0.7771 - get_f1: 0.6361 - val_loss: 0.8095 - val_get_f1: 0.6063\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7451 - get_f1: 0.6523\n",
            "Epoch 13: val_loss improved from 0.80953 to 0.78122, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 14s 907ms/step - loss: 0.7451 - get_f1: 0.6523 - val_loss: 0.7812 - val_get_f1: 0.6128\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6989 - get_f1: 0.6571\n",
            "Epoch 14: val_loss improved from 0.78122 to 0.75132, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 735ms/step - loss: 0.6989 - get_f1: 0.6571 - val_loss: 0.7513 - val_get_f1: 0.6217\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6444 - get_f1: 0.6817\n",
            "Epoch 15: val_loss improved from 0.75132 to 0.71593, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 708ms/step - loss: 0.6444 - get_f1: 0.6817 - val_loss: 0.7159 - val_get_f1: 0.6380\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5903 - get_f1: 0.6939\n",
            "Epoch 16: val_loss improved from 0.71593 to 0.70365, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 685ms/step - loss: 0.5903 - get_f1: 0.6939 - val_loss: 0.7036 - val_get_f1: 0.6418\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5526 - get_f1: 0.7002\n",
            "Epoch 17: val_loss improved from 0.70365 to 0.68807, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 726ms/step - loss: 0.5526 - get_f1: 0.7002 - val_loss: 0.6881 - val_get_f1: 0.6473\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5219 - get_f1: 0.7416\n",
            "Epoch 18: val_loss did not improve from 0.68807\n",
            "16/16 [==============================] - 2s 97ms/step - loss: 0.5219 - get_f1: 0.7416 - val_loss: 0.6886 - val_get_f1: 0.7062\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5024 - get_f1: 0.7934\n",
            "Epoch 19: val_loss improved from 0.68807 to 0.67181, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 722ms/step - loss: 0.5024 - get_f1: 0.7934 - val_loss: 0.6718 - val_get_f1: 0.7483\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4731 - get_f1: 0.8272\n",
            "Epoch 20: val_loss improved from 0.67181 to 0.66847, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 701ms/step - loss: 0.4731 - get_f1: 0.8272 - val_loss: 0.6685 - val_get_f1: 0.7564\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4550 - get_f1: 0.8387\n",
            "Epoch 21: val_loss improved from 0.66847 to 0.66436, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 744ms/step - loss: 0.4550 - get_f1: 0.8387 - val_loss: 0.6644 - val_get_f1: 0.7720\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4339 - get_f1: 0.8409\n",
            "Epoch 22: val_loss did not improve from 0.66436\n",
            "16/16 [==============================] - 2s 98ms/step - loss: 0.4339 - get_f1: 0.8409 - val_loss: 0.6739 - val_get_f1: 0.7740\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4287 - get_f1: 0.8420\n",
            "Epoch 23: val_loss improved from 0.66436 to 0.66215, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bilstm/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 734ms/step - loss: 0.4287 - get_f1: 0.8420 - val_loss: 0.6621 - val_get_f1: 0.7651\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4012 - get_f1: 0.8532\n",
            "Epoch 24: val_loss did not improve from 0.66215\n",
            "16/16 [==============================] - 2s 99ms/step - loss: 0.4012 - get_f1: 0.8532 - val_loss: 0.6939 - val_get_f1: 0.7780\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3925 - get_f1: 0.8584\n",
            "Epoch 25: val_loss did not improve from 0.66215\n",
            "16/16 [==============================] - 2s 95ms/step - loss: 0.3925 - get_f1: 0.8584 - val_loss: 0.6863 - val_get_f1: 0.7626\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3825 - get_f1: 0.8499\n",
            "Epoch 26: val_loss did not improve from 0.66215\n",
            "16/16 [==============================] - 2s 98ms/step - loss: 0.3825 - get_f1: 0.8499 - val_loss: 0.6883 - val_get_f1: 0.7691\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3692 - get_f1: 0.8593\n",
            "Epoch 27: val_loss did not improve from 0.66215\n",
            "16/16 [==============================] - 2s 98ms/step - loss: 0.3692 - get_f1: 0.8593 - val_loss: 0.6918 - val_get_f1: 0.7666\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3549 - get_f1: 0.8659Restoring model weights from the end of the best epoch: 23.\n",
            "\n",
            "Epoch 28: val_loss did not improve from 0.66215\n",
            "16/16 [==============================] - 2s 97ms/step - loss: 0.3549 - get_f1: 0.8659 - val_loss: 0.7000 - val_get_f1: 0.7706\n",
            "Epoch 28: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 26: val_loss did not improve from 0.61862\n",
        "16/16 [==============================] - 2s 126ms/step - loss: 0.3744 - get_f1: 0.8567 - val_loss: 0.6383 - val_get_f1: 0.7855\n",
        "Epoch 26: early stopping"
      ],
      "metadata": {
        "id": "709Sde1sbN5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "plot_graphs(history, 'get_f1')\n",
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "67d6f6e7-eee1-4b33-d9ad-bec039ea2e60",
        "id": "7DUQqPTQbN5Z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/Colab Notebooks/MA/bilstm/bilstm_sp.h5\")"
      ],
      "metadata": {
        "id": "tftnaUU2bN5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a = keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/MA/bilstm/bilstm_sp.h5\",  custom_objects={\"get_f1\": get_f1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856cded0-7b9b-4bd8-8d10-79eba67c40c5",
        "id": "KXatHE2zbN5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d62831d2-f7a4-477f-ccf9-6d359509bd9b",
        "id": "sOAhkxXMbN5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 1s 7ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79772661-d53f-4df0-b577-f645b0ef26a1",
        "id": "cK59gxCFbN5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.81      0.73        96\n",
            "           1       0.00      0.00      0.00        32\n",
            "           2       0.84      0.89      0.86       183\n",
            "\n",
            "    accuracy                           0.77       311\n",
            "   macro avg       0.50      0.57      0.53       311\n",
            "weighted avg       0.70      0.77      0.73       311\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy')\n",
        "print(accuracy_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1)))\n",
        "print('f1 score')\n",
        "print(f1_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n",
        "print('recall')\n",
        "print(recall_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n",
        "print('precision')\n",
        "print(precision_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76325681-dac2-4fee-af38-378ab2f88819",
        "id": "204tqAB8bN5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "0.7717041800643086\n",
            "f1 score\n",
            "0.7323723921334031\n",
            "recall\n",
            "0.7717041800643086\n",
            "precision\n",
            "0.6988122821475857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU\n"
      ],
      "metadata": {
        "id": "kAnxxOtDfZl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = train_sp.append(test_sp, ignore_index=True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "a415901f-57d4-4124-d78d-e58f14ac39d4",
        "id": "ep2pplK4fZmI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                               text   aspectTerm  \\\n",
              "0     142087  I am very satisfied with it ... I have used th...        price   \n",
              "1     142089  Cheap processing, wobbling unstable unstable l...   processing   \n",
              "2     142089  Cheap processing, wobbling unstable unstable l...         lids   \n",
              "3     142089  Cheap processing, wobbling unstable unstable l...  baking time   \n",
              "4     142091  The iron does not remain, the waffles are too ...      waffles   \n",
              "...      ...                                                ...          ...   \n",
              "1550  145346  It is very simple to handle and the price qual...      coating   \n",
              "1551  145357  Easy to clean, heat enough and the materials s...        clean   \n",
              "1552  145357  Easy to clean, heat enough and the materials s...         heat   \n",
              "1553  145357  Easy to clean, heat enough and the materials s...    materials   \n",
              "1554  145357  Easy to clean, heat enough and the materials s...        price   \n",
              "\n",
              "      From   To        SP                                         text_wo_sw  \\\n",
              "0       82   87  positive  satisfied ... device times ... price device .....   \n",
              "1        6   16  negative  cheap processing, wobbling unstable unstable l...   \n",
              "2       45   49  negative  cheap processing, wobbling unstable unstable l...   \n",
              "3      142  153  negative  cheap processing, wobbling unstable unstable l...   \n",
              "4       30   37  negative     iron remain, waffles small waffles brown, six.   \n",
              "...    ...  ...       ...                                                ...   \n",
              "1550    81   88  positive  simple handle price quality relationship good....   \n",
              "1551     8   13  positive  easy clean, heat materials good quality plasti...   \n",
              "1552    15   19  positive  easy clean, heat materials good quality plasti...   \n",
              "1553    35   44  positive  easy clean, heat materials good quality plasti...   \n",
              "1554   123  128  positive  easy clean, heat materials good quality plasti...   \n",
              "\n",
              "                                                    coc  \n",
              "0     satisfied...device times...device...can't expe...  \n",
              "1                cheap, wobbling unstable unstable lids  \n",
              "2     cheap processing, wobbling unstable unstable f...  \n",
              "3     independently time open incredible unreasonabl...  \n",
              "4                iron remain, small waffles brown, six.  \n",
              "...                                                 ...  \n",
              "1550  handle price quality relationship good. non -s...  \n",
              "1551                  easy, heat materials good quality  \n",
              "1552  easy clean, materials good quality plasticucho...  \n",
              "1553  easy clean, heat good quality plasticuchos. go...  \n",
              "1554         materials good quality plasticuchos. good.  \n",
              "\n",
              "[1555 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e8efb3ee-97b5-4027-ba73-a5e56140349f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>aspectTerm</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>SP</th>\n",
              "      <th>text_wo_sw</th>\n",
              "      <th>coc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142087</td>\n",
              "      <td>I am very satisfied with it ... I have used th...</td>\n",
              "      <td>price</td>\n",
              "      <td>82</td>\n",
              "      <td>87</td>\n",
              "      <td>positive</td>\n",
              "      <td>satisfied ... device times ... price device .....</td>\n",
              "      <td>satisfied...device times...device...can't expe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>processing</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap, wobbling unstable unstable lids</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>lids</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap processing, wobbling unstable unstable f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>baking time</td>\n",
              "      <td>142</td>\n",
              "      <td>153</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>independently time open incredible unreasonabl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142091</td>\n",
              "      <td>The iron does not remain, the waffles are too ...</td>\n",
              "      <td>waffles</td>\n",
              "      <td>30</td>\n",
              "      <td>37</td>\n",
              "      <td>negative</td>\n",
              "      <td>iron remain, waffles small waffles brown, six.</td>\n",
              "      <td>iron remain, small waffles brown, six.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1550</th>\n",
              "      <td>145346</td>\n",
              "      <td>It is very simple to handle and the price qual...</td>\n",
              "      <td>coating</td>\n",
              "      <td>81</td>\n",
              "      <td>88</td>\n",
              "      <td>positive</td>\n",
              "      <td>simple handle price quality relationship good....</td>\n",
              "      <td>handle price quality relationship good. non -s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1551</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>clean</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy, heat materials good quality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1552</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>heat</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy clean, materials good quality plasticucho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1553</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>materials</td>\n",
              "      <td>35</td>\n",
              "      <td>44</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy clean, heat good quality plasticuchos. go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1554</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>price</td>\n",
              "      <td>123</td>\n",
              "      <td>128</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>materials good quality plasticuchos. good.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1555 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8efb3ee-97b5-4027-ba73-a5e56140349f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e8efb3ee-97b5-4027-ba73-a5e56140349f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e8efb3ee-97b5-4027-ba73-a5e56140349f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 1000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 10\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(df['coc'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e75c826-a753-4c12-f291-80a909c1a8b3",
        "id": "IO4thXHgfZmI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1765 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = tokenizer.texts_to_sequences(df['coc'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70a1934-5dd9-48a7-e3d6-5f279e11c98f",
        "id": "OZJiq_OCfZmJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data tensor: (1555, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(df['SP']).values\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e098131-91ba-41a5-ffb3-9a2ef415f697",
        "id": "13SSXS6wfZmJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (1555, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5ed89c-24b9-4521-8929-971ed95a4c19",
        "id": "5HcCEnQHfZmJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 29)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90042a83-3291-4088-d744-9c38b3f70110",
        "id": "jwtQfgI0fZmJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1244, 10) (1244, 3)\n",
            "(311, 10) (311, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first layer is the embedded layer that uses 100 length vectors to represent each word.\n",
        "SpatialDropout1D performs variational dropout in NLP models.\n",
        "The next layer is the LSTM layer with 100 memory units.\n",
        "The output layer must create 3 output values, one for each class.\n",
        "Activation function is softmax for multi-class classification.\n",
        "Because it is a multi-class classification problem, categorical_crossentropy is used as the loss function.\n"
      ],
      "metadata": {
        "id": "1NN2yet2fZmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a8f548-baa3-4547-82a9-323f16c39541",
        "id": "6NhTlOj9fZmK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, 10, 100)           100000    \n",
            "                                                                 \n",
            " spatial_dropout1d_10 (Spati  (None, 10, 100)          0         \n",
            " alDropout1D)                                                    \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 100)               60600     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160,903\n",
            "Trainable params: 160,903\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam = k.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999)\n",
        "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=[get_f1])"
      ],
      "metadata": {
        "id": "_Fgq4DqGfZmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the best only\n",
        "\n",
        "filepath=\"/content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 5, verbose = 1,restore_best_weights = True)\n",
        "callbacks_list = [earlystop, checkpoint]"
      ],
      "metadata": {
        "id": "isqVG0rafZmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=callbacks_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa85e53-68f3-40d8-bf35-f1d9e0eee36f",
        "id": "9MbTvXXmfZmL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0950 - get_f1: 0.0000e+00\n",
            "Epoch 1: val_loss improved from inf to 1.08665, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 504ms/step - loss: 1.0950 - get_f1: 0.0000e+00 - val_loss: 1.0867 - val_get_f1: 0.0000e+00\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0781 - get_f1: 0.0000e+00\n",
            "Epoch 2: val_loss improved from 1.08665 to 1.07116, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 279ms/step - loss: 1.0781 - get_f1: 0.0000e+00 - val_loss: 1.0712 - val_get_f1: 0.0000e+00\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0604 - get_f1: 0.0000e+00\n",
            "Epoch 3: val_loss improved from 1.07116 to 1.05286, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 329ms/step - loss: 1.0604 - get_f1: 0.0000e+00 - val_loss: 1.0529 - val_get_f1: 0.0000e+00\n",
            "Epoch 4/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.0383 - get_f1: 0.0000e+00\n",
            "Epoch 4: val_loss improved from 1.05286 to 1.02939, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 280ms/step - loss: 1.0371 - get_f1: 0.0000e+00 - val_loss: 1.0294 - val_get_f1: 0.0000e+00\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0076 - get_f1: 0.0000e+00\n",
            "Epoch 5: val_loss improved from 1.02939 to 0.99605, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 283ms/step - loss: 1.0076 - get_f1: 0.0000e+00 - val_loss: 0.9961 - val_get_f1: 0.0000e+00\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9653 - get_f1: 0.0503\n",
            "Epoch 6: val_loss improved from 0.99605 to 0.95201, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.9653 - get_f1: 0.0503 - val_loss: 0.9520 - val_get_f1: 0.3012\n",
            "Epoch 7/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.9162 - get_f1: 0.4778\n",
            "Epoch 7: val_loss improved from 0.95201 to 0.90683, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 289ms/step - loss: 0.9149 - get_f1: 0.4883 - val_loss: 0.9068 - val_get_f1: 0.6000\n",
            "Epoch 8/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.8711 - get_f1: 0.6287\n",
            "Epoch 8: val_loss improved from 0.90683 to 0.88631, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 276ms/step - loss: 0.8686 - get_f1: 0.6323 - val_loss: 0.8863 - val_get_f1: 0.5919\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8505 - get_f1: 0.6197\n",
            "Epoch 9: val_loss improved from 0.88631 to 0.86648, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 280ms/step - loss: 0.8505 - get_f1: 0.6197 - val_loss: 0.8665 - val_get_f1: 0.5958\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8295 - get_f1: 0.6311\n",
            "Epoch 10: val_loss improved from 0.86648 to 0.84903, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 0.8295 - get_f1: 0.6311 - val_loss: 0.8490 - val_get_f1: 0.5972\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8098 - get_f1: 0.6295\n",
            "Epoch 11: val_loss improved from 0.84903 to 0.83626, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 285ms/step - loss: 0.8098 - get_f1: 0.6295 - val_loss: 0.8363 - val_get_f1: 0.5958\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7920 - get_f1: 0.6285\n",
            "Epoch 12: val_loss improved from 0.83626 to 0.81994, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 278ms/step - loss: 0.7920 - get_f1: 0.6285 - val_loss: 0.8199 - val_get_f1: 0.5985\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7758 - get_f1: 0.6360\n",
            "Epoch 13: val_loss improved from 0.81994 to 0.80385, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.7758 - get_f1: 0.6360 - val_loss: 0.8039 - val_get_f1: 0.6062\n",
            "Epoch 14/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.7559 - get_f1: 0.6360\n",
            "Epoch 14: val_loss improved from 0.80385 to 0.78901, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 282ms/step - loss: 0.7524 - get_f1: 0.6417 - val_loss: 0.7890 - val_get_f1: 0.6074\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7236 - get_f1: 0.6459\n",
            "Epoch 15: val_loss improved from 0.78901 to 0.76940, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 281ms/step - loss: 0.7236 - get_f1: 0.6459 - val_loss: 0.7694 - val_get_f1: 0.6151\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7026 - get_f1: 0.6666\n",
            "Epoch 16: val_loss improved from 0.76940 to 0.74825, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 282ms/step - loss: 0.7026 - get_f1: 0.6666 - val_loss: 0.7482 - val_get_f1: 0.6263\n",
            "Epoch 17/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6699 - get_f1: 0.6729\n",
            "Epoch 17: val_loss improved from 0.74825 to 0.72870, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 322ms/step - loss: 0.6697 - get_f1: 0.6718 - val_loss: 0.7287 - val_get_f1: 0.6324\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6420 - get_f1: 0.6837\n",
            "Epoch 18: val_loss improved from 0.72870 to 0.70882, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 283ms/step - loss: 0.6420 - get_f1: 0.6837 - val_loss: 0.7088 - val_get_f1: 0.6397\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6165 - get_f1: 0.6938\n",
            "Epoch 19: val_loss improved from 0.70882 to 0.69006, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 285ms/step - loss: 0.6165 - get_f1: 0.6938 - val_loss: 0.6901 - val_get_f1: 0.6447\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5997 - get_f1: 0.6944\n",
            "Epoch 20: val_loss improved from 0.69006 to 0.67299, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.5997 - get_f1: 0.6944 - val_loss: 0.6730 - val_get_f1: 0.6447\n",
            "Epoch 21/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5663 - get_f1: 0.7162\n",
            "Epoch 21: val_loss improved from 0.67299 to 0.65552, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 277ms/step - loss: 0.5687 - get_f1: 0.7173 - val_loss: 0.6555 - val_get_f1: 0.6662\n",
            "Epoch 22/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5569 - get_f1: 0.7424\n",
            "Epoch 22: val_loss improved from 0.65552 to 0.63828, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 280ms/step - loss: 0.5581 - get_f1: 0.7439 - val_loss: 0.6383 - val_get_f1: 0.7122\n",
            "Epoch 23/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5339 - get_f1: 0.7719\n",
            "Epoch 23: val_loss improved from 0.63828 to 0.62380, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 346ms/step - loss: 0.5285 - get_f1: 0.7778 - val_loss: 0.6238 - val_get_f1: 0.7253\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5062 - get_f1: 0.8034\n",
            "Epoch 24: val_loss improved from 0.62380 to 0.61357, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.5062 - get_f1: 0.8034 - val_loss: 0.6136 - val_get_f1: 0.7477\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4926 - get_f1: 0.8243\n",
            "Epoch 25: val_loss improved from 0.61357 to 0.59767, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 282ms/step - loss: 0.4926 - get_f1: 0.8243 - val_loss: 0.5977 - val_get_f1: 0.7531\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4765 - get_f1: 0.8327\n",
            "Epoch 26: val_loss improved from 0.59767 to 0.58678, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 277ms/step - loss: 0.4765 - get_f1: 0.8327 - val_loss: 0.5868 - val_get_f1: 0.7703\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4617 - get_f1: 0.8348\n",
            "Epoch 27: val_loss improved from 0.58678 to 0.57967, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 0.4617 - get_f1: 0.8348 - val_loss: 0.5797 - val_get_f1: 0.7805\n",
            "Epoch 28/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4409 - get_f1: 0.8453\n",
            "Epoch 28: val_loss improved from 0.57967 to 0.56770, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 281ms/step - loss: 0.4395 - get_f1: 0.8466 - val_loss: 0.5677 - val_get_f1: 0.7954\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4386 - get_f1: 0.8382\n",
            "Epoch 29: val_loss improved from 0.56770 to 0.55616, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 281ms/step - loss: 0.4386 - get_f1: 0.8382 - val_loss: 0.5562 - val_get_f1: 0.8045\n",
            "Epoch 30/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4286 - get_f1: 0.8480\n",
            "Epoch 30: val_loss did not improve from 0.55616\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.4295 - get_f1: 0.8476 - val_loss: 0.5574 - val_get_f1: 0.8077\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4150 - get_f1: 0.8532\n",
            "Epoch 31: val_loss improved from 0.55616 to 0.55083, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.4150 - get_f1: 0.8532 - val_loss: 0.5508 - val_get_f1: 0.8040\n",
            "Epoch 32/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4003 - get_f1: 0.8559\n",
            "Epoch 32: val_loss improved from 0.55083 to 0.54946, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 286ms/step - loss: 0.3934 - get_f1: 0.8622 - val_loss: 0.5495 - val_get_f1: 0.8059\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3879 - get_f1: 0.8557\n",
            "Epoch 33: val_loss improved from 0.54946 to 0.54655, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 279ms/step - loss: 0.3879 - get_f1: 0.8557 - val_loss: 0.5466 - val_get_f1: 0.8099\n",
            "Epoch 34/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3887 - get_f1: 0.8442\n",
            "Epoch 34: val_loss did not improve from 0.54655\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.3890 - get_f1: 0.8447 - val_loss: 0.5511 - val_get_f1: 0.8076\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3722 - get_f1: 0.8608\n",
            "Epoch 35: val_loss improved from 0.54655 to 0.54089, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.3722 - get_f1: 0.8608 - val_loss: 0.5409 - val_get_f1: 0.8072\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3661 - get_f1: 0.8638\n",
            "Epoch 36: val_loss did not improve from 0.54089\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.3661 - get_f1: 0.8638 - val_loss: 0.5441 - val_get_f1: 0.8099\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3480 - get_f1: 0.8694\n",
            "Epoch 37: val_loss did not improve from 0.54089\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.3480 - get_f1: 0.8694 - val_loss: 0.5474 - val_get_f1: 0.8083\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3513 - get_f1: 0.8707\n",
            "Epoch 38: val_loss improved from 0.54089 to 0.53472, saving model to /content/drive/MyDrive/Colab Notebooks/MA/gru/checkpoint_sp/\n",
            "16/16 [==============================] - 4s 280ms/step - loss: 0.3513 - get_f1: 0.8707 - val_loss: 0.5347 - val_get_f1: 0.8094\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3401 - get_f1: 0.8754\n",
            "Epoch 39: val_loss did not improve from 0.53472\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.3401 - get_f1: 0.8754 - val_loss: 0.5517 - val_get_f1: 0.8042\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3386 - get_f1: 0.8674\n",
            "Epoch 40: val_loss did not improve from 0.53472\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.3386 - get_f1: 0.8674 - val_loss: 0.5406 - val_get_f1: 0.8081\n",
            "Epoch 41/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3398 - get_f1: 0.8681\n",
            "Epoch 41: val_loss did not improve from 0.53472\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.3355 - get_f1: 0.8700 - val_loss: 0.5517 - val_get_f1: 0.8046\n",
            "Epoch 42/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3258 - get_f1: 0.8747\n",
            "Epoch 42: val_loss did not improve from 0.53472\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.3292 - get_f1: 0.8715 - val_loss: 0.5467 - val_get_f1: 0.8140\n",
            "Epoch 43/100\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3236 - get_f1: 0.8763Restoring model weights from the end of the best epoch: 38.\n",
            "\n",
            "Epoch 43: val_loss did not improve from 0.53472\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.3342 - get_f1: 0.8686 - val_loss: 0.5401 - val_get_f1: 0.8154\n",
            "Epoch 43: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 26: val_loss did not improve from 0.61862\n",
        "16/16 [==============================] - 2s 126ms/step - loss: 0.3744 - get_f1: 0.8567 - val_loss: 0.6383 - val_get_f1: 0.7855\n",
        "Epoch 26: early stopping"
      ],
      "metadata": {
        "id": "ZSUEKuMwfZmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "plot_graphs(history, 'get_f1')\n",
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "XjTpyKf_fZmL",
        "outputId": "b85e0ae6-3286-4cab-faf6-1edd4995bf6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3gU5fbA8e/ZTQNSaKEm9E6QFnpRUZEioCBSVERRFGk2FK96r41rb1yQohcFpAqKqAhyEaUGCaG3UKQktBAgBEL6+/tjVs0PAgTIZpLs+TzPPrs7OzN7Msqemfed97xijEEppZTnctgdgFJKKXtpIlBKKQ+niUAppTycJgKllPJwmgiUUsrDedkdwLUqXbq0qVKlit1hKKVUgbJhw4aTxpjg7D4rcImgSpUqREZG2h2GUkoVKCJy8HKfadOQUkp5OE0ESinl4TQRKKWUhytwfQRKKc+UlpZGTEwMycnJdoeSr/n5+RESEoK3t3eOt3FbIhCRKcBdwAljTFg2n9cBvgCaAC8ZY953VyxKqYIvJiaGgIAAqlSpgojYHU6+ZIwhPj6emJgYqlatmuPt3Nk09CXQ6QqfnwJGAJoAlFJXlZycTKlSpTQJXIGIUKpUqWu+anJbIjDGrMD6sb/c5yeMMeuBNHfFoJQqXDQJXN31HKMC0VksIoNFJFJEIuPi4q5rH/HnUnjjhx0kJGneUUqprApEIjDGTDbGhBtjwoODsx0Yd1Wr98Xzxeo/uPWDX5kbeZjMTJ2HQSl1bfz9/e0OwS0KRCLIDd3rBLL6tj+oVcqH5+dt4d6Ja9gWm2B3WEopZTuPSQRs/5byq15iVvpTzGpznEPx5+k+bhX//G6bNhcppa6JMYZRo0YRFhZGgwYNmDNnDgBHjx6lffv2NGrUiLCwMFauXElGRgYDBw78a92PPvrI5ugv5c7bR2cBtwClRSQG+BfgDWCMmSgi5YBIIBDIFJGngHrGmLNuCajxA+BfFln6Cq02PE1ExeZ8WWwQ/444yI9bjjK6cx16NQnB4dDOKKXyu9e+386OI7n7U1GvQiD/6lY/R+t+8803bNq0ic2bN3Py5EmaNWtG+/btmTlzJnfeeScvvfQSGRkZJCUlsWnTJmJjY9m2bRsAZ86cydW4c4PbEoExpt9VPj8GhLjr+y8hArU6QvUOsOkrvJb/m0djH6d3nS7842wvRs3bwteRMfy7Zxg1ygTkWVhKqYJn1apV9OvXD6fTSdmyZbn55ptZv349zZo145FHHiEtLY27776bRo0aUa1aNfbv38/w4cPp2rUrHTt2tDv8S3jeyGKnFzQdCGH3wtpxBK3+hHEZSxlZ5z4eOXA7nT85zZCbq/PkrTXw83baHa1SKhs5PXPPa+3bt2fFihX8+OOPDBw4kGeeeYYBAwawefNmlixZwsSJE5k7dy5TpkyxO9T/x3P6CC7m6w+3jIYRG5FG/al1cBYrir3A6Cp7GfvLXrp8spI1+07aHaVSKh9q164dc+bMISMjg7i4OFasWEHz5s05ePAgZcuW5bHHHuPRRx8lKiqKkydPkpmZSa9evXjzzTeJioqyO/xLeN4VwcUCykH3sRD+CI7vhjEo9hW61+rMo3F96P/ZOu5tGsJLXepSopiP3ZEqpfKJe+65h7Vr19KwYUNEhHfffZdy5coxdepU3nvvPby9vfH392fatGnExsby8MMPk5mZCcBbb71lc/SXEmMK1v304eHhxm0T02SkweqP4bd3MT7F+KHCSJ7eWYvAIj6MuTuMzg3Ku+d7lVJXtXPnTurWrWt3GAVCdsdKRDYYY8KzW99zm4ay4/SG9qPg8ZVIqRp02/cqm2p8TuOg8wyZEcVHS6N1IJpSqtDRRJCdMnXgkSXQ6W38j0bw+bmhvFV9G58s28PwWRu5kJphd4RKKZVrNBFcjsMJLYfAkDVI+Ub0i/0339X6icXbYuk9aQ1HEy7YHaFSSuUKTQRXU7IqDFgAzR6l4aHpRFSdwvG4k/QYt5pNh/PfwBCllLpWmghywukNXT+ALu8TfPQ3VpV+m1DHSfpMWsvCzUfsjk4ppW6IJoJr0fwxeGAevueP8rXzJXqXOcKIWRv5aGk0Be3uK6WU+pMmgmtVvQM8+j8cfoG8kTCaf1ffzifL9jB22V67I1NKqeuiieB6BNeCR5choS3oHzuGz0KX8NH/ovl85X67I1NK5RNXmrvgwIEDhIVdMpW7bTQRXK+iJeHBb6HxA9wRN5V3QiN488edzFx3yO7IlFLqmmiJiRvh9IZuYyHpFPdFjyemcnleWgBFfBzc0zjvCqsq5XF+Gg3HtubuPss1gM5vX/bj0aNHExoaytChQwF49dVX8fLyYvny5Zw+fZq0tDTefPNNevTocU1fm5yczJAhQ4iMjMTLy4sPP/yQW2+9le3bt/Pwww+TmppKZmYm8+fPp0KFCtx3333ExMSQkZHBK6+8Qp8+fW7ozwZNBDfO4YSenyFTOvHMmX9zNOQDnvt6C0W8vegUVs7u6JRSuaRPnz489dRTfyWCuXPnsmTJEkaMGEFgYCAnT56kZcuWdO/e/ZomkB8/fjwiwtatW9m1axcdO3YkOjqaiRMnMnLkSO6//35SU1PJyMhg0aJFVKhQgR9//BGAhITcmWVRE0Fu8PWHfrOQzzrwbtoYTlZ4m+GzovhsQDi31C5jd3RKFT5XOHN3l8aNG3PixAmOHDlCXFwcJUqUoFy5cjz99NOsWLECh8NBbGwsx48fp1y5nJ8Erlq1iuHDhwNQp04dKleuTHR0NK1atWLMmDHExMTQs2dPatasSYMGDXj22Wd54YUXuOuuu2jXrl2u/G3aR5BbiodCv1k4zh3nc79PqBvsx+PTN7Buf7zdkSmlcknv3r2ZN28ec+bMoU+fPsyYMYO4uDg2bNjApk2bKFu2LMnJybnyXf3792fhwoUUKVKELl268Msvv1CrVi2ioqJo0KABL7/8Mq+//nqufJfbEoGITBGREyKy7TKfi4iMFZG9IrJFRJq4K5Y8ExIOd0/AKyaCuRVmEVqiCIOmRvLHyfN2R6aUygV9+vRh9uzZzJs3j969e5OQkECZMmXw9vZm+fLlHDx48Jr32a5dO2bMmAFAdHQ0hw4donbt2uzfv59q1aoxYsQIevTowZYtWzhy5AhFixblgQceYNSoUbk2t4E7rwi+BDpd4fPOQE3XYzAwwY2x5J2wnnDLP/DbMZdvblqHQ2DU15vJ0KqlShV49evXJzExkYoVK1K+fHnuv/9+IiMjadCgAdOmTaNOnTrXvM8nn3ySzMxMGjRoQJ8+ffjyyy/x9fVl7ty5hIWF0ahRI7Zt28aAAQPYunUrzZs3p1GjRrz22mu8/PLLufJ3uXU+AhGpAvxgjLnkhlkRmQT8aoyZ5Xq/G7jFGHP0Svt063wEucUYmP8obJvH2vCP6beqDP+8qx6PtK1qd2RKFVg6H0HOFaT5CCoCh7O8j3Etu4SIDBaRSBGJjIuLy5PgbogI9BgPIc1ouelF+ldP4d0luzigTURKqXyoQHQWG2MmG2PCjTHhwcHBdoeTM95+0OcrxMuHV5mMjxOen7dFJ7ZRyoNs3bqVRo0a/b9HixYt7A7rEnbePhoLhGZ5H+JaVngElIOOb+KzcDhTGtzJvetrMW3tAQa20SYipa6HMeaa7tG3W4MGDdi0aVOefuf1NPfbeUWwEBjgunuoJZBwtf6BAqnxg1ClHU2jP6RHdQfvLN7NwXhtIlLqWvn5+REfH6+Vfq/AGEN8fDx+fn7XtJ3bOotFZBZwC1AaOA78C/AGMMZMFCutj8O6sygJeNgYc9Ve4ALRWXyx+H3waSsuVLuD5tEDqFchkFmPtcThKDhnNkrZLS0tjZiYmFy7T7+w8vPzIyQkBG9v7/+3/EqdxW5rGjLG9LvK5wYY6q7vz1dKVYebn6fIL28wIbwjD6xOZ8a6gzzYqordkSlVYHh7e1O1qjarukOB6CwuFNqMhDL1aRP9Nh1rFOWtn3Zx+FSS3VEppZQmgjzj9Ibu/0ESj/JRyQU4RPQuIqVUvqCJIC+FNIUWT1Bsy1Q+ap3C2v3xfL3h8NW3U0opN9JEkNc6vAxBIdy+dwwtKhXj/Z+jOZ+SbndUSikPpokgr/n6Q9cPkbhdfFTxV+ISU5i0Qqe4VErZRxOBHWp1hLBeVNgynkG1U5m8Yh/HEvSWOKWUPTQR2KXTO+BdlFFpE8nMNHzw8267I1JKeShNBHbxD4Y7XsPvSAQf1trOvKgYth/JnWnnlFLqWmgisFPjARDaki7HPqWy3wX+vWinDp9XSuU5TQR2cjig28c4Us7yWfnvWL03nl93F4Ay20qpQkUTgd3K1IXWw6l5ZCF3l/iDMYt2kp6RaXdUSikPookgP2j/PBSvzJte/+XQidPMidRBZkqpvKOJID/wKQpdP8A/cT+vl17GR0ujSUxOszsqpZSH0ESQX9S8A+rdzX0X5lDs/CEm/aaDzJRSeUMTQX7S6W0cXr5MKjGTz1bu48iZC3ZHpJTyAJoI8pPA8nDbP6mTFEkXWcM7i3fZHZFSygO4NRGISCcR2S0ie0VkdDafVxaRZSKyRUR+FZEQd8ZTIIQ/AhWa8KbfDJZv2sPaffF2R6SUKuTclghExAmMBzoD9YB+IlLvotXeB6YZY24CXgfeclc8BYbDCd0+pmj6GSYX/ZT3FqwlTW8nVUq5kTuvCJoDe40x+40xqcBsoMdF69QDfnG9Xp7N556pfEOk6wc0Zxv/OTuCRT/MtzsipVQh5s5EUBHIekN8jGtZVpuBnq7X9wABIlLKjTEVHOGP4Hh0KV7eftwV9RhnF78JmRl2R6WUKoTs7ix+DrhZRDYCNwOxwCW/diIyWEQiRSQyLs6DSjBUaEzao7/yg2lDYMR7MLU7JMTaHZVSqpBxZyKIBUKzvA9xLfuLMeaIMaanMaYx8JJr2ZmLd2SMmWyMCTfGhAcHB7sx5PwnpFwZYm79hGdSnyAjNgomtoFdi+wOSylViLgzEawHaopIVRHxAfoCC7OuICKlReTPGF4EprgxngLr0XZV2VSyMw95v0dmYCjM7geLRkGajjNQSt04tyUCY0w6MAxYAuwE5hpjtovI6yLS3bXaLcBuEYkGygJj3BVPQebr5eT1HmGsOl2CT6tPhJZPwu+TYfKtcGyr3eEppQo4KWj178PDw01kZKTdYdhi6Mwo/rfjOEufvplKp9fAgifhwmm47Z/QcqhV1loppbIhIhuMMeHZfaa/HAXIK13r4eUQ/rVwG6b6bTBkLdTsCD+/DNN7aEeyUuq6aCIoQMoF+fH0HbVYvjuOn3cch2KloM9X0G0sxETChNaw/Vu7w1RKFTCaCAqYh1pXoXbZAF6Yv4XvNsViAJo+BE+sgpLV4OuB8O0TkHTK5kiVUgWFJoICxtvpYOKDTalcqhgjZ29i0NRIq0ppqeow6GdoPwq2zIFxzWDLXChgfUBKqbyniaAAqlq6GN8Mac0rd9Vj7b54On60gukRB8kUL+jwMgz+FUpUhm8eg+l3Q/w+u0NWSuVjmggKKKdDGNS2Kj8/3Z5GocV5ZcE2+k6OYF/cOSjfEAYthS7vQ2wUfNoKfnsX0lPsDlsplQ9pIijgQksWZfqg5rx3703sPp5I509WMn75XjJwQPPHYNh6qNMVlo+BCW3gj5V2h6yUymc0ERQCIkLv8FCWPtOeO+qW5b0lu7n/8whOnE2GgHLQ+wu4fz5kpMLUu+Cre+GPFdp/oJQCdEBZoTR/QwwvL9hGMV8nH/VpRLuarvpMqUkQMR7WTYLzcVC+EbQeDvXuBqeXvUErpdzqSgPKNBEUUnuOJzJ0ZhR7Tpxj2K01GHlbTbycrgvAtGTYMhvWjIP4PRBUCVo9CY0fBF9/ewNXSrmFJgIPdSE1g38t3MbcyBhaVC3J2H6NKRvo9/cKmZmwZwmsHguH1oBfELQaZl0leBexL3ClVK7TRODh/mwqKupjNRW1r5VNKe+YSFj1Eez6wbpC6PgG1OsBInkfsFIq12mtIQ/Xq2kI3w9vQyl/Hx764nfeWrSTlPSL5v8JCYe+M+ChH8AvEL5+CKZ2g2Pb7AlaKZVnNBF4iBplAvhuaFv6NqvEpBX7uXv8GnYfS7x0xartYPBv0PVDOL4dJrWDH57RkhVKFWKaCDxIER8nb/VswOcDwjlxNplu41bx31V/kJl5UfOg0wuaDYLhG6DZY7DhSxjb2LrbKCPNltiVUu6jicAD3V6vLIufak+7GqV544cdPDhlHUcTspntrGhJ6PKuVdCufEP46XlrUNre/+V90Eopt9FE4KGCA3z5/KFw/n1PA6IOnqHTxyv5YcuR7FcuWw8GfAd9Z1qD0r7qBTN6w8k9eRu0Usot3JoIRKSTiOwWkb0iMjqbzyuJyHIR2SgiW0SkizvjUf+fiNC/RSUWjWxHldLFGDZzI8NnbSQuMZuaRCJWqYqh6+CO1+HgWvi0JSx+0ZolTSlVYLnt9lERcQLRwB1ADNZk9v2MMTuyrDMZ2GiMmSAi9YBFxpgqV9qv3j7qHmkZmXy6fB/jl++liI+Tl7rUpXd4CHK520fPnYBf3oSoaVCkBHR4CZo8BE7vvA1cKZUjdt0+2hzYa4zZb4xJBWYDPS5axwCBrtdBwGXaJpS7eTsdjLy9JotGtqV22QCen7+Ffp9FsD/uXPYb+JeB7mPh8RVQph78+CyMbw5b51kD1ZRSBYY7E0FF4HCW9zGuZVm9CjwgIjHAImB4djsSkcEiEikikXFxce6IVbnUKBPA7MEteatnA7YfOUunT1byn2V7SE2/zI97+Ztg4A/QbzZ4FYH5g2Bye9izVIvaKVVA2N1Z3A/40hgTAnQBpovIJTEZYyYbY8KNMeHBwdmMilW5yuEQ+jWvxLJnb6ZjvbJ8sDSarmNXsv7AZcYSiEDtzvDESuj5GSSfhRn3whdd4FBE3gavlLpm7kwEsUBolvchrmVZDQLmAhhj1gJ+QGk3xqSuQZkAP8b1b8KUgeEkpWbQe+Janpq9MftbTQEcTrjpPhgWaU2KE78XptwJM/vAqf15G7xSKsfcmQjWAzVFpKqI+AB9gYUXrXMIuA1AROpiJQJt+8lnOtQpy89Pt2d4hxos2naMDu//xthle0hOy8h+Ay8fa1KckZvgtn/CwTUwoS1smKrNRUrlQ24tOue6HfRjwAlMMcaMEZHXgUhjzELXnUKfAf5YHcfPG2N+vtI+9a4hex0+lcRbP+1k0dZjVCxehH90qUuXBuUuf3cRQEIMLBhiTYZTuwt0Gwv+2sSnVF7S6qMq10Xsj+e173ew8+hZmlcpyT+71SOsYtDlN8jMhHUT4H+vWUXtuo+D2p3yLmClPJxWH1W5rmW1UvwwvC3/vqcBe+PO0WP8aqauOcBlTywcDmg1FAb/Cv5lYVYf+H4kpJ7Py7CVUtnQRKCum9NhjUxe/twtdKhThn8t3M4r320jLeMK4wjK1oPHfoHWI6w+g4ltIWZD3gWtlLqEJgJ1w4KKeDPpgaY8cXN1voo4xMAvfich6QpVSr18rYlvBv5gVTOdcies/VQ7kpWyiSYClSscDmF05zp80Lsh6/84zT2frr78qOQ/VWlrjT2o2RGWvAhzHoALZ/ImYKXUXzQRqFzVq2kIMx9rQcKFNO4ev5rVe09eeYMiJayZ0TqOgejFMKk9xEblTbBKKUATgXKD8ColWTC0DeWDijBgyu9Mjzh45Q1EoPUwePgnyMywmorWTdamIqXyiCYC5RahJYsy/8nW3FIrmFcWbOPDpdE52Ki51VRU7Vb4aRR8PRCSE9weq1KeThOBcht/Xy8mDwinT3goY5ft4T/LcjCRTdGSVgG721+Dnd9bTUWH1rk/WKU8mCYC5VZOh/BWzwb0bFyRD5ZGM+m3fVffyOGAtk/Bw4vAZMIXnWDZG5Ce6v6AlfJAmgiU2zkcwnu9G9KtYQXe+mkXU1b9kbMNK7WEJ1ZDo/6w8n347+0Qt9u9wSrlgTQRqDzhdAgf3teQzmHleP2HHUxfeyBnG/oFQo/x0Ocrq2bRpPYQMVEnv1EqF+UoEYjISBEJFMt/RSRKRDq6OzhVuHg7HXzStzG31y3DK99tZ/bvh3K+cd1uMGQtVL0ZFr8AX/WEszqhnVK5IadXBI8YY84CHYESwIPA226LShVaPl4Oxt/fhJtrBfPit1uZvyEm5xsHlIX+c+Cuj+DwOvi0FWz/1n3BKuUhcpoI/qwx3AWYbozZnmWZUtfE18vJpAeb0qZ6aUbN28yc9ddwZSAC4Y/AE6ugVHXrFtNvh0BKotviVaqwy2ki2CAiP2MlgiUiEgBoI626bn7eTj4bEE7r6qV5Yf5WRn29mQupl5noJjulqsMjS6D987BltlW87vDv7gtYqUIsp4lgEDAaaGaMSQK8gYfdFpXyCEV8nHz5cDNGdKjBvKgYeoxfxZ7j13Bm7/SGDi/BQNdtplM6wfK3ICPdfUErVQjlNBG0AnYbY86IyAPAy4AO+VQ3zMvp4JmOtZn2SHPiz6XSfdxq5l1LvwFA5VZWU1GD3vDb29a4A50jWakcy2kimAAkiUhD4FlgHzDtahuJSCcR2S0ie0VkdDaffyQim1yPaBHR0pMeql3NYBaNbEfD0CCe+3ozz329maTUaziz9wuCnpPg3ikQFw0T28GGL7VekVI5kNNEkG6sqad6AOOMMeOBgCttICJOYDzQGagH9HPNUfwXY8zTxphGxphGwH+Ab671D1CFR9lAP2Y82pIRt9VkflQMPcatJvpamooAwnrBkNVQobE1A9pXvazxB0qpy8ppIkgUkRexbhv9UUQcWP0EV9Ic2GuM2W+MSQVmYyWSy+kHzMphPKqQcjqEZ+6oxbRHmnPqfCr3TljDvqvNa3Cx4qEwYCF0eR8ORVi3mUZN06sDpS4jp4mgD5CCNZ7gGBACvHeVbSoCh7O8j3Etu4SIVAaqAr9c5vPBIhIpIpFxcXE5DFkVZO1qBvPtk23wdjp45Mv1nDp/jXWGHA5o/ph1dVC+ISwcDjPuhYRY9wSsVAGWo0Tg+vGfAQSJyF1AsjHmqn0E16AvMM8Yk+39g8aYycaYcGNMeHBwcC5+rcrPKpUqyuQB4RxNSObx6ZGkpF/D7aV/KlnVujro/B4cXAOftoSo6Xp1oFQWOS0xcR/wO9AbuA9YJyL3XmWzWCA0y/sQ17Ls9EWbhVQ2mlYuYU1/eeA0o+dvxVzPD7jDAS0GW1cH5RrAwmEw/R44uTf3A1aqAMpp09BLWGMIHjLGDMBq/3/lKtusB2qKSFUR8cH6sV948UoiUgerbMXanIetPEm3hhV49o5afLsxlrHLbuDHu2Q1eOgHq+8gNgomtIJf3oTUpNwLVqkCKKeJwGGMOZHlffzVtjXGpAPDgCXATmCuMWa7iLwuIt2zrNoXmG2u61RPeYphHWrQq0kIH/0vmu823UA7/599B8PWQ/17YMV78GkL2L0494JVqoCRnPz+ish7wE383XzTB9hijHnBjbFlKzw83ERGRub116p8IDU9kwf/u46Nh84w87EWhFcpeeM7/WMlLHoO4nZB7a7Q+W0oXunG96tUPiMiG4wx4dl+ltMTcRHpBbRxvV1pjLGl7KMmAs92+nwqPSesIeFCGt8+2ZrKpYrd+E7TUyHiU/jtHasT+eZR0Go4ePnc+L6VyidyJRHkF5oI1B8nz3PPp6spWcyHuY+3orS/b+7s+MxhWPKiNVdycB2462OrfIVShcCVEsEV2/lFJFFEzmbzSBSRs+4JV6krq1q6GJMfDOfImQv0mxxBXGJK7uy4eKg1E1q/OZB63qpZtHA4JJ3Knf0rlU9drcM3wBgTmM0jwBgTmFdBKnWx5lVL8sXA5sScvkDfyWs5cTY593ZeuxMMXQetR8DGGTCuGWyeo2MPVKGlcxarAqtV9VJMfaQ5RxOS6Ts5gmMJuZgMfIpBxzfg8d+gRBX4djBM66FjD1ShpIlAFWjNq5Zk2iPNOX42mb6T13I04ULufkG5BjBoKXT9EI5sssYeLH8L0nIx6ShlM00EqsALr1KSaYNaEH8ulT6TIog9k8vJwOGAZoNg2O9Qt7s158GEVrB3We5+j1I20USgCoWmlUsw/dEWnE5Kpc+ktRw+5YbRwgHl4N7/woMLAIGvelpzJp89mvvfpVQe0kSgCo1GocWZ8WgLzl5Io+/kCA7Gn3fPF1W/FYasgVtfgl2LrM7kiIk6RaYqsDQRqELlppDizHysJUmp6fSasJZtsW6aUdXbD25+HoZGQKUWsPgF+OxW+GOFe75PKTfSRKAKnbCKQXz9RGt8nELfyRGs2XfSfV9WshrcPw96T4WkeJjazXocWue+71Qql2kiUIVSjTL+zH+yNRWK+zFwynoWbXVjO74I1L8bhkdBp7fhxE6Y0hFm9LbuNFIqn9NEoAqt8kFF+Prx1twUEsTQmVFMjzjo3i/09oOWQ2DkZrj9VTj8O0y+GeY8AMd3uPe7lboBmghUoRZU1Jvpg1pwW50yvLJgGx8ujb6+yW2uhU8xaPs0PLUFbnkR9v0KE1rDzD6w/Vsdg6DyHS06pzxCekYm//h2K3MjY+jfohJv9AjD6ZC8+fKkU1Z1040zIPEI+AZB/R5wU1+o1Moap6CUm2n1UaUAYwzvLdnNp7/uo0XVkrxxdxi1ygbkXQCZGXBgJWyeDTsWQtp5a+6Dm/pAo/ut+ZWVchNNBEplMXf9YcYs2sn5lHQeblOFkbfXwt/XK2+DSD0Pu36EzbNg/68gDmj2KNz8AhTNhQl3lLqIbYlARDoBnwBO4HNjzNvZrHMf8CpggM3GmP5X2qcmApUbTp1P5d3Fu5i9/jBlAnx5qWtdujesgEgeNRdldfaINSlO1DTwDbT6FZoNAqd33seiCi1bEoGIOIFo4A4gBmsy+37GmB1Z1qkJzAU6GGNOi0iZi+ZGvoQmApWbNh46zT+/287W2AR7mouyOr4dlvzDukIoVRM6vgm17rRuT1XqBl33xDQ3qDmw1xiz3xiTCswGely0zmPAeGPMaYCrJQGlclvjSiVYMLQNY+4JY9exRDp/spIPf97t/juLslO2vlXHqN8cwMCsPjD9bji2Le9jUR7FnbIiVjgAABczSURBVImgInA4y/sY17KsagG1RGS1iES4mpIuISKDRSRSRCLj4uLcFK7yVE6HcH+Lyix/7hZ6NKzA2F/28p9fbJp3QMSaGOfJCOj0jjUgbWIb+KKrNTlOWi5XVlUK+8cReAE1gVuAfsBnIlL84pWMMZONMeHGmPDg4OA8DlF5ipLFfPjgvob0ahLCh0ujmbnukH3BOL2h5RMwYiN0eAXOxlqT47xfG358Vkcsq1zlzlslYoHQLO9DXMuyigHWGWPSgD9EJBorMax3Y1xKXZaI8HavBpw6n8LLC7ZSspg3ncLK2xdQ0ZLQ/jlo+wwcXG11KG/8CtZ/bk2a03iA1Y9QorJ9MaoCz52dxV5YncW3YSWA9UB/Y8z2LOt0wupAfkhESgMbgUbGmPjL7Vc7i1VeSEpN5/7P17H9yFmmPdKcltVK2R3S3y6chq3zrKRwbIu1rHglqNIeqraDKm0hKMTeGFW+Y+fto12Aj7FuH51ijBkjIq8DkcaYhWLdq/cB0AnIAMYYY2ZfaZ+aCFReOX0+ld6T1nI8IZk5j7eiXoVAu0O61Ild8MdvVvnrg6utJAFQoqqVFOr1gGoddPSy0gFlSl2vI2cu0GvCGtIzDd8MaU1oyaJ2h3R5mZlwYjv8sdIawXxgNaQkQKka0HwwNOwHfvkwmak8oYlAqRsQfTyR3hPXUqKoN/OGtKa0v6/dIeVMegrs+A7WTYLYSPDxh0b9raRQuqbd0ak8polAqRu04eAp7v98HVVL+zPqzlq0rxmMl7MANbfEboB1k2H7N5CRCtU7QPPHoeYd4HDaHZ3KA5oIlMoFy3ed4NmvN3PqfCplAnzp2SSE3uEhVA/2tzu0nDt3AjZMhcj/QuJRCKoETR+CJgPAv4zd0Sk30kSgVC5JTc/kl13H+Toyhl+j48jINDStXILeTUPoelN5AvwKSH2gjDSr6F3kf62OZoc31OsO4YOgcmsta1EIaSJQyg1OnE3m242xfL0hhr0nzlHE28kH9zWkSwMbxx1cj5N7IHIKbJoByQkQXBfCegJilcpOTXI9u15npFrlMEJbQKWWeiVRQGgiUMqNjDFsOnyGN3/cyebDZ5g8oCkd6pS1O6xrl5oE2+ZbVwlHNlrLHN7gUxS8i1kzr/kUtUpmH98BGSnWOiWrWxPsVGppPRevBF4+uReXMZB8BoqUyL193qjTB6xKsddaMvzCaeuW39K1oFjejk3RRKBUHjibnMb9n61j9/FEvhzYjNY1Stsd0vVLSQQvv8uXwk5PgaOb4dBaOBRhPf85hgHA6eNKHAHWs6+/9VykBARUgIByEOh6DihvPZw+cPoPiNsNJ3dDXLT1fHIPpJ6DsmFw033QoLe17Y0wxoo3Iw0CriFpnz8J//uXNbrb6QN1u0GTh6BKu8uP1TDGOj4bpsKOBZDumqrUv5x1ZVUuzPrbyta3qs7mZhLNQhOBUnnk9PlU+kxeS8zpC0wf1IKmlfPRWaw7ZWZC/B44vA4Sj1s/3KnnXM1J5yDF9Top3uqkTku6dB/iBJPx9/uAChBcC0rXBv9g2L3Yug0WgWo3WzO71e0GvtmUDU9PseoznT0CCTGQcBjOHHa9dj3Szlvr1uoMrYdfuW8kMwM2fAnLXrf+nhZPWElky2yrOa1EVWjyoDXTXEA5a5vzJ62Jh6KmwcloKyne1Btq3A6n9ltlx49vsxJfRqq1jdMHyjeCyq2sq6vQFrk2UZEmAqXy0Imzydw3aS3x51OZPbgl9SsE2R1S/mIMpJyFxGPWD3XiUeuRet4a/Fa6tjXOIbvBb/H7YMsc63H6AHgVgTpdoWgp64c/IcZ6Pp9NleJiwVbpjaAQCAq1HhdOWf0jSfFQobGVEOr2AGeWMmyxG1yF/jZaZ/5d3ocydazP0i7Azu+ts/2Dq6xkVquTdVa/8wfITIOQ5tadWfXvsa6KLpaRBvF7rcRwdLOVTGOjrG0Bguv83ex2A+VDNBEolcdiTidx38S1pKRnMufxltQoY9NkN4WVMXD4d+uMfPu31hl7YEUIquh6DrGeAyu4fvQrgneR7PeVmmSdua8dD6f2WbfUthwCde+ClR9aVwL+ZeHOMRDW6/JXDSf3wsZpsGmm9ePesJ91W27Zetf+96VdsBLPX01v66xR4q2GWXFcB00EStngj5Pn6T1xLU4HfP14ayqVysflKQoyY3LndtfMTIj+CdaMg0NrrGXitJqBbhmd8/Icma7mrdwcqJeZCXE7wbsolKx6XbvQRKCUTXYdO0vfyREE+Hkx9/FWlA+6zFmpyl9iNlhJof49ViduIWDXVJVKebw65QKZ+nBzTp9P4+7xq4k6dPrqGyn7hTSFDi8XmiRwNZoIlHKzhqHFmft4K3y8HPSdFGHvzGdKZUMTgVJ5oF6FQL4f1pYW1Uryj2+38uI3W0hJz7j6hkrlAU0ESuWR4kV9+PLh5jx5S3Vm/X6YPpMiOJaQbHdYSmkiUCovOR3C853qMOH+Juw5nshd/1nF73+csjss5eHcmghEpJOI7BaRvSIyOpvPB4pInIhscj0edWc8SuUXnRuUZ8HQNgT4edH/swgm/LqP5DRtKlL2cFsiEBEnMB7oDNQD+olIdiMr5hhjGrken7srHqXym5plA/huWBs61CnDO4t30fad5Uz6bR/nUtLtDk15GHdeETQH9hpj9htjUoHZQA83fp9SBU6gnzeTB4QzZ3BL6pYP4K2fdtH2nV8Yu2wPCRfS7A5PeQh3JoKKwOEs72Ncyy7WS0S2iMg8EQnNbkciMlhEIkUkMi4umxoiShVwLaqVYvqgFnz7ZGvCK5fgw6XRtHn7F95dvIv4cyl2h6cKObs7i78HqhhjbgKWAlOzW8kYM9kYE26MCQ8ODs7TAJXKS40rleDzh5qxaEQ7bq4dzITf9tHu3eWM+2WP9iEot3FnIogFsp7hh7iW/cUYE2+M+fN053OgqRvjUarAqFchkPH9m7D06fa0rxnM+z9Hc9sHv/HjlqMUtLIwKv9zZyJYD9QUkaoi4gP0BRZmXUFEss7p1x3Y6cZ4lCpwapQJYOKDTZn5WAsC/LwYOjOKPpMj2BabYHdoqhBxWyIwxqQDw4AlWD/wc40x20XkdRHp7lpthIhsF5HNwAhgoLviUaoga129ND+OaMeYe8LYe+Ic3catYvT8LcQlav+BunFafVSpAibhQhpjl+1h6poDFPF2Mrh9NR5pW5Vivl5X31h5LC1DrVQhtC/uHG8t2sX/dh6ntL8PQ2+tQf8WlfD1ysU6+KrQ0ESgVCG24eBp3luyi4j9p6hYvAgjb6tJzyYV8XLafVOgyk90PgKlCrGmlUsw67GWfDWoBaX9fXh+/hbu/HgFi7YeJTOzYJ3oKXtoIlCqEBAR2tYszYKhbZj4QFMcIjw5I4qHvvid0+dT7Q5P5XOaCJQqRESETmHlWPxUe97oUZ91+0/Rbdwqth/R203V5WkiUKoQcjqEB1tVYc7jLUnPMPSasIYFG2OvvqHySJoIlCrEGlcqwffD23JTxeI8NWcTr3+/g7SMTLvDUvmMJgKlCrngAF9mPNaCga2rMGX1Hzzw+TpOaiE7lYWOQFHKA3g7HbzavT4NQ4MYPX8r3f6zin90qUvJYj74ejnw8XLg6+V0PTsILOKNvw5Q8xj6X1opD3JP4xBqlgng8ekbGD5r42XXcwg0q1KSzmHl6BRWnnJBfnkYpcprOqBMKQ+UlJrO7mOJpKZnkpqRSUqa6zk9g9T0TA6fusCS7cfYc+IcAI0rFadT/XJ0DitPpVJFbY5eXQ8dWayUui57T5xj8bajLN5+jG2xZwGoXyGQR9tVpXvDijgdYnOEKqc0ESilbtjhU0ks3naM+VEx7DqWSM0y/jxzRy06hZVDRBNCfqeJQCmVazIzDT9tO8aHS3ezL+48YRUDebZjbW6pFawJIR/TWkNKqVzjcAhdbyrPkqfa837vhpxJSuPhL9bTe+JaIvbH2x2eug6aCJRS18XL6eDepiH88uwtvHF3GIdPJ9F3cgTDZ23kXEq63eGpa+DWRCAinURkt4jsFZHRV1ivl4gYEcn2skUplX/5eDl4sGVlfht1K8/cUYsftxyhx7hVRB9PtDs0lUNuSwQi4gTGA52BekA/EamXzXoBwEhgnbtiUUq5n5+3kxG31WTGoy1JuJBOj3Gr+W6T1jcqCNx5RdAc2GuM2W+MSQVmAz2yWe8N4B0g2Y2xKKXySKvqpVg0oi0NKgYxcvYmXl6wlZT0DLvDUlfgzkRQETic5X2Ma9lfRKQJEGqM+dGNcSil8liZQD9mPtaCx9tX46uIQ9w3cS0xp5PsDktdhm2dxSLiAD4Ens3BuoNFJFJEIuPi4twfnFLqhnk5HbzYpS6THmzK/rjzdB27itm/H2Jf3DmdOS2fcds4AhFpBbxqjLnT9f5FAGPMW673QcA+4Jxrk3LAKaC7MeayAwV0HIFSBc+Bk+cZMiOKnUet0cnFfJzUqxBI/QpB1K8QSFjFIKqWLkZ6puFCagbJaX8+MrmQloExhqaVS+g8zDfAlgFlIuIFRAO3AbHAeqC/MWb7Zdb/FXjuSkkANBEoVVBlZBqijyeyLTaB7UfOsi02gR1Hz5KUmrP+g9plA3i9R31aVCvl5kgLpyslArdVHzXGpIvIMGAJ4ASmGGO2i8jrQKQxZqG7vlsplf84HULd8oHULR9Ib9eyjEzDgfjzbItN4PCpJHy9nPh5O/DzdlLEx4mfl/Ucl5jCe0t202dyBD0aVeAfXepSNlArouYWLTGhlCoQLqRmMOG3fUz8bR/eDuGp22sxsE0VvLW5KEe0xIRSqsAr4uPkmTtqsfTp9rSsVooxi3bS5ZOVrNl30u7QCjy9IlBKFUjLdh7nte93cOhUEnXKBdCkcgkahxancaUSVCtdDIeWyP5/tPqoUqpQSk7LYPrag6zce5KNh06TmGzVOAr086JRpRI0Ci3OHXXL0iAkyOZI7aeJQClV6GVmGvafPEfUoTNsPHSGjYdOE308EQMMaFmZUZ3qePQ8zLbcNaSUUnnJ4RBqlAmgRpkA7gsPBSDhQhofLY1m6toDLN1xnDH3NODWOmXsDTQf0s5ipVShFVTEm1e712feE60p6uvFw1+uZ+TsjcSfS7E7tHxFm4aUUh4hJT2DT5fv49Nf9+Lv68W/utWnR6MKiAjGGE4npXHkzAViz1zgyJkLnE5K4+ZawTSpVLxQzLymfQRKKeWy+1giL8zfwqbDZ6hbPpCU9AyOnLlAclpmtuvXKRdAv+aVuLtxRYKKeOdxtLlHE4FSSmWRkWmYuuYAP207SnCALxWCilChuPWoWLwIFYr74evt5PvNR5j1+yG2xCTg5+2ga4MK9G9RqUBeJWgiUEqpG7AtNoFZvx/iu01HOJeSTu2yATQMDcLf1xt/Xyf+fl74+3pTzNdJgJ8XxYv6UD7Ij2B/33xTKE8TgVJK5YLzKel8v/kI8zbEEHvmAueS0zmXms7lfkYdAmUC/CgX5Ee5QOs5tGRRGlcqTv0Kgfh6OXP83SfPpWAMBAf4XlfsmgiUUspNMjMNF9IyOJeSTmJyOonJaZxOSuVoQjLHEpI5mpDM8bPW89EzFzjvqrbq4+XgpopBNK1cgiaVS9CkUgmCA3xJTstg74lz7Dx6ll3HEtl9LJFdx85y8lwqQ2+tzqg761xXnDqOQCml3MThEIr5elHM14uygVdf/8TZZKIOnWbDQevxxeoDTFqxH7DO9uPPpfDnvD2+Xg5qlwvg1tplqFM+kJbVSrrlb9BEoJRSeahMoB+dwsrTKaw8YN3Wui32LFEHT7Pz2FlCihehTvlAapcLoEqpYjjzoGaSJgKllLKRr5eTppVL0LRyCdtiyB/d2UoppWyjiUAppTycWxOBiHQSkd0isldERmfz+RMislVENonIKhGp5854lFJKXcptiUBEnMB4oDNQD+iXzQ/9TGNMA2NMI+Bd4EN3xaOUUip77rwiaA7sNcbsN8akArOBHllXMMaczfK2GFCwBjUopVQh4M67hioCh7O8jwFaXLySiAwFngF8gA7Z7UhEBgODASpVqpTrgSqllCezvbPYGDPeGFMdeAF4+TLrTDbGhBtjwoODg/M2QKWUKuTcmQhigdAs70Ncyy5nNnC3G+NRSimVDXc2Da0HaopIVawE0Bfon3UFEalpjNnjetsV2MNVbNiw4aSIHLzOmEoDJ69zW0+hx+jK9PhcnR6jK7Pr+FS+3AduSwTGmHQRGQYsAZzAFGPMdhF5HYg0xiwEhonI7UAacBp4KAf7ve62IRGJvFzRJWXRY3RlenyuTo/RleXH4+PWEhPGmEXAoouW/TPL65Hu/H6llFJXZ3tnsVJKKXt5WiKYbHcABYAeoyvT43N1eoyuLN8dnwI3MY1SSqnc5WlXBEoppS6iiUAppTycxySCq1VC9UQiMkVETojItizLSorIUhHZ43q2b7YMm4lIqIgsF5EdIrJdREa6lusxAkTET0R+F5HNruPzmmt5VRFZ5/q3NkdEfOyO1U4i4hSRjSLyg+t9vjs+HpEIclgJ1RN9CXS6aNloYJkxpiawzPXeU6UDzxpj6gEtgaGu/2/0GFlSgA7GmIZAI6CTiLQE3gE+MsbUwBofNMjGGPODkcDOLO/z3fHxiERADiqheiJjzArg1EWLewBTXa+n4sFlP4wxR40xUa7XiVj/mCuixwgAYznneuvtehis4pHzXMs99vgAiEgIVtWEz13vhXx4fDwlEWRXCbWiTbHkd2WNMUddr48BZe0MJr8QkSpAY2Adeoz+4mr22AScAJYC+4Azxph01yqe/m/tY+B5INP1vhT58Ph4SiJQ18FY9xZ7/P3FIuIPzAeeumgODY8/RsaYDNfEUiFYV951bA4p3xCRu4ATxpgNdsdyNW4tMZGPXGslVE92XETKG2OOikh5rDM9jyUi3lhJYIYx5hvXYj1GFzHGnBGR5UAroLiIeLnOej3531oboLuIdAH8gEDgE/Lh8fGUK4K/KqG6euj7Agttjim/Wsjfxf8eAr6zMRZbudpz/wvsNMZknUZVjxEgIsEiUtz1ughwB1Y/ynLgXtdqHnt8jDEvGmNCjDFVsH5zfjHG3E8+PD4eM7LYlZU/5u9KqGNsDsl2IjILuAWrLO5x4F/AAmAuUAk4CNxnjLm4Q9kjiEhbYCWwlb/beP+B1U/g8cdIRG7C6ux0Yp1UzjXGvC4i1bBuyCgJbAQeMMak2Bep/UTkFuA5Y8xd+fH4eEwiUEoplT1PaRpSSil1GZoIlFLKw2kiUEopD6eJQCmlPJwmAqWU8nCaCJRyEZEMEdmU5ZFrxeREpErWKq9K5SeeMrJYqZy44CqXoJRH0SsCpa5CRA6IyLsistVVf7+Ga3kVEflFRLaIyDIRqeRaXlZEvnXV6d8sIq1du3KKyGeu2v0/u0bjIiIjXHMebBGR2Tb9mcqDaSJQ6m9FLmoa6pPlswRjTANgHNYIdYD/AFONMTcBM4CxruVjgd9cdfqbANtdy2sC440x9YEzQC/X8tFAY9d+nnDXH6fU5ejIYqVcROScMcY/m+UHsCZg2e8qQnfMGFNKRE4C5Y0xaa7lR40xpUUkDgjJWjbAVcZ6qWsyG0TkBcDbGPOmiCwGzmGV91iQpca/UnlCrwiUyhlzmdfXIms9mQz+7qPrijWDXhNgvYho353KU5oIlMqZPlme17per8GqKglwP1aBOrCmrxwCf03cEnS5nYqIAwg1xiwHXgCCgEuuSpRyJz3zUOpvRVyzbf1psTHmz1tIS4jIFqyz+n6uZcOBL0RkFBAHPOxaPhKYLCKDsM78hwBHyZ4T+MqVLAQYa4w5k2t/kVI5oH0ESl2Fq48g3Bhz0u5YlHIHbRpSSikPp1cESinl4fSKQCmlPJwmAqWU8nCaCJRSysNpIlBKKQ+niUAppTzc/wETApvuCUukEwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/Colab Notebooks/MA/gru/gru_sp.h5\")"
      ],
      "metadata": {
        "id": "8ghbIl4-fZmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a = keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/MA/gru/gru_sp.h5\",  custom_objects={\"get_f1\": get_f1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn_78OLSfZmM",
        "outputId": "49c6502e-b5bc-477f-9ca8-f0a8e95a17f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6OCugTCfZmM",
        "outputId": "0aa16159-fc31-4778-e066-fc1b3927aa3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XKwwlFgfZmM",
        "outputId": "457c1c45-b56c-494e-b174-7bfb1209c7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.76      0.70        96\n",
            "           1       0.00      0.00      0.00        32\n",
            "           2       0.81      0.89      0.85       183\n",
            "\n",
            "    accuracy                           0.76       311\n",
            "   macro avg       0.49      0.55      0.52       311\n",
            "weighted avg       0.68      0.76      0.72       311\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy')\n",
        "print(accuracy_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1)))\n",
        "print('f1 score')\n",
        "print(f1_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n",
        "print('recall')\n",
        "print(recall_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n",
        "print('precision')\n",
        "print(precision_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ47dv8bfZmM",
        "outputId": "ed5e98a4-aabd-4660-ba1c-963a7eff4a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "0.7556270096463023\n",
            "f1 score\n",
            "0.7157532960465571\n",
            "recall\n",
            "0.7556270096463023\n",
            "precision\n",
            "0.6802131927436909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bi-GRU\n"
      ],
      "metadata": {
        "id": "PdnWqzBDf712"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = train_sp.append(test_sp, ignore_index=True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "a1089880-ead5-446a-eae3-4bfc515b27fe",
        "id": "g5W07q5Xf712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                               text   aspectTerm  \\\n",
              "0     142087  I am very satisfied with it ... I have used th...        price   \n",
              "1     142089  Cheap processing, wobbling unstable unstable l...   processing   \n",
              "2     142089  Cheap processing, wobbling unstable unstable l...         lids   \n",
              "3     142089  Cheap processing, wobbling unstable unstable l...  baking time   \n",
              "4     142091  The iron does not remain, the waffles are too ...      waffles   \n",
              "...      ...                                                ...          ...   \n",
              "1550  145346  It is very simple to handle and the price qual...      coating   \n",
              "1551  145357  Easy to clean, heat enough and the materials s...        clean   \n",
              "1552  145357  Easy to clean, heat enough and the materials s...         heat   \n",
              "1553  145357  Easy to clean, heat enough and the materials s...    materials   \n",
              "1554  145357  Easy to clean, heat enough and the materials s...        price   \n",
              "\n",
              "      From   To        SP                                         text_wo_sw  \\\n",
              "0       82   87  positive  satisfied ... device times ... price device .....   \n",
              "1        6   16  negative  cheap processing, wobbling unstable unstable l...   \n",
              "2       45   49  negative  cheap processing, wobbling unstable unstable l...   \n",
              "3      142  153  negative  cheap processing, wobbling unstable unstable l...   \n",
              "4       30   37  negative     iron remain, waffles small waffles brown, six.   \n",
              "...    ...  ...       ...                                                ...   \n",
              "1550    81   88  positive  simple handle price quality relationship good....   \n",
              "1551     8   13  positive  easy clean, heat materials good quality plasti...   \n",
              "1552    15   19  positive  easy clean, heat materials good quality plasti...   \n",
              "1553    35   44  positive  easy clean, heat materials good quality plasti...   \n",
              "1554   123  128  positive  easy clean, heat materials good quality plasti...   \n",
              "\n",
              "                                                    coc  \n",
              "0     satisfied...device times...device...can't expe...  \n",
              "1                cheap, wobbling unstable unstable lids  \n",
              "2     cheap processing, wobbling unstable unstable f...  \n",
              "3     independently time open incredible unreasonabl...  \n",
              "4                iron remain, small waffles brown, six.  \n",
              "...                                                 ...  \n",
              "1550  handle price quality relationship good. non -s...  \n",
              "1551                  easy, heat materials good quality  \n",
              "1552  easy clean, materials good quality plasticucho...  \n",
              "1553  easy clean, heat good quality plasticuchos. go...  \n",
              "1554         materials good quality plasticuchos. good.  \n",
              "\n",
              "[1555 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b73b1fa-e04f-43bb-81c4-3e652ad6eaba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>aspectTerm</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>SP</th>\n",
              "      <th>text_wo_sw</th>\n",
              "      <th>coc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142087</td>\n",
              "      <td>I am very satisfied with it ... I have used th...</td>\n",
              "      <td>price</td>\n",
              "      <td>82</td>\n",
              "      <td>87</td>\n",
              "      <td>positive</td>\n",
              "      <td>satisfied ... device times ... price device .....</td>\n",
              "      <td>satisfied...device times...device...can't expe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>processing</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap, wobbling unstable unstable lids</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>lids</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>cheap processing, wobbling unstable unstable f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142089</td>\n",
              "      <td>Cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>baking time</td>\n",
              "      <td>142</td>\n",
              "      <td>153</td>\n",
              "      <td>negative</td>\n",
              "      <td>cheap processing, wobbling unstable unstable l...</td>\n",
              "      <td>independently time open incredible unreasonabl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>142091</td>\n",
              "      <td>The iron does not remain, the waffles are too ...</td>\n",
              "      <td>waffles</td>\n",
              "      <td>30</td>\n",
              "      <td>37</td>\n",
              "      <td>negative</td>\n",
              "      <td>iron remain, waffles small waffles brown, six.</td>\n",
              "      <td>iron remain, small waffles brown, six.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1550</th>\n",
              "      <td>145346</td>\n",
              "      <td>It is very simple to handle and the price qual...</td>\n",
              "      <td>coating</td>\n",
              "      <td>81</td>\n",
              "      <td>88</td>\n",
              "      <td>positive</td>\n",
              "      <td>simple handle price quality relationship good....</td>\n",
              "      <td>handle price quality relationship good. non -s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1551</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>clean</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy, heat materials good quality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1552</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>heat</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy clean, materials good quality plasticucho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1553</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>materials</td>\n",
              "      <td>35</td>\n",
              "      <td>44</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>easy clean, heat good quality plasticuchos. go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1554</th>\n",
              "      <td>145357</td>\n",
              "      <td>Easy to clean, heat enough and the materials s...</td>\n",
              "      <td>price</td>\n",
              "      <td>123</td>\n",
              "      <td>128</td>\n",
              "      <td>positive</td>\n",
              "      <td>easy clean, heat materials good quality plasti...</td>\n",
              "      <td>materials good quality plasticuchos. good.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1555 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b73b1fa-e04f-43bb-81c4-3e652ad6eaba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6b73b1fa-e04f-43bb-81c4-3e652ad6eaba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6b73b1fa-e04f-43bb-81c4-3e652ad6eaba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 1000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 10\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(df['coc'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fdeea2b-17b3-4951-9574-b04381be250a",
        "id": "TbjDTpWLf713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1765 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = tokenizer.texts_to_sequences(df['coc'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783a7d71-0f5d-49dc-9bd8-24b854a1ae2e",
        "id": "J71gpJohf713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data tensor: (1555, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(df['SP']).values\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7543d5-a2de-4367-dcfb-c554e0117a51",
        "id": "LPsu_ifGf714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (1555, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25097078-b255-4aa6-d63c-a2b7d21a971a",
        "id": "HFSruPNif714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 29)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fcd94d-3e5e-4926-c08b-108e5ab9680e",
        "id": "RAdafpXPf715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1244, 10) (1244, 3)\n",
            "(311, 10) (311, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first layer is the embedded layer that uses 100 length vectors to represent each word.\n",
        "SpatialDropout1D performs variational dropout in NLP models.\n",
        "The next layer is the LSTM layer with 100 memory units.\n",
        "The output layer must create 3 output values, one for each class.\n",
        "Activation function is softmax for multi-class classification.\n",
        "Because it is a multi-class classification problem, categorical_crossentropy is used as the loss function.\n"
      ],
      "metadata": {
        "id": "H-Mo9CDEf716"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Bidirectional(GRU(100, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac956077-afc7-4cc0-b54c-d5dcee77d819",
        "id": "gd-vRs67f716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 10, 100)           100000    \n",
            "                                                                 \n",
            " spatial_dropout1d (SpatialD  (None, 10, 100)          0         \n",
            " ropout1D)                                                       \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 200)              121200    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 603       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 221,803\n",
            "Trainable params: 221,803\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam = k.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999)\n",
        "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=[get_f1])"
      ],
      "metadata": {
        "id": "0yIZkHdlf716"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the best only\n",
        "\n",
        "filepath=\"/content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 5, verbose = 1,restore_best_weights = True)\n",
        "callbacks_list = [earlystop, checkpoint]"
      ],
      "metadata": {
        "id": "rzYh1C_xf716"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=callbacks_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6f4a6d-68d0-42d2-d31a-ead728f837dc",
        "id": "B9v5I8Gcf717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0892 - get_f1: 0.0000e+00\n",
            "Epoch 1: val_loss improved from inf to 1.06996, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 21s 976ms/step - loss: 1.0892 - get_f1: 0.0000e+00 - val_loss: 1.0700 - val_get_f1: 0.0000e+00\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0530 - get_f1: 0.0000e+00\n",
            "Epoch 2: val_loss improved from 1.06996 to 1.03647, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 685ms/step - loss: 1.0530 - get_f1: 0.0000e+00 - val_loss: 1.0365 - val_get_f1: 0.0000e+00\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0177 - get_f1: 0.0000e+00\n",
            "Epoch 3: val_loss improved from 1.03647 to 1.00175, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 740ms/step - loss: 1.0177 - get_f1: 0.0000e+00 - val_loss: 1.0018 - val_get_f1: 0.0000e+00\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9791 - get_f1: 0.0000e+00\n",
            "Epoch 4: val_loss improved from 1.00175 to 0.96568, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 686ms/step - loss: 0.9791 - get_f1: 0.0000e+00 - val_loss: 0.9657 - val_get_f1: 0.0000e+00\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9408 - get_f1: 0.1927\n",
            "Epoch 5: val_loss improved from 0.96568 to 0.92960, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 685ms/step - loss: 0.9408 - get_f1: 0.1927 - val_loss: 0.9296 - val_get_f1: 0.4727\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9011 - get_f1: 0.5426\n",
            "Epoch 6: val_loss improved from 0.92960 to 0.90120, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 745ms/step - loss: 0.9011 - get_f1: 0.5426 - val_loss: 0.9012 - val_get_f1: 0.5680\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8730 - get_f1: 0.6115\n",
            "Epoch 7: val_loss improved from 0.90120 to 0.88456, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 676ms/step - loss: 0.8730 - get_f1: 0.6115 - val_loss: 0.8846 - val_get_f1: 0.5838\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8551 - get_f1: 0.6250\n",
            "Epoch 8: val_loss improved from 0.88456 to 0.87517, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 728ms/step - loss: 0.8551 - get_f1: 0.6250 - val_loss: 0.8752 - val_get_f1: 0.5919\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8471 - get_f1: 0.6275\n",
            "Epoch 9: val_loss improved from 0.87517 to 0.86340, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 678ms/step - loss: 0.8471 - get_f1: 0.6275 - val_loss: 0.8634 - val_get_f1: 0.5919\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8339 - get_f1: 0.6164\n",
            "Epoch 10: val_loss improved from 0.86340 to 0.85435, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 746ms/step - loss: 0.8339 - get_f1: 0.6164 - val_loss: 0.8543 - val_get_f1: 0.5919\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8235 - get_f1: 0.6267\n",
            "Epoch 11: val_loss improved from 0.85435 to 0.84312, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 689ms/step - loss: 0.8235 - get_f1: 0.6267 - val_loss: 0.8431 - val_get_f1: 0.5966\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8053 - get_f1: 0.6264\n",
            "Epoch 12: val_loss improved from 0.84312 to 0.83258, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 740ms/step - loss: 0.8053 - get_f1: 0.6264 - val_loss: 0.8326 - val_get_f1: 0.5944\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7864 - get_f1: 0.6245\n",
            "Epoch 13: val_loss improved from 0.83258 to 0.81722, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 702ms/step - loss: 0.7864 - get_f1: 0.6245 - val_loss: 0.8172 - val_get_f1: 0.5954\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7606 - get_f1: 0.6359\n",
            "Epoch 14: val_loss improved from 0.81722 to 0.79547, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 733ms/step - loss: 0.7606 - get_f1: 0.6359 - val_loss: 0.7955 - val_get_f1: 0.5996\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7321 - get_f1: 0.6397\n",
            "Epoch 15: val_loss improved from 0.79547 to 0.77238, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 691ms/step - loss: 0.7321 - get_f1: 0.6397 - val_loss: 0.7724 - val_get_f1: 0.6085\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7033 - get_f1: 0.6612\n",
            "Epoch 16: val_loss improved from 0.77238 to 0.74747, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 757ms/step - loss: 0.7033 - get_f1: 0.6612 - val_loss: 0.7475 - val_get_f1: 0.6301\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6692 - get_f1: 0.6799\n",
            "Epoch 17: val_loss improved from 0.74747 to 0.72537, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 680ms/step - loss: 0.6692 - get_f1: 0.6799 - val_loss: 0.7254 - val_get_f1: 0.6366\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6387 - get_f1: 0.6894\n",
            "Epoch 18: val_loss improved from 0.72537 to 0.70294, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 741ms/step - loss: 0.6387 - get_f1: 0.6894 - val_loss: 0.7029 - val_get_f1: 0.6410\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6158 - get_f1: 0.6982\n",
            "Epoch 19: val_loss improved from 0.70294 to 0.68311, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 686ms/step - loss: 0.6158 - get_f1: 0.6982 - val_loss: 0.6831 - val_get_f1: 0.6539\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5894 - get_f1: 0.7344\n",
            "Epoch 20: val_loss improved from 0.68311 to 0.66501, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 694ms/step - loss: 0.5894 - get_f1: 0.7344 - val_loss: 0.6650 - val_get_f1: 0.6792\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5673 - get_f1: 0.7559\n",
            "Epoch 21: val_loss improved from 0.66501 to 0.64552, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 755ms/step - loss: 0.5673 - get_f1: 0.7559 - val_loss: 0.6455 - val_get_f1: 0.7091\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5550 - get_f1: 0.7873\n",
            "Epoch 22: val_loss improved from 0.64552 to 0.63137, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 681ms/step - loss: 0.5550 - get_f1: 0.7873 - val_loss: 0.6314 - val_get_f1: 0.7404\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5219 - get_f1: 0.8110\n",
            "Epoch 23: val_loss improved from 0.63137 to 0.62015, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 726ms/step - loss: 0.5219 - get_f1: 0.8110 - val_loss: 0.6202 - val_get_f1: 0.7536\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5043 - get_f1: 0.8194\n",
            "Epoch 24: val_loss improved from 0.62015 to 0.61070, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 10s 688ms/step - loss: 0.5043 - get_f1: 0.8194 - val_loss: 0.6107 - val_get_f1: 0.7669\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4962 - get_f1: 0.8203\n",
            "Epoch 25: val_loss improved from 0.61070 to 0.59281, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 733ms/step - loss: 0.4962 - get_f1: 0.8203 - val_loss: 0.5928 - val_get_f1: 0.7852\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4720 - get_f1: 0.8282\n",
            "Epoch 26: val_loss improved from 0.59281 to 0.58236, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 698ms/step - loss: 0.4720 - get_f1: 0.8282 - val_loss: 0.5824 - val_get_f1: 0.7956\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4550 - get_f1: 0.8304\n",
            "Epoch 27: val_loss improved from 0.58236 to 0.57789, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 12s 770ms/step - loss: 0.4550 - get_f1: 0.8304 - val_loss: 0.5779 - val_get_f1: 0.7910\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4445 - get_f1: 0.8476\n",
            "Epoch 28: val_loss improved from 0.57789 to 0.56599, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 721ms/step - loss: 0.4445 - get_f1: 0.8476 - val_loss: 0.5660 - val_get_f1: 0.7948\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4291 - get_f1: 0.8402\n",
            "Epoch 29: val_loss improved from 0.56599 to 0.55977, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 12s 780ms/step - loss: 0.4291 - get_f1: 0.8402 - val_loss: 0.5598 - val_get_f1: 0.7938\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4105 - get_f1: 0.8440\n",
            "Epoch 30: val_loss improved from 0.55977 to 0.55021, saving model to /content/drive/MyDrive/Colab Notebooks/MA/bigru/checkpoint_sp/\n",
            "16/16 [==============================] - 11s 719ms/step - loss: 0.4105 - get_f1: 0.8440 - val_loss: 0.5502 - val_get_f1: 0.7968\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4044 - get_f1: 0.8506\n",
            "Epoch 31: val_loss did not improve from 0.55021\n",
            "16/16 [==============================] - 1s 73ms/step - loss: 0.4044 - get_f1: 0.8506 - val_loss: 0.5533 - val_get_f1: 0.7910\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3763 - get_f1: 0.8570\n",
            "Epoch 32: val_loss did not improve from 0.55021\n",
            "16/16 [==============================] - 1s 74ms/step - loss: 0.3763 - get_f1: 0.8570 - val_loss: 0.5512 - val_get_f1: 0.7942\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3655 - get_f1: 0.8634\n",
            "Epoch 33: val_loss did not improve from 0.55021\n",
            "16/16 [==============================] - 1s 73ms/step - loss: 0.3655 - get_f1: 0.8634 - val_loss: 0.5562 - val_get_f1: 0.7981\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3503 - get_f1: 0.8568\n",
            "Epoch 34: val_loss did not improve from 0.55021\n",
            "16/16 [==============================] - 1s 74ms/step - loss: 0.3503 - get_f1: 0.8568 - val_loss: 0.5624 - val_get_f1: 0.8013\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3531 - get_f1: 0.8618Restoring model weights from the end of the best epoch: 30.\n",
            "\n",
            "Epoch 35: val_loss did not improve from 0.55021\n",
            "16/16 [==============================] - 1s 75ms/step - loss: 0.3531 - get_f1: 0.8618 - val_loss: 0.5764 - val_get_f1: 0.8006\n",
            "Epoch 35: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 26: val_loss did not improve from 0.61862\n",
        "16/16 [==============================] - 2s 126ms/step - loss: 0.3744 - get_f1: 0.8567 - val_loss: 0.6383 - val_get_f1: 0.7855\n",
        "Epoch 26: early stopping"
      ],
      "metadata": {
        "id": "koxu6u0_f717"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "plot_graphs(history, 'get_f1')\n",
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "QdlTYGhIf717",
        "outputId": "0200c7d8-0c67-44ac-9e59-9d387da63cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVxU9f7H8deHRRFFFEFQUdHcBVdwyS3LTM20NDXXNMsWy1bbbve23Lr1q1u3ullmLmlZalpquWVmmeYCrogr4gaogChuINv398cZb2QIqAwzMJ/n48GDmTPnzHyYbN5zvt/z/X7FGINSSinX5eboApRSSjmWBoFSSrk4DQKllHJxGgRKKeXiNAiUUsrFeTi6gKvl7+9vQkJCHF2GUkqVKps3b04xxgTk91ipC4KQkBCioqIcXYZSSpUqInL4So9p05BSSrk4DQKllHJxdgsCEZkuIkkisvMKjzcRkfUiclFEnrFXHUoppQpmzz6Cz4GPgFlXeDwVmADcaccalFJlRFZWFvHx8WRkZDi6FKfm5eVFcHAwnp6eRT7GbkFgjFkjIiEFPJ4EJInI7faqQSlVdsTHx+Pj40NISAgi4uhynJIxhpMnTxIfH0+9evWKfFyp6CMQkXEiEiUiUcnJyY4uRynlABkZGVSrVk1DoAAiQrVq1a76rKlUBIExZooxJtwYEx4QkO9lsEopF6AhULhreY9KRRAUh5RzF3n1+xguZuc4uhSllHIqLhMEG+NSmbHuEE/O3UZOrq7BoJS6epUqVXJ0CXZht85iEfkauAnwF5F44GXAE8AYM1lEgoAooDKQKyJPAM2MMWfsUc/tLWpwLK0pry/ZTWWvaN4cEKanmUophR3PCIwxQ40xNYwxnsaYYGPMNGPMZGPMZNvjx23bKxtjqthu2yUELrm/S33Gd7+BOZFH+b/le+35UkqpMswYw8SJEwkNDSUsLIy5c+cCcOzYMbp27UqrVq0IDQ3lt99+Iycnh9GjR/9v3//85z8Orv6vSt1cQ9frmZ6NOXUhi8m/HqCqtycPdrvB0SUppa7Sq9/HsCuxeL83NqtZmZfvaF6kfb/99lu2bdvG9u3bSUlJISIigq5du/LVV19x22238be//Y2cnBwuXLjAtm3bSEhIYOdOa2zt6dOni7Xu4uAyfQRkpcPGTxFj+Gf/UPq2qMGby/YwN/KIoytTSpUya9euZejQobi7uxMYGEi3bt2IjIwkIiKCGTNm8MorrxAdHY2Pjw/169cnLi6Oxx57jOXLl1O5cmVHl/8XrnNGsHMBLHsWkvfg3udd3hvcijMZ2bzwbTS+FTzpFVrD0RUqpYqoqN/cS1rXrl1Zs2YNS5YsYfTo0Tz11FOMGjWK7du3s2LFCiZPnsy8efOYPn26o0v9E9c5I2g1HDo/CVHTYenTlHODySPa0Kp2FSZ8vY21+1McXaFSqpTo0qULc+fOJScnh+TkZNasWUO7du04fPgwgYGBPPDAA9x///1s2bKFlJQUcnNzGThwIK+//jpbtmxxdPl/4TpnBCJwy8vW7bVWZ413n3eZMbodQ6asZ9wXUXz1QAda1a7iwCKVUqXBXXfdxfr162nZsiUiwttvv01QUBAzZ87knXfewdPTk0qVKjFr1iwSEhIYM2YMubm5ALz55psOrv6vxJjSdU19eHi4ua6FaYyBVa9aYRB+H/R5l6Rzmdw9eT1nMrL45sGONAz0Kb6ClVLFYvfu3TRt2tTRZZQK+b1XIrLZGBOe3/6u0zR0yaUzgzzNRNUrlePLse3xdHdj5LRNHE294OgqlVKqxLheEMAfYdDpCVsYPEOdql58MbYd6Vk5DJ+6kRNndKpbpZRrcM0gACsMerxiC4NpsPQZmlSvxOdjIjh57iIjpm4k9Xymo6tUSim7c90ggHzDoHWwL1PvjeBI6gXunb6JMxlZjq5SKaXsyrWDAPINg471/fhkRBt2HzvD2M8juZCZ7egqlVLKbjQIIE8YPG6FwfLnublxdT64pzWbD5/iwS826/TVSqkyy3XGERRGBHq8CjlZsOFj8PDi9h6vcP5iC55dsIMJX29l0rA2eLhrdiqlyhb9VMtLBG77lzW+YN378OvbDI6ozct3NGNFzAkmzt9Brq5loJQqgoLWLjh06BChoaElWE3B9IzgciLQ513IyoBf/gWeXozp9DjnL2bz7x/3UbG8O//sH6prGSilygwNgvy4uUH/jyA7A1b+Azy8GN99HGcvZvPpr3FULO/BC711hKNSDrPseTgeXbzPGRQGvd+64sPPP/88tWvXZvz48QC88soreHh4sHr1ak6dOkVWVhavv/46/fv3v6qXzcjI4OGHHyYqKgoPDw/ee+89unfvTkxMDGPGjCEzM5Pc3FwWLFhAzZo1GTx4MPHx8eTk5PD3v/+dIUOGXNefDRoEV+bmDgOmQE4mLHsW8fDi+V6jOG8Lg0AfL+7rXM/RVSqlSsiQIUN44okn/hcE8+bNY8WKFUyYMIHKlSuTkpJChw4d6Nev31W1GEyaNAkRITo6mj179tCzZ0/27dvH5MmTefzxxxk+fDiZmZnk5OSwdOlSatasyZIlSwBIS0srlr9Ng6Ag7p5w93SYMwy+fxzx8OLVfoNJOnORfy7ZRXDVCvRsHuToKpVyPQV8c7eX1q1bk5SURGJiIsnJyVStWpWgoCCefPJJ1qxZg5ubGwkJCZw4cYKgoKJ/Lqxdu5bHHnsMgCZNmlC3bl327dtHx44deeONN4iPj2fAgAE0bNiQsLAwnn76aZ577jn69u1Lly5diuVvs1tnsYhMF5EkEdl5hcdFRD4UkVgR2SEibexVy3XxKA9DvoSQzrDwIdx3L+KDe1rTIrgKE+ZsZftR51ttSCllH4MGDWL+/PnMnTuXIUOGMHv2bJKTk9m8eTPbtm0jMDCQjIzimZ5m2LBhLF68mAoVKtCnTx9+/vlnGjVqxJYtWwgLC+Oll17itddeK5bXsudVQ58DvQp4vDfQ0PYzDvjEjrVcH88KMHQOBLeDBWOpcPBHpo4KJ8CnPGNnRuokdUq5iCFDhjBnzhzmz5/PoEGDSEtLo3r16nh6erJ69WoOHz581c/ZpUsXZs+eDcC+ffs4cuQIjRs3Ji4ujvr16zNhwgT69+/Pjh07SExMxNvbmxEjRjBx4sRiW9vAnovXrwFSC9ilPzDLWDYAVUTEeZcJK18Jhs+zOpTmjSIgeQMzRrcjMzuX0TM2kXZBp6JQqqxr3rw5Z8+epVatWtSoUYPhw4cTFRVFWFgYs2bNokmTJlf9nI888gi5ubmEhYUxZMgQPv/8c8qXL8+8efMIDQ2lVatW7Ny5k1GjRhEdHU27du1o1aoVr776Ki+99FKx/F12XY9AREKAH4wxf7lgVkR+AN4yxqy13V8FPGeM+ctiAyIyDuusgTp16rS9ltQtNhdS4fPb4fQRuHcxGy6GMHLaRtrWrcrM+9pR3sPdcbUpVYbpegRFVybXIzDGTDHGhBtjwgMCAhxbjLcfjPgWvKvBl3fTwSeFd+5uyYa4VJ5fEE1pW+hHKaUcGQQJQO0894Nt25xf5RowaqF1VdGsO7mzXg5P39qI77Ym8J+f9ju6OqWUk4iOjqZVq1Z/+mnfvr2jy/oLR14+uhh4VETmAO2BNGPMMQfWc3X86ltnBp/3gVl38uh9yzl6KpgPV+0nuGoFBofXLvw5lFJXxRhTqkb1h4WFsW3bthJ9zWtplbDn5aNfA+uBxiISLyJjReQhEXnItstSIA6IBT4DHrFXLXYTFArD5sGZROTLAbzRuw6dG/jz4rfRrN2f4ujqlCpTvLy8OHnypDa/FsAYw8mTJ/Hy8rqq41xv8Xp72P8TfH0PBEdwZtBcBk3dRuLpdL55uCNNgio7ujqlyoSsrCzi4+OL7Tr9ssrLy4vg4GA8PT3/tL2gzmINguKycwHMHwsNe5LYayp3Tt6Eh5vw3fhOBFa+unRWSqniVuqvGioVQgfC7e/C/hXU/OVppt/blrT0LMbMiOTcRV3hTCnlvDQIilPEWLj57xA9j9Ad/2LSsNbsPXGW8bO3kJ2T6+jqlFIqXxoExa3L09DxUdg0hZuOTef1O0P5dV8yf1+0Uzu5lFJOSWcfLW4i0PN1SD8Nv77F0F5Vie9+E5NWHyC4qjfjuzdwdIVKKfUnekZgDyJwxwfQpC8sf45nArfSv1VN3lmxl0XbSseYOaWU69AgsBd3Dxg4Dep1RRaN598tEmlfz4+J3+xgQ9xJR1enlFL/o0FgT55ecM9XUKMlngvGMK3bRepU82bcrChik846ujqllAI0COyvvA8Mnw9VQ6j07Qi+7FOech7ujJ4RSfLZi46uTimlNAhKRMVqMPI7qFCFoMXDmH1nVU6ey2TszEjSM3McXZ1SysVpEJQU31owciGIG41/HMWU/kFEJ6Tx1Lxt5ObqZaVKKcfRIChJ/g1gxAK4eIYuGx7gnz0CWbbzOO+u3OvoypRSLkyDoKTVaAnD5sLpIwzf/yQPtK7IpNUHWLA53tGVKaVclAaBI9S9EYbMRk7G8mLieIbUOcPz3+5g08GClnhWSin70CBwlIY9YMwyJDeHt04/w0Cf3Tz4RRSHT553dGVKKRejQeBINVvBAz8jfvV58+LrDDHLuO/zSNLSsxxdmVLKhWgQOFrlmtaZQaNePG+mc+/pSUyYHUmWzlaqlCohGgTOoHwlGPIldHyUUe4/Mvrw87z53SadrVQpVSLsGgQi0ktE9opIrIg8n8/jdUVklYjsEJFfRCTYnvU4NTd3uO0N6Ps+3dx3MnjH/Xyz6ndHV6WUcgH2XLzeHZgE9AaaAUNFpNllu/0bmGWMaQG8Brxpr3pKjfAxMGIBdTxS6f7bUDavXeHoipRSZZw9zwjaAbHGmDhjTCYwB+h/2T7NgJ9tt1fn87hLcmvQHbl/JTnuXrReOYTEacMgWQedKaXsw55BUAs4mud+vG1bXtuBAbbbdwE+IlLt8icSkXEiEiUiUcnJyXYp1tlUqNkcz4fXsLDiIHyP/ISZ1B7zzRhI2u3o0pRSZYyjO4ufAbqJyFagG5AA/GUWNmPMFGNMuDEmPCAgoKRrdJhqAUH0eXIybzaax8fZd3Bx9zLMxx3hm9FwYpejy1NKlRH2DIIEoHae+8G2bf9jjEk0xgwwxrQG/mbbdtqONZU6Xp7u/HNYN7jlZTpceJ8F3oPJ3b8SPukI80bBiRhHl6iUKuXsuWZxJNBQROphBcA9wLC8O4iIP5BqjMkFXgCm27GeUktEGN+9AfX9K/LkPF+met/Ol22i8N85A3YtggY9oFEv67dfPUeXq5QqZewWBMaYbBF5FFgBuAPTjTExIvIaEGWMWQzcBLwpIgZYA4y3Vz1lQe+wGgRX9eb+WZF0i+rEJ3ePpOvJ+bB9DsT+ZO3kd4MVCA16QEgnKFfRsUUrpZyelLZBS+Hh4SYqKsrRZTjU8bQMHpgVRUxiGi/2acrYTiHIqYNWGMT+BAd/g+x0cC9nTXB36YzBv6GjS1dKOYiIbDbGhOf7mAZB6ZSemcNT87axbOdxBrSuxfO9m1C9spf1YFYGHFlvC4ZVkGy70qhGSwgbBM0HWAvlKKVchgZBGZWba3h/1X4+Xh2Lp7sbYzvXY1y3+lT28vzzjqePwu7vIfobSNwCCIR0hrC7oWk/8PZzSP1KqZKjQVDGHUo5z7sr9/H99kSqensyvnsDRnSoi5en+193PnkAoudboXByP7h5QsNbrVBo1Ev7FJQqozQIXMTOhDT+b/keftufQq0qFXiiR0MGtAnG3U3+urMxcGy7FQg7F8DZY1afQp0OUL873NAdglqCm6OHmiilioMGgYv5PTaFt5bvYUd8Go0CKzHxtib0aFodkXwCASA3Bw7/DvuWQ9wvcGKntb2CH9S/yQqF+t2hSu38j1dKOT0NAhdkjGFp9HH+/eNeDqacp1FgJW68wZ929fyICPEjwKf8lQ8+e8IKhLjVcGA1nDtuba/WABreBs3vguBwuFKwKKWcjgaBC8vKyeWbqHiWRCey5fBp0rOsGTzq+1ckIsSPdvWsn+CqFfI/YzDGmt8objUc+BkOroGcTPCtDc36W1cg1WqjoaCUk9MgUIAVCjsT0th0MJXIQ6lsOpjKmYxsAGr4etGtUQCDI2rTunaVKzcjZaTB3mUQ8511aWpuFvjWgeZ3Wj81NRSUckYaBCpfubmGfUlniTyYyoaDqazek8SFzBwaB/owJKI2A9rUoop3uSs/Qfpp2LvUCoUDP0NuNlSpY50pNLkDgiO0s1kpJ6FBoIrk3MVsvt+eyJxNR9gen0Y5Dzd6hwYxJKI2HetXu/JZAkD6KdizFGK+hbhfrTOFitWhSR9o0hfqdQWPAvollFJ2pUGgrtquxDPMjTzCd1sTOJORTUg1bwZH1GZA62CCfL0KPjgjDfavtAaxxf4EmeegnA806glNbocGt4JX5ZL5Q5RSgAaBug4ZWTks23mMrzcdZdPBVABa1q5Cz2aB9GwWSIPqlQo+U8jKgIO/WqGwdxlcSLHGKzTpCzc+ZnU0K6XsToNAFYu45HMs23mcH3edYPtRa9mIkGre9GwexK3NAmlTp2r+g9cuyc2BoxutqbO3fQUXz0BIF+j0BDS4RTuZlbIjDQJV7E6cyWDlrhOs3HWC3w+kkJVjqFaxHLc0rc6g8NpEhBQyf1FGGmyeCRs+tkY1V28OnSZA6EBw9yz4WKXUVdMgUHZ1NiOLX/cl82PMCVbvTeJsRjY9mgbyfO/GNKjuU/DB2Zmwcz6s+9CaJbVyMHR4GNreC+ULOVYpVWQaBKrEpGfmMOP3g3yy+gDnM7MZElGbJ3o0IrByIR3MxlgdzOs+gMNrobwvdHkSOj4G7vZcSE8p16BBoEpc6vlM/vvzfr7ccBh3N+H+zvV5sFt9fC6fIjs/8ZthzTuwb5m1hkK/j6BGC/sXrVQZVlAQ2HW0j4j0EpG9IhIrIs/n83gdEVktIltFZIeI9LFnPark+FUsx8t3NGfVUzfRs1kQH62Opds7vzDz90NkZucWfHBwWxg2BwbPgjPHYMpNsOo16wokpVSxs9sZgYi4A/uAW4F4rMXshxpjduXZZwqw1RjziYg0A5YaY0IKel49IyiddsSf5s2le1gfd5K61bx5rlcTeocGFXzpKcCFVPjxJdg2G/wbWWcHddqXTNFKlSGOOiNoB8QaY+KMMZnAHKD/ZfsY4NLIIl8g0Y71KAdqEVyFrx5oz4wxEXh5uPPI7C0M/nQ9O+JPF3ygtx/c+TGMWABZ6TD9Nlj6LFw8VzKFK+UC7BkEtYCjee7H27bl9QowQkTigaXAY3asRzmYiNC9cXWWPt6Ff90VxsGU8/T7aB1Pzd3GsbT0gg9u0AMeWQ/tHoBNU+Djjtakd0qp6+boGcGGAp8bY4KBPsAXIvKXmkRknIhEiUhUcnJyiRepipe7mzCsfR1WP3MTD990Az9EH6P7v3/hvZX7uJCZfeUDy/tAn3dgzDJr3qIvB8Dix+Di2ZIrXqkyyJ5BkADkXdIq2LYtr7HAPABjzHrAC/C//ImMMVOMMeHGmPCAgAA7latKmo+XJ8/1asKqp7pxS9NAPly1n+7//oX5m+PJzS2g76puR3horTUieeuX8Ekna4U1pdQ1sWcQRAINRaSeiJQD7gEWX7bPEeAWABFpihUE+pXfxdT282bSsDYseLgjQb4VeOab7fSbtPZ/01jky9MLbn3VOjsQgRl9YOU/IPtiyRWuVBlhtyAwxmQDjwIrgN3APGNMjIi8JiL9bLs9DTwgItuBr4HRprQNbFDFpm1dP757+EbeH9KKlLOZ3D35d2b+fogC/0nU6QAPrbNGIq/7AKZ0h+M7S65opcoAHVCmnNLpC5k8NW87P+9J4o6WNXlrQBgVyxcywnjfClj0qLU2ws1/gxsngJt7yRSslJNz2IAypa5VFe9yTB0VzsTbGrNkRyL9J60jNqmQTuFGt8EjG6zFcH56xWouSj1YIvUqVZppECin5eYmjO/egC/Htuf0hUz6fbSORdsuv97gMhWrwaCZcNcUSNptdSRvnV0yBStVSmkQKKd3YwN/fnisC81qVObxOdv4x6KdXMzOufIBItByCDzyu7XwzaJH4LuHdBCaUlegQaBKhSBfL74e14H7O9dj1vrDDP50AwmnCxmE5hsMoxbBTS/CjrnwmXYkK5UfDQJVani6u/FS32Z8MrwNB5LOcfuHv7F2f0rBB7m5w03PwajFkHEGpt4CUTOsaa+VUoAGgSqFeofVYPGjnajuU557Z2zi601HCj+oXhdrEFrdG+GHJ2DBWCsYlFIaBKp0qh9QiQUP30jnBv688G00/1q6u+DRyACVAmD4ArjlZYhZCFO6QeK2kilYKSemQaBKLR8vT6bdG87IDnWZsiaOh2dvJj2zgE5kADc36PIUjF5ijUKeditsnKJNRcqlaRCoUs3D3Y3X+jfnH32b8eOuEwyZsp6kM0VYwObSfEX1u8OyiTB7EKQVcmmqUmWUBoEq9USE+zrX47OR4cQmnePOSevYfawI7f/efjB0DvR+Gw6vg487wOaZenagXE6RgkBEHheRymKZJiJbRKSnvYtT6mr0aBbIvAc7kmMMd3/yO6v3JBV+kJsbtH8QHl5nrY/8/QT44i44XYQOaKXKiKKeEdxnjDkD9ASqAiOBt+xWlVLXKLSWL4vGdybEvyJjZ0Yya/2hoh3oV9+6xPT2dyE+0lr4JnIq5BayvrJSZUBRg+DSwrJ9gC+MMTF5tinlVIJ8vZj3YEdublKdfyyK4dXvY8gp7IoisM4OIu63VkILDoclT8OsfjpfkSrzihoEm0XkR6wgWCEiPoB+VVJOq2J5Dz4dGc59neoxY90hHvwiivMXC1j9LK8qdWDkQrjjQzi2HT65ETZM1rMDVWYVaRpq2/KRrYA4Y8xpEfEDgo0xO+xd4OV0Gmp1tWatP8Qri2NoVrMy0+6NILCyV9EPTouH75+A2JVQtxP0/8hqRlKqlCmOaag7AnttITACeAlIK64ClbKnUR1DmHpvOHHJ57mrqFcUXeIbDMO/gf4fW/MUfdLJGnegZweqDClqEHwCXBCRllirih0AZtmtKqWK2c1NAvnmIeuKokGT1/PL3iJcUXSJCLQebvUd1L3RGncwqx+cOmS3epUqSUUNgmzbEpL9gY+MMZMAH/uVpVTxa17Tl4XjO1HHz5uxM6P4csPhq3sC31owfD70+681NcXHN+qVRapMKGoQnBWRF7AuG11i6zPwLOwgEeklIntFJFZEns/n8f+IyDbbzz4RKWC1cqWuXw3fCsx7qCPdGgXw0sKdvLFkV+FzFOUlAm1GWWcHtdtZVxZ90R9OXWWoKOVEihoEQ4CLWOMJjgPBwDsFHSAi7sAkoDfQDBgqIs3y7mOMedIY08oY0wr4L/DtVdav1FWrVN6DKSPbcm/Hunz228GizVF0uSq1YeR30Pd9SNhiXVmk01urUqpIQWD78J8N+IpIXyDDGFNYH0E7INYYE2eMyQTmYDUtXclQ4Oui1KPU9fJwd+PV/qG8fIc1R9HYmZFXHwYiED7GOjuo1caa3np6LzhW4hfTKXVdijrFxGBgEzAIGAxsFJG7CzmsFnA0z/1427b8nr8uUA/4+QqPjxORKBGJSk5OLkrJShXJmE71eG9wS9bHnWTcF1FkZF1lGIBt3MEiq+/g5H5reuslT8OF1OIvWCk7KGrT0N+ACGPMvcaYUVjf9v9ejHXcA8w3xuT7f6ExZooxJtwYEx4QEFCML6sU3NU6mLcHtmBtbAoPfrG54PWQr8TNzeo7eGwzRDwAUdPhv22t5qLca3g+pUpQUYPAzRiT93q7k0U4NgGoned+sG1bfu5Bm4WUAw0Kr82bd4Xx675kHvlyC5nZ13glUIWq0OdtePA3qN7Uai767GY4Glm8BStVjIoaBMtFZIWIjBaR0cASYGkhx0QCDUWknoiUw/qwX3z5TiLSBGsiu/VFL1up4ndPuzq8fmcoq/Yk8djXW8jKuY7LQoNCrcVvBk6DcydgWg9Y+Aicu4rxC0qVkKJ2Fk8EpgAtbD9TjDHPFXJMNvAosALYDcwzxsSIyGsi0i/PrvcAc0xR5rpQys5GdKjLq/2asyLmBE/M2Ub29YSBCITdDY9GQacnYMc8q7not/cgK734ilbqOhVpriFnonMNqZIw9bc4Xl+ym34ta/KfIa1wdyuGyXZTYuHHv8G+5eBTE7q/CK2GgZv79T+3UoW45rmGROSsiJzJ5+esiFzFhC1KlS73d6nPC72bsHh7IhO/2V60aawL498Ahs2F0Uuhck1Y/Kg1/mDvMh1/oBzKo6AHjTE6jYRyWQ92u4HsXMM7K/bi5ia8PbAFbsVxZhDSCe7/CXYvhlWvwdf3QJ0b4dZXrdHKSpUwXbNYqQKM796AJ3o0ZP7meJ6at+36OpDzEoFm/eGRDXD7e3AyFqbdCnNHQMr+4nkNpYpIg0CpQjx+S0Mm3taYhdsSeWBWFBcyi7jATVG4e0LEWJiwFbr/DQ6shkntYemzkH6q+F5HqQJoEChVCBFhfPcGvDUgjDX7khk+dSOnL2QW74uUrwTdnoUJ26DtvRD5GXzYBiKn6YA0ZXcaBEoV0T3t6vDJiLbEJJ5h0OT1HEuzwyWglQKg73/gwTXWgLQlT8Gn3eDQuuJ/LaVsNAiUugq3NQ9i1n3tOJ6WwcCPfyc26Zx9XigozBqQdvcMq4no8z7wzWg4fbTQQ5W6WhoESl2lDvWrMefBDmTmGAZN/p1tR+20jIYIhA6ARyOh2/PWZaYfRcAv/6cD0lSx0iBQ6ho0r+nLgoc74uPlybDPNrBmnx1nxS3nDd1fsAKh0W3wy7+sQFg/CdJ1LSd1/TQIlLpGdatVZP5DHalbrSJjZ0ayaNuV5lQsJlXqwOCZcO8P4BsMK16E95rC90/AiRj7vrYq03SKCaWu05mMLO6fGcWmg6k8ctMNPN6jIeU9SmDaiGPbYdNnEP0NZGdASBdo9wA0vh3cCxwrqlxQQVNMaBAoVQwysnL4x6KdzIuKp0mQD+8NbkWzmpVL5sUvpMLWLyByKpw+ApVrWSuntRltXYWkFBoESpWYn3ad4CzcRbIAABcwSURBVPlvo0lLz+SJHo14sGt9PNxLqAU2Nwf2rYBNUyBuNbh5wg3dofld0LgPVKhSMnUop6RBoFQJSj2fyUsLo1kafZzWdarw3uBW1POvWLJFJO+DrbMgZhGkHbFCocEttlDoDV6+JVuPcjgNAqVKmDGGxdsT+ceiGC5m5/BC76aM7FC3eCatu7pCIGELxHwLMQvhTDy4l4Mb8oZCCTVhKYfSIFDKQU6cyeDZ+Tv4dV8ynRv48/bdLahZpYJjijEG4qNg10KI+Q7OJFhnCnU7QoMe1k/1Ztb4BVXmaBAo5UDGGL7adIQ3luzG3U145+4W9Aqt4diicnMhIQp2fw+xqyDJdvmpT02rCalBD6h/k/YrlCEaBEo5gSMnL/DYnK1sP3qah2+6gWd6Ni6elc+KQ1oCHFgFsT/BgV/gYhqIu7U+QuPe0HaMNiGVcg4LAhHpBXwAuANTjTFv5bPPYOAVwADbjTHDCnpODQJVml3MzuHV73fx1cYjdGnozwf3tMavYjlHl/VnOVlWE1LsTxC70hqv4F0NujxjTZntUd7RFapr4JAgEBF3YB9wKxAPRAJDjTG78uzTEJgH3GyMOSUi1Y0xSQU9rwaBKgvmRh7h74tiCKhUnk9HtiW0lhNfxZOwGX56BQ6uAd861lrLLQbrWsulzDWvWXyd2gGxxpg4Y0wmMAfof9k+DwCTjDGnAAoLAaXKiiERdZj/UEeMMQz45He+iXLiWUVrtYVRi2HEt1afwcKHYHIX2Ltc11ouI+wZBLWAvP+6423b8moENBKRdSKywdaU9BciMk5EokQkKjnZjpN7KVWCWgRX4fvHOhMRUpWJ83fw0sJoMrOLaSnM4iZidSKP+xUGToOsC/D1EJjRG45sdHR16jo5etI5D6AhcBMwFPhMRP5ymYIxZooxJtwYEx4QoEPmVdlRrVJ5Zo5px4Pd6vPlhiMMmbKe42kZji7rytzcIOxuaybU29+Fkwdgek/4ciBs+9qa7kKVOvYMggSgdp77wbZtecUDi40xWcaYg1h9Cg3tWJNSTsfD3Y0Xejdl0rA27D1+lr7//Y2fdp1wdFkFc/eEiPvh8W1w80vW7KcLH4J3GsDnfWHDZGveI1Uq2LOz2APrg/0WrACIBIYZY2Ly7NMLqwP5XhHxB7YCrYwxJ6/0vNpZrMqy/SfO8uhXW9l74ix3ta7Fy3c0o4q3k11VlJ/cXEjcCnuXwJ4lkLzH2h7UApr0hSa3Q2BzHazmQI68fLQP8D7W5aPTjTFviMhrQJQxZrGICPAu0AvIAd4wxswp6Dk1CFRZl5mdy0erY/l4dSxVvMvx+p3NHT8A7WqlxP4RCkc3AQYqBUK1BlA1JM9PPet3RX8NCTvTAWVKlUIxiWk8O38HMYlnuL1FDV7r15xqlUrhNfznkmDvUqtT+dQh6+ds4p/38axoBUJQKDQfYHVMu3s6oFgnZQzE/WJNMR7Q6JqeQoNAqVIqKyeXT389wAer9uPj5cmr/ZrTt0UNpLR/e85Kt/oQLgVD6kE4dRCOboT0U1DBz5oUr8VgqN3edc8WMtKsTvjIqXByP4SPhb7vXdNTaRAoVcrtO3GWid9sZ3t8Grc1D+Sfd4ZS3cfL0WUVv+xMa6qLHfNg7zLITreW6AwbBGGDoXoTR1dYMo7vhMjPrPch6wLUCrdWn2t2J3he2393DQKlyoDsnFymrj3Ieyv34eXhxoA2wQxsE0xorcql/wwhPxfPwu4fIHqe1SxiciEoDJrcATVaWs1IlWuVnbOF7EzYvdj69n9kPXh4WZfqRtwPNVtf99NrEChVhhxIPse7P+7lp11JZObk0iiwEgPaBHNnq1oE+ZbBswSAsyesNRV2zIPELX9sr1DVCofAMOt3UCj4NwaPUnClVU42nIyFEzutK66iv4FzJ6y+koj7odVw8PYrtpfTIFCqDEq7kMUP0Yl8uyWBzYdPIQKdG/gzsE0wPZsH4l2ujC5gn3EGknbB8eg/fpJ2QbZtIJ6bJ9RoAQ1uhYa3Wt+mHT0vUvppa6zF8Wg4EW01/STv+XPNN9xsNf/ccIs1cK+YaRAoVcYdTDnPd1viWbAlgYTT6VQs506/VjV5sU9TfLxc4OqbnGxIPWALhh1w+HdrBlWM1fHc4BYrGBrcYl2qWpjsi9bVTiYHvKpA+cqFfzjn5lgd4CcPWB27J2Otn5RYa2W4S7yrQWCo7QwmzLrt38juZzEaBEq5iNxcw6ZDqSzYHM93WxNoEezLzPvauUYYXO5CKhz4GfavtKbUvpACCNRqY4VC1brWh/25JKtJ5tyJP25nnL7sycQKgwq+1nrPXlWs3xWqWN/2T8ZCahzkZP5xSHlf8G9gjZ0IaPLHh75PkEP6NTQIlHJBy3ce59GvthBmC4PKrhgGl+TmwrGtsN+2xsKlswUAT29rsFulQKhUPc/tAHDzsC7hTD9t/c5Is0Li0u3001C+ElRr+MeHfrWG1m8nGySnQaCUi/ox5jjjv9pCs5q+zLqvHb4VXDgM8rqQao1XqBRofZC7AEetR6CUcrCezYP4ZHhbdiWmMXLaRtIuZDm6JOfg7QfVbnCZECiMBoFSZVyPZoFMHtGWPcfOMnzaBk5fyCz8IOVSNAiUcgG3NA3k05Ft2XfiHMM+28ip8xoG6g8aBEq5iO5NqjNlZFtik88xbOpGUjUMlI0GgVIu5KbG1Zk6Kpy45HMM+2wDJ89ddHRJygloECjlYro2CmDavREcTDnPPVM2sOmgLi/p6jQIlHJBnRv6M2NMBGnpWQz+dD33Tt/EzoQ0R5elHESDQCkXdeMN/vw6sTsv9G7C9vjT9P3vWsbP3kJs0jlHl6ZKmA4oU0pxJiOLqb8dZNpvcaRn5TCwTTCP92hIcFVvR5emionDBpSJSC8R2SsisSLyfD6PjxaRZBHZZvu53571KKXyV9nLk6dubcSaZ7tzX6d6LNqeSPd//8Iri2NIOpvh6PKUndntjEBE3IF9wK1APBAJDDXG7Mqzz2gg3BjzaFGfV88IlLK/Y2npfLgqlnlRRxEgtJYv7ev50b6+H+Ehfq49b1EpVdAZgT0nLG8HxBpj4mxFzAH6A7sKPEop5XA1fCvw5oAwxnWtzzdRR9l0MJXp6w7y6Zo4RKBZjcq0q+dH+3rVaFfPD7+KpWAhGHVF9gyCWsDRPPfjgfb57DdQRLpinT08aYw5evkOIjIOGAdQp04dO5SqlMpPPf+KPNvLWic4PTOHrUdPselgKhvjUvlq4xFmrDsEQKvaVXhrYBhNgio7sFp1rezZNHQ30MsYc7/t/kigfd5mIBGpBpwzxlwUkQeBIcaYmwt6Xm0aUso5XMzOITo+jY0HU/n890OkpWfxtz5NGdWxbtlcQ7mUc1RncQJQO8/9YNu2/zHGnDTGXBraOBVoa8d6lFLFqLyHO+Ehfozv3oBlj3eh0w3VeHlxDA/MitLpK0oZewZBJNBQROqJSDngHmBx3h1EpEaeu/2A3XasRyllJ/6VyjN9dAT/6NuMNftS6PX+GtbFpji6LFVEdgsCY0w28CiwAusDfp4xJkZEXhORfrbdJohIjIhsByYAo+1Vj1LKvkSE+zrX47vxN+Lj5cGIaRv5v+V7yMrJdXRpqhA6oEwpVewuZGbzzx928fWmo7SsXYUP72lF3WoVHV2WS9MVypRSJcq7nAdvDmjBx8PbcDD5HLd/uJZvoo6Sm1u6vni6Cg0CpZTd9AmrwbInutKsRmUmzt9B7w9+Y/nOYxoITkaDQCllV7WqVODrcR34cGhrsnJzeejLLfT971pW7jpBaWuaLqs0CJRSdufuJvRrWZOVT3bjP0NaciEzmwdmRdF/0jpW70nSQHAw7SxWSpW47JxcvtuawIc/7+doajqtalfhqVsb0aWhvw5Gs5OCOos1CJRSDpOVk8uCzfH89+dYEk6n0yLYlz5hNejZLJD6AZUcXV6ZokGglHJqmdm5zIs6ytzIo0TbVkprWL0SPZsH0rNZEC2CffVM4TppECilSo2E0+msjDnOj7tOsPFgKjm5hhq+XtzaLJDbmgfRrp4fnu7avXm1NAiUUqXSqfOZ/LwniRUxx1mzP5mMrFwCfMoz+sYQRrSvi6+3rotQVBoESqlSLz0zh1/3JfPVpiOs2ZeMdzl37omow32dQ3RJzSLQIFBKlSm7Es/w2W9xfL89EQP0bVGDcV3r07ymr6NLc1oaBEqpMinxdDrT1x7k601HOJ+ZQ5eG/ozrWp/ODfQy1MtpECilyrS09CxmbzzMjHWHSD57kRbBvrzYpykd6ldzdGlOQ4NAKeUSLmbnsHBrAu//tJ9jaRnc2iyQF3o30TEJaBAopVxMRlYO09Ye5OPVsVzMzmVEh7pMuKUhfhXLObo0h9FpqJVSLsXL053x3Rvwy8TuDImozaz1h+j2zmo+/fUAGVk5ji7P6WgQKKXKrACf8rxxVxgrnuhKeN2qvLlsDz3e+9W62qiUtYbYk12bhkSkF/AB4A5MNca8dYX9BgLzgQhjTIHtPto0pJS6Vmv3p/DG0t3sPnaGWlUq0KxmZZrWqEyzGj40CapMHT9v3NzK5tVGBTUNedjxRd2BScCtQDwQKSKLjTG7LtvPB3gc2GivWpRSCqBzQ39+eKwzC7cmsHpvEruPnWHV7hNcWifHu5w7jYN8aFrDCojujQNcYrCa3YIAaAfEGmPiAERkDtAf2HXZfv8E/g+YaMdalFIKsNZGGNg2mIFtgwFrxPL+pLPsPnaG3ces3z9sT+SrjUfw8fJg0rA2dG0U4OCq7cueQVALOJrnfjzQPu8OItIGqG2MWSIiGgRKqRJXoZw7LYKr0CK4yv+2GWM4kHyeR7/awpjPI3n5jmaM6hjiuCLtzGGdxSLiBrwHPF2EfceJSJSIRCUnJ9u/OKWUSxMRGlSvxPyHb6R74wD+sSiGlxftJDsn19Gl2YU9gyABqJ3nfrBt2yU+QCjwi4gcAjoAi0XkL50ZxpgpxphwY0x4QEDZPkVTSjmPSuU9+HRkOOO61mfm+sPcNzOKMxlZji6r2NkzCCKBhiJST0TKAfcAiy89aIxJM8b4G2NCjDEhwAagX2FXDSmlVElydxNe7NOU/xsYxu+xKQz4+HeOnLzg6LKKld2CwBiTDTwKrAB2A/OMMTEi8pqI9LPX6yqllD0MiajDF2Pbk3LuIv0nrWXTwVRHl1RsdIoJpZS6CodSznPfzEiOpl7gX3eFMSi8duEHOQGdYkIppYpJiH9Fvnu4E+3q+TFx/g7Gf7WF5TuPk55ZeqeusOflo0opVSb5envy+Zh2/PvHvczZdJQlO45RwdOdmxoH0Cs0iJubVMfHq/Qso6lNQ0opdR2ycnLZGJfK8phjrIg5QfLZi5Rzd6NzQ396NQ+iR7NAp5j1VKehVkqpEpCba9hy5BTLdx5n2c7jJJxOx91N6N64OuO61icipKrDVk7TIFBKqRJmjCEm8QxLoo8xZ9MRTl3IomXtKjzYtT63NQ/CvYQnt9MgUEopB0rPzGH+lnim/RbHoZMXqOPnzdjO9RgUHox3uZLpqtUgUEopJ5CTa1i56wRT1hxgy5HTVPH2ZGSHuozqGEKAT3mMMZzJyCb1fCYnz13k5PnMP93uWL8aPZsHXdNrO2QaaqWUUn/m7ib0Cg2iV2gQmw+nMmVNHB+tjuXTNXFU9fYk9XwmWTn5fzmvVN6DKhXKXXMQFESDQCmlHKBtXT8+HenHwZTzfLH+MOcuZlGtUnmqVSyHX8Vyf7rtV7EcXp7udqtFg0AppRyonn9F/nFHM4fWoCOLlVLKxWkQKKWUi9MgUEopF6dBoJRSLk6DQCmlXJwGgVJKuTgNAqWUcnEaBEop5eJK3VxDIpIMHL7Gw/2BlGIspyRozSWjtNVc2uoFrbmkXKnmusaYgPwOKHVBcD1EJOpKky45K625ZJS2mktbvaA1l5RrqVmbhpRSysVpECillItztSCY4ugCroHWXDJKW82lrV7QmkvKVdfsUn0ESiml/srVzgiUUkpdRoNAKaVcnMsEgYj0EpG9IhIrIs87up6iEJFDIhItIttExCkXahaR6SKSJCI782zzE5GVIrLf9ruqI2vM6wr1viIiCbb3eZuI9HFkjZcTkdoislpEdolIjIg8btvuzO/zlWp2yvdaRLxEZJOIbLfV+6ptez0R2Wj73JgrIuUcXeslBdT8uYgczPMetyr0uVyhj0BE3IF9wK1APBAJDDXG7HJoYYUQkUNAuDHGaQe0iEhX4BwwyxgTatv2NpBqjHnLFrpVjTHPObLOS65Q7yvAOWPMvx1Z25WISA2ghjFmi4j4AJuBO4HROO/7fKWaB+OE77WICFDRGHNORDyBtcDjwFPAt8aYOSIyGdhujPnEkbVeUkDNDwE/GGPmF/W5XOWMoB0Qa4yJM8ZkAnOA/g6uqUwwxqwBUi/b3B+Yabs9E+sDwClcoV6nZow5ZozZYrt9FtgN1MK53+cr1eyUjOWc7a6n7ccANwOXPlCd7T2+Us1XzVWCoBZwNM/9eJz4H2UeBvhRRDaLyDhHF3MVAo0xx2y3jwOBjiymiB4VkR22piOnaWK5nIiEAK2BjZSS9/mymsFJ32sRcReRbUASsBI4AJw2xmTbdnG6z43LazbGXHqP37C9x/8RkfKFPY+rBEFp1dkY0wboDYy3NWuUKsZqe3T29sdPgBuAVsAx4F3HlpM/EakELACeMMacyfuYs77P+dTstO+1MSbHGNMKCMZqRWji4JIKdXnNIhIKvIBVewTgBxTaXOgqQZAA1M5zP9i2zakZYxJsv5OA77D+cZYGJ2xtxJfaipMcXE+BjDEnbP9D5QKf4YTvs60NeAEw2xjzrW2zU7/P+dVcGt5rY8xpYDXQEagiIh62h5z2cyNPzb1szXLGGHMRmEER3mNXCYJIoKHtCoBywD3AYgfXVCARqWjrZENEKgI9gZ0FH+U0FgP32m7fCyxyYC2FuvRhanMXTvY+2zoFpwG7jTHv5XnIad/nK9XsrO+1iASISBXb7QpYF5bsxvpwvdu2m7O9x/nVvCfPlwPB6tMo9D12iauGAGyXqb0PuAPTjTFvOLikAolIfayzAAAP4CtnrFlEvgZuwpr69gTwMrAQmAfUwZoyfLAxxik6aK9Q701YTRUGOAQ8mKft3eFEpDPwGxAN5No2v4jV5u6s7/OVah6KE77XItICqzPYHesL8jxjzGu2/w/nYDWxbAVG2L5pO1wBNf8MBAACbAMeytOpnP9zuUoQKKWUyp+rNA0ppZS6Ag0CpZRycRoESinl4jQIlFLKxWkQKKWUi9MgUMpGRHLyzNi4TYpxlloRCZE8M54q5Uw8Ct9FKZeRbhuur5RL0TMCpQoh1roQb4u1NsQmEWlg2x4iIj/bJvdaJSJ1bNsDReQ72zzx20XkRttTuYvIZ7a543+0jQZFRCaINW//DhGZ46A/U7kwDQKl/lDhsqahIXkeSzPGhAEfYY1QB/gvMNMY0wKYDXxo2/4h8KsxpiXQBoixbW8ITDLGNAdOAwNt258HWtue5yF7/XFKXYmOLFbKRkTOGWMq5bP9EHCzMSbONpHacWNMNRFJwVp8Jcu2/Zgxxl9EkoHgvFMR2KZiXmmMaWi7/xzgaYx5XUSWYy2WsxBYWNh0AEoVNz0jUKpozBVuX428c9Tk8Ecf3e3AJKyzh8g8s10qVSI0CJQqmiF5fq+33f4dayZbgOFYk6wBrAIehv8tHOJ7pScVETegtjFmNda88b7AX85KlLIn/eah1B8q2FZ7umS5MebSJaRVRWQH1rf6obZtjwEzRGQikAyMsW1/HJgiImOxvvk/jLUIS37cgS9tYSHAh7a55ZUqMdpHoFQhbH0E4caYFEfXopQ9aNOQUkq5OD0jUEopF6dnBEop5eI0CJRSysVpECillIvTIFBKKRenQaCUUi7u/wEnNAPB64UKwgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/Colab Notebooks/MA/bigru/bigru_sp.h5\")"
      ],
      "metadata": {
        "id": "GpjKv6W4f717"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a = keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/MA/bigru/bigru_sp.h5\",  custom_objects={\"get_f1\": get_f1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNhqlW-Vf717",
        "outputId": "3279e4ec-2ae7-4609-e6a0-ed19b082a435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrtqCs5ff717",
        "outputId": "0a582b3b-7960-4de4-cd83-434fd716580a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 1s 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFGRLjJMf718",
        "outputId": "c49d8a7a-e2c6-4f55-f884-6f3524fd9691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.77      0.72        96\n",
            "           1       0.00      0.00      0.00        32\n",
            "           2       0.82      0.90      0.85       183\n",
            "\n",
            "    accuracy                           0.77       311\n",
            "   macro avg       0.50      0.56      0.52       311\n",
            "weighted avg       0.69      0.77      0.72       311\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy')\n",
        "print(accuracy_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1)))\n",
        "print('f1 score')\n",
        "print(f1_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n",
        "print('recall')\n",
        "print(recall_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n",
        "print('precision')\n",
        "print(precision_score(np.argmax(Y_test, axis=-1), np.argmax(p, axis=-1),average='weighted'))\n"
      ],
      "metadata": {
        "id": "Ze6I_NVvf718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3108e2e-f15a-4d62-b364-747c63d3d53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "0.7652733118971061\n",
            "f1 score\n",
            "0.7243838385415041\n",
            "recall\n",
            "0.7652733118971061\n",
            "precision\n",
            "0.6877660804425693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z_-fhWMgGjD"
      },
      "source": [
        "## Transfomers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-6NHi-DgIjP"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 10"
      ],
      "metadata": {
        "id": "V-15Z8oTXVwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "def tokenize(data,max_len = MAX_LEN):\n",
        "    input_ids = list()\n",
        "    attention_mask = list()\n",
        "    for i in tqdm(range(len(data))):\n",
        "        encoded = tokenizer.encode_plus(data[i],\n",
        "                                        add_special_tokens = True,\n",
        "                                        max_length = MAX_LEN,\n",
        "                                        is_split_into_words=True,\n",
        "                                        return_attention_mask=True,\n",
        "                                        padding = 'max_length',\n",
        "                                        truncation=True,return_tensors = 'np')\n",
        "                        \n",
        "        \n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_mask.append(encoded['attention_mask'])\n",
        "    return np.vstack(input_ids),np.vstack(attention_mask)"
      ],
      "metadata": {
        "id": "fkGqlUa2SLY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = train_sp.append(test_sp, ignore_index=True)\n",
        "df.loc[df['SP'] == 'positive', 'SP'] = 1\n",
        "df.loc[df['SP'] == 'negative', 'SP'] = -1\n",
        "df.loc[df['SP'] == 'neutral', 'SP'] = 0\n",
        "\n",
        "data = df.groupby([\"coc\",\"id\",\"aspectTerm\",\"From\",\"To\"])[\"coc\"].apply(list).values\n",
        "labels = df.groupby([\"coc\",\"id\",\"aspectTerm\",\"From\",\"To\"])[\"SP\"].apply(list).values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=29)"
      ],
      "metadata": {
        "id": "hnNxNO0oQvTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HGUyJ7UXNj1",
        "outputId": "56819d8b-f150-462f-d763-b796abf636b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1088,), (467,), (1088,), (467,))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids,attention_mask = tokenize(X_train,max_len = MAX_LEN)\n",
        "val_input_ids,val_attention_mask = tokenize(X_test,max_len = MAX_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHw9qXyEcVDs",
        "outputId": "d7f298f9-cd70-4045-98c1-4fc269379456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1088/1088 [00:00<00:00, 7202.73it/s]\n",
            "100%|██████████| 467/467 [00:00<00:00, 8129.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhpwEYqRh9JM",
        "outputId": "aa778353-f0bb-42b4-ac70-c40c6ec2d8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " array([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzr8QhmtcqVz",
        "outputId": "69c7ca39-9320-462e-e996-361529abc0ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1088, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Checking Padding and Truncation length's\n",
        "was = list()\n",
        "for i in range(len(input_ids)):\n",
        "    was.append(len(input_ids[i]))\n",
        "set(was)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpuhVD7Ncvjv",
        "outputId": "9205117f-6784-49b0-8595-2fd35a133b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{10}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Padding with 2\n",
        "test_tag = list()\n",
        "for i in range(len(y_test)):\n",
        "    test_tag.append(np.array(y_test[i] + [0] * (MAX_LEN-len(y_test[i]))))\n",
        "    \n",
        "# TEST:  Checking Padding Length\n",
        "was = list()\n",
        "for i in range(len(test_tag)):\n",
        "    was.append(len(test_tag[i]))\n",
        "set(was)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5OB5QxQczH9",
        "outputId": "1be11585-e6f1-4b54-e9cc-68aafe4c3303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{10}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Padding with 2\n",
        "train_tag = list()\n",
        "for i in range(len(y_train)):\n",
        "    train_tag.append(np.array(y_train[i] + [0] * (MAX_LEN-len(y_train[i]))))\n",
        "    \n",
        "# TEST:  Checking Padding Length\n",
        "was = list()\n",
        "for i in range(len(train_tag)):\n",
        "    was.append(len(train_tag[i]))\n",
        "set(was)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppjUOqVSioeG",
        "outputId": "225a58a9-3d61-4c82-ae85-6383f44f91a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{10}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ON THE RIGHT TRACK, TRY TO FIGURE OUT ONE TAG"
      ],
      "metadata": {
        "id": "z9zIKZgVeHDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_tag "
      ],
      "metadata": {
        "id": "mccBgEcoc_4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_tags = 3\n",
        "def create_model(bert_model,max_len = MAX_LEN):\n",
        "    input_ids = tf.keras.Input(shape = (max_len,),dtype = 'int32')\n",
        "    attention_masks = tf.keras.Input(shape = (max_len,),dtype = 'int32')\n",
        "    bert_output = bert_model(input_ids,attention_mask = attention_masks,return_dict =True)\n",
        "    embedding = tf.keras.layers.Dropout(0.3)(bert_output[\"last_hidden_state\"])\n",
        "    output = tf.keras.layers.Dense(n_tags,activation = 'softmax')(embedding)\n",
        "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = [output])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "AGX9l2xQR-KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "model = create_model(bert_model,MAX_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dANEqBzvdZK1",
        "outputId": "7f395785-733b-4389-aabe-f812b437559e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBPQrao5WTCQ",
        "outputId": "41658235-e27a-4211-a106-cb75b1f8af90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_6 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_13[0][0]',               \n",
            "                                thPoolingAndCrossAt               'input_14[0][0]']               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 10,                                                \n",
            "                                768),                                                             \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " dropout_265 (Dropout)          (None, 10, 768)      0           ['tf_bert_model_6[0][0]']        \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 10, 3)        2307        ['dropout_265[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,484,547\n",
            "Trainable params: 109,484,547\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the best only\n",
        "\n",
        "filepath=\"/content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 10, verbose = 1,restore_best_weights = True)\n",
        "callbacks_list = [earlystop, checkpoint]"
      ],
      "metadata": {
        "id": "JTRWv90qdpDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_bert = model.fit([input_ids,attention_mask],np.array(train_tag),validation_data = ([val_input_ids,val_attention_mask],np.array(test_tag)),epochs = 50,batch_size = 30,callbacks = callbacks_list,verbose = 1, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jeqwTSPdrW_",
        "outputId": "dab64e61-b258-4fa0-b905-a4785bd47693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37/37 [==============================] - ETA: 0s - loss: nan - accuracy: 0.8913\n",
            "Epoch 1: val_loss did not improve from inf\n",
            "37/37 [==============================] - 24s 210ms/step - loss: nan - accuracy: 0.8913 - val_loss: nan - val_accuracy: 0.9071\n",
            "Epoch 2/50\n",
            "37/37 [==============================] - ETA: 0s - loss: nan - accuracy: 0.9097\n",
            "Epoch 2: val_loss did not improve from inf\n",
            "37/37 [==============================] - 4s 115ms/step - loss: nan - accuracy: 0.9097 - val_loss: nan - val_accuracy: 0.9071\n",
            "Epoch 3/50\n",
            "37/37 [==============================] - ETA: 0s - loss: nan - accuracy: 0.9097\n",
            "Epoch 3: val_loss did not improve from inf\n",
            "37/37 [==============================] - 4s 116ms/step - loss: nan - accuracy: 0.9097 - val_loss: nan - val_accuracy: 0.9071\n",
            "Epoch 4/50\n",
            "37/37 [==============================] - ETA: 0s - loss: nan - accuracy: 0.9097\n",
            "Epoch 4: val_loss did not improve from inf\n",
            "37/37 [==============================] - 4s 115ms/step - loss: nan - accuracy: 0.9097 - val_loss: nan - val_accuracy: 0.9071\n",
            "Epoch 5/50\n",
            "37/37 [==============================] - ETA: 0s - loss: nan - accuracy: 0.9097\n",
            "Epoch 5: val_loss did not improve from inf\n",
            "37/37 [==============================] - 4s 118ms/step - loss: nan - accuracy: 0.9097 - val_loss: nan - val_accuracy: 0.9071\n",
            "Epoch 6/50\n",
            "37/37 [==============================] - ETA: 0s - loss: nan - accuracy: 0.9097\n",
            "Epoch 6: val_loss did not improve from inf\n",
            "37/37 [==============================] - 4s 116ms/step - loss: nan - accuracy: 0.9097 - val_loss: nan - val_accuracy: 0.9071\n",
            "Epoch 7/50\n",
            "37/37 [==============================] - ETA: 0s - loss: nan - accuracy: 0.9097\n",
            "Epoch 7: val_loss did not improve from inf\n",
            "37/37 [==============================] - 4s 117ms/step - loss: nan - accuracy: 0.9097 - val_loss: nan - val_accuracy: 0.9071\n",
            "Epoch 8/50\n",
            "37/37 [==============================] - ETA: 0s - loss: nan - accuracy: 0.9097\n",
            "Epoch 8: val_loss did not improve from inf\n",
            "37/37 [==============================] - 4s 117ms/step - loss: nan - accuracy: 0.9097 - val_loss: nan - val_accuracy: 0.9071\n",
            "Epoch 9/50\n",
            "37/37 [==============================] - ETA: 0s - loss: nan - accuracy: 0.9097\n",
            "Epoch 9: val_loss did not improve from inf\n",
            "37/37 [==============================] - 4s 115ms/step - loss: nan - accuracy: 0.9097 - val_loss: nan - val_accuracy: 0.9071\n",
            "Epoch 10/50\n",
            "37/37 [==============================] - ETA: 0s - loss: nan - accuracy: 0.9097Restoring model weights from the end of the best epoch: 1.\n",
            "\n",
            "Epoch 10: val_loss did not improve from inf\n",
            "37/37 [==============================] - 5s 122ms/step - loss: nan - accuracy: 0.9097 - val_loss: nan - val_accuracy: 0.9071\n",
            "Epoch 10: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " a.predict([val_input_ids,val_attention_mask])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "6My6Kn8Odw3N",
        "outputId": "db9f5d98-00b7-497b-ce41-fdec1c906676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-e1d0c8479ed3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_attention_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/Colab Notebooks/MA/bert/bert_sp.h5\")"
      ],
      "metadata": {
        "id": "2tAunx6Kd0OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/MA/bert/bert_ate.h5\",  custom_objects={\"TFBertModel\": TFBertModel})"
      ],
      "metadata": {
        "id": "caP-k5Emd1XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "plot_graphs(history_bert, 'accuracy')\n",
        "plot_graphs(history_bert, 'loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "JU9HEACXd3D1",
        "outputId": "20aafe48-7010-4e46-f6f7-2f040abce62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pred(val_input_ids,val_attention_mask):\n",
        "    return a.predict([val_input_ids,val_attention_mask])"
      ],
      "metadata": {
        "id": "TgZtDrnNd4nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testing(val_input_ids,val_attention_mask,enc_tag,y_test):\n",
        "    val_input = val_input_ids.reshape(1,maxlen)\n",
        "    val_attention = val_attention_mask.reshape(1,maxlen)\n",
        "    \n",
        "    # Print Original Sentence\n",
        "    sentence = tokenizer.decode(val_input_ids[val_input_ids > 0])\n",
        "    #print(\"Original Text : \",str(sentence))\n",
        "    #print(\"\\n\")\n",
        "    true_enc_tag = enc_tag.inverse_transform(y_test)\n",
        "\n",
        "    #print(\"Original Tags : \" ,str(true_enc_tag))\n",
        "    #print(\"\\n\")\n",
        "    \n",
        "    pred_with_pad = np.argmax(pred(val_input,val_attention),axis = -1) \n",
        "    pred_without_pad = pred_with_pad[pred_with_pad>-1]\n",
        "    pred_enc_tag = enc_tag.inverse_transform(pred_without_pad)\n",
        "    pred_final = pred_enc_tag[0:len(true_enc_tag)]\n",
        "    #print(\"Predicted Tags : \",pred_enc_tag[0:len(true_enc_tag)])\n",
        "    return true_enc_tag, pred_final\n",
        "  "
      ],
      "metadata": {
        "id": "9wrb0wNRd6vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing(val_input_ids[2],val_attention_mask[2],enc_tag,y_test[2])"
      ],
      "metadata": {
        "id": "PtCkhcRRd90v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i in range(len(y_test)):\n",
        "  result = testing(val_input_ids[i],val_attention_mask[i],enc_tag,y_test[i])\n",
        "  results.append(result)\n",
        "\n",
        "y_trues = []\n",
        "for i in range(len(results)):\n",
        "  y_true = results[i][0]\n",
        "  y_trues.append(y_true)\n",
        "\n",
        "y_preds = []\n",
        "for i in range(len(results)):\n",
        "  y_pred = results[i][1]\n",
        "  y_preds.append(y_pred)\n"
      ],
      "metadata": {
        "id": "8UmXg4IVd8Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = pd.DataFrame({\"col1\": np.concatenate(y_trues, axis=0),\"col2\": np.concatenate(y_preds, axis=0)})"
      ],
      "metadata": {
        "id": "Z-WwMlVReB4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy')\n",
        "print(accuracy_score(final.col1, final.col2))\n",
        "print('f1 score')\n",
        "print(f1_score(final.col1, final.col2,average=None))\n",
        "print('recall')\n",
        "print(recall_score(final.col1, final.col2,average=None))\n",
        "print('precision')\n",
        "print(precision_score(final.col1, final.col2,average=None))"
      ],
      "metadata": {
        "id": "pwvqpaVJeCWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(final.col1, final.col2))"
      ],
      "metadata": {
        "id": "G_sqzkiNeFUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bert"
      ],
      "metadata": {
        "id": "sgoX4tkLlweT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_sp.loc[train_sp['SP'] == 'positive', 'SP'] = 1\n",
        "train_sp.loc[train_sp['SP'] == 'negative', 'SP'] = -1\n",
        "train_sp.loc[train_sp['SP'] == 'neutral', 'SP'] = 0\n",
        "train = train_sp[['coc','SP']]\n",
        "\n",
        "test_sp.loc[test_sp['SP'] == 'positive', 'SP'] = 1\n",
        "test_sp.loc[test_sp['SP'] == 'negative', 'SP'] = -1\n",
        "test_sp.loc[test_sp['SP'] == 'neutral', 'SP'] = 0\n",
        "test = test_sp[['coc','SP']]\n"
      ],
      "metadata": {
        "id": "kcJx9jwdlyMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
        "  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
        "                                                          text_a = x[DATA_COLUMN], \n",
        "                                                          text_b = None,\n",
        "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
        "                                                          text_a = x[DATA_COLUMN], \n",
        "                                                          text_b = None,\n",
        "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
        "  \n",
        "  return train_InputExamples, validation_InputExamples"
      ],
      "metadata": {
        "id": "k8I9e-_dnJlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " train_InputExamples, validation_InputExamples = convert_data_to_examples(train, \n",
        "                                                                           test, \n",
        "                                                                           'coc', \n",
        "                                                                           'SP')"
      ],
      "metadata": {
        "id": "LcADuFGwnd8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
        "    features = [] # -> will hold InputFeatures to be converted later\n",
        "\n",
        "    for e in examples:\n",
        "        # Documentation is really strong for this method, so please take a look at it\n",
        "        input_dict = tokenizer.encode_plus(\n",
        "            e.text_a,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length, # truncates if len(s) > max_length\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
        "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
        "            )\n",
        "        )\n",
        "    def gen():\n",
        "        for f in features:\n",
        "            yield (\n",
        "                {\n",
        "                    \"input_ids\": f.input_ids,\n",
        "                    \"attention_mask\": f.attention_mask,\n",
        "                    \"token_type_ids\": f.token_type_ids,\n",
        "                },\n",
        "                f.label,\n",
        "            )\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
        "        (\n",
        "            {\n",
        "                \"input_ids\": tf.TensorShape([None]),\n",
        "                \"attention_mask\": tf.TensorShape([None]),\n",
        "                \"token_type_ids\": tf.TensorShape([None]),\n",
        "            },\n",
        "            tf.TensorShape([]),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "DATA_COLUMN = 'coc'\n",
        "LABEL_COLUMN = 'SP'"
      ],
      "metadata": {
        "id": "RBgi7GNfoQsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\n",
        "\n",
        "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
        "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
        "\n",
        "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
        "validation_data = validation_data.batch(32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jRUeamVpBvK",
        "outputId": "2129524e-4e1c-4b01-a159-bb9cec994f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJP3pfu6qJJV",
        "outputId": "23f335da-82be-410d-9f68-5514a268b0f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<RepeatDataset element_spec=({'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 615
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = c.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "4a2080c7600346ddb9462604c689401e",
            "cb40b713a61e44b7ae916749401eb455",
            "a6beb98ef00c4fabbe72f27ed6cf146f",
            "17368d89249a47f28b784da91c3cb5bd",
            "fd7168a42ea24a6d9bb664ffc6db3f2e",
            "240618320aa84a9e82b0f68fda84fa17",
            "f2eb00500c5c45ac9aaf278ee4fab029",
            "ca71014306834b23bf7f1be1f4fe3a12",
            "1be66cb92845434a98d4b508e050c6c8",
            "fe71e1c17eaf461bb927710c6d686e7b",
            "0f62dab8c6c3441d929304f9422726fa",
            "2f6f5f4064414a2095f8b2485bce8428",
            "df43d542a6ff437a96de31e478bcf402",
            "24866ea0233a4cc587b24988a0b41244",
            "005ba8f20f224b4bae136e21a638a436",
            "b5379d7d39c246139ff19c8e874276b2",
            "b2008997e149413686ea2c5b408c257d",
            "f74eaeffdcd544c198e48494256256e7",
            "7c0bd779859c45d8bf20266f8d38c15a",
            "2493ceefb324487a8cdbbd038c161599",
            "461ef3279a224139a37355cb438182dc",
            "ed44ad561cfe4f93b8ad335e45406121"
          ]
        },
        "id": "EYXuXLV6qaGh",
        "outputId": "ba8e2164-f4a7-455c-b6b4-e5709dabe6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a2080c7600346ddb9462604c689401e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/536M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f6f5f4064414a2095f8b2485bce8428"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-616-82b5504dd69b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2604\u001b[0m                         \u001b[0m_commit_hash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommit_hash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2605\u001b[0m                     )\n\u001b[0;32m-> 2606\u001b[0;31m                     \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcached_file_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2608\u001b[0m                     \u001b[0;31m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             )\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"downloading %s to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m   1243\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEffectiveLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     )\n\u001b[0;32m--> 495\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_bytes_read\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength_remaining\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength_remaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Vf4YPtyDqgKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001,clipvalue=0.5), \n",
        "              loss=\"sparse_categorical_crossentropy\", \n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
        "\n",
        "model.fit(train_data, epochs=100, validation_data=validation_data)"
      ],
      "metadata": {
        "id": "qI0Q0WX2pYvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert"
      ],
      "metadata": {
        "id": "uaGo3YvWvyI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WxNfxkaUwDBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#train_sp.loc[train_sp['SP'] == 'positive', 'SP'] = 1\n",
        "#train_sp.loc[train_sp['SP'] == 'negative', 'SP'] = -1\n",
        "#train_sp.loc[train_sp['SP'] == 'neutral', 'SP'] = 0\n",
        "#train = train_sp[['coc','SP']]\n",
        "\n",
        "df = train_sp#.append(test_sp, ignore_index=True)\n",
        "df.loc[df['SP'] == 'positive', 'SP'] = 1\n",
        "df.loc[df['SP'] == 'negative', 'SP'] = -1\n",
        "df.loc[df['SP'] == 'neutral', 'SP'] = 0\n",
        "df = df[['coc','SP']]\n",
        "\n",
        "df = df.astype({'SP':'int'})\n"
      ],
      "metadata": {
        "id": "GktSNH2nwDQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test data\n",
        "\n",
        "df_test = test_sp#.append(test_sp, ignore_index=True)\n",
        "df_test.loc[df_test['SP'] == 'positive', 'SP'] = 1\n",
        "df_test.loc[df_test['SP'] == 'negative', 'SP'] = -1\n",
        "df_test.loc[df_test['SP'] == 'neutral', 'SP'] = 0\n",
        "df_test = df_test[['coc','SP']]\n",
        "\n",
        "df_test = df_test.astype({'SP':'int'})\n"
      ],
      "metadata": {
        "id": "8flg-rQ-O5N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8YwAeKlDiGT",
        "outputId": "0b00c5e6-73d8-47fb-f60c-8020450d797e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(467, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 671
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "FBSYlhUyvzI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_input_ids = np.zeros((len(df), 256))\n",
        "X_attn_masks = np.zeros((len(df), 256))"
      ],
      "metadata": {
        "id": "Bokb5yVZwKW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "X_input_ids_test = np.zeros((len(df_test), 256))\n",
        "X_attn_masks_test = np.zeros((len(df_test), 256))"
      ],
      "metadata": {
        "id": "rcMGeaiWPB6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_dataset(df, ids, masks, tokenizer):\n",
        "    for i, text in tqdm(enumerate(df['coc'])):\n",
        "        tokenized_text = tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=256, \n",
        "            truncation=True, \n",
        "            padding='max_length', \n",
        "            add_special_tokens=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        ids[i, :] = tokenized_text.input_ids\n",
        "        masks[i, :] = tokenized_text.attention_mask\n",
        "    return ids, masks"
      ],
      "metadata": {
        "id": "mImnmwftwVr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_input_ids, X_attn_masks = preprocessing_dataset(df, X_input_ids, X_attn_masks, tokenizer)\n",
        "#test\n",
        "X_input_ids_test, X_attn_masks_test = preprocessing_dataset(df_test, X_input_ids_test, X_attn_masks_test, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReAxHbVfwZsB",
        "outputId": "c1507e21-75f0-466f-8dce-baa89ea3e62a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "266it [00:00, 2651.75it/s]\u001b[A\n",
            "545it [00:00, 2732.56it/s]\u001b[A\n",
            "819it [00:00, 2388.97it/s]\u001b[A\n",
            "1088it [00:00, 2371.69it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "467it [00:00, 2796.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.zeros((len(df), 3))\n",
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU8TL685w-tR",
        "outputId": "792df5b6-e5ae-42d6-bb1d-08d3bc74dafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1088, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 677
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = np.zeros((len(df_test), 3))\n",
        "labels_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBRi9FwJPL1x",
        "outputId": "7ad5164b-d462-4a29-c180-b68a6da52a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(467, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 678
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.get_dummies(df['SP']).values\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GkM90UCRx29",
        "outputId": "cae52297-30a6-43de-df34-198efb568b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (1088, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = pd.get_dummies(df_test['SP']).values\n",
        "print('Shape of label tensor:', labels_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dwxTnhaR-on",
        "outputId": "c8931fbb-5177-48de-9c6b-ea69e614f4f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (467, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_bnU8uISAsN",
        "outputId": "25ff56a3-ecb4-49c6-ab52-3df4aff40051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 681
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.get_dummies(df['SP']).values\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1Em2UtgR9Vj",
        "outputId": "cc11668e-91a6-4dfb-f3fd-3e0bd068f07a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (1088, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
        "dataset.take(1) # one sample data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzFB6iRdxVyF",
        "outputId": "fdb9f2fd-6de6-4793-f6dc-0b5cef35023f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 683
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((X_input_ids_test, X_attn_masks_test, labels_test))\n",
        "dataset_test.take(1) # one sample data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzLTVIahPjoz",
        "outputId": "977747f8-b92b-472d-e2cb-6cdcf0120e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 684
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ1Df8Ljd5CB",
        "outputId": "c99c65c1-0f4d-4a2f-f79f-dde353812ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1088"
            ]
          },
          "metadata": {},
          "execution_count": 685
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attn_masks\n",
        "    }, labels"
      ],
      "metadata": {
        "id": "3CVVBArD1Q_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(SentimentDatasetMapFunction) "
      ],
      "metadata": {
        "id": "P0orQVSg1Sma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "dataset_test = dataset_test.map(SentimentDatasetMapFunction) "
      ],
      "metadata": {
        "id": "AOkZstPKPp07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2kkStE51U0T",
        "outputId": "432da3fe-839e-4f07-ef92-ee36fcd28dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 689
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nmDVLHPPvXs",
        "outputId": "560e7518-c1e4-4885-a5ef-db912ce7cb6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 690
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ8M1kom1Wlf",
        "outputId": "0edc4580-4a47-456e-bbb7-3f3561228abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 691
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlSozbtjPyav",
        "outputId": "62c53700-405b-4409-8da5-a750a425466e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 692
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(10000).batch(1, drop_remainder=True)\n",
        "#test\n",
        "dataset_test = dataset_test.shuffle(10000).batch(1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "09PgjwZP51Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ua7Lt0PeGr-",
        "outputId": "616c5a91-6366-4d6f-bfbd-432474c03256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 694
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiYW_2vt53-i",
        "outputId": "0685e93c-91cf-4593-ba25-e71e3a768db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None)}, TensorSpec(shape=(1, 3), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 695
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23k6lEV3P4Tj",
        "outputId": "79dc6815-4421-4814-f84d-96e62b1a7eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None)}, TensorSpec(shape=(1, 3), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 696
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = np.round_(0.8 * len(dataset))\n",
        "p\n",
        "#train_size = int((len(df)//16)*p) # for each 16 batch of data we will have len(df)//16 samples, take 80% of that for train."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NiIQHzN1ckY",
        "outputId": "7c7fe9f0-d465-4910-c528-94baf484fbb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "870.0"
            ]
          },
          "metadata": {},
          "execution_count": 697
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#int((len(df)//16)*p)"
      ],
      "metadata": {
        "id": "TN6mKOtw3PE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = dataset.take(p)\n",
        "validation_dataset = dataset.skip(p)\n",
        "#training_dataset = dataset.take(len(dataset))"
      ],
      "metadata": {
        "id": "vpAAsGq_2dte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCILBuZHexbi",
        "outputId": "fccc9e8f-6c1c-4acb-b014-7682d2e218a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "870"
            ]
          },
          "metadata": {},
          "execution_count": 700
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txh3BMtbTWRX",
        "outputId": "6f7421e1-c906-413c-f0ec-8b7b9f1781e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {},
          "execution_count": 701
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEjLaI_V4JOg",
        "outputId": "9360393b-c680-4021-a019-606e22f929c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None)}, TensorSpec(shape=(1, 3), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 702
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "testing_dataset = dataset_test.take(len(dataset_test))\n"
      ],
      "metadata": {
        "id": "S8aGD-a0P-_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(testing_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_muuK64QHoa",
        "outputId": "06bf8903-7647-4b9a-eb96-d60aa86e4391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 704
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertModel.from_pretrained('bert-base-cased') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp6S_l4M2r8p",
        "outputId": "c3858927-9f2f-4fa7-808d-4913ad9e7357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining 2 input layers for input_ids and attn_masks\n",
        "input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
        "attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
        "\n",
        "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] \n",
        "\n",
        "#intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
        "\n",
        "embedding = tf.keras.layers.Dropout(0.3)(bert_embds) ##\n",
        "output_layer = tf.keras.layers.Dense(3, activation='softmax', name='output_layer')(embedding)\n",
        "\n",
        "\n",
        "sentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
        "\n",
        "\n",
        "sentiment_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDgztgAv2zCh",
        "outputId": "f548b92e-9291-4013-b8ed-0479fb52013b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " dropout_418 (Dropout)          (None, 768)          0           ['bert[0][1]']                   \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 3)            2307        ['dropout_418[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,312,579\n",
            "Trainable params: 108,312,579\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim = tf.keras.optimizers.Adam(learning_rate=0.000001) #0.000005 try w other learning rate\n",
        "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')"
      ],
      "metadata": {
        "id": "xKbN8u3-2-mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the best only\n",
        "\n",
        "filepath=\"/content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 3, verbose = 1,restore_best_weights = True)\n",
        "callbacks_list = [earlystop, checkpoint]"
      ],
      "metadata": {
        "id": "QEMcav_P9N2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sentiment_model.compile(optimizer=optim, loss=\"categorical_crossentropy\", metrics=[acc],)"
      ],
      "metadata": {
        "id": "jo5l1L1c3BLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "history = sentiment_model.fit(\n",
        "    training_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    batch_size = batch_size,\n",
        "    epochs=100,\n",
        "    callbacks = callbacks_list\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx7Fjp4s3Dp9",
        "outputId": "6ae94530-6729-4344-f556-4455e8386e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 1.0592 - accuracy: 0.5115\n",
            "Epoch 1: val_loss improved from inf to 0.80614, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 119s 124ms/step - loss: 1.0592 - accuracy: 0.5115 - val_loss: 0.8061 - val_accuracy: 0.6101\n",
            "Epoch 2/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.9256 - accuracy: 0.5632\n",
            "Epoch 2: val_loss improved from 0.80614 to 0.65958, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 105s 121ms/step - loss: 0.9256 - accuracy: 0.5632 - val_loss: 0.6596 - val_accuracy: 0.7339\n",
            "Epoch 3/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.7150 - accuracy: 0.7046\n",
            "Epoch 3: val_loss did not improve from 0.65958\n",
            "870/870 [==============================] - 72s 83ms/step - loss: 0.7150 - accuracy: 0.7046 - val_loss: 0.7084 - val_accuracy: 0.7064\n",
            "Epoch 4/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.6428 - accuracy: 0.7529\n",
            "Epoch 4: val_loss improved from 0.65958 to 0.45257, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 105s 120ms/step - loss: 0.6428 - accuracy: 0.7529 - val_loss: 0.4526 - val_accuracy: 0.8028\n",
            "Epoch 5/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.5709 - accuracy: 0.7828\n",
            "Epoch 5: val_loss did not improve from 0.45257\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.5709 - accuracy: 0.7828 - val_loss: 0.4872 - val_accuracy: 0.8073\n",
            "Epoch 6/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.4871 - accuracy: 0.8080\n",
            "Epoch 6: val_loss improved from 0.45257 to 0.26468, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 104s 119ms/step - loss: 0.4871 - accuracy: 0.8080 - val_loss: 0.2647 - val_accuracy: 0.9220\n",
            "Epoch 7/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.4310 - accuracy: 0.8437\n",
            "Epoch 7: val_loss did not improve from 0.26468\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.4310 - accuracy: 0.8437 - val_loss: 0.2843 - val_accuracy: 0.9037\n",
            "Epoch 8/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.3880 - accuracy: 0.8517\n",
            "Epoch 8: val_loss improved from 0.26468 to 0.24932, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 106s 122ms/step - loss: 0.3880 - accuracy: 0.8517 - val_loss: 0.2493 - val_accuracy: 0.8945\n",
            "Epoch 9/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.3578 - accuracy: 0.8713\n",
            "Epoch 9: val_loss improved from 0.24932 to 0.24621, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 104s 120ms/step - loss: 0.3578 - accuracy: 0.8713 - val_loss: 0.2462 - val_accuracy: 0.9174\n",
            "Epoch 10/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.8908\n",
            "Epoch 10: val_loss improved from 0.24621 to 0.17989, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 105s 121ms/step - loss: 0.3111 - accuracy: 0.8908 - val_loss: 0.1799 - val_accuracy: 0.9312\n",
            "Epoch 11/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.2674 - accuracy: 0.8920\n",
            "Epoch 11: val_loss improved from 0.17989 to 0.16665, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 104s 120ms/step - loss: 0.2674 - accuracy: 0.8920 - val_loss: 0.1667 - val_accuracy: 0.9450\n",
            "Epoch 12/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.2257 - accuracy: 0.9184\n",
            "Epoch 12: val_loss improved from 0.16665 to 0.09622, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 105s 121ms/step - loss: 0.2257 - accuracy: 0.9184 - val_loss: 0.0962 - val_accuracy: 0.9725\n",
            "Epoch 13/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.2293 - accuracy: 0.9195\n",
            "Epoch 13: val_loss did not improve from 0.09622\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.2293 - accuracy: 0.9195 - val_loss: 0.1152 - val_accuracy: 0.9633\n",
            "Epoch 14/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.2069 - accuracy: 0.9287\n",
            "Epoch 14: val_loss improved from 0.09622 to 0.05126, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 103s 119ms/step - loss: 0.2069 - accuracy: 0.9287 - val_loss: 0.0513 - val_accuracy: 0.9862\n",
            "Epoch 15/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.9437\n",
            "Epoch 15: val_loss did not improve from 0.05126\n",
            "870/870 [==============================] - 73s 83ms/step - loss: 0.1788 - accuracy: 0.9437 - val_loss: 0.0592 - val_accuracy: 0.9862\n",
            "Epoch 16/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.1435 - accuracy: 0.9644\n",
            "Epoch 16: val_loss did not improve from 0.05126\n",
            "870/870 [==============================] - 72s 83ms/step - loss: 0.1435 - accuracy: 0.9644 - val_loss: 0.0889 - val_accuracy: 0.9862\n",
            "Epoch 17/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.9563Restoring model weights from the end of the best epoch: 14.\n",
            "\n",
            "Epoch 17: val_loss did not improve from 0.05126\n",
            "870/870 [==============================] - 72s 83ms/step - loss: 0.1675 - accuracy: 0.9563 - val_loss: 0.0759 - val_accuracy: 0.9771\n",
            "Epoch 17: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model.save(\"/content/drive/MyDrive/Colab Notebooks/MA/bert/bert_sp.h5\")"
      ],
      "metadata": {
        "id": "1yqTlt4h9fGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#a = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/MA/bert/bert_sp.h5\",  custom_objects={\"TFBertModel\": TFBertModel})"
      ],
      "metadata": {
        "id": "RYdVEPLpZETL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " a =sentiment_model.predict(testing_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_2QzEdp9W8h",
        "outputId": "b723e27e-2bfc-4103-ccef-fbd4610f386b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "467/467 [==============================] - 10s 19ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "plot_graphs(history, 'accuracy')\n",
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "xZqmdR849hPm",
        "outputId": "faf21b10-8680-4ae8-a1cc-78b7a21e5f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arg = np.argmax(a,axis=1)"
      ],
      "metadata": {
        "id": "7Uwb7p1JbEvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = df_test.SP.values\n",
        "\n",
        "new_y_true = []\n",
        "\n",
        "for y in y_true:\n",
        "    new_y_true.append(y+1)\n",
        "\n",
        "print(new_y_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hd3p7a8hzys",
        "outputId": "6eabb735-f178-4be2-ea80-f79e6de4a6d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 0, 0, 1, 1, 2, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 0, 1, 0, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 0, 1, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 0, 2, 0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 2, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Q-_I1ZEeheHQ",
        "outputId": "82b827f6-aeb0-4135-bb1a-f4034f7929b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   coc  SP\n",
              "0                 succeed straight away. gladly again.   1\n",
              "1    evenly browned, unfortunately regulation tempe...  -1\n",
              "2                             great use, ok small use.  -1\n",
              "3    unfortunately regulation temperature. terms gr...   0\n",
              "4    unfortunately regulation temperature. terms gr...   0\n",
              "..                                                 ...  ..\n",
              "462  handle price quality relationship good. non -s...   1\n",
              "463                  easy, heat materials good quality   1\n",
              "464  easy clean, materials good quality plasticucho...   1\n",
              "465  easy clean, heat good quality plasticuchos. go...   1\n",
              "466         materials good quality plasticuchos. good.   1\n",
              "\n",
              "[467 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc864bb7-cd7a-44a4-bca9-621b09c05f83\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coc</th>\n",
              "      <th>SP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>succeed straight away. gladly again.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>evenly browned, unfortunately regulation tempe...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>great use, ok small use.</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>unfortunately regulation temperature. terms gr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>unfortunately regulation temperature. terms gr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>handle price quality relationship good. non -s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>easy, heat materials good quality</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>easy clean, materials good quality plasticucho...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>easy clean, heat good quality plasticuchos. go...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>materials good quality plasticuchos. good.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>467 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc864bb7-cd7a-44a4-bca9-621b09c05f83')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fc864bb7-cd7a-44a4-bca9-621b09c05f83 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fc864bb7-cd7a-44a4-bca9-621b09c05f83');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 717
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(new_y_true, arg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1aGgSp0iXrp",
        "outputId": "e100c030-ee76-4245-dda1-65e3181029b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.28      0.29       130\n",
            "           1       0.00      0.00      0.00        35\n",
            "           2       0.67      0.70      0.68       302\n",
            "\n",
            "    accuracy                           0.53       467\n",
            "   macro avg       0.32      0.33      0.32       467\n",
            "weighted avg       0.51      0.53      0.52       467\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "           0       0.30      0.28      0.29       130\n",
        "           1       0.00      0.00      0.00        35\n",
        "           2       0.67      0.70      0.68       302\n",
        "\n",
        "    accuracy                           0.53       467\n",
        "   macro avg       0.32      0.33      0.32       467\n",
        "weighted avg       0.51      0.53      0.52       467\n",
        "lr 0.000001\n"
      ],
      "metadata": {
        "id": "qFdAW-ndi1Dv",
        "outputId": "9587c368-c317-4eda-80ea-9b2607fbec31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    accuracy                           0.56       467\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert 2 - 0.53 acc --> worst"
      ],
      "metadata": {
        "id": "iPpDziFe_0tY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "77ktCTdy_0vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#train_sp.loc[train_sp['SP'] == 'positive', 'SP'] = 1\n",
        "#train_sp.loc[train_sp['SP'] == 'negative', 'SP'] = -1\n",
        "#train_sp.loc[train_sp['SP'] == 'neutral', 'SP'] = 0\n",
        "#train = train_sp[['coc','SP']]\n",
        "\n",
        "df = train_sp#.append(test_sp, ignore_index=True)\n",
        "df.loc[df['SP'] == 'positive', 'SP'] = 1\n",
        "df.loc[df['SP'] == 'negative', 'SP'] = -1\n",
        "df.loc[df['SP'] == 'neutral', 'SP'] = 0\n",
        "df = df[['coc','SP']]\n",
        "\n",
        "df = df.astype({'SP':'int'})\n"
      ],
      "metadata": {
        "id": "HTs1SlH3_0vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test data\n",
        "\n",
        "df_test = test_sp#.append(test_sp, ignore_index=True)\n",
        "df_test.loc[df_test['SP'] == 'positive', 'SP'] = 1\n",
        "df_test.loc[df_test['SP'] == 'negative', 'SP'] = -1\n",
        "df_test.loc[df_test['SP'] == 'neutral', 'SP'] = 0\n",
        "df_test = df_test[['coc','SP']]\n",
        "\n",
        "df_test = df_test.astype({'SP':'int'})\n"
      ],
      "metadata": {
        "id": "OrzoT97W_0vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b00c5e6-73d8-47fb-f60c-8020450d797e",
        "id": "2HtITSAL_0vn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(467, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 671
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "Rg7ItI9A_0wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_input_ids = np.zeros((len(df), 256))\n",
        "X_attn_masks = np.zeros((len(df), 256))"
      ],
      "metadata": {
        "id": "9m7O_UgZ_0wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "X_input_ids_test = np.zeros((len(df_test), 256))\n",
        "X_attn_masks_test = np.zeros((len(df_test), 256))"
      ],
      "metadata": {
        "id": "b89D60gP_0wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_dataset(df, ids, masks, tokenizer):\n",
        "    for i, text in tqdm(enumerate(df['coc'])):\n",
        "        tokenized_text = tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=256, \n",
        "            truncation=True, \n",
        "            padding='max_length', \n",
        "            add_special_tokens=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        ids[i, :] = tokenized_text.input_ids\n",
        "        masks[i, :] = tokenized_text.attention_mask\n",
        "    return ids, masks"
      ],
      "metadata": {
        "id": "bzKPEikm_0x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_input_ids, X_attn_masks = preprocessing_dataset(df, X_input_ids, X_attn_masks, tokenizer)\n",
        "#test\n",
        "X_input_ids_test, X_attn_masks_test = preprocessing_dataset(df_test, X_input_ids_test, X_attn_masks_test, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1507e21-75f0-466f-8dce-baa89ea3e62a",
        "id": "GfckkKKs_0x4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "266it [00:00, 2651.75it/s]\u001b[A\n",
            "545it [00:00, 2732.56it/s]\u001b[A\n",
            "819it [00:00, 2388.97it/s]\u001b[A\n",
            "1088it [00:00, 2371.69it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "467it [00:00, 2796.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.zeros((len(df), 3))\n",
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792df5b6-e5ae-42d6-bb1d-08d3bc74dafe",
        "id": "hFxEbjKf_0x6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1088, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 677
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = np.zeros((len(df_test), 3))\n",
        "labels_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad5164b-d462-4a29-c180-b68a6da52a3f",
        "id": "_KUK9E9U_0yA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(467, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 678
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.get_dummies(df['SP']).values\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae52297-30a6-43de-df34-198efb568b7e",
        "id": "NTnQzPCv_0yB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (1088, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = pd.get_dummies(df_test['SP']).values\n",
        "print('Shape of label tensor:', labels_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8931fbb-5177-48de-9c6b-ea69e614f4f4",
        "id": "3muBfQ0-_0yC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (467, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25ff56a3-ecb4-49c6-ab52-3df4aff40051",
        "id": "pgv4gPrM_0yD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 681
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.get_dummies(df['SP']).values\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc11668e-91a6-4dfb-f3fd-3e0bd068f07a",
        "id": "zuaRula2_0y5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (1088, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
        "dataset.take(1) # one sample data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdb9f2fd-6de6-4793-f6dc-0b5cef35023f",
        "id": "9pG10mRC_0y6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 683
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((X_input_ids_test, X_attn_masks_test, labels_test))\n",
        "dataset_test.take(1) # one sample data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "977747f8-b92b-472d-e2cb-6cdcf0120e3d",
        "id": "d6jZPxsF_0y6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 684
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c99c65c1-0f4d-4a2f-f79f-dde353812ecf",
        "id": "xNGJHKvJ_0zR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1088"
            ]
          },
          "metadata": {},
          "execution_count": 685
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attn_masks\n",
        "    }, labels"
      ],
      "metadata": {
        "id": "OC7qezBU_0zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(SentimentDatasetMapFunction) "
      ],
      "metadata": {
        "id": "FD2fhiYv_0zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "dataset_test = dataset_test.map(SentimentDatasetMapFunction) "
      ],
      "metadata": {
        "id": "4vH5G_Zu_0zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432da3fe-839e-4f07-ef92-ee36fcd28dff",
        "id": "ZfpBYQbQ_0zs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 689
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560e7518-c1e4-4885-a5ef-db912ce7cb6a",
        "id": "wTxG5L4s_0zs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 690
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0edc4580-4a47-456e-bbb7-3f3561228abd",
        "id": "8PW3moHp_0zt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 691
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c53700-405b-4409-8da5-a750a425466e",
        "id": "-Bo2TyaM_0zz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 692
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(10000).batch(1, drop_remainder=True)\n",
        "#test\n",
        "dataset_test = dataset_test.shuffle(10000).batch(1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "oRNXJeWl_0zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616c5a91-6366-4d6f-bfbd-432474c03256",
        "id": "LcWaj6Rq_0z0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 694
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0685e93c-91cf-4593-ba25-e71e3a768db9",
        "id": "i_HR_hjW_00K"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None)}, TensorSpec(shape=(1, 3), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 695
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79dc6815-4421-4814-f84d-96e62b1a7eb3",
        "id": "Jg8hpGBI_00L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None)}, TensorSpec(shape=(1, 3), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 696
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = np.round_(0.8 * len(dataset))\n",
        "p\n",
        "#train_size = int((len(df)//16)*p) # for each 16 batch of data we will have len(df)//16 samples, take 80% of that for train."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7fe9f0-d465-4910-c528-94baf484fbb8",
        "id": "e_gVmfpB_00L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "870.0"
            ]
          },
          "metadata": {},
          "execution_count": 697
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#int((len(df)//16)*p)"
      ],
      "metadata": {
        "id": "d7ZP2uBa_00M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = dataset.take(p)\n",
        "validation_dataset = dataset.skip(p)\n",
        "#training_dataset = dataset.take(len(dataset))"
      ],
      "metadata": {
        "id": "uC4QR8Jq_00R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fccc9e8f-6c1c-4acb-b014-7682d2e218a6",
        "id": "ofd_kJgb_00S"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "870"
            ]
          },
          "metadata": {},
          "execution_count": 700
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7421e1-c906-413c-f0ec-8b7b9f1781e5",
        "id": "ewZwFivk_00S"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {},
          "execution_count": 701
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9360393b-c680-4021-a019-606e22f929c8",
        "id": "VDED2rx7_00U"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None)}, TensorSpec(shape=(1, 3), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 702
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "testing_dataset = dataset_test.take(len(dataset_test))\n"
      ],
      "metadata": {
        "id": "F2jcRZOE_00U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(testing_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06bf8903-7647-4b9a-eb96-d60aa86e4391",
        "id": "5sbKneSN_00U"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 704
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertModel.from_pretrained('bert-base-cased') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3858927-9f2f-4fa7-808d-4913ad9e7357",
        "id": "YitVMges_00t"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining 2 input layers for input_ids and attn_masks\n",
        "input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
        "attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
        "\n",
        "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] \n",
        "\n",
        "#intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
        "\n",
        "embedding = tf.keras.layers.Dropout(0.3)(bert_embds) ##\n",
        "output_layer = tf.keras.layers.Dense(3, activation='softmax', name='output_layer')(embedding)\n",
        "\n",
        "\n",
        "sentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
        "\n",
        "\n",
        "sentiment_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f548b92e-9291-4013-b8ed-0479fb52013b",
        "id": "D0Q6uQ3N_00t"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " dropout_418 (Dropout)          (None, 768)          0           ['bert[0][1]']                   \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 3)            2307        ['dropout_418[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,312,579\n",
            "Trainable params: 108,312,579\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim = tf.keras.optimizers.Adam(learning_rate=0.000001) #0.000005 try w other learning rate\n",
        "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')"
      ],
      "metadata": {
        "id": "yqflXMHQ_00v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the best only\n",
        "\n",
        "filepath=\"/content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 3, verbose = 1,restore_best_weights = True)\n",
        "callbacks_list = [earlystop, checkpoint]"
      ],
      "metadata": {
        "id": "D8tWjF2o_00v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sentiment_model.compile(optimizer=optim, loss=\"categorical_crossentropy\", metrics=[acc],)"
      ],
      "metadata": {
        "id": "GpW2v0_G_00v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "history = sentiment_model.fit(\n",
        "    training_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    batch_size = batch_size,\n",
        "    epochs=100,\n",
        "    callbacks = callbacks_list\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae94530-6729-4344-f556-4455e8386e91",
        "id": "GilcFutE_00x"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 1.0592 - accuracy: 0.5115\n",
            "Epoch 1: val_loss improved from inf to 0.80614, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 119s 124ms/step - loss: 1.0592 - accuracy: 0.5115 - val_loss: 0.8061 - val_accuracy: 0.6101\n",
            "Epoch 2/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.9256 - accuracy: 0.5632\n",
            "Epoch 2: val_loss improved from 0.80614 to 0.65958, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 105s 121ms/step - loss: 0.9256 - accuracy: 0.5632 - val_loss: 0.6596 - val_accuracy: 0.7339\n",
            "Epoch 3/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.7150 - accuracy: 0.7046\n",
            "Epoch 3: val_loss did not improve from 0.65958\n",
            "870/870 [==============================] - 72s 83ms/step - loss: 0.7150 - accuracy: 0.7046 - val_loss: 0.7084 - val_accuracy: 0.7064\n",
            "Epoch 4/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.6428 - accuracy: 0.7529\n",
            "Epoch 4: val_loss improved from 0.65958 to 0.45257, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 105s 120ms/step - loss: 0.6428 - accuracy: 0.7529 - val_loss: 0.4526 - val_accuracy: 0.8028\n",
            "Epoch 5/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.5709 - accuracy: 0.7828\n",
            "Epoch 5: val_loss did not improve from 0.45257\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.5709 - accuracy: 0.7828 - val_loss: 0.4872 - val_accuracy: 0.8073\n",
            "Epoch 6/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.4871 - accuracy: 0.8080\n",
            "Epoch 6: val_loss improved from 0.45257 to 0.26468, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 104s 119ms/step - loss: 0.4871 - accuracy: 0.8080 - val_loss: 0.2647 - val_accuracy: 0.9220\n",
            "Epoch 7/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.4310 - accuracy: 0.8437\n",
            "Epoch 7: val_loss did not improve from 0.26468\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.4310 - accuracy: 0.8437 - val_loss: 0.2843 - val_accuracy: 0.9037\n",
            "Epoch 8/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.3880 - accuracy: 0.8517\n",
            "Epoch 8: val_loss improved from 0.26468 to 0.24932, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 106s 122ms/step - loss: 0.3880 - accuracy: 0.8517 - val_loss: 0.2493 - val_accuracy: 0.8945\n",
            "Epoch 9/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.3578 - accuracy: 0.8713\n",
            "Epoch 9: val_loss improved from 0.24932 to 0.24621, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 104s 120ms/step - loss: 0.3578 - accuracy: 0.8713 - val_loss: 0.2462 - val_accuracy: 0.9174\n",
            "Epoch 10/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.8908\n",
            "Epoch 10: val_loss improved from 0.24621 to 0.17989, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 105s 121ms/step - loss: 0.3111 - accuracy: 0.8908 - val_loss: 0.1799 - val_accuracy: 0.9312\n",
            "Epoch 11/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.2674 - accuracy: 0.8920\n",
            "Epoch 11: val_loss improved from 0.17989 to 0.16665, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 104s 120ms/step - loss: 0.2674 - accuracy: 0.8920 - val_loss: 0.1667 - val_accuracy: 0.9450\n",
            "Epoch 12/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.2257 - accuracy: 0.9184\n",
            "Epoch 12: val_loss improved from 0.16665 to 0.09622, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 105s 121ms/step - loss: 0.2257 - accuracy: 0.9184 - val_loss: 0.0962 - val_accuracy: 0.9725\n",
            "Epoch 13/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.2293 - accuracy: 0.9195\n",
            "Epoch 13: val_loss did not improve from 0.09622\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.2293 - accuracy: 0.9195 - val_loss: 0.1152 - val_accuracy: 0.9633\n",
            "Epoch 14/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.2069 - accuracy: 0.9287\n",
            "Epoch 14: val_loss improved from 0.09622 to 0.05126, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 103s 119ms/step - loss: 0.2069 - accuracy: 0.9287 - val_loss: 0.0513 - val_accuracy: 0.9862\n",
            "Epoch 15/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.9437\n",
            "Epoch 15: val_loss did not improve from 0.05126\n",
            "870/870 [==============================] - 73s 83ms/step - loss: 0.1788 - accuracy: 0.9437 - val_loss: 0.0592 - val_accuracy: 0.9862\n",
            "Epoch 16/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.1435 - accuracy: 0.9644\n",
            "Epoch 16: val_loss did not improve from 0.05126\n",
            "870/870 [==============================] - 72s 83ms/step - loss: 0.1435 - accuracy: 0.9644 - val_loss: 0.0889 - val_accuracy: 0.9862\n",
            "Epoch 17/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.9563Restoring model weights from the end of the best epoch: 14.\n",
            "\n",
            "Epoch 17: val_loss did not improve from 0.05126\n",
            "870/870 [==============================] - 72s 83ms/step - loss: 0.1675 - accuracy: 0.9563 - val_loss: 0.0759 - val_accuracy: 0.9771\n",
            "Epoch 17: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model.save(\"/content/drive/MyDrive/Colab Notebooks/MA/bert/bert_sp.h5\")"
      ],
      "metadata": {
        "id": "XLBGjTOZ_00x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#a = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/MA/bert/bert_sp.h5\",  custom_objects={\"TFBertModel\": TFBertModel})"
      ],
      "metadata": {
        "id": "KoFF1RDh_00y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " a =sentiment_model.predict(testing_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b723e27e-2bfc-4103-ccef-fbd4610f386b",
        "id": "mi6VHBam_00z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "467/467 [==============================] - 10s 19ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "plot_graphs(history, 'accuracy')\n",
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "faf21b10-8680-4ae8-a1cc-78b7a21e5f97",
        "id": "lxFSdidH_00z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUZdbA8d/JpFFCSCMBAoQaetEIKKtSZAVFsbwIthVs62vXXcu6rvK6rLq79l0bohQbuigu6woIgoKASIcQQpGaQEgBQhJSZ573jzuEAIEkZG5mkjnfz2c+mblz596ToPfMfcp5xBiDUkop/xXg7QCUUkp5lyYCpZTyc5oIlFLKz2kiUEopP6eJQCml/FygtwOoqejoaJOQkODtMJRSql5Zs2ZNtjEmprL3bEsEIvIBMArINMb0rOR9AV4HrgCOAeONMWurOm5CQgKrV6/2dLhKKdWgicieM71nZ9PQNGDEWd4fCXR2P+4G3rYxFqWUUmdgWyIwxiwBDp1ll9HADGP5CWguIi3tikcppVTlvNlZ3BrYV+F1mnubUkqpOlQvOotF5G6s5iPatm172vulpaWkpaVRVFRU16GpSoSGhhIfH09QUJC3Q1FKVYM3E0E60KbC63j3ttMYYyYDkwGSkpJOK46UlpZGWFgYCQkJWH3QyluMMeTk5JCWlkb79u29HY5Sqhq82TQ0B/iNWAYCucaYA+dyoKKiIqKiojQJ+AARISoqSu/OlKpH7Bw++ikwGIgWkTTgWSAIwBjzDvAN1tDRHVjDRyfU8ny1+bjyIP23UKp+sS0RGGNurOJ9A9xn1/mVUn4i7yBs+BRKCrwdyekkAKI6Qat+ENkBAnyzmEO96CxWSqnT5GfCstdh1ftQVgj44p1ohS7NkGbQso/1aNXPekS094nkoImgnikrKyMwUP/ZlB/Lz4Jlr1kJwFkMvcfBJb+HqI7ejux0zlLI2gr718GB9dbPn9+z4oYTyaFVX2jZ98SdQx03r+oVxYOuueYa9u3bR1FREQ899BB333038+bN46mnnsLpdBIdHc13331Hfn4+DzzwAKtXr0ZEePbZZ7n++utp2rQp+fn5AMyaNYuvv/6aadOmMX78eEJDQ1m3bh2DBg1i3LhxPPTQQxQVFdGoUSOmTp1KYmIiTqeTJ554gnnz5hEQEMBdd91Fjx49eOONN/jqq68AWLBgAW+99RazZ8/25p9KqZrLz4Llx+8AiqD3WLjkMd9MAMc5giCup/XgVmubsxSyUq2ksN+dHFZOrpAcwqFVnxOJoVVf687BxuTQ4BLB//1nMyn7j3r0mN1bNePZq3pUud8HH3xAZGQkhYWFXHDBBYwePZq77rqLJUuW0L59ew4dsiZa//nPfyY8PJxNmzYBcPjw4SqPnZaWxvLly3E4HBw9epSlS5cSGBjIwoULeeqpp/jiiy+YPHkyu3fvZv369QQGBnLo0CEiIiK49957ycrKIiYmhqlTp3L77bfX7g+iVF0qyHY3AU2xEkCvG6wEEN3J25GdG0cQxPWyHuf9xtrmLIXMLSffOax8B5wl1vuh4VZiuOgB6Dzc4yE1uETgTW+88Ub5N+19+/YxefJkLrnkkvLx9JGRkQAsXLiQmTNnln8uIiKiymOPGTMGh8MBQG5uLrfddhvbt29HRCgtLS0/7j333FPedHT8fLfeeisfffQREyZMYMWKFcyYMcNDv7FSNirIhuVvWE0pZUXQa4w7AXT2dmSe5wiClr2tB7dZ28pKIGvLiTuHA+utv4MNGlwiqM43dzt8//33LFy4kBUrVtC4cWMGDx5M3759SU1NrfYxKg67PHUcfpMmTcqf/+lPf2LIkCHMnj2b3bt3M3jw4LMed8KECVx11VWEhoYyZswY7WNQ52bHQlj5LoS1tJorWvWDFt0hMMSz5ynIOZEASo9Br/+BSx6HmC6ePY+vCww+0bl8vs2nsvfw/iM3N5eIiAgaN25MamoqP/30E0VFRSxZsoRdu3aVNw1FRkYyfPhw3nzzTV577TXAahqKiIggNjaWLVu2kJiYyOzZswkLCzvjuVq3tsoyTZs2rXz78OHDeffddxkyZEh501BkZCStWrWiVatWTJo0iYULF9r+t1ANzJF9MP8p2DIHwlrBvp9h7XTrvYAgiO1xcmdni+7WRaymCnJgxT+s9vLSY9Dzerj0cYhJ9Ozvo06jicBDRowYwTvvvEO3bt1ITExk4MCBxMTEMHnyZK677jpcLhctWrRgwYIFPP3009x333307NkTh8PBs88+y3XXXceLL77IqFGjiImJISkpqbzj+FSPP/44t912G5MmTeLKK68s337nnXeybds2evfuTVBQEHfddRf3338/ADfffDNZWVl069atTv4eqgEoK4EV/4QlfwdjYNgzcOH94AiGw7vdbdnu9uzNs2HNNOtzjmArObTse+LOIabbmZPDsUOw/B/w82RrLkDP6+DSJzQB1CGx5nXVH0lJSebUhWm2bNmiF7gq3H///fTr14877rijTs6n/yb13C+L4ZvHIGc7dB0FI16A5qcXfCxnjJUcKnZ27t8AxbnW+45giO158p1DWJzVIbryXSsB9LjWSgAtutbJr2i3whIni1Izad44iMS4MKKbergJrYZEZI0xJqmy9/SOwA+cf/75NGnShJdfftnboShfl5sO3/7R+oYf0R5unlW9USoiENneevS8ztpmDBzedfIwyU1fwOoPKn4QelzjTgAN44tDVl4xH67YzYc/7eHwsdLy7dFNg0mMCyMxthld48JIjAujS2wYjYId3gvWTROBH1izZo23Q1C+rqwEVr4N3/8VjBOGPG0NVQwKPfdjiliToyI7WO39AC7XieRwaKd1txHb3TO/g5ftyMzn/R938sXadEqdLoZ3i2X8RQm4DKRmHGVrRh5bD+bxyc97KCp1AdafKCGqCYmxVmI4niDaRTXBEVB3k8o0ESjl73Ytgf/+HrK3QuIVVjNQRII95woIsCaA+fIksBowxrBy1yHeW7KT71IzCQkMYMz58dzxq/Z0iGlavt+vOkeXP3e6DHsPHWNrxlFSM/JIPWAliPkpGRxvqQ8NCqBzi5OTQ2JcGDFNQ2wp6qiJQCl/dfQAfPs0JM+C5u3gxs8g8WzLjKvjypwu5iZn8N7SnWxMyyWySTAPX9aZWwe2I6qKvgBHgNA+ugnto5swoueJ1XkLS5xsz8wjNSPPunvIyOP7rVnMWpNWvs8zo7pz+688v86HJgKl/I2z1Oqg/f4F6/ngP8CghyCokbcj83n5xWV8vmof7/+4i/QjhbSPbsJfru3J9efFExpUu7b+RsEOesc3p3d885O25+QXszXDShAXdYqq1TnORBOBUv5k9zL45veQmQKdfw0j/2q14auzOni0iGnLd/PxT3s4WlTGBQkRPHtVdy7rFkuAzW35UU1DuKhTCBd1iq5653OkiUApf5B3EBb8CTZ+BuFtYdynkDiyzqtc1jdbM/J4b+lO/r0+HafLMLJnS+68uD392lZdFqY+0UTgBRWrjCplq8LDsP4T+P5Fq07NJY/Drx6B4MbejsxnGWNY/ksOk5fs5IdtWTQKcnDzgHbcPqg9baMa5t9NE4Ef07UNGqicX2DrXNg2D/Yst4aDdhwGV/y9wYzWscOenALmJmfw1bp0UjPyiG4awmOXJ3LzgLY0b3wOJTPqkYZ3FZj7JGRs8uwx43rByBfP+PaTTz5JmzZtuO8+a+XNiRMnEhgYyOLFizl8+DClpaVMmjSJ0aNHV3mq/Px8Ro8eXennZsyYwUsvvYSI0Lt3bz788EMOHjzIPffcw86dOwF4++23adWqFaNGjSI5ORmAl156ifz8fCZOnFheDO/HH3/kxhtvpEuXLkyaNImSkhKioqL4+OOPiY2NrXTNhNzcXDZu3FheI+m9994jJSWFV199tVZ/XlVLzjJI+/nExT97m7W9RXerEzjxCohP0magSuzIzGde8gG+2ZRBygGrfH2f+HD+dn1vru7bqtYdwPVFw0sEXjB27Fgefvjh8kTw+eefM3/+fB588EGaNWtGdnY2AwcO5Oqrr65yDHBoaCizZ88+7XMpKSlMmjSJ5cuXEx0dXb62wYMPPsill17K7NmzcTqd5OfnV7m+QUlJCcfLdBw+fJiffvoJEWHKlCn87W9/4+WXX650zYSgoCD+8pe/8Pe//52goCCmTp3Ku+++W9s/nzoXRbmw4zvrwr/9W6sJKCAIEgZB0h3WMFC75gLUY8YYth3M55tNB5ibfIBtB60m2qR2ETx9ZTdG9IwjPqJhNv+cTcNLBGf55m6Xfv36kZmZyf79+8nKyiIiIoK4uDgeeeQRlixZQkBAAOnp6Rw8eJC4uLizHssYw1NPPXXa5xYtWsSYMWOIjrZGDhxfa2DRokXl6ws4HA7Cw8OrTARjx44tf56WlsbYsWM5cOAAJSUl5WsnnGnNhKFDh/L111/TrVs3SktL6dWrVw3/WuqcHdplXfi3zoU9y8BVBo0iofPl1oW/4zAIbebtKH2OMYbN+48yN/kAczdlsDO7ABHonxDJ/13dg8t7xBEXXosZ1A1Aw0sEXjJmzBhmzZpFRkYGY8eO5eOPPyYrK4s1a9YQFBREQkLCaWsMVOZcP1dRYGAgLper/PXZ1jZ44IEHePTRR7n66qv5/vvvmThx4lmPfeedd/L888/TtWtXJkyYUKO4VA25nJC2GrbNha3zrEVKAKK7wIX3QZeR0KY/BPhH80VNGGPYkJbL3E0HmJucwd5Dx3AECBd2iOKOi9vz6+5xxIR5twicL9FE4CFjx47lrrvuIjs7mx9++IHPP/+cFi1aEBQUxOLFi9mzZ0+1jpObm1vp54YOHcq1117Lo48+SlRUVPlaA8OGDePtt9/m4YcfLm8aio2NJTMzk5ycHJo2bcrXX3/NiBGVzxituLbB9OnTy7efac2EAQMGsG/fPtauXcvGjRtr8ydTlSnOg18WWRf+7fPhWA6IA9pdBOc9D11GaIfvGbhchrV7D/PNpgzmb84g/UghQQ5hUKdo7h/Sicu6xxLZpGF3+p4rTQQe0qNHD/Ly8mjdujUtW7bk5ptv5qqrrqJXr14kJSXRtWv1Suue6XM9evTgj3/8I5deeikOh4N+/foxbdo0Xn/9de6++27ef/99HA4Hb7/9NhdeeCHPPPMM/fv3p3Xr1mc998SJExkzZgwREREMHTqUXbt2AZxxzQSAG264gfXr11driU1VDUf2Whf+bXNh94/WOrWh4daEry4joNNl0Kh51cfxM06XYVd2PhvTclm79zDfbj5IZl4xwYEBXNI5hkeHd+GybrGENw7ydqg+T9cjUDU2atQoHnnkEYYNG3bGffTf5CxcLti/9sQon4PW6C4iO1qTvBJHQpuB4NDvace5XIad2QUkp+eyMS2X5PRckvfncqzECUCjIAeXdolhZK84hnZtQVioXvxPpesRKI84cuQI/fv3p0+fPmdNAqoSJQXWYi/b5sK2b6EgEyQA2l4Iw/9sXfwb4qLs58DlMuzOKWBTei6b0nLZmJ5Lyv6j5BeXARASGECPVs24IakNPVuH0zs+nI4xTeu0bHNDo4nASzZt2sStt9560raQkBBWrlzppYiq1rx5c7Zt21bzD2bvgIXPQpfLrbr0wU2q/kxDkJtufePfNg92/gDOYghpZjX1JI60fjaO9HaUXuVyGfYcOua+6B9hU3oum9OPklfhot+tZTOuO691+UW/U0xTAh0BXo68YWkwicAYY0udbrv06tWL9evXezsMW5zW3Lj8dUj92nrMewp63wBJE6yJeg2Jy2Ut03h8iGeGuzM9IgGSbreGeLa96NwWdvcBxhjW7TtCbmEppWUuSpwuSp0uSspclDgNJWXW6+PvlbjfKy3/adz7WtsKistIzcgjr8i66Ae7L/qj+7Wid+vm9GwdTufYpgTpRd92DSIRhIaGkpOTQ1RUVL1KBg2RMYacnBxCQ93jsouOWssT9rsF+t5iLXC+7iNY/T60TrISQo/r6nftm+J8WDXFKu2ct99q8onvD5dNtIZ4xiTW+1m9G9OO8H//SWHNnrPPUakoODCAEEcAQYEBBDsCCAoUghzW8+DAAEIDHVzdpxW9WofTKz6cLrFhetH3kgbRWVxaWkpaWlqNx9sre4SGhhIfH09QUJC1Pu3Xj8CdiyD+fGuHY4dgw0xYM9UqhxASfuIuIbaHd4OviZICKwEse90a5tlhCPQea432aWJP3fi6lplXxN/nbWXW2jSimgTzyPAudGvZrPxiHlT+UwhxOMov9oEBol/KfMzZOosbRCJQPuzdS6wmk3uWnv6t2BirKNqaaZDyb6sNPb6/+y7hWt9dKKWkAFa9704A2dBxqLW4S5v+3o7MY4rLnExbtpt/LNpBcZmT2we15/6hnXQ0Tj2mo4aUd+xfBwc2wBUvVd40ImLVxkkYZC2Qsv4T6y7hq/+FeU9Cnxvh/PHQwkeGoZYcs5q0lr0OBVnWHcDgP0DbAd6OzGOMMXy3JZNJ/01hd84xhnVtwdOjutM+2k86+P2UJgJlnzXTILCR1exTlcaRcNH9VumE3T9aCWHV+7DyHWtMfdIE6D7aO3cJJcesJq5lr7kTwGB3AhhY97HYaPvBPJ77OoWl27PpGNOEaRMuYHBiC2+HpeqArYlAREYArwMOYIox5sVT3m8HfADEAIeAW4wxaacdSNU/xXmwaZY1XDQ0vPqfE4H2F1uPgmz3XcI0mP1bmPsE9L0J+t1q3SXY3QZdWmglgB9fs8b9t7/USgDtLrT3vHUs91gpr323jRkr9tA42MEzo7pz64XttOPWj9iWCETEAbwJDAfSgFUiMscYk1Jht5eAGcaY6SIyFHgBuPX0o6l6J/kLKMm3mnbOVZNoGPQgXPQA7Fpi3SX8/B789BY0ioCWfaFVX/fPftC8rWeSQ2khrJ5q3QHkH4T2l8Dg6Va9nwbE6TJ8+vNeXv52K7mFpdzYvy2PDu9CVFMtxuZv7Lwj6A/sMMbsBBCRmcBooGIi6A486n6+GPjKxnhUXVozDVr0sBZEqS0R6HCp9cjPsuYj7F9nPZb/wyrHDBWSQz8rQbTqB+Ftqp8cSgutuH981UoACRfD/0y1+jAamOW/ZPPcf1JIzchjQPtInr2qB91baQlrf2VnImgN7KvwOg04tVdtA3AdVvPRtUCYiEQZY3Iq7iQidwN3A7Rt29a2gJWH7F9vXaRH/t3zzTdNY6z+AtwlsEuLIHPziXMeWA/L36iQHCJPJIXjSSI8/uS4SosqJIAMdwL4ABJ+5dnYfcC+Q8d4/pstzE3OoHXzRrx183mM7BmnQz39nLc7i38P/FNExgNLgHTAeepOxpjJwGSwho/WZYDqHKydDoGh1eskrq2gUGh9vvU4rrQIDm6GA+67hv0brHZ+4/5Pq3HUiaQQ0tQ9EewAtBsE10+x+icamGMlZby1+BcmL92JQ4TfDe/CXZd08JulGNXZ2ZkI0oE2FV7Hu7eVM8bsx7ojQESaAtcbY47YGJOyW3E+bPyXNVvYW6WTg0KtyWvxFZNDoZUc9q+z7h4OrLfuAIzTKvtw3WSrL6AeMcZYZRvcZR1KnS6Kj5d0cLooLTOUOJ38klnAKwu2kXG0iGv6tuKJkV1pGe6jczSUV9iZCFYBnUWkPVYCGAfcVHEHEYkGDhljXMAfsEYQqfps85dQkle7TmI7BDWy+isq9lmUFlp9Ac3b+UwJiILiMr5cm8Z/Nx2goNhZXqvn+AX+pLo9TlfVB3TrHR/Omzf34/x2/l3kTlXOtkRgjCkTkfuB+VjDRz8wxmwWkeeA1caYOcBg4AURMVhNQ/fZFY+qI2umQUy3+jHLNqiRzyzwvju7gBkr9vCv1fvIKy6ja1wYrZo3IsghBAc6rBIO7pIOQRXKO4S4yzsEV6jpE3y8to/7eZOQQPq1aU6AlmlWZ2BrH4Ex5hvgm1O2PVPh+Sxglp0xqDp0YCOkr4ERf/WZb9i+zOUy/Lgjm2nLd7N4ayYOEa7o1ZLbLkrgvLbNtQNX1RlvdxarhqQuO4nrsfziMr5Yk8b0FbvZmVVAdNMQHhjamZsHtCW2Wai3w1N+SBOB8oySAtj4OXS/xu8XWzmTXdkFTF++m1lr0sgvLqNPfDivju3DFb1aEhKoo3eU92giUJ6xeTYUH/W9TmIvc7kMS7ZnMW35br7fmkWQQ7jS3fzTr22Et8NTCtBEoDxlzTSITmxwhdjOVV5Rqbv5Zw+7sq3mn4cv68xNA9rSIkybf5Rv0USgai8jGdJWweUv+H0n8c6sfGas2FPe/NO3TXNeH9eXkT1bEhyoRdyUb9JEoGpv7XRwhECfcd6OxCtcLsMP27OYtmw3P2yzmn9G9W7FbRcl0LeNlybVKVUDmghU7ZQcgw2fWWsF+Fkn8dGiUmatTuPDn6zmn5iwEB65rAs3DmijzT+qXtFEoGon5SsozvWrTuIdmfnMWLGbL9akUVDi5Ly2zXlYm39UPaaJQNXOmmkQ1bnB1eo/lctl+H5bJlOX7Wbp9myCHQGM6tOS8Rcl0Dtem39U/aaJQJ27gymwbyX8+i8NtpP4aFEp/1qdxowVu9mTc4zYZiH8bngXbhzQlmhdwEU1EJoI1LlbOx0cwdYi8w3Mjsw8pi/fwxdr0zhW4uT8dhH8/teJjOgZp0s4qgZHE4E6N6WFsOFT6HY1NInydjQe4XQZvt+aybTlJ5p/rurTivEXJdArvgbrLitVz2giUOcm5d9Q1DA6iXMLS/nX6n3MWLGHvYeOEdcslMcuT2TcBW10/V7lFzQRqHOzZhpEdqyXyzmWOl3szCogNeMoK3cdYvbadApLnVyQEMETI7ry6x6x2vyj/IomAlVzmamwdwUM/7NPdxIbYziQW8TWjDxSM/LYmnGU1Iw8fsnKp9RprXgaHBjA6D7W5K+erbX5R/knTQSq5tZOh4Ag6HtT1fvWkaNFpSdd8Ldm5LE1I4+jRWXl+7QMDyUxLoxLE2PoGhdGYmwzOrZoopU/ld/TRKBqprQI1n8C3a6CJtF1fnpjDDsy80k5cNR90bce6UcKy/cJCwkkMS6Mq/q0si74cc1IjA0jvHFQncerVH2giUDVzJY5UHTEK53EhSVOnv4qmS/WpgEQGCB0jGnK+e0iuGlAW7q1tC76rcJDdXUvpWpAE4GqmTXTILIDJFxcp6fdk1PAPR+tJTXjKPcN6chVfVrRIbqplnRQygM0Eajqy9oGe5bBZf8HAXV3AV6UepCHZ65HRPhg/AUMSWxRZ+dWyh9oIlDVV95JfHOdnM7pMry+cBtvLNpBj1bNeOeW82kT2bhOzq2UP9FEoKrneCdx1yuhaYztpztcUMJDn61nybYsxpwfz5+v6UlokI7uUcoOmghU9aR+DYWH6qSTeFNaLvd8tIasvGJeuK4X4y5oo52/StlIE4GqnjXTICIB2l9q62k+W7WXP/17M9FNgvnXPRfSR1f4Usp2mghU1bJ3wO6lMOxZ2zqJi0qdTJyzmZmr9nFx52heH9ePyCbBtpxLKXUyTQSqamunQUCgbZ3E+w4d496P17IpPZf7h3TikeFdcARoU5BSdUUTgTq7smKrkzhxJITFevzwP2zL4qGZ63C6DFN+k8Rl3T1/DqXU2WkiUGeX+jUcy/F4J7HLZfjn4h28unAbibFhvHPL+SREN/HoOZRS1aOJQJ3dmmkQ3hY6DPXYIXOPlfLI5+tZlJrJtf1a8/y1vWgUrENDlfIWTQTqzHJ+gV1LYOjTHusk3rw/l//9aC0Hcgt5bnQPbh3YToeGKuVlmgjUma2dDuKAvrd45HBfrEnjqdmbiGgczGe/vZDz2kZ45LhKqdrRRKAqV1YC6z62OombtazVoYpKnUz6bwof/bSXCztE8Y+b+hGtS0Aq5TM0Efij0kLIy4D8g5B3wHqedwDyKr7OgOLarUnschlmr0vn5W+3sj+3iN9e2oHHfp1IoC4DqZRPsTURiMgI4HXAAUwxxrx4yvttgelAc/c+TxpjvrEzpgYvNx1y06wL+pku9EVHTv9cQBCEtbSGiMZ0gQ6XQlRn6DjsnMJYviObv3yzhc37j9InPpxXx/ZlQIeoWv5ySik72JYIRMQBvAkMB9KAVSIyxxiTUmG3p4HPjTFvi0h34Bsgwa6YGrwNM2H2b0/eFhAEYXHQNBai3IvNh8W6L/px0DTOet440iPrD28/mMcLc1NZlJpJ6+aNeH1cX67q3YoAnSCmlM+y846gP7DDGLMTQERmAqOBionAAM3cz8OB/TbG07A5y2Dx8xDXC4ZNtC7yYXHQKLJO1g7IzCvi1QXb+WzVXpqEBPLUFV35zYUJWjFUqXrAzkTQGthX4XUaMOCUfSYC34rIA0AT4LLKDiQidwN3A7Rt29bjgTYIm2fDkT0w9mPoXOmf0RbHSsp4b8ku3l3yC6VOF7ddlMCDQzsToXWClKo3qpUIRORL4H1grjHG5cHz3whMM8a8LCIXAh+KSM9Tz2GMmQxMBkhKSjIePH/D4HLBj69ATFdIvKJOTul0GWat2cfL324jM6+YK3rF8fjlXXV2sFL1UHXvCN4CJgBviMi/gKnGmK1VfCYdaFPhdbx7W0V3ACMAjDErRCQUiAYyqxmXAtg+HzJT4Np366QZ6IdtWbzwzRZSM/Lo17Y5b99yHue3i7T9vEope1QrERhjFgILRSQc61v8QhHZB7wHfGSMKa3kY6uAziLSHisBjANuOmWfvcAwYJqIdANCgaxz+k38lTGw9GWrDETP6209Vcr+o7wwdwtLt2fTNrIxb918HiN7xunMYKXquWr3EYhIFHALcCuwDvgY+BVwGzD41P2NMWUicj8wH2to6AfGmM0i8hyw2hgzB/gd8J6IPILVcTzeGKNNPzWx+0dIWwVXvASOIFtOkZFbxMvfbmXW2jSahQbxp1HduWVgW0ICtSNYqYagun0Es4FE4EPgKmPMAfdbn4nI6jN9zj0n4JtTtj1T4XkKMKimQasKfnwFmsRAP8+Ugagov7iMd3/4hfeW7sTlgrsu7sB9gzsR3tiehKOU8o7q3hG8YYxZXNkbxpgkD8ajamL/OvhlEVw2EYIaeeywxhj+vX4/k4SNdqMAABmcSURBVP6bQnZ+CVf1acXjlyfSJrKxx86hlPId1U0E3UVknTHmCICIRAA3GmPesi80VaWlr0BIOCTd4bFDZuQW8cfZm/guNZO+bZoz5bYL6KvrBivVoFU3EdxljHnz+AtjzGERuQtrNJHyhqxtsOU/cPHvILRZ1ftXwRjDzFX7eP6/Wyh1uXj6ym5MGNRel4xUyg9UNxE4RESOd+S6y0fojCFvWvYaBIbCwP+t9aH2HTrGk19uZNmOHAZ2iOSv1/emXZTOB1DKX1Q3EczD6hh+1/36t+5tyhuO7IONn1lNQk2iz/kwLpdh+ord/G3eVhwBwl+u7cmNF7TVukBK+ZnqJoInsC7+x79+LgCm2BKRqtryf1g/L3rgnA/xS1Y+T8zayOo9hxmcGMPz1/aiVXPPdTgrpeqP6k4ocwFvux/Km/KzYO0M6D0Wmrepev9TlDldTF66k9cWbqdRkINXbujDtf1a66QwpfxYdecRdAZeALpjzf4FwBjTwaa41JmsfBvKimDQwzX+6JYDR3l81kY2pecyokccz13TgxZhoVV/UCnVoFW3aWgq8CzwKjAEq+6QLjNV14py4ecp0O0qa/GYaiopc/HPxTt4a/EOmjcO4q2bz+OKXrVbflIp1XBUNxE0MsZ85x45tAeYKCJrgGeq+qDyoFXvW8tHXvxotT+yft8RHp+1gW0H87m2X2ueGdVdS0QrpU5S3URQLCIBwHZ3/aB0oKl9YanTlBbCT29Bx6HQql+VuxeVOnllwTamLN1Ji7BQPhifxNCusXUQqFKqvqluIngIaAw8CPwZq3noNruCUpVY9xEUZMGvqr4b+HnXIZ74YiO7sgu4sX9b/nBFV5qFan0gpVTlqkwE7sljY40xvwfysfoHVF1ylsKyNyC+v7Xm8BkUljh5Ye4WZqzYQ5vIRnxy5wAu6nTu8wyUUv6hykRgjHGKyJmvPsp+yV9A7l644m9nXGC+qNTJnTNWsfyXHCYMSuCxyxNpHGznSqRKqYaiuleKdSIyB/gXUHB8ozHmS1uiUie4XFZxuRY9oPPlle5S6nRx/yfrWLYjh5fG9OF/zo+v4yCVUvVZdRNBKJADDK2wzQCaCOy29RvI3grXTal0GUqny/D7f21g4ZaDPDe6hyYBpVSNVXdmsfYLeIMx1sIzEQnQ49pK3jY8/VUy/16/n8dHJPKbCxPqPESlVP1X3ZnFU7HuAE5ijLnd4xGpE3b9AOlrYNSr4Dj5n8oYw/PfbOHTn/dy7+CO3Du4k5eCVErVd9VtGvq6wvNQ4Fpgv+fDUSdZ+go0jYU+N5321hvf7eC9pbu47cJ2PHZ5oheCU0o1FNVtGvqi4msR+RT40ZaIlCVtjXVHMPzPEHRyPaApS3fy6sJtXH9ePM9e1UMLximlauVc6wV1Blp4MpB6pfAIbJoFZSX2nePHVyC0OSSd3D0z8+e9TPrvFkb2jOOv1/fStQOUUrVW3T6CPE7uI8jAWqPAP33/olUFNKozXPF36DjEs8fPTIXUr+HSJyAkrHzzfzbs5w+zN3FplxheG9eXQIfW/VNK1V51m4bCqt7LTzhLYdO/oPX5UHgYPrwGul8Dlz8P4a09c44fX4WgxjDgnvJNC1MO8shn67kgIZJ3bjmfkECHZ86llPJ71fpKKSLXikh4hdfNReQa+8LyYTsWwrFsuOQx+N8VMORp2DYP/nkBLHu99s1Fh/dYieb8CdA4EoDlO7K595O1dG/VjPdvS6JRsCYBpZTnVLdt4VljTO7xF8aYI1jrE/ifDZ9C42jodJnViXvpY3Dfz9BhMCx4Bt75Fez84dyPv/wNkAC48D4A1u49zJ0zVpMQ1ZjpE/oTpsXjlFIeVt1EUNl+/lfIpvAwbJ0LvcaAo8IFOaId3PgJ3PQ5OIthxtUw63Y4WsMRtvmZVpXRPuMgvDWb9+cy/oOfiQkL4aM7Bug6AkopW1Q3EawWkVdEpKP78Qqwxs7AfFLyl+AssS7UlelyOdy7Egb/AbZ8bTUXLf+n1a9QHT+9ZR3/V4/wS1Y+v3n/Z5qEBPLxnQNo0UyXlFRK2aO6ieABoAT4DJgJFAH32RWUz9owE1p0h5Z9zrxPUCgMfhLuWwntBsG3f4R3LobdVUy7KDxiLUPZfTT7pCW3TFmJCHx85wDiIxp79vdQSqkKqpUIjDEFxpgnjTFJxpgLjDFPGWMKqv5kA5K9A9J+tu4GqjOBK7I93Pw53DgTSgtg2pXwxV2Ql1H5/qumQEkeh/rdzy3vr6SguIwZtw+gQ4wuBKeUsld1Rw0tEJHmFV5HiMh8+8LyQRtnWp24vW6o2ecSR1rNRZc8DilfWc1FP70NzrIT+5Qcg5/eoqT9MMb9p4CsvGKm3d6f7q2aefZ3UEqpSlS3aSjaPVIIAGPMYfxpZrHLZTULdRgCzVrW/PPBjWHoH+Hen6BNf5j3JLx7CexZYb2/7kM4lsOfci5nd84xpvwmifPaRnj2d1BKqTOobiJwiUjb4y9EJIFKqpE2WHuWQe4+6HNj7Y4T1RFungVjP4biozB1BMy+B9ey19kS1IMvstvw9s3n6fKSSqk6Vd0hoH8EfhSRHwABLgburupDIjICeB1wAFOMMS+e8v6rwPH6DI2BFsaY5viaDZ9CcBh0vbL2xxKBbqOsshRLX8Yse4MAVyl/K72JV8f2ZVi32NqfQymlaqC6JSbmiUgS1sV/HfAVUHi2z7gXvX8TGA6kAatEZI4xJqXCcR+psP8DQL8a/wZ2KymAlH9Dj2usJh5PCW4Cw55hVtklrPhhLpePvpWr+rTy3PGVUqqaqlt07k7gISAeWA8MBFZw8tKVp+oP7DDG7HQfYyYwGkg5w/434ouzlVP/CyX5la4J4Amf/hJEUewoXhnQzpbjK6VUVarbR/AQcAGwxxgzBOub+5Gzf4TWwL4Kr9Pc204jIu2A9sCiM7x/t4isFpHVWVlZ1QzZQ9Z/As3bQtsLPX7og0eLWLv3CCN7xnn82EopVV3VTQRFxpgiABEJMcakAp5cFmscMMsY46zsTWPMZPcchqSYmBgPnrYKR/fDzu+tTuJKFo6vrfmbrTkFI3tpIlBKeU91O4vT3PMIvgIWiMhhYE8Vn0kH2lR4He/eVplx+OJM5Y2fAQZ6j7Xl8HM3ZdAxpgmdWmiVb6WU91S3s/ha99OJIrIYCAfmVfGxVUBnEWmPlQDGAac1tItIVyACq8/BdxhjzR1oM8Aa9ulhOfnFrNyVo4vOK6W8rsYVRI0x1aqxbIwpE5H7gflYw0c/MMZsFpHngNXGmDnuXccBM40xvjUvYf86yEqFUa/ZcviFWw7iMjBC+weUUl5maylpY8w3wDenbHvmlNcT7YzhnG2YCY4Q6HFt1fueg7nJGbSJbEQPLSOhlPIyXfS2MmUl1iphXa+ARp6f35ZbWMqyHdmM6BGHVKeAnVJK2UgTQWV2LIDCQ7UvKXEGi1IPUuo0jOh5DnWLlFLKwzQRVGbDp9AkBjoOs+Xw85IziG0WQr82vldNQynlfzQRnOrYIdg6zyo37fB8F8qxkjJ+2JbF5T3iCAjQZiGllPdpIjhV8hfgKj3zcpS19P3WLIpKXTpaSCnlMzQRnGrDpxDbE1r2tuXw85IziGwSTP+ESFuOr5RSNaWJoKKsbZC+xra7geIyJ4tSM/l191gCHfqnV0r5Br0aVbTh03NbjrKaftyeTX5xGZdrs5BSyodoIjjO5bJqC3UcBmH2LA4zNzmDsNBABnXUFciUUr5DE8Fxu5fA0XToa8/cgVKni4VbDnJZt1iCA/XPrpTyHXpFOm7DTAgJh8QrbDn8yp2HOHKsVEcLKaV8jiYCgOJ8SJljLUcZ1MiWU8xNPkCjIAeXdK7D9RSUUqoaNBEAbPkPlBbYVlLC6TLM33yQIV1jaBTssOUcSil1rjQRgDVaKCIB2g605fBr9x4mO79YawsppXySJoLcNNi1xLobsKkS6NxNGQQ7AhiSqM1CSinfo4nA5uUojTHM35zBxZ2jCQsNsuUcSilVG/6dCIyB9Z9C24sgsr0tp9iYlkv6kUIdLaSU8ln+nQjS10LOdttKSgDM25yBI0AY3t2eSWpKKVVb/p0INnwCgaHWsFEbGGOYl5zBhR2iaN442JZzKKVUbflvIigrtkpOd70SQsNtOcXWg3nsyi7QZiGllE/z30Sw/VsoPGzb3AGwSk6LwK97aLOQUsp3+W8iWP8pNI2FDkNsO8W85AyS2kXQIizUtnMopVRt+WciKMiB7fOh1xhblqME2JVdQGpGnk4iU0r5PP9MBMmzwFUGfW+y7RRzkw8AaP+AUsrn+Wci2PApxPWC2B62nWJ+cgZ94sNp3dyeInZKKeUp/pcIMlNh/zroY9/dQPqRQjak5epKZEqpesH/EsGGT0Ec0Ot/bDvFvOQMAEZq/4BSqh7wr0Tgclq1hTpdBk1b2HaaeckH6BoXRvvoJradQymlPMW/EsGuHyDvgG3LUQJk5hWxes9h7SRWStUb/pUINsy0ZhF3GWnbKb7dfBBjdLSQUqr+8J9EUJxnrUTW4zoIsm+C17zkDNpHNyExNsy2cyillCf5TyJImQOlx2wtKXG4oIQVO3MY0TMOsWmRG6WU8jT/SQRhcdB7HLTpb9spFm45iNNlGNFDm4WUUvWHrYlAREaIyFYR2SEiT55hnxtEJEVENovIJ7YF02kYXPeubctRgtUs1Lp5I3rH21PNVCml7GBPoR1ARBzAm8BwIA1YJSJzjDEpFfbpDPwBGGSMOSwi9o3ptFleUSlLt2dzy8B22iyklKpX7Lwj6A/sMMbsNMaUADOB0afscxfwpjHmMIAxJtPGeGy1eGsWJU4XI3tps5BSqn6xMxG0BvZVeJ3m3lZRF6CLiCwTkZ9EZERlBxKRu0VktYiszsrKsinc2pmXfIDopiGc1zbC26EopVSNeLuzOBDoDAwGbgTeE5Hmp+5kjJlsjEkyxiTFxMTUcYhVKyxxsjg1i8t7xOII0GYhpVT9YmciSAfaVHgd795WURowxxhTaozZBWzDSgz1yg/bsigsdWptIaVUvWRnIlgFdBaR9iISDIwD5pyyz1dYdwOISDRWU9FOG2OyxfzNGYQ3CmJAh0hvh6KUUjVmWyIwxpQB9wPzgS3A58aYzSLynIhc7d5tPpAjIinAYuAxY0yOXTHZoaTMxcItBxnePZYgh7db2pRSquZsGz4KYIz5BvjmlG3PVHhugEfdj3pp2S/Z5BWVMVJrCyml6in9CltL8zZl0DQkkEGdor0dilJKnRNNBLVQ5nSxYMtBhnRtQWiQw9vhKKXUOdFEUAs/7z7EoYISbRZSStVrmghqYV5yBiGBAQxO9L25DUopVV2aCM6Ry2WYl5zBpV1iaBxsa5+7UkrZShPBOVq37wiZecVaW0gpVe9pIjhH85IPEOQQhnaN9XYoSilVK5oIzoExhrnJGQzqFE14oyBvh6OUUrWiieAcbN5/lLTDhboSmVKqQdBEUENFpU4mztlMSGAAw7trs5BSqv7T4S414HQZHp65ntV7DvPmTecR1TTE2yEppVSt6R1BNRlj+PPXKczbnMHTV3bjyt5aclop1TBoIqim93/cxbTlu7l9UHvuvLiDt8NRSimP0URQDf/ZsJ9J/93CFb3iePrKbt4ORymlPEoTQRVW7szhd59v4IKECF65oS8BuhSlUqqB0URwFtsP5nHXjNW0iWzEe79J0gqjSqkGSRPBGRw8WsT4qasICXIwbUJ/mjcO9nZISillC00ElcgrKmX81FUcOVbC1PEX0CaysbdDUkop2+g8glOUlLm49+O1bDuYxwfjL6Bn63Bvh6SUUrbSO4IKjDE8+eVGlm7P5oXrenFpF11nQCnV8GkiqOCVBdv4cm06j1zWhRuS2ng7HKWUqhOaCNw+WbmXfyzawdikNjw4rJO3w1FKqTqjiQD4bstBnv5qE4MTY5h0bU9EdK6AUsp/+H0i2LDvCPd/so7urZrx5k3nEeTw+z+JUsrP+PVVb09OAbdPW0VU02A+GH8BTUJ0EJVSyv/4bSI4VFDC+KmrcBrD9Nv70yIs1NshKaWUV/jlV+DCEid3TF9F+pFCPrlzAB1jmno7JKWU8hq/uyNwugwPzVzH+n1HeH1sX5ISIr0dklJKeZVfJQJjDBPnbObblIM8M6o7I3vp4jJKKeVXieDdJTv58Kc93H1JByYMau/tcJRSyif4TSL49/p0XpybyqjeLXlyRFdvh6OUUj7DbxJBbLNQhneP5eUb+ujiMkopVYHfjBoa2CGKgR2ivB2GUkr5HFvvCERkhIhsFZEdIvJkJe+PF5EsEVnvftxpZzxKKaVOZ9sdgYg4gDeB4UAasEpE5hhjUk7Z9TNjzP12xaGUUurs7Lwj6A/sMMbsNMaUADOB0TaeTyml1DmwMxG0BvZVeJ3m3naq60Vko4jMEpFKFwEQkbtFZLWIrM7KyrIjVqWU8lveHjX0HyDBGNMbWABMr2wnY8xkY0ySMSYpJkZXDVNKKU+yMxGkAxW/4ce7t5UzxuQYY4rdL6cA59sYj1JKqUrYmQhWAZ1FpL2IBAPjgDkVdxCRijUerga22BiPUkqpStg2asgYUyYi9wPzAQfwgTFms4g8B6w2xswBHhSRq4Ey4BAw3q54lFJKVU6MMd6OoUZEJAvYc44fjwayPRiOp2hcNaNx1ZyvxqZx1Uxt4mpnjKm0k7XeJYLaEJHVxpgkb8dxKo2rZjSumvPV2DSumrErLm+PGlJKKeVlmgiUUsrP+VsimOztAM5A46oZjavmfDU2jatmbInLr/oIlFJKnc7f7giUUkqdQhOBUkr5Ob9JBFWtjeANItJGRBaLSIqIbBaRh7wdU0Ui4hCRdSLytbdjOU5EmrsLFKaKyBYRudDbMQGIyCPuf8NkEflUREK9FMcHIpIpIskVtkWKyAIR2e7+GeEjcf3d/e+4UURmi0hzX4irwnu/ExEjItG+EpeIPOD+m20Wkb956nx+kQgqrI0wEugO3Cgi3b0bFWDNqP6dMaY7MBC4z0fiOu4hfK/sx+vAPGNMV6APPhCfiLQGHgSSjDE9sWbSj/NSONOAEadsexL4zhjTGfjO/bquTeP0uBYAPd1FJ7cBf6jroKg8LtyVkH8N7K3rgNymcUpcIjIEq5R/H2NMD+AlT53MLxIBPro2gjHmgDFmrft5HtZFrbJS3XVOROKBK7GKAfoEEQkHLgHeBzDGlBhjjng3qnKBQCMRCQQaA/u9EYQxZglWuZaKRnOisu904Jo6DYrK4zLGfGuMKXO//AmrMKXX43J7FXgc8MpomjPE9b/Ai8cLdRpjMj11Pn9JBNVdG8FrRCQB6Aes9G4k5V7D+h/B5e1AKmgPZAFT3U1WU0SkibeDMsakY3072wscAHKNMd96N6qTxBpjDrifZwCx3gzmDG4H5no7CAARGQ2kG2M2eDuWU3QBLhaRlSLyg4hc4KkD+0si8Gki0hT4AnjYGHPUB+IZBWQaY9Z4O5ZTBALnAW8bY/oBBXinmeMk7jb30ViJqhXQRERu8W5UlTPWeHGfGjMuIn/Eaib92AdiaQw8BTzj7VgqEQhEYjUjPwZ8LiLiiQP7SyKocm0EbxGRIKwk8LEx5ktvx+M2CLhaRHZjNaMNFZGPvBsSYN3JpRljjt81zcJKDN52GbDLGJNljCkFvgQu8nJMFR08XvLd/dNjTQq1JSLjgVHAzcY3JjV1xEroG9z//ccDa0UkzqtRWdKAL43lZ6y7dY90ZPtLIqhybQRvcGfz94EtxphXvB3PccaYPxhj4o0xCVh/q0XGGK9/wzXGZAD7RCTRvWkYkOLFkI7bCwwUkcbuf9Nh+EAndgVzgNvcz28D/u3FWMqJyAis5serjTHHvB0PgDFmkzGmhTEmwf3ffxpwnvu/PW/7ChgCICJdgGA8VCHVLxKBu0Pq+NoIW4DPjTGbvRsVYH3zvhXrG/d69+MKbwfl4x4APhaRjUBf4Hkvx4P7DmUWsBbYhPX/lVdKFIjIp8AKIFFE0kTkDuBFYLiIbMe6e3nRR+L6JxAGLHD/t/+Oj8TldWeI6wOgg3tI6UzgNk/dRWmJCaWU8nN+cUeglFLqzDQRKKWUn9NEoJRSfk4TgVJK+TlNBEop5ec0ESjlJiLOCsN413uySq2IJFRW4VIpXxDo7QCU8iGFxpi+3g5CqbqmdwRKVUFEdovI30Rkk4j8LCKd3NsTRGSRu57+dyLS1r091l1ff4P7cbzchENE3nPXkv9WRBq5939QrDUpNorITC/9msqPaSJQ6oRGpzQNja3wXq4xphfWbNjX3Nv+AUx319P/GHjDvf0N4AdjTB+sWkjHZ7F3Bt5015I/Alzv3v4k0M99nHvs+uWUOhOdWayUm4jkG2OaVrJ9NzDUGLPTXSQwwxgTJSLZQEtjTKl7+wFjTLSIZAHxx+vGu4+RACxwLw6DiDwBBBljJonIPCAfq5bMV8aYfJt/VaVOoncESlWPOcPzmiiu8NzJiT66K7FW0DsPWOVe3EapOqOJQKnqGVvh5wr38+WcWJLyZmCp+/l3WKtJHV/3OfxMBxWRAKCNMWYx8AQQDpx2V6KUnfSbh1InNBKR9RVezzPGHB9CGuGueFoM3Oje9gDWammPYa2cNsG9/SFgsrtipBMrKRygcg7gI3eyEOANH1p+U/kJ7SNQqgruPoIkY4xHar8r5Wu0aUgppfyc3hEopZSf0zsCpZTyc5oIlFLKz2kiUEopP6eJQCml/JwmAqWU8nP/D3pf7JnZkI5hAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arg = np.argmax(a,axis=1)"
      ],
      "metadata": {
        "id": "xOttRtDR_000"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = df_test.SP.values\n",
        "\n",
        "new_y_true = []\n",
        "\n",
        "for y in y_true:\n",
        "    new_y_true.append(y+1)\n",
        "\n",
        "print(new_y_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eabb735-f178-4be2-ea80-f79e6de4a6d9",
        "id": "lV5q7DB0_000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 0, 0, 1, 1, 2, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 0, 1, 0, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 0, 1, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 0, 2, 0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 2, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "82b827f6-aeb0-4135-bb1a-f4034f7929b0",
        "id": "HhA99qu0_001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   coc  SP\n",
              "0                 succeed straight away. gladly again.   1\n",
              "1    evenly browned, unfortunately regulation tempe...  -1\n",
              "2                             great use, ok small use.  -1\n",
              "3    unfortunately regulation temperature. terms gr...   0\n",
              "4    unfortunately regulation temperature. terms gr...   0\n",
              "..                                                 ...  ..\n",
              "462  handle price quality relationship good. non -s...   1\n",
              "463                  easy, heat materials good quality   1\n",
              "464  easy clean, materials good quality plasticucho...   1\n",
              "465  easy clean, heat good quality plasticuchos. go...   1\n",
              "466         materials good quality plasticuchos. good.   1\n",
              "\n",
              "[467 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc864bb7-cd7a-44a4-bca9-621b09c05f83\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coc</th>\n",
              "      <th>SP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>succeed straight away. gladly again.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>evenly browned, unfortunately regulation tempe...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>great use, ok small use.</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>unfortunately regulation temperature. terms gr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>unfortunately regulation temperature. terms gr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>handle price quality relationship good. non -s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>easy, heat materials good quality</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>easy clean, materials good quality plasticucho...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>easy clean, heat good quality plasticuchos. go...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>materials good quality plasticuchos. good.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>467 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc864bb7-cd7a-44a4-bca9-621b09c05f83')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fc864bb7-cd7a-44a4-bca9-621b09c05f83 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fc864bb7-cd7a-44a4-bca9-621b09c05f83');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 717
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(new_y_true, arg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e100c030-ee76-4245-dda1-65e3181029b4",
        "id": "a4qHTbgh_002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.28      0.29       130\n",
            "           1       0.00      0.00      0.00        35\n",
            "           2       0.67      0.70      0.68       302\n",
            "\n",
            "    accuracy                           0.53       467\n",
            "   macro avg       0.32      0.33      0.32       467\n",
            "weighted avg       0.51      0.53      0.52       467\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "           0       0.30      0.28      0.29       130\n",
        "           1       0.00      0.00      0.00        35\n",
        "           2       0.67      0.70      0.68       302\n",
        "\n",
        "    accuracy                           0.53       467\n",
        "   macro avg       0.32      0.33      0.32       467\n",
        "weighted avg       0.51      0.53      0.52       467\n",
        "lr 0.000001\n"
      ],
      "metadata": {
        "outputId": "9587c368-c317-4eda-80ea-9b2607fbec31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "0m3aT-eB_002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    accuracy                           0.56       467\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert 2 - 0.64 acc"
      ],
      "metadata": {
        "id": "fd5CVKoL_2f1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Pshew0rr_2f1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Device:', tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DzBwJYBH2Z3",
        "outputId": "2bfffb3c-d1f3-433a-f158-becb590617ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of replicas: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dl = pd.concat([train_sp,test_sp])\n",
        "df_dl = df_dl[['coc','SP']]"
      ],
      "metadata": {
        "id": "UFKyhmioIACN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df_dl.loc[df_dl['SP'] == 'positive', 'SP'] = 1\n",
        "df_dl.loc[df_dl['SP'] == 'negative', 'SP'] = -1\n",
        "df_dl.loc[df_dl['SP'] == 'neutral', 'SP'] = 0\n",
        "df_dl = df_dl[['coc','SP']]\n",
        "\n",
        "df_dl = df_dl.astype({'SP':'int'})\n"
      ],
      "metadata": {
        "id": "ZQFIHUQ2_2f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "w-8NqHrhIuTc",
        "outputId": "35bc9383-6d12-444b-c1f6-a258f958a24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   coc  SP\n",
              "0    satisfied...device times...device...can't expe...   1\n",
              "1               cheap, wobbling unstable unstable lids  -1\n",
              "2    cheap processing, wobbling unstable unstable f...  -1\n",
              "3    independently time open incredible unreasonabl...  -1\n",
              "4               iron remain, small waffles brown, six.  -1\n",
              "..                                                 ...  ..\n",
              "462  handle price quality relationship good. non -s...   1\n",
              "463                  easy, heat materials good quality   1\n",
              "464  easy clean, materials good quality plasticucho...   1\n",
              "465  easy clean, heat good quality plasticuchos. go...   1\n",
              "466         materials good quality plasticuchos. good.   1\n",
              "\n",
              "[1555 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8fa1d6f9-49d9-445e-9388-be63e812907c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coc</th>\n",
              "      <th>SP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>satisfied...device times...device...can't expe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cheap, wobbling unstable unstable lids</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cheap processing, wobbling unstable unstable f...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>independently time open incredible unreasonabl...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>iron remain, small waffles brown, six.</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>handle price quality relationship good. non -s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>easy, heat materials good quality</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>easy clean, materials good quality plasticucho...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>easy clean, heat good quality plasticuchos. go...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>materials good quality plasticuchos. good.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1555 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fa1d6f9-49d9-445e-9388-be63e812907c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8fa1d6f9-49d9-445e-9388-be63e812907c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8fa1d6f9-49d9-445e-9388-be63e812907c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 898
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dl.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55308b81-e0dd-4598-d947-8c583ee1fe36",
        "id": "f-XBGuAG_2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1555, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 899
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(df_dl.coc,df_dl.SP,random_state=29,test_size=0.3)\n",
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQVGgVCJKkg4",
        "outputId": "d0d4a14b-744e-4169-c54d-887825d6750d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1088,), (467,), (1088,), (467,))"
            ]
          },
          "metadata": {},
          "execution_count": 900
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "MeLL3JOx_2f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_input_ids = np.zeros((len(X_train), 256))\n",
        "X_attn_masks = np.zeros((len(X_train), 256))"
      ],
      "metadata": {
        "id": "wbi1Hkvp_2f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_dataset(df, ids, masks, tokenizer):\n",
        "    for i, text in tqdm(enumerate(df)):\n",
        "        tokenized_text = tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=256, \n",
        "            truncation=True, \n",
        "            padding='max_length', \n",
        "            add_special_tokens=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        ids[i, :] = tokenized_text.input_ids\n",
        "        masks[i, :] = tokenized_text.attention_mask\n",
        "    return ids, masks"
      ],
      "metadata": {
        "id": "fVrf4KeT_2f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_input_ids, X_attn_masks = preprocessing_dataset(X_train, X_input_ids, X_attn_masks, tokenizer)\n",
        "#test\n",
        "X_input_ids_test, X_attn_masks_test = preprocessing_dataset(X_test, X_input_ids_test, X_attn_masks_test, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca227cea-d7ed-4b1b-afae-e07b955d2cc4",
        "id": "q53BE2ql_2f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1088it [00:00, 2782.71it/s]\n",
            "467it [00:00, 2782.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.zeros((len(X_train), 3))\n",
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd05606-d9a0-4183-84c6-9b544090add8",
        "id": "Sc4bVZHT_2f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1088, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 905
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = np.zeros((len(X_test), 3))\n",
        "labels_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874ffd4a-7090-407b-b3a0-a5f0f7da9cb8",
        "id": "U7juBsAK_2f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(467, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 906
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.get_dummies(y_train).values\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420eba49-f3a5-4118-fb60-8fa4e89dfe78",
        "id": "RVgChPm6_2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (1088, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = pd.get_dummies(y_test).values\n",
        "print('Shape of label tensor:', labels_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f991b036-7fe8-4103-d9cc-75bc481c18ac",
        "id": "bXuJoYgR_2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (467, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e469be-33dd-4bdc-9c5c-9fafd17e5771",
        "id": "tNowq-4g_2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [0, 0, 1],\n",
              "       ...,\n",
              "       [0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [0, 0, 1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 909
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.get_dummies(df['SP']).values\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c268e314-daf5-4096-fe20-fc9e854f608e",
        "id": "T8L6_42C_2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (1088, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
        "dataset.take(1) # one sample data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be52d8a9-d915-4307-ed50-19fe3172db56",
        "id": "vG7-pq3K_2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 911
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu2wCqkBMFqG",
        "outputId": "03e371a3-2774-44cc-e84f-fc86c2117c4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1088"
            ]
          },
          "metadata": {},
          "execution_count": 912
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((X_input_ids_test, X_attn_masks_test, labels_test))\n",
        "dataset_test.take(1) # one sample data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "824d711c-b741-4f8e-884c-ffe8d1886c54",
        "id": "1QAf4n3S_2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 913
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ba476a-d00b-4366-a592-af485ddffe8e",
        "id": "XwYduG9D_2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1088"
            ]
          },
          "metadata": {},
          "execution_count": 914
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attn_masks\n",
        "    }, labels"
      ],
      "metadata": {
        "id": "m9kfWG7T_2f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(SentimentDatasetMapFunction) "
      ],
      "metadata": {
        "id": "ZFrUS2xW_2f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "dataset_test = dataset_test.map(SentimentDatasetMapFunction) "
      ],
      "metadata": {
        "id": "_dyn-aQC_2f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489e4c54-ae3f-48d7-d9be-d6d10f5bb69a",
        "id": "qL7u-nbX_2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 918
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80822ea0-68fb-4e67-807f-70a7542eef0d",
        "id": "7-pjfAZh_2f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 919
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ae0d77a-aa3e-4765-a957-d180d8998b39",
        "id": "fFVNvFea_2f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 920
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "789e2dc3-d82d-4275-8e42-75fcde458e04",
        "id": "ATlk1MKD_2f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 921
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(10000).batch(1, drop_remainder=True)\n",
        "#test\n",
        "dataset_test = dataset_test.shuffle(10000).batch(1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "sfmYSGNR_2f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80510010-76ce-4173-9c14-c5e63b165a7f",
        "id": "Se8dYqHe_2f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 923
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b724d1-dbfc-480d-92b9-a49652040ed0",
        "id": "t2sXkA2__2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None)}, TensorSpec(shape=(1, 3), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 924
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ef7d9e-76f3-4a3e-f651-afa57550012c",
        "id": "uDvA0atg_2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None)}, TensorSpec(shape=(1, 3), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 925
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = np.round_(0.8 * len(dataset))\n",
        "p\n",
        "#train_size = int((len(df)//16)*p) # for each 16 batch of data we will have len(df)//16 samples, take 80% of that for train."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aea80aa-72a9-4a2b-c61f-41a58660f115",
        "id": "w6KbEmo3_2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "870.0"
            ]
          },
          "metadata": {},
          "execution_count": 926
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#int((len(df)//16)*p)"
      ],
      "metadata": {
        "id": "Vt0oniOO_2f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = dataset.take(p)\n",
        "validation_dataset = dataset.skip(p)\n",
        "#training_dataset = dataset.take(len(dataset))"
      ],
      "metadata": {
        "id": "K7CHRDF3_2f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea173966-a4c0-4eae-e4d6-995fe398deeb",
        "id": "-gycdesw_2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "870"
            ]
          },
          "metadata": {},
          "execution_count": 929
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "392eac44-0659-40f8-aa8d-430817bee9ba",
        "id": "CV0aTN91_2gF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {},
          "execution_count": 930
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0abbd9ce-8714-4783-d84e-6ec4e8c35fef",
        "id": "ba3y8bO8_2gG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None)}, TensorSpec(shape=(1, 3), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 931
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "testing_dataset = dataset_test.take(len(dataset_test))\n"
      ],
      "metadata": {
        "id": "6gfon3np_2gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(testing_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9b17e1-9156-4b6e-a1e5-66a9c55ec151",
        "id": "ooXNKLjq_2gG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 933
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertModel.from_pretrained('bert-base-cased') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4311f24b-434c-4bdf-d600-3b75504ff439",
        "id": "BPKgGtkQ_2gG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining 2 input layers for input_ids and attn_masks\n",
        "input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
        "attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
        "\n",
        "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] \n",
        "\n",
        "#intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
        "\n",
        "embedding = tf.keras.layers.Dropout(0.3)(bert_embds) ##\n",
        "output_layer = tf.keras.layers.Dense(3, activation='softmax', name='output_layer')(embedding)\n",
        "\n",
        "\n",
        "sentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
        "\n",
        "\n",
        "sentiment_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed9ebba-feab-4d54-af9f-37e8d5df09a1",
        "id": "dxvqW79M_2hf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " dropout_570 (Dropout)          (None, 768)          0           ['bert[0][1]']                   \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 3)            2307        ['dropout_570[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,312,579\n",
            "Trainable params: 108,312,579\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim = tf.keras.optimizers.Adam(learning_rate=0.000001) #0.000005 try w other learning rate\n",
        "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')"
      ],
      "metadata": {
        "id": "batLNufj_2hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the best only\n",
        "\n",
        "filepath=\"/content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 3, verbose = 1,restore_best_weights = True)\n",
        "callbacks_list = [earlystop, checkpoint]"
      ],
      "metadata": {
        "id": "WZ2DTLET_2hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sentiment_model.compile(optimizer=optim, loss=\"categorical_crossentropy\", metrics=[acc],)"
      ],
      "metadata": {
        "id": "Mix8ayvM_2hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "history = sentiment_model.fit(\n",
        "    training_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    batch_size = batch_size,\n",
        "    epochs=100,\n",
        "    callbacks = callbacks_list\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bdc2d49-6cba-4321-968b-401abe3fbffd",
        "id": "RDbNm6Gw_2hs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 1.2109 - accuracy: 0.4253\n",
            "Epoch 1: val_loss improved from inf to 0.86516, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 119s 124ms/step - loss: 1.2109 - accuracy: 0.4253 - val_loss: 0.8652 - val_accuracy: 0.6101\n",
            "Epoch 2/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 1.0428 - accuracy: 0.5126\n",
            "Epoch 2: val_loss improved from 0.86516 to 0.81358, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 106s 122ms/step - loss: 1.0428 - accuracy: 0.5126 - val_loss: 0.8136 - val_accuracy: 0.6422\n",
            "Epoch 3/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.9333 - accuracy: 0.5621\n",
            "Epoch 3: val_loss did not improve from 0.81358\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.9333 - accuracy: 0.5621 - val_loss: 0.8515 - val_accuracy: 0.5917\n",
            "Epoch 4/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.9134 - accuracy: 0.5851\n",
            "Epoch 4: val_loss did not improve from 0.81358\n",
            "870/870 [==============================] - 72s 83ms/step - loss: 0.9134 - accuracy: 0.5851 - val_loss: 0.8204 - val_accuracy: 0.6147\n",
            "Epoch 5/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.8944 - accuracy: 0.5805Restoring model weights from the end of the best epoch: 2.\n",
            "\n",
            "Epoch 5: val_loss did not improve from 0.81358\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.8944 - accuracy: 0.5805 - val_loss: 0.8302 - val_accuracy: 0.5963\n",
            "Epoch 5: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model.save(\"/content/drive/MyDrive/Colab Notebooks/MA/bert/bert_sp.h5\")"
      ],
      "metadata": {
        "id": "I0z06S1k_2ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#a = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/MA/bert/bert_sp.h5\",  custom_objects={\"TFBertModel\": TFBertModel})"
      ],
      "metadata": {
        "id": "ShX_uoCm_2ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " a =sentiment_model.predict(testing_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60fda6c-b89d-4246-c49c-1b497f7f915f",
        "id": "JwZZeGx3_2hv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "467/467 [==============================] - 10s 19ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "plot_graphs(history, 'accuracy')\n",
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "outputId": "ce2a6f75-b8c9-4a23-ae5b-94c02956e138",
        "id": "v3qmNDvU_2hw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arg = np.argmax(a,axis=1)"
      ],
      "metadata": {
        "id": "oHUaK3VW_2hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = df_test.SP.values\n",
        "\n",
        "new_y_true = []\n",
        "\n",
        "for y in y_true:\n",
        "    new_y_true.append(y+1)\n",
        "\n",
        "print(new_y_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7faa5991-f474-4609-da2c-beda82b5ed1d",
        "id": "MzL3el_f_2hx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 0, 0, 1, 1, 2, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 0, 1, 0, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 0, 1, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 0, 2, 0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 2, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "3d88b72b-061f-41d3-d8ab-9876b84f7d20",
        "id": "jIZjbRCO_2hx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   coc  SP\n",
              "0                 succeed straight away. gladly again.   1\n",
              "1    evenly browned, unfortunately regulation tempe...  -1\n",
              "2                             great use, ok small use.  -1\n",
              "3    unfortunately regulation temperature. terms gr...   0\n",
              "4    unfortunately regulation temperature. terms gr...   0\n",
              "..                                                 ...  ..\n",
              "462  handle price quality relationship good. non -s...   1\n",
              "463                  easy, heat materials good quality   1\n",
              "464  easy clean, materials good quality plasticucho...   1\n",
              "465  easy clean, heat good quality plasticuchos. go...   1\n",
              "466         materials good quality plasticuchos. good.   1\n",
              "\n",
              "[467 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a22c052-5528-4bed-a457-dcb28435b758\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coc</th>\n",
              "      <th>SP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>succeed straight away. gladly again.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>evenly browned, unfortunately regulation tempe...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>great use, ok small use.</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>unfortunately regulation temperature. terms gr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>unfortunately regulation temperature. terms gr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>handle price quality relationship good. non -s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>easy, heat materials good quality</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>easy clean, materials good quality plasticucho...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>easy clean, heat good quality plasticuchos. go...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>materials good quality plasticuchos. good.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>467 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a22c052-5528-4bed-a457-dcb28435b758')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a22c052-5528-4bed-a457-dcb28435b758 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a22c052-5528-4bed-a457-dcb28435b758');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 946
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(new_y_true, arg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec3979f7-fd30-4d53-b083-ff4fe75ca67b",
        "id": "y16F_lTv_2hx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.01      0.02       130\n",
            "           1       0.00      0.00      0.00        35\n",
            "           2       0.65      0.99      0.78       302\n",
            "\n",
            "    accuracy                           0.64       467\n",
            "   macro avg       0.33      0.33      0.27       467\n",
            "weighted avg       0.51      0.64      0.51       467\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "          precision    recall  f1-score   support\n",
        "\n",
        "           0       0.33      0.01      0.02       130\n",
        "           1       0.00      0.00      0.00        35\n",
        "           2       0.65      0.99      0.78       302\n",
        "\n",
        "    accuracy                           0.64       467\n",
        "   macro avg       0.33      0.33      0.27       467\n",
        "weighted avg       0.51      0.64      0.51       467"
      ],
      "metadata": {
        "outputId": "43edad04-357e-4ef3-cf31-cc2e2a4e21dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "AfbcT9jn_2hx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    accuracy                           0.53       467\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert 2"
      ],
      "metadata": {
        "id": "ygPNWnLuVW77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Device:', tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ece11e-7fa4-4fb6-c922-af516c30b89f",
        "id": "DYIOQ8nyVW8A"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of replicas: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dl = pd.concat([train_sp,test_sp])\n",
        "df_dl = df_dl[['coc','SP']]"
      ],
      "metadata": {
        "id": "B6I_8IrKVW8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df_dl.loc[df_dl['SP'] == 'positive', 'SP'] = 1\n",
        "df_dl.loc[df_dl['SP'] == 'negative', 'SP'] = -1\n",
        "df_dl.loc[df_dl['SP'] == 'neutral', 'SP'] = 0\n",
        "df_dl = df_dl[['coc','SP']]\n",
        "\n",
        "df_dl = df_dl.astype({'SP':'int'})\n"
      ],
      "metadata": {
        "id": "W7HP83wBVW8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "8322ebde-f6d4-49ab-c987-1846d63c3de3",
        "id": "Vmt-2FTVVW8C"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   coc  SP\n",
              "0    satisfied...device times...device...can't expe...   1\n",
              "1               cheap, wobbling unstable unstable lids  -1\n",
              "2    cheap processing, wobbling unstable unstable f...  -1\n",
              "3    independently time open incredible unreasonabl...  -1\n",
              "4               iron remain, small waffles brown, six.  -1\n",
              "..                                                 ...  ..\n",
              "462  handle price quality relationship good. non -s...   1\n",
              "463                  easy, heat materials good quality   1\n",
              "464  easy clean, materials good quality plasticucho...   1\n",
              "465  easy clean, heat good quality plasticuchos. go...   1\n",
              "466         materials good quality plasticuchos. good.   1\n",
              "\n",
              "[1555 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-82de8472-681b-43e7-8ed2-cd66f17b8a84\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coc</th>\n",
              "      <th>SP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>satisfied...device times...device...can't expe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cheap, wobbling unstable unstable lids</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cheap processing, wobbling unstable unstable f...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>independently time open incredible unreasonabl...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>iron remain, small waffles brown, six.</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>handle price quality relationship good. non -s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>easy, heat materials good quality</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>easy clean, materials good quality plasticucho...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>easy clean, heat good quality plasticuchos. go...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>materials good quality plasticuchos. good.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1555 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82de8472-681b-43e7-8ed2-cd66f17b8a84')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-82de8472-681b-43e7-8ed2-cd66f17b8a84 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-82de8472-681b-43e7-8ed2-cd66f17b8a84');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 816
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dl.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89efbc38-08c7-4653-d35c-42a286c6cd77",
        "id": "nR2hrNmJVW8C"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1555, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 817
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(df_dl.coc,df_dl.SP,random_state=29,test_size=0.3)\n",
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60cb307-dca0-46f6-b819-99b44906c287",
        "id": "2OLrEDFBVW8D"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1088,), (467,), (1088,), (467,))"
            ]
          },
          "metadata": {},
          "execution_count": 818
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "WeleW5M0VW8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_input_ids = np.zeros((len(X_train), 256))\n",
        "X_attn_masks = np.zeros((len(X_train), 256))"
      ],
      "metadata": {
        "id": "od_ZRp2YVW8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_input_ids_test = np.zeros((len(X_test), 256))\n",
        "X_attn_masks_test = np.zeros((len(X_test), 256))"
      ],
      "metadata": {
        "id": "1mZ2p1nhj5Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_dataset(df, ids, masks, tokenizer):\n",
        "    for i, text in tqdm(enumerate(df)):\n",
        "        tokenized_text = tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=256, \n",
        "            truncation=True, \n",
        "            padding='max_length', \n",
        "            add_special_tokens=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        ids[i, :] = tokenized_text.input_ids\n",
        "        masks[i, :] = tokenized_text.attention_mask\n",
        "    return ids, masks"
      ],
      "metadata": {
        "id": "Q1FzurvwVW8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_input_ids, X_attn_masks = preprocessing_dataset(X_train, X_input_ids, X_attn_masks, tokenizer)\n",
        "#test\n",
        "X_input_ids_test, X_attn_masks_test = preprocessing_dataset(X_test, X_input_ids_test, X_attn_masks_test, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa2532e3-a485-4ba0-e9a7-c40baf71195c",
        "id": "IQyoUE76VW8F"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1088it [00:00, 2662.32it/s]\n",
            "467it [00:00, 2779.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.zeros((len(X_train), 3))\n",
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58210bc4-82ac-4b89-f662-280de60d7eb2",
        "id": "_c61JFjmVW8F"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1088, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 824
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = np.zeros((len(X_test), 3))\n",
        "labels_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d037fc-535e-43dc-cf12-a4a234ddbbff",
        "id": "280zgYWwVW8H"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(467, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 825
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.get_dummies(y_train).values\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29000204-4d35-40e1-f8cc-2a7f56ba4fa9",
        "id": "VNUDSycnVW8I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (1088, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = pd.get_dummies(y_test).values\n",
        "print('Shape of label tensor:', labels_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6f9417-3bda-4846-e500-8a422fae8f0a",
        "id": "KzS-bWOZVW8I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (467, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d39eebe-2343-45a2-912c-59f685c0343f",
        "id": "6Cgn4lPRVW8J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [0, 0, 1],\n",
              "       ...,\n",
              "       [0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [0, 0, 1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 828
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
        "dataset.take(1) # one sample data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1b8d85-3b38-48ab-e021-c2e5f2b26c5e",
        "id": "pcfwekoPVW8K"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 829
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff007081-623c-42cf-a372-873c3cd066e3",
        "id": "R44BFgk_VW8L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1088"
            ]
          },
          "metadata": {},
          "execution_count": 830
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((X_input_ids_test, X_attn_masks_test, labels_test))\n",
        "dataset_test.take(1) # one sample data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514016fd-f56f-4e67-b7e3-cc6b026c65c9",
        "id": "dYfN95FCVW8L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 831
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e280cff3-dad6-43a7-fa13-b1b42c7b750d",
        "id": "XCuQkC-tVW8M"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1088"
            ]
          },
          "metadata": {},
          "execution_count": 832
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attn_masks\n",
        "    }, labels"
      ],
      "metadata": {
        "id": "lVMZZafDVW8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(SentimentDatasetMapFunction) "
      ],
      "metadata": {
        "id": "ouaHa9moVW8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "dataset_test = dataset_test.map(SentimentDatasetMapFunction) "
      ],
      "metadata": {
        "id": "boORCSuvVW8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38747ff7-540b-41c6-9b23-178701238ad0",
        "id": "vHsB2zDoVW8N"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 836
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7480dc83-5304-44f1-fa6f-1631960ab18b",
        "id": "_qfZbhoeVW8O"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 837
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edf79400-a7ac-4b6e-d599-81046dc9ba3d",
        "id": "xjINFY0MVW8O"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 838
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5f9e14-a006-4424-b6f8-08edf935c10e",
        "id": "uSobgXKaVW8O"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(3,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 839
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(10000).batch(1, drop_remainder=True)\n",
        "#test\n",
        "dataset_test = dataset_test.shuffle(10000).batch(1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "bB80GFB-VW8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef0d1d9-30d2-478f-f26c-9dbea150773b",
        "id": "pqoWtNdaVW8P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 841
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6065278b-44ea-4627-d602-afc86028edf5",
        "id": "htpNma4mVW8P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None)}, TensorSpec(shape=(1, 3), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 842
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e586298-b90c-4da1-e66f-d8e217f50b81",
        "id": "48TlWgBCVW8P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None)}, TensorSpec(shape=(1, 3), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 843
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = np.round_(0.8 * len(dataset))\n",
        "p\n",
        "#train_size = int((len(df)//16)*p) # for each 16 batch of data we will have len(df)//16 samples, take 80% of that for train."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e9b8064-36d7-4f03-e223-212b564be6ad",
        "id": "L6q4S5t9VW8P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "870.0"
            ]
          },
          "metadata": {},
          "execution_count": 844
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#int((len(df)//16)*p)"
      ],
      "metadata": {
        "id": "JcQtM-YcVW8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = dataset.take(p)\n",
        "validation_dataset = dataset.skip(p)\n",
        "#training_dataset = dataset.take(len(dataset))"
      ],
      "metadata": {
        "id": "12GtHSEZVW8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0106e05a-386f-4a6f-8fea-bcb378223e5d",
        "id": "aO6MeU8_VW8Q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "870"
            ]
          },
          "metadata": {},
          "execution_count": 847
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c34629-2129-46fb-bc98-2fffb11aad6a",
        "id": "TEqXTAU0VW8Q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {},
          "execution_count": 848
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de23365e-c74a-44a4-9412-4358746ca677",
        "id": "SikKWehRVW8Q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(1, 256), dtype=tf.float64, name=None)}, TensorSpec(shape=(1, 3), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 849
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "testing_dataset = dataset_test.take(len(dataset_test))\n"
      ],
      "metadata": {
        "id": "LcY84PEjVW8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(testing_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea2d1a0-2899-4d60-85e0-71a32edf109c",
        "id": "-zmo_kTiVW8R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 851
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f6d7bb-1db9-4e8a-db7f-efe754188343",
        "id": "GsIgpW4bVW8R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining 2 input layers for input_ids and attn_masks\n",
        "input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
        "attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
        "\n",
        "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] \n",
        "\n",
        "#intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
        "\n",
        "embedding = tf.keras.layers.Dropout(0.5)(bert_embds) ##\n",
        "output_layer = tf.keras.layers.Dense(3, activation='sigmoid', name='output_layer')(embedding)\n",
        "\n",
        "\n",
        "sentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
        "\n",
        "\n",
        "sentiment_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51b4f0e-c635-424f-c2d1-abb3d977befe",
        "id": "1NhKheVqVW8R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " dropout_584 (Dropout)          (None, 768)          0           ['bert[0][1]']                   \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 3)            2307        ['dropout_584[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,312,579\n",
            "Trainable params: 108,312,579\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim = tf.keras.optimizers.Adam(learning_rate=0.000006) #0.000005 try w other learning rate\n",
        "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')"
      ],
      "metadata": {
        "id": "pD1uGKTiVW8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the best only\n",
        "\n",
        "filepath=\"/content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 3, verbose = 1,restore_best_weights = True)\n",
        "callbacks_list = [earlystop, checkpoint]"
      ],
      "metadata": {
        "id": "i28rrqjdVW8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sentiment_model.compile(optimizer=optim, loss=\"categorical_crossentropy\", metrics=[acc],)"
      ],
      "metadata": {
        "id": "uVlRUNnqVW8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "history = sentiment_model.fit(\n",
        "    training_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    batch_size = batch_size,\n",
        "    epochs=100,\n",
        "    callbacks = callbacks_list\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a55a1fe-cb2c-4bf4-8ea1-d55c6146b6da",
        "id": "A1nOakbIVW8S"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.8971 - accuracy: 0.6402\n",
            "Epoch 1: val_loss improved from inf to 0.54922, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 120s 126ms/step - loss: 0.8971 - accuracy: 0.6402 - val_loss: 0.5492 - val_accuracy: 0.7798\n",
            "Epoch 2/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.6000 - accuracy: 0.7839\n",
            "Epoch 2: val_loss improved from 0.54922 to 0.39118, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 107s 123ms/step - loss: 0.6000 - accuracy: 0.7839 - val_loss: 0.3912 - val_accuracy: 0.8440\n",
            "Epoch 3/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.4104 - accuracy: 0.8575\n",
            "Epoch 3: val_loss improved from 0.39118 to 0.28438, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 109s 125ms/step - loss: 0.4104 - accuracy: 0.8575 - val_loss: 0.2844 - val_accuracy: 0.8945\n",
            "Epoch 4/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.3784 - accuracy: 0.8690\n",
            "Epoch 4: val_loss improved from 0.28438 to 0.16040, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 107s 123ms/step - loss: 0.3784 - accuracy: 0.8690 - val_loss: 0.1604 - val_accuracy: 0.9404\n",
            "Epoch 5/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.9138\n",
            "Epoch 5: val_loss did not improve from 0.16040\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.2437 - accuracy: 0.9138 - val_loss: 0.1714 - val_accuracy: 0.9358\n",
            "Epoch 6/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.2040 - accuracy: 0.9299\n",
            "Epoch 6: val_loss improved from 0.16040 to 0.10248, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 106s 122ms/step - loss: 0.2040 - accuracy: 0.9299 - val_loss: 0.1025 - val_accuracy: 0.9587\n",
            "Epoch 7/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9598\n",
            "Epoch 7: val_loss improved from 0.10248 to 0.06157, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 108s 124ms/step - loss: 0.1344 - accuracy: 0.9598 - val_loss: 0.0616 - val_accuracy: 0.9817\n",
            "Epoch 8/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9724\n",
            "Epoch 8: val_loss improved from 0.06157 to 0.04108, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 107s 123ms/step - loss: 0.0921 - accuracy: 0.9724 - val_loss: 0.0411 - val_accuracy: 0.9908\n",
            "Epoch 9/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9690\n",
            "Epoch 9: val_loss did not improve from 0.04108\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.1152 - accuracy: 0.9690 - val_loss: 0.0509 - val_accuracy: 0.9862\n",
            "Epoch 10/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9759\n",
            "Epoch 10: val_loss improved from 0.04108 to 0.02866, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 106s 122ms/step - loss: 0.0746 - accuracy: 0.9759 - val_loss: 0.0287 - val_accuracy: 0.9908\n",
            "Epoch 11/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9724\n",
            "Epoch 11: val_loss did not improve from 0.02866\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.0866 - accuracy: 0.9724 - val_loss: 0.0460 - val_accuracy: 0.9862\n",
            "Epoch 12/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9816\n",
            "Epoch 12: val_loss did not improve from 0.02866\n",
            "870/870 [==============================] - 73s 83ms/step - loss: 0.0582 - accuracy: 0.9816 - val_loss: 0.0410 - val_accuracy: 0.9862\n",
            "Epoch 13/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9805\n",
            "Epoch 13: val_loss improved from 0.02866 to 0.02502, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 108s 124ms/step - loss: 0.0515 - accuracy: 0.9805 - val_loss: 0.0250 - val_accuracy: 0.9908\n",
            "Epoch 14/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9816\n",
            "Epoch 14: val_loss did not improve from 0.02502\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.0672 - accuracy: 0.9816 - val_loss: 0.0524 - val_accuracy: 0.9908\n",
            "Epoch 15/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.9782\n",
            "Epoch 15: val_loss improved from 0.02502 to 0.02280, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 107s 123ms/step - loss: 0.0546 - accuracy: 0.9782 - val_loss: 0.0228 - val_accuracy: 0.9862\n",
            "Epoch 16/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9839\n",
            "Epoch 16: val_loss did not improve from 0.02280\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.0583 - accuracy: 0.9839 - val_loss: 0.0322 - val_accuracy: 0.9862\n",
            "Epoch 17/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9805\n",
            "Epoch 17: val_loss improved from 0.02280 to 0.01637, saving model to /content/drive/MyDrive/Colab Notebooks/MA/tfbert/checkpoint_sp/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r870/870 [==============================] - 107s 123ms/step - loss: 0.0381 - accuracy: 0.9805 - val_loss: 0.0164 - val_accuracy: 0.9908\n",
            "Epoch 18/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9885\n",
            "Epoch 18: val_loss did not improve from 0.01637\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.0332 - accuracy: 0.9885 - val_loss: 0.0375 - val_accuracy: 0.9862\n",
            "Epoch 19/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9897\n",
            "Epoch 19: val_loss did not improve from 0.01637\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.0374 - accuracy: 0.9897 - val_loss: 0.0339 - val_accuracy: 0.9908\n",
            "Epoch 20/100\n",
            "870/870 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9828Restoring model weights from the end of the best epoch: 17.\n",
            "\n",
            "Epoch 20: val_loss did not improve from 0.01637\n",
            "870/870 [==============================] - 73s 84ms/step - loss: 0.0482 - accuracy: 0.9828 - val_loss: 0.0415 - val_accuracy: 0.9908\n",
            "Epoch 20: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model.save(\"/content/drive/MyDrive/Colab Notebooks/MA/bert/bert_sp.h5\")"
      ],
      "metadata": {
        "id": "dhqqqU0LVW8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#a = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/MA/bert/bert_sp.h5\",  custom_objects={\"TFBertModel\": TFBertModel})"
      ],
      "metadata": {
        "id": "5inEjcicVW8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " a =sentiment_model.predict(testing_dataset)"
      ],
      "metadata": {
        "id": "6Qv4k1spVW8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba5cfda0-81f0-4a86-b8e4-4ba9a767aab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "467/467 [==============================] - 10s 19ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "plot_graphs(history, 'accuracy')\n",
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "id": "6PeUD86rVW8T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "ad2fcd51-e34f-4ba1-8306-a9e07c9dce02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arg = np.argmax(a,axis=1)"
      ],
      "metadata": {
        "id": "5tCOvPlXVW8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = y_test.values\n",
        "\n",
        "new_y_true = []\n",
        "\n",
        "for y in y_true:\n",
        "    new_y_true.append(y+1)\n",
        "\n",
        "print(new_y_true)"
      ],
      "metadata": {
        "id": "Ca90oNOYVW8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433b5b77-6337-42f6-efbc-13a801005779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 2, 2, 2, 2, 0, 2, 2, 0, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 1, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 1, 0, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 1, 2, 2, 0, 2, 0, 2, 0, 1, 0, 2, 0, 2, 1, 2, 0, 0, 1, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 0, 1, 2, 0, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0, 0, 0, 2, 0, 1, 2, 2, 1, 2, 1, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 1, 1, 2, 0, 2, 0, 0, 0, 0, 0, 1, 2, 1, 2, 0, 2, 2, 0, 1, 2, 2, 0, 2, 2, 0, 1, 2, 2, 0, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 0, 1, 2, 2, 2, 0, 2, 0, 0, 2, 0, 1, 0, 0, 1, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 1, 0, 2, 0, 2, 2, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 1, 2, 0, 0, 2, 1, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 1, 0, 2, 2, 2, 0, 1, 2, 2, 2, 0, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 1, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 0, 1, 2, 1, 1, 2, 0, 1, 0, 0, 0, 2, 0, 2, 2, 1, 0, 0, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0, 0, 2, 0, 2, 2, 1, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 0, 2, 2, 1, 0, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 1, 0, 2, 0, 2, 2, 2, 0, 0, 1, 2, 1, 2, 2, 2, 2, 0, 0, 1, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 1, 1, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 0, 1, 2, 2, 0, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXodify-odqD",
        "outputId": "3b030d7e-1a26-475b-a609-cbf81fd49055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 864
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(new_y_true, arg))"
      ],
      "metadata": {
        "id": "uwiqYhPDVW8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4aa4c2-eec4-49b1-94dd-c511e380fb16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.37      0.36       140\n",
            "           1       0.17      0.10      0.12        51\n",
            "           2       0.62      0.64      0.63       276\n",
            "\n",
            "    accuracy                           0.50       467\n",
            "   macro avg       0.38      0.37      0.37       467\n",
            "weighted avg       0.49      0.50      0.49       467\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.32      0.34      0.33       140\n",
        "           1       0.21      0.14      0.17        51\n",
        "           2       0.62      0.64      0.63       276\n",
        "\n",
        "    accuracy                           0.50       467\n",
        "   macro avg       0.39      0.37      0.38       467\n",
        "weighted avg       0.49      0.50      0.49       467\n",
        "0.000004"
      ],
      "metadata": {
        "id": "Dzi55uUHVW8U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "d370b285-ef70-4d7d-a03b-30f5134c481c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    accuracy                           0.50       467\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test w TFBertForSequenceClassification tmr"
      ],
      "metadata": {
        "id": "9wQtj9sYows1"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "MokBMjza0iSJ",
        "BPV4J98R0koB",
        "VRjblq_KX3Nx",
        "NWhnGVzRmLrJ",
        "A0DkNtLpk7Le",
        "EZ9OZK0G6S-6",
        "dxn-__i1lBvH",
        "BHSYpo3ZlA1U",
        "NWm6tDTUXyje",
        "6FI4y1mNf2TZ",
        "BVZfNCSG6OCN",
        "cYG7HaglZj73",
        "xh0GRs_nlPig",
        "vNvQsxEP3aQj",
        "yZ8r9cfKbN5E",
        "kAnxxOtDfZl9",
        "f-6NHi-DgIjP",
        "sgoX4tkLlweT",
        "uaGo3YvWvyI2",
        "iPpDziFe_0tY",
        "fd5CVKoL_2f1"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "140yLcaUTsamaknyjjmiIl6vklVvrRMd2",
      "authorship_tag": "ABX9TyNengxt4N161uE/uhPZE/jn",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4a2080c7600346ddb9462604c689401e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb40b713a61e44b7ae916749401eb455",
              "IPY_MODEL_a6beb98ef00c4fabbe72f27ed6cf146f",
              "IPY_MODEL_17368d89249a47f28b784da91c3cb5bd"
            ],
            "layout": "IPY_MODEL_fd7168a42ea24a6d9bb664ffc6db3f2e"
          }
        },
        "cb40b713a61e44b7ae916749401eb455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_240618320aa84a9e82b0f68fda84fa17",
            "placeholder": "​",
            "style": "IPY_MODEL_f2eb00500c5c45ac9aaf278ee4fab029",
            "value": "Downloading: 100%"
          }
        },
        "a6beb98ef00c4fabbe72f27ed6cf146f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca71014306834b23bf7f1be1f4fe3a12",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1be66cb92845434a98d4b508e050c6c8",
            "value": 570
          }
        },
        "17368d89249a47f28b784da91c3cb5bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe71e1c17eaf461bb927710c6d686e7b",
            "placeholder": "​",
            "style": "IPY_MODEL_0f62dab8c6c3441d929304f9422726fa",
            "value": " 570/570 [00:00&lt;00:00, 24.3kB/s]"
          }
        },
        "fd7168a42ea24a6d9bb664ffc6db3f2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "240618320aa84a9e82b0f68fda84fa17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2eb00500c5c45ac9aaf278ee4fab029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca71014306834b23bf7f1be1f4fe3a12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1be66cb92845434a98d4b508e050c6c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe71e1c17eaf461bb927710c6d686e7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f62dab8c6c3441d929304f9422726fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f6f5f4064414a2095f8b2485bce8428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df43d542a6ff437a96de31e478bcf402",
              "IPY_MODEL_24866ea0233a4cc587b24988a0b41244",
              "IPY_MODEL_005ba8f20f224b4bae136e21a638a436"
            ],
            "layout": "IPY_MODEL_b5379d7d39c246139ff19c8e874276b2"
          }
        },
        "df43d542a6ff437a96de31e478bcf402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2008997e149413686ea2c5b408c257d",
            "placeholder": "​",
            "style": "IPY_MODEL_f74eaeffdcd544c198e48494256256e7",
            "value": "Downloading:   6%"
          }
        },
        "24866ea0233a4cc587b24988a0b41244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c0bd779859c45d8bf20266f8d38c15a",
            "max": 536063208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2493ceefb324487a8cdbbd038c161599",
            "value": 30670848
          }
        },
        "005ba8f20f224b4bae136e21a638a436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_461ef3279a224139a37355cb438182dc",
            "placeholder": "​",
            "style": "IPY_MODEL_ed44ad561cfe4f93b8ad335e45406121",
            "value": " 30.7M/536M [1:06:14&lt;00:06, 82.3MB/s]"
          }
        },
        "b5379d7d39c246139ff19c8e874276b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2008997e149413686ea2c5b408c257d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f74eaeffdcd544c198e48494256256e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c0bd779859c45d8bf20266f8d38c15a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2493ceefb324487a8cdbbd038c161599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "461ef3279a224139a37355cb438182dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed44ad561cfe4f93b8ad335e45406121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}